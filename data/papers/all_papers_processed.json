[
  {
    "text": "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\u2217\nGoogle Brain\navaswani@google.comNoam Shazeer\u2217\nGoogle Brain\nnoam@google.comNiki Parmar\u2217\nGoogle Research\nnikip@google.comJakob Uszkoreit\u2217\nGoogle Research\nusz@google.com\nLlion Jones\u2217\nGoogle Research\nllion@google.comAidan N. Gomez\u2217 \u2020\nUniversity of Toronto\naidan@cs.toronto.edu\u0141ukasz Kaiser\u2217\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin\u2217 \u2021\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n\u2020Work performed while at Google Brain.\n\u2021Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n1 Introduction\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2\nFigure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512 .\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position ican depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by\u221adk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\nthe matrix of outputs as:\nAttention( Q, K, V ) = softmax(QKT\n\u221adk)V (1)\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof1\u221adk. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients4. To counteract this effect, we scale the dot products by1\u221adk.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q\u00b7k=Pdk\ni=1qiki, has mean 0and variance dk.\n4\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\nwhere head i= Attention( QWQ\ni, KWK\ni, V WV\ni)\nWhere the projections are parameter matrices WQ\ni\u2208Rdmodel\u00d7dk,WK\ni\u2208Rdmodel\u00d7dk,WV\ni\u2208Rdmodel\u00d7dv\nandWO\u2208Rhdv\u00d7dmodel.\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n\u2022In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n\u2022The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n\u2022Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by\u221admodel.\n5\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2\u00b7d) O(1) O(1)\nRecurrent O(n\u00b7d2) O(n) O(n)\nConvolutional O(k\u00b7n\u00b7d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r\u00b7n\u00b7d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i)=sin(pos/100002i/d model)\nPE(pos,2i+1)=cos(pos/100002i/d model)\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0to10000 \u00b72\u03c0. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\nPEpos.\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi\u2208Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6\nlength nis smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k\u00b7n\u00b7d+n\u00b7d2). Even with k=n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [ 20] with \u03b21= 0.9,\u03b22= 0.98and\u03f5= 10\u22129. We varied the learning\nrate over the course of training, according to the formula:\nlrate =d\u22120.5\nmodel\u00b7min(step_num\u22120.5, step _num\u00b7warmup _steps\u22121.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup _steps = 4000 .\n5.4 Regularization\nWe employ three types of regularization during training:\n7\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModelBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0\u00b71020\nGNMT + RL [38] 24.6 39.92 2.3\u00b710191.4\u00b71020\nConvS2S [9] 25.16 40.46 9.6\u00b710181.5\u00b71020\nMoE [32] 26.03 40.56 2.0\u00b710191.2\u00b71020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0\u00b71020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8\u00b710201.1\u00b71021\nConvS2S Ensemble [9] 26.36 41.29 7.7\u00b710191.2\u00b71021\nTransformer (base model) 27.3 38.1 3.3\u00b71018\nTransformer (big) 28.4 41.8 2.3\u00b71019\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop= 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls= 0.1[36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop= 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4and length penalty \u03b1= 0.6[38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d model dff h d k dvPdrop \u03f5lstrain PPL BLEU params\nsteps (dev) (dev) \u00d7106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B)16 5.16 25.1 58\n32 5.01 25.4 60\n(C)2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21and\u03b1= 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor .\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450 , 2016.\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733 , 2016.\n10\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR , abs/1406.1078, 2014.\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357 , 2016.\n[7]Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL , 2016.\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850 , 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 770\u2013778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\n9(8):1735\u20131780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing , pages 832\u2013841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\n[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS) , 2016.\n[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR) , 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nInInternational Conference on Learning Representations , 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722 , 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130 , 2017.\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\n11\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313\u2013330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\npages 152\u2013159. ACL, June 2006.\n[27] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing , 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433\u2013440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859 , 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research , 15(1):1929\u20131958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28 , pages 2440\u20132448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\u20133112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems , 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144 , 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers) , pages 434\u2013443. ACL, August 2013.\n12\nAttention Visualizations\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for\nthe word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\n13\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15\n",
    "title": "Attention Is All You Need",
    "arxiv_id": "1706.03762",
    "category": "famous",
    "authors": [
      "Ashish Vaswani",
      "Noam Shazeer",
      "Niki Parmar",
      "Jakob Uszkoreit",
      "Llion Jones",
      "Aidan N. Gomez",
      "Lukasz Kaiser",
      "Illia Polosukhin"
    ],
    "published": "2017-06-12",
    "num_pages": 15,
    "num_chars": 39487
  },
  {
    "text": "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\nfjacobdevlin,mingweichang,kentonl,kristout g@google.com\nAbstract\nWe introduce a new language representa-\ntion model called BERT , which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\nBERT is conceptually simple and empirically\npowerful. It obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n1 Introduction\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).There are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and\ufb01ne-tuning . The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning allpre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches. The ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations from Transformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953). The\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the maskedarXiv:1810.04805v2  [cs.CL]  24 May 2019\nword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\ntrain a deep bidirectional Transformer. In addi-\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level andtoken-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\u2022 BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert .\n2 Related Work\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n2.1 Unsupervised Feature-based Approaches\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods. Pre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).These approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n2.2 Unsupervised Fine-tuning Approaches\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\nbedding parameters from unlabeled text (Col-\nlobert and Weston, 2008).\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch. At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\net al., 2018a). Left-to-right language model-\nBERT BERT \nE[CLS] E1 E[SEP] ... ENE1\u2019... EM\u2019\nC\nT1\nT[SEP] ...\n TN\nT1\u2019...\n TM\u2019\n[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \nQuestion Paragraph Start/End Span \nBERT \nE[CLS] E1 E[SEP] ... ENE1\u2019... EM\u2019\nC\nT1\nT[SEP] ...\n TN\nT1\u2019...\n TM\u2019\n[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \nMasked Sentence A Masked Sentence B \nPre-training Fine-Tuning NSP Mask LM Mask LM \nUnlabeled Sentence A and B Pair SQuAD \nQuestion Answer Pair NER MNLI Figure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n2.3 Transfer Learning from Supervised Data\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n3 BERT\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning . Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-mal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\nModel Architecture BERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthetensor2tensor library.1Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERT BASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERT LARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\nBERT BASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\ni.e., 3072 for the H= 768 and 4096 for the H= 1024 .\n4We note that in the literature the bidirectional Trans-\nInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g.,hQuestion, Answeri) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ( [CLS] ). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ( [SEP] ). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence Aor sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token asC2RH,\nand the \ufb01nal hidden vector for the ithinput token\nasTi2RH.\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n3.1 Pre-training BERT\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\nTask #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right orright-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\nformer is often referred to as a \u201cTransformer encoder\u201d while\nthe left-context-only version is referred to as a \u201cTransformer\ndecoder\u201d since it can be used for text generation.In order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.\nAlthough this allows us to obtain a bidirec-\ntional pre-trained model, a downside is that we\nare creating a mismatch between pre-training and\n\ufb01ne-tuning, since the [MASK] token does not ap-\npear during \ufb01ne-tuning. To mitigate this, we do\nnot always replace \u201cmasked\u201d words with the ac-\ntual[MASK] token. The training data generator\nchooses 15% of the token positions at random for\nprediction. If the i-th token is chosen, we replace\nthei-th token with (1) the [MASK] token 80% of\nthe time (2) a random token 10% of the time (3)\nthe unchanged i-th token 10% of the time. Then,\nTiwill be used to predict the original token with\ncross entropy loss. We compare variations of this\nprocedure in Appendix C.2.\nTask #2: Next Sentence Prediction (NSP)\nMany important downstream tasks such as Ques-\ntion Answering (QA) and Natural Language Infer-\nence (NLI) are based on understanding the rela-\ntionship between two sentences, which is not di-\nrectly captured by language modeling. In order\nto train a model that understands sentence rela-\ntionships, we pre-train for a binarized next sen-\ntence prediction task that can be trivially gener-\nated from any monolingual corpus. Speci\ufb01cally,\nwhen choosing the sentences AandBfor each pre-\ntraining example, 50% of the time Bis the actual\nnext sentence that follows A(labeled as IsNext ),\nand 50% of the time it is a random sentence from\nthe corpus (labeled as NotNext ). As we show\nin Figure 1, Cis used for next sentence predic-\ntion (NSP).5Despite its simplicity, we demon-\nstrate in Section 5.1 that pre-training towards this\ntask is very bene\ufb01cial to both QA and NLI.6\n5The \ufb01nal model achieves 97%-98% accuracy on NSP.\n6The vector Cis not a meaningful sentence representation\nwithout \ufb01ne-tuning, since it was trained with NSP.\n[CLS] helikesplay## ing[SEP] mydogiscute[SEP]Input \nE[CLS] Ehe Elikes Eplay E## ing E[SEP] Emy Edog Eis Ecute E[SEP] Token \nEmbeddings \nEA EB EB EB EB EB EA EA EA EA EASegment \nEmbeddings \nE0 E6 E7 E8 E9 E10 E1 E2 E3 E4 E5Position \nEmbeddings Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-\ntion embeddings and the position embeddings.\nThe NSP task is closely related to representation-\nlearning objectives used in Jernite et al. (2017) and\nLogeswaran and Lee (2018). However, in prior\nwork, only sentence embeddings are transferred to\ndown-stream tasks, where BERT transfers all pa-\nrameters to initialize end-task model parameters.\nPre-training data The pre-training procedure\nlargely follows the existing literature on language\nmodel pre-training. For the pre-training corpus we\nuse the BooksCorpus (800M words) (Zhu et al.,\n2015) and English Wikipedia (2,500M words).\nFor Wikipedia we extract only the text passages\nand ignore lists, tables, and headers. It is criti-\ncal to use a document-level corpus rather than a\nshuf\ufb02ed sentence-level corpus such as the Billion\nWord Benchmark (Chelba et al., 2013) in order to\nextract long contiguous sequences.\n3.2 Fine-tuning BERT\nFine-tuning is straightforward since the self-\nattention mechanism in the Transformer al-\nlows BERT to model many downstream tasks\u2014\nwhether they involve single text or text pairs\u2014by\nswapping out the appropriate inputs and outputs.\nFor applications involving text pairs, a common\npattern is to independently encode text pairs be-\nfore applying bidirectional cross attention, such\nas Parikh et al. (2016); Seo et al. (2017). BERT\ninstead uses the self-attention mechanism to unify\nthese two stages, as encoding a concatenated text\npair with self-attention effectively includes bidi-\nrectional cross attention between two sentences.\nFor each task, we simply plug in the task-\nspeci\ufb01c inputs and outputs into BERT and \ufb01ne-\ntune all the parameters end-to-end. At the in-\nput, sentence Aand sentence Bfrom pre-training\nare analogous to (1) sentence pairs in paraphras-\ning, (2) hypothesis-premise pairs in entailment, (3)\nquestion-passage pairs in question answering, and(4) a degenerate text- ?pair in text classi\ufb01cation\nor sequence tagging. At the output, the token rep-\nresentations are fed into an output layer for token-\nlevel tasks, such as sequence tagging or question\nanswering, and the [CLS] representation is fed\ninto an output layer for classi\ufb01cation, such as en-\ntailment or sentiment analysis.\nCompared to pre-training, \ufb01ne-tuning is rela-\ntively inexpensive. All of the results in the pa-\nper can be replicated in at most 1 hour on a sin-\ngle Cloud TPU, or a few hours on a GPU, starting\nfrom the exact same pre-trained model.7We de-\nscribe the task-speci\ufb01c details in the correspond-\ning subsections of Section 4. More details can be\nfound in Appendix A.5.\n4 Experiments\nIn this section, we present BERT \ufb01ne-tuning re-\nsults on 11 NLP tasks.\n4.1 GLUE\nThe General Language Understanding Evaluation\n(GLUE) benchmark (Wang et al., 2018a) is a col-\nlection of diverse natural language understanding\ntasks. Detailed descriptions of GLUE datasets are\nincluded in Appendix B.1.\nTo \ufb01ne-tune on GLUE, we represent the input\nsequence (for single sentence or sentence pairs)\nas described in Section 3, and use the \ufb01nal hid-\nden vectorC2RHcorresponding to the \ufb01rst\ninput token ( [CLS] ) as the aggregate representa-\ntion. The only new parameters introduced during\n\ufb01ne-tuning are classi\ufb01cation layer weights W2\nRK\u0002H, whereKis the number of labels. We com-\npute a standard classi\ufb01cation loss with CandW,\ni.e.,log(softmax( CWT)).\n7For example, the BERT SQuAD model can be trained in\naround 30 minutes on a single Cloud TPU to achieve a Dev\nF1 score of 91.0%.\n8See (10) in https://gluebenchmark.com/faq .\nSystem MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average\n392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k -\nPre-OpenAI SOTA 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0\nBiLSTM+ELMo+Attn 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0\nOpenAI GPT 82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1\nBERT BASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6\nBERT LARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1\nTable 1: GLUE Test results, scored by the evaluation server ( https://gluebenchmark.com/leaderboard ).\nThe number below each task denotes the number of training examples. The \u201cAverage\u201d column is slightly different\nthan the of\ufb01cial GLUE score, since we exclude the problematic WNLI set.8BERT and OpenAI GPT are single-\nmodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\naccuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\nWe use a batch size of 32 and \ufb01ne-tune for 3\nepochs over the data for all GLUE tasks. For each\ntask, we selected the best \ufb01ne-tuning learning rate\n(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.\nAdditionally, for BERT LARGE we found that \ufb01ne-\ntuning was sometimes unstable on small datasets,\nso we ran several random restarts and selected the\nbest model on the Dev set. With random restarts,\nwe use the same pre-trained checkpoint but per-\nform different \ufb01ne-tuning data shuf\ufb02ing and clas-\nsi\ufb01er layer initialization.9\nResults are presented in Table 1. Both\nBERT BASE and BERT LARGE outperform all sys-\ntems on all tasks by a substantial margin, obtaining\n4.5% and 7.0% respective average accuracy im-\nprovement over the prior state of the art. Note that\nBERT BASE and OpenAI GPT are nearly identical\nin terms of model architecture apart from the at-\ntention masking. For the largest and most widely\nreported GLUE task, MNLI, BERT obtains a 4.6%\nabsolute accuracy improvement. On the of\ufb01cial\nGLUE leaderboard10, BERT LARGE obtains a score\nof 80.5, compared to OpenAI GPT, which obtains\n72.8 as of the date of writing.\nWe \ufb01nd that BERT LARGE signi\ufb01cantly outper-\nforms BERT BASE across all tasks, especially those\nwith very little training data. The effect of model\nsize is explored more thoroughly in Section 5.2.\n4.2 SQuAD v1.1\nThe Stanford Question Answering Dataset\n(SQuAD v1.1) is a collection of 100k crowd-\nsourced question/answer pairs (Rajpurkar et al.,\n2016). Given a question and a passage from\n9The GLUE data set distribution does not include the Test\nlabels, and we only made a single GLUE evaluation server\nsubmission for each of BERT BASE and BERT LARGE .\n10https://gluebenchmark.com/leaderboardWikipedia containing the answer, the task is to\npredict the answer text span in the passage.\nAs shown in Figure 1, in the question answer-\ning task, we represent the input question and pas-\nsage as a single packed sequence, with the ques-\ntion using the Aembedding and the passage using\ntheBembedding. We only introduce a start vec-\ntorS2RHand an end vector E2RHduring\n\ufb01ne-tuning. The probability of word ibeing the\nstart of the answer span is computed as a dot prod-\nuct between TiandSfollowed by a softmax over\nall of the words in the paragraph: Pi=eS\u0001TiP\njeS\u0001Tj.\nThe analogous formula is used for the end of the\nanswer span. The score of a candidate span from\npositionito positionjis de\ufb01ned as S\u0001Ti+E\u0001Tj,\nand the maximum scoring span where j\u0015iis\nused as a prediction. The training objective is the\nsum of the log-likelihoods of the correct start and\nend positions. We \ufb01ne-tune for 3 epochs with a\nlearning rate of 5e-5 and a batch size of 32.\nTable 2 shows top leaderboard entries as well\nas results from top published systems (Seo et al.,\n2017; Clark and Gardner, 2018; Peters et al.,\n2018a; Hu et al., 2018). The top results from the\nSQuAD leaderboard do not have up-to-date public\nsystem descriptions available,11and are allowed to\nuse any public data when training their systems.\nWe therefore use modest data augmentation in\nour system by \ufb01rst \ufb01ne-tuning on TriviaQA (Joshi\net al., 2017) befor \ufb01ne-tuning on SQuAD.\nOur best performing system outperforms the top\nleaderboard system by +1.5 F1 in ensembling and\n+1.3 F1 as a single system. In fact, our single\nBERT model outperforms the top ensemble sys-\ntem in terms of F1 score. Without TriviaQA \ufb01ne-\n11QANet is described in Yu et al. (2018), but the system\nhas improved substantially after publication.\nSystem Dev Test\nEM F1 EM F1\nTop Leaderboard Systems (Dec 10th, 2018)\nHuman - - 82.3 91.2\n#1 Ensemble - nlnet - - 86.0 91.7\n#2 Ensemble - QANet - - 84.5 90.5\nPublished\nBiDAF+ELMo (Single) - 85.6 - 85.8\nR.M. Reader (Ensemble) 81.2 87.9 82.3 88.5\nOurs\nBERT BASE (Single) 80.8 88.5 - -\nBERT LARGE (Single) 84.1 90.9 - -\nBERT LARGE (Ensemble) 85.8 91.8 - -\nBERT LARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8\nBERT LARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2\nTable 2: SQuAD 1.1 results. The BERT ensemble\nis 7x systems which use different pre-training check-\npoints and \ufb01ne-tuning seeds.\nSystem Dev Test\nEM F1 EM F1\nTop Leaderboard Systems (Dec 10th, 2018)\nHuman 86.3 89.0 86.9 89.5\n#1 Single - MIR-MRC (F-Net) - - 74.8 78.0\n#2 Single - nlnet - - 74.2 77.1\nPublished\nunet (Ensemble) - - 71.4 74.9\nSLQA+ (Single) - 71.4 74.4\nOurs\nBERT LARGE (Single) 78.7 81.9 80.0 83.1\nTable 3: SQuAD 2.0 results. We exclude entries that\nuse BERT as one of their components.\ntuning data, we only lose 0.1-0.4 F1, still outper-\nforming all existing systems by a wide margin.12\n4.3 SQuAD v2.0\nThe SQuAD 2.0 task extends the SQuAD 1.1\nproblem de\ufb01nition by allowing for the possibility\nthat no short answer exists in the provided para-\ngraph, making the problem more realistic.\nWe use a simple approach to extend the SQuAD\nv1.1 BERT model for this task. We treat ques-\ntions that do not have an answer as having an an-\nswer span with start and end at the [CLS] to-\nken. The probability space for the start and end\nanswer span positions is extended to include the\nposition of the [CLS] token. For prediction, we\ncompare the score of the no-answer span: snull=\nS\u0001C+E\u0001Cto the score of the best non-null span\n12The TriviaQA data we used consists of paragraphs from\nTriviaQA-Wiki formed of the \ufb01rst 400 tokens in documents,\nthat contain at least one of the provided possible answers.System Dev Test\nESIM+GloVe 51.9 52.7\nESIM+ELMo 59.1 59.2\nOpenAI GPT - 78.0\nBERT BASE 81.6 -\nBERT LARGE 86.6 86.3\nHuman (expert)y- 85.0\nHuman (5 annotations)y- 88.0\nTable 4: SWAG Dev and Test accuracies.yHuman per-\nformance is measured with 100 samples, as reported in\nthe SWAG paper.\n^si;j=maxj\u0015iS\u0001Ti+E\u0001Tj. We predict a non-null\nanswer when ^si;j> s null+\u001c, where the thresh-\nold\u001cis selected on the dev set to maximize F1.\nWe did not use TriviaQA data for this model. We\n\ufb01ne-tuned for 2 epochs with a learning rate of 5e-5\nand a batch size of 48.\nThe results compared to prior leaderboard en-\ntries and top published work (Sun et al., 2018;\nWang et al., 2018b) are shown in Table 3, exclud-\ning systems that use BERT as one of their com-\nponents. We observe a +5.1 F1 improvement over\nthe previous best system.\n4.4 SWAG\nThe Situations With Adversarial Generations\n(SWAG) dataset contains 113k sentence-pair com-\npletion examples that evaluate grounded common-\nsense inference (Zellers et al., 2018). Given a sen-\ntence, the task is to choose the most plausible con-\ntinuation among four choices.\nWhen \ufb01ne-tuning on the SWAG dataset, we\nconstruct four input sequences, each containing\nthe concatenation of the given sentence (sentence\nA) and a possible continuation (sentence B). The\nonly task-speci\ufb01c parameters introduced is a vec-\ntor whose dot product with the [CLS] token rep-\nresentation Cdenotes a score for each choice\nwhich is normalized with a softmax layer.\nWe \ufb01ne-tune the model for 3 epochs with a\nlearning rate of 2e-5 and a batch size of 16. Re-\nsults are presented in Table 4. BERT LARGE out-\nperforms the authors\u2019 baseline ESIM+ELMo sys-\ntem by +27.1% and OpenAI GPT by 8.3%.\n5 Ablation Studies\nIn this section, we perform ablation experiments\nover a number of facets of BERT in order to better\nunderstand their relative importance. Additional\nDev Set\nTasks MNLI-m QNLI MRPC SST-2 SQuAD\n(Acc) (Acc) (Acc) (Acc) (F1)\nBERT BASE 84.4 88.4 86.7 92.7 88.5\nNo NSP 83.9 84.9 86.5 92.6 87.9\nLTR & No NSP 82.1 84.3 77.5 92.1 77.8\n+ BiLSTM 82.1 84.1 75.7 91.6 84.9\nTable 5: Ablation over the pre-training tasks using the\nBERT BASE architecture. \u201cNo NSP\u201d is trained without\nthe next sentence prediction task. \u201cLTR & No NSP\u201d is\ntrained as a left-to-right LM without the next sentence\nprediction, like OpenAI GPT. \u201c+ BiLSTM\u201d adds a ran-\ndomly initialized BiLSTM on top of the \u201cLTR + No\nNSP\u201d model during \ufb01ne-tuning.\nablation studies can be found in Appendix C.\n5.1 Effect of Pre-training Tasks\nWe demonstrate the importance of the deep bidi-\nrectionality of BERT by evaluating two pre-\ntraining objectives using exactly the same pre-\ntraining data, \ufb01ne-tuning scheme, and hyperpa-\nrameters as BERT BASE :\nNo NSP : A bidirectional model which is trained\nusing the \u201cmasked LM\u201d (MLM) but without the\n\u201cnext sentence prediction\u201d (NSP) task.\nLTR & No NSP : A left-context-only model which\nis trained using a standard Left-to-Right (LTR)\nLM, rather than an MLM. The left-only constraint\nwas also applied at \ufb01ne-tuning, because removing\nit introduced a pre-train/\ufb01ne-tune mismatch that\ndegraded downstream performance. Additionally,\nthis model was pre-trained without the NSP task.\nThis is directly comparable to OpenAI GPT, but\nusing our larger training dataset, our input repre-\nsentation, and our \ufb01ne-tuning scheme.\nWe \ufb01rst examine the impact brought by the NSP\ntask. In Table 5, we show that removing NSP\nhurts performance signi\ufb01cantly on QNLI, MNLI,\nand SQuAD 1.1. Next, we evaluate the impact\nof training bidirectional representations by com-\nparing \u201cNo NSP\u201d to \u201cLTR & No NSP\u201d. The LTR\nmodel performs worse than the MLM model on all\ntasks, with large drops on MRPC and SQuAD.\nFor SQuAD it is intuitively clear that a LTR\nmodel will perform poorly at token predictions,\nsince the token-level hidden states have no right-\nside context. In order to make a good faith at-\ntempt at strengthening the LTR system, we added\na randomly initialized BiLSTM on top. This does\nsigni\ufb01cantly improve results on SQuAD, but theresults are still far worse than those of the pre-\ntrained bidirectional models. The BiLSTM hurts\nperformance on the GLUE tasks.\nWe recognize that it would also be possible to\ntrain separate LTR and RTL models and represent\neach token as the concatenation of the two mod-\nels, as ELMo does. However: (a) this is twice as\nexpensive as a single bidirectional model; (b) this\nis non-intuitive for tasks like QA, since the RTL\nmodel would not be able to condition the answer\non the question; (c) this it is strictly less powerful\nthan a deep bidirectional model, since it can use\nboth left and right context at every layer.\n5.2 Effect of Model Size\nIn this section, we explore the effect of model size\non \ufb01ne-tuning task accuracy. We trained a number\nof BERT models with a differing number of layers,\nhidden units, and attention heads, while otherwise\nusing the same hyperparameters and training pro-\ncedure as described previously.\nResults on selected GLUE tasks are shown in\nTable 6. In this table, we report the average Dev\nSet accuracy from 5 random restarts of \ufb01ne-tuning.\nWe can see that larger models lead to a strict ac-\ncuracy improvement across all four datasets, even\nfor MRPC which only has 3,600 labeled train-\ning examples, and is substantially different from\nthe pre-training tasks. It is also perhaps surpris-\ning that we are able to achieve such signi\ufb01cant\nimprovements on top of models which are al-\nready quite large relative to the existing literature.\nFor example, the largest Transformer explored in\nVaswani et al. (2017) is (L=6, H=1024, A=16)\nwith 100M parameters for the encoder, and the\nlargest Transformer we have found in the literature\nis (L=64, H=512, A=2) with 235M parameters\n(Al-Rfou et al., 2018). By contrast, BERT BASE\ncontains 110M parameters and BERT LARGE con-\ntains 340M parameters.\nIt has long been known that increasing the\nmodel size will lead to continual improvements\non large-scale tasks such as machine translation\nand language modeling, which is demonstrated\nby the LM perplexity of held-out training data\nshown in Table 6. However, we believe that\nthis is the \ufb01rst work to demonstrate convinc-\ningly that scaling to extreme model sizes also\nleads to large improvements on very small scale\ntasks, provided that the model has been suf\ufb01-\nciently pre-trained. Peters et al. (2018b) presented\nmixed results on the downstream task impact of\nincreasing the pre-trained bi-LM size from two\nto four layers and Melamud et al. (2016) men-\ntioned in passing that increasing hidden dimen-\nsion size from 200 to 600 helped, but increasing\nfurther to 1,000 did not bring further improve-\nments. Both of these prior works used a feature-\nbased approach \u2014 we hypothesize that when the\nmodel is \ufb01ne-tuned directly on the downstream\ntasks and uses only a very small number of ran-\ndomly initialized additional parameters, the task-\nspeci\ufb01c models can bene\ufb01t from the larger, more\nexpressive pre-trained representations even when\ndownstream task data is very small.\n5.3 Feature-based Approach with BERT\nAll of the BERT results presented so far have used\nthe \ufb01ne-tuning approach, where a simple classi\ufb01-\ncation layer is added to the pre-trained model, and\nall parameters are jointly \ufb01ne-tuned on a down-\nstream task. However, the feature-based approach,\nwhere \ufb01xed features are extracted from the pre-\ntrained model, has certain advantages. First, not\nall tasks can be easily represented by a Trans-\nformer encoder architecture, and therefore require\na task-speci\ufb01c model architecture to be added.\nSecond, there are major computational bene\ufb01ts\nto pre-compute an expensive representation of the\ntraining data once and then run many experiments\nwith cheaper models on top of this representation.\nIn this section, we compare the two approaches\nby applying BERT to the CoNLL-2003 Named\nEntity Recognition (NER) task (Tjong Kim Sang\nand De Meulder, 2003). In the input to BERT, we\nuse a case-preserving WordPiece model, and we\ninclude the maximal document context provided\nby the data. Following standard practice, we for-\nmulate this as a tagging task but do not use a CRF\nHyperparams Dev Set Accuracy\n#L #H #A LM (ppl) MNLI-m MRPC SST-2\n3 768 12 5.84 77.9 79.8 88.4\n6 768 3 5.24 80.6 82.2 90.7\n6 768 12 4.68 81.9 84.8 91.3\n12 768 12 3.99 84.4 86.7 92.9\n12 1024 16 3.54 85.7 86.9 93.3\n24 1024 16 3.23 86.6 87.8 93.7\nTable 6: Ablation over BERT model size. #L = the\nnumber of layers; #H = hidden size; #A = number of at-\ntention heads. \u201cLM (ppl)\u201d is the masked LM perplexity\nof held-out training data.System Dev F1 Test F1\nELMo (Peters et al., 2018a) 95.7 92.2\nCVT (Clark et al., 2018) - 92.6\nCSE (Akbik et al., 2018) - 93.1\nFine-tuning approach\nBERT LARGE 96.6 92.8\nBERT BASE 96.4 92.4\nFeature-based approach (BERT BASE)\nEmbeddings 91.0 -\nSecond-to-Last Hidden 95.6 -\nLast Hidden 94.9 -\nWeighted Sum Last Four Hidden 95.9 -\nConcat Last Four Hidden 96.1 -\nWeighted Sum All 12 Layers 95.5 -\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\nResults are presented in Table 7. BERT LARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n6 Conclusion\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\nReferences\nAlan Akbik, Duncan Blythe, and Roland V ollgraf.\n2018. Contextual string embeddings for sequence\nlabeling. In Proceedings of the 27th International\nConference on Computational Linguistics , pages\n1638\u20131649.\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444 .\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch , 6(Nov):1817\u20131853.\nLuisa Bentivogli, Bernardo Magnini, Ido Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing , pages 120\u2013128. Association for Computa-\ntional Linguistics.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nInEMNLP . Association for Computational Linguis-\ntics.\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural language.\nComputational linguistics , 18(4):467\u2013479.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017) , pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005 .\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\nQuora question pairs.\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.Kevin Clark, Minh-Thang Luong, Christopher D Man-\nning, and Quoc Le. 2018. Semi-supervised se-\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1914\u2013\n1925.\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Pro-\nceedings of the 25th international conference on\nMachine learning , pages 160\u2013167. ACM.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo \u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing , pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems , pages 3079\u20133087.\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. 2009. ImageNet: A Large-Scale Hierarchical\nImage Database. In CVPR09 .\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nInProceedings of the Third International Workshop\non Paraphrasing (IWP2005) .\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe.arXiv preprint arXiv:1801.07736 .\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR , abs/1606.08415.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nfrom unlabelled data. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies . Association for Computa-\ntional Linguistics.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nFuru Wei, and Ming Zhou. 2018. Reinforced\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI .\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR ,\nabs/1705.00557.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors. In\nAdvances in neural information processing systems ,\npages 3294\u20133302.\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning , pages\n1188\u20131196.\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge. In\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning , volume 46, page 47.\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\ntations. In International Conference on Learning\nRepresentations .\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS .\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL .\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26 , pages 3111\u20133119. Curran Associates,\nInc.\nAndriy Mnih and Geoffrey E Hinton. 2009. A scal-\nable hierarchical distributed language model. In\nD. Koller, D. Schuurmans, Y . Bengio, and L. Bot-\ntou, editors, Advances in Neural Information Pro-\ncessing Systems 21 , pages 1081\u20131088. Curran As-\nsociates, Inc.\nAnkur P Parikh, Oscar T \u00a8ackstr \u00a8om, Dipanjan Das, and\nJakob Uszkoreit. 2016. A decomposable attention\nmodel for natural language inference. In EMNLP .\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 1532\u2013\n1543.\nMatthew Peters, Waleed Ammar, Chandra Bhagavat-\nula, and Russell Power. 2017. Semi-supervised se-\nquence tagging with bidirectional language models.\nInACL.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018a. Deep contextualized word rep-\nresentations. In NAACL .Matthew Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018b. Dissecting contextual\nword embeddings: Architecture and representation.\nInProceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing , pages\n1499\u20131509.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding with unsupervised learning. Technical re-\nport, OpenAI.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing , pages 2383\u20132392.\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\nHannaneh Hajishirzi. 2017. Bidirectional attention\n\ufb02ow for machine comprehension. In ICLR .\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing ,\npages 1631\u20131642.\nFu Sun, Linyang Li, Xipeng Qiu, and Yang Liu.\n2018. U-net: Machine reading comprehension\nwith unanswerable questions. arXiv preprint\narXiv:1810.06638 .\nWilson L Taylor. 1953. Cloze procedure: A new\ntool for measuring readability. Journalism Bulletin ,\n30(4):415\u2013433.\nErik F Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the conll-2003 shared task:\nLanguage-independent named entity recognition. In\nCoNLL .\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\nWord representations: A simple and general method\nfor semi-supervised learning. In Proceedings of the\n48th Annual Meeting of the Association for Compu-\ntational Linguistics , ACL \u201910, pages 384\u2013394.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems , pages 6000\u20136010.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. 2008. Extracting and\ncomposing robust features with denoising autoen-\ncoders. In Proceedings of the 25th international\nconference on Machine learning , pages 1096\u20131103.\nACM.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018a.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP ,\npages 353\u2013355.\nWei Wang, Ming Yan, and Chen Wu. 2018b. Multi-\ngranularity hierarchical attention fusion networks\nfor reading comprehension and question answering.\nInProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) . Association for Computational Lin-\nguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2018. Neural network acceptability judg-\nments. arXiv preprint arXiv:1805.12471 .\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2018. A broad-coverage challenge corpus\nfor sentence understanding through inference. In\nNAACL .\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google\u2019s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint\narXiv:1609.08144 .\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\nLipson. 2014. How transferable are features in deep\nneural networks? In Advances in neural information\nprocessing systems , pages 3320\u20133328.\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V\nLe. 2018. QANet: Combining local convolution\nwith global self-attention for reading comprehen-\nsion. In ICLR .\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. Swag: A large-scale adversarial dataset\nfor grounded commonsense inference. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) .\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE\ninternational conference on computer vision , pages\n19\u201327.\nAppendix for \u201cBERT: Pre-training of\nDeep Bidirectional Transformers for\nLanguage Understanding\u201d\nWe organize the appendix into three sections:\n\u2022 Additional implementation details for BERT\nare presented in Appendix A;\u2022 Additional details for our experiments are\npresented in Appendix B; and\n\u2022 Additional ablation studies are presented in\nAppendix C.\nWe present additional ablation studies for\nBERT including:\n\u2013Effect of Number of Training Steps; and\n\u2013Ablation for Different Masking Proce-\ndures.\nA Additional Details for BERT\nA.1 Illustration of the Pre-training Tasks\nWe provide examples of the pre-training tasks in\nthe following.\nMasked LM and the Masking Procedure As-\nsuming the unlabeled sentence is my dog is\nhairy , and during the random masking procedure\nwe chose the 4-th token (which corresponding to\nhairy ), our masking procedure can be further il-\nlustrated by\n\u2022 80% of the time: Replace the word with the\n[MASK] token, e.g., my dog is hairy !\nmy dog is [MASK]\n\u2022 10% of the time: Replace the word with a\nrandom word, e.g., my dog is hairy !my\ndog is apple\n\u2022 10% of the time: Keep the word un-\nchanged, e.g., my dog is hairy !my dog\nis hairy . The purpose of this is to bias the\nrepresentation towards the actual observed\nword.\nThe advantage of this procedure is that the\nTransformer encoder does not know which words\nit will be asked to predict or which have been re-\nplaced by random words, so it is forced to keep\na distributional contextual representation of ev-\neryinput token. Additionally, because random\nreplacement only occurs for 1.5% of all tokens\n(i.e., 10% of 15%), this does not seem to harm\nthe model\u2019s language understanding capability. In\nSection C.2, we evaluate the impact this proce-\ndure.\nCompared to standard langauge model training,\nthe masked LM only make predictions on 15% of\ntokens in each batch, which suggests that more\npre-training steps may be required for the model\nBERT (Ours) \nTrm Trm Trm\nTrm Trm Trm...\n...Trm Trm Trm\nTrm Trm Trm...\n...OpenAI GPT \nLstm ELMo \nLstm Lstm \nLstm Lstm Lstm Lstm Lstm Lstm \nLstm Lstm Lstm  T1 T2 TN...\n...\n......\n...\n E1 E2 EN... T1 T2TN...\n E1 E2 EN ... T1 T2 TN...\n E1 E2 EN...Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-\nleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and\nOpenAI GPT are \ufb01ne-tuning approaches, while ELMo is a feature-based approach.\nto converge. In Section C.1 we demonstrate that\nMLM does converge marginally slower than a left-\nto-right model (which predicts every token), but\nthe empirical improvements of the MLM model\nfar outweigh the increased training cost.\nNext Sentence Prediction The next sentence\nprediction task can be illustrated in the following\nexamples.\nInput =[CLS] the man went to [MASK] store [SEP]\nhe bought a gallon [MASK] milk [SEP]\nLabel =IsNext\nInput =[CLS] the man [MASK] to the store [SEP]\npenguin [MASK] are flight ##less birds [SEP]\nLabel =NotNext\nA.2 Pre-training Procedure\nTo generate each training input sequence, we sam-\nple two spans of text from the corpus, which we\nrefer to as \u201csentences\u201d even though they are typ-\nically much longer than single sentences (but can\nbe shorter also). The \ufb01rst sentence receives the A\nembedding and the second receives the Bembed-\nding. 50% of the time Bis the actual next sentence\nthat follows Aand 50% of the time it is a random\nsentence, which is done for the \u201cnext sentence pre-\ndiction\u201d task. They are sampled such that the com-\nbined length is\u0014512 tokens. The LM masking is\napplied after WordPiece tokenization with a uni-\nform masking rate of 15%, and no special consid-\neration given to partial word pieces.\nWe train with batch size of 256 sequences (256\nsequences * 512 tokens = 128,000 tokens/batch)\nfor 1,000,000 steps, which is approximately 40epochs over the 3.3 billion word corpus. We\nuse Adam with learning rate of 1e-4, \f1= 0:9,\n\f2= 0:999, L2 weight decay of 0:01, learning\nrate warmup over the \ufb01rst 10,000 steps, and linear\ndecay of the learning rate. We use a dropout prob-\nability of 0.1 on all layers. We use a gelu acti-\nvation (Hendrycks and Gimpel, 2016) rather than\nthe standard relu , following OpenAI GPT. The\ntraining loss is the sum of the mean masked LM\nlikelihood and the mean next sentence prediction\nlikelihood.\nTraining of BERT BASE was performed on 4\nCloud TPUs in Pod con\ufb01guration (16 TPU chips\ntotal).13Training of BERT LARGE was performed\non 16 Cloud TPUs (64 TPU chips total). Each pre-\ntraining took 4 days to complete.\nLonger sequences are disproportionately expen-\nsive because attention is quadratic to the sequence\nlength. To speed up pretraing in our experiments,\nwe pre-train the model with sequence length of\n128 for 90% of the steps. Then, we train the rest\n10% of the steps of sequence of 512 to learn the\npositional embeddings.\nA.3 Fine-tuning Procedure\nFor \ufb01ne-tuning, most model hyperparameters are\nthe same as in pre-training, with the exception of\nthe batch size, learning rate, and number of train-\ning epochs. The dropout probability was always\nkept at 0.1. The optimal hyperparameter values\nare task-speci\ufb01c, but we found the following range\nof possible values to work well across all tasks:\n\u2022Batch size : 16, 32\n13https://cloudplatform.googleblog.com/2018/06/Cloud-\nTPU-now-offers-preemptible-pricing-and-global-\navailability.html\n\u2022Learning rate (Adam) : 5e-5, 3e-5, 2e-5\n\u2022Number of epochs : 2, 3, 4\nWe also observed that large data sets (e.g.,\n100k+ labeled training examples) were far less\nsensitive to hyperparameter choice than small data\nsets. Fine-tuning is typically very fast, so it is rea-\nsonable to simply run an exhaustive search over\nthe above parameters and choose the model that\nperforms best on the development set.\nA.4 Comparison of BERT, ELMo ,and\nOpenAI GPT\nHere we studies the differences in recent popular\nrepresentation learning models including ELMo,\nOpenAI GPT and BERT. The comparisons be-\ntween the model architectures are shown visually\nin Figure 3. Note that in addition to the architec-\nture differences, BERT and OpenAI GPT are \ufb01ne-\ntuning approaches, while ELMo is a feature-based\napproach.\nThe most comparable existing pre-training\nmethod to BERT is OpenAI GPT, which trains a\nleft-to-right Transformer LM on a large text cor-\npus. In fact, many of the design decisions in BERT\nwere intentionally made to make it as close to\nGPT as possible so that the two methods could be\nminimally compared. The core argument of this\nwork is that the bi-directionality and the two pre-\ntraining tasks presented in Section 3.1 account for\nthe majority of the empirical improvements, but\nwe do note that there are several other differences\nbetween how BERT and GPT were trained:\n\u2022 GPT is trained on the BooksCorpus (800M\nwords); BERT is trained on the BooksCor-\npus (800M words) and Wikipedia (2,500M\nwords).\n\u2022 GPT uses a sentence separator ( [SEP] ) and\nclassi\ufb01er token ( [CLS] ) which are only in-\ntroduced at \ufb01ne-tuning time; BERT learns\n[SEP] ,[CLS] and sentence A/Bembed-\ndings during pre-training.\n\u2022 GPT was trained for 1M steps with a batch\nsize of 32,000 words; BERT was trained for\n1M steps with a batch size of 128,000 words.\n\u2022 GPT used the same learning rate of 5e-5 for\nall \ufb01ne-tuning experiments; BERT chooses a\ntask-speci\ufb01c \ufb01ne-tuning learning rate which\nperforms the best on the development set.To isolate the effect of these differences, we per-\nform ablation experiments in Section 5.1 which\ndemonstrate that the majority of the improvements\nare in fact coming from the two pre-training tasks\nand the bidirectionality they enable.\nA.5 Illustrations of Fine-tuning on Different\nTasks\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks. In\nthe \ufb01gure,Erepresents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\nB Detailed Experimental Setup\nB.1 Detailed Descriptions for the GLUE\nBenchmark Experiments.\nOur GLUE results in Table1 are obtained\nfrom https://gluebenchmark.com/\nleaderboard and https://blog.\nopenai.com/language-unsupervised .\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment ,contradiction , or\nneutral with respect to the \ufb01rst one.\nQQP Quora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\nBERT \nE[CLS] E1 E[SEP] ... ENE1\u2019... EM\u2019\nC\nT1\nT[SEP] ...\n TN\nT1\u2019...\n TM\u2019\n[CLS] Tok \n1 [SEP] ...Tok \nNTok \n1...Tok\nM\nQuestion Paragraph BERT \nE[CLS] E1 E2 EN\nC\nT1\n T2\n TN\nSingle Sentence ...\n...BERT \nTok 1 Tok 2 Tok N ... [CLS]E[CLS] E1 E2 EN\nC\nT1\n T2\n TN\nSingle Sentence \nB-PER O O...\n... E[CLS] E1 E[SEP] Class \nLabel \n... ENE1\u2019... EM\u2019\nC\nT1\nT[SEP] ...\n TN\nT1\u2019...\n TM\u2019\nStart/End Span Class \nLabel \nBERT \nTok 1 Tok 2 Tok N ... [CLS]Tok 1[CLS] [CLS] Tok \n1 [SEP] ...Tok \nNTok \n1...Tok\nM\nSentence 1 \n...Sentence 2 \nFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\nMRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotationsfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\nRTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset,15and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n15https://gluebenchmark.com/faq\njority class.\nC Additional Ablation Studies\nC.1 Effect of Number of Training Steps\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nforksteps. This allows us to answer the following\nquestions:\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERT BASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\nC.2 Ablation for Different Masking\nProcedures\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n200 400 600 800 1;0007678808284\nPre-training Steps (Thousands)MNLI Dev Accuracy\nBERT BASE (Masked LM)\nBERT BASE (Left-to-Right)\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nksteps. The x-axis is the value of k.Note that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\nMasking Rates Dev Set Results\nMASK SAME RND MNLI NER\nFine-tune Fine-tune Feature-based\n80% 10% 10% 84.2 95.4 94.9\n100% 0% 0% 84.3 94.9 94.0\n80% 0% 20% 84.1 95.2 94.6\n80% 20% 0% 84.4 95.2 94.7\n0% 20% 80% 83.7 94.8 94.6\n0% 0% 100% 83.6 94.9 94.6\nTable 8: Ablation over different masking strategies.\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe[MASK] symbol for MLM; S AME means that\nwe keep the target token as is; R NDmeans that\nwe replace the target token with another random\ntoken.\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the M ASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe R NDstrategy performs much worse than our\nstrategy as well.\n",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "arxiv_id": "1810.04805",
    "category": "famous",
    "authors": [
      "Jacob Devlin",
      "Ming-Wei Chang",
      "Kenton Lee",
      "Kristina Toutanova"
    ],
    "published": "2018-10-11",
    "num_pages": 16,
    "num_chars": 64064
  },
  {
    "text": "Language Models are Few-Shot Learners\nTom B. Brown\u0003Benjamin Mann\u0003Nick Ryder\u0003Melanie Subbiah\u0003\nJared KaplanyPrafulla Dhariwal Arvind Neelakantan Pranav Shyam Girish Sastry\nAmanda Askell Sandhini Agarwal Ariel Herbert-Voss Gretchen Krueger Tom Henighan\nRewon Child Aditya Ramesh Daniel M. Ziegler Jeffrey Wu Clemens Winter\nChristopher Hesse Mark Chen Eric Sigler Mateusz Litwin Scott Gray\nBenjamin Chess Jack Clark Christopher Berner\nSam McCandlish Alec Radford Ilya Sutskever Dario Amodei\nOpenAI\nAbstract\nRecent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training\non a large corpus of text followed by \ufb01ne-tuning on a speci\ufb01c task. While typically task-agnostic\nin architecture, this method still requires task-speci\ufb01c \ufb01ne-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language task from only\na few examples or from simple instructions \u2013 something which current NLP systems still largely\nstruggle to do. Here we show that scaling up language models greatly improves task-agnostic,\nfew-shot performance, sometimes even reaching competitiveness with prior state-of-the-art \ufb01ne-\ntuning approaches. Speci\ufb01cally, we train GPT-3, an autoregressive language model with 175 billion\nparameters, 10x more than any previous non-sparse language model, and test its performance in\nthe few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or \ufb01ne-tuning,\nwith tasks and few-shot demonstrations speci\ufb01ed purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation, question-answering, and\ncloze tasks, as well as several tasks that require on-the-\ufb02y reasoning or domain adaptation, such as\nunscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same\ntime, we also identify some datasets where GPT-3\u2019s few-shot learning still struggles, as well as some\ndatasets where GPT-3 faces methodological issues related to training on large web corpora. Finally,\nwe \ufb01nd that GPT-3 can generate samples of news articles which human evaluators have dif\ufb01culty\ndistinguishing from articles written by humans. We discuss broader societal impacts of this \ufb01nding\nand of GPT-3 in general.\n\u0003Equal contribution\nyJohns Hopkins University, OpenAI\nAuthor contributions listed at end of paper.arXiv:2005.14165v4  [cs.CL]  22 Jul 2020\nContents\n1 Introduction 3\n2 Approach 6\n2.1 Model and Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.2 Training Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.3 Training Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.4 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3 Results 10\n3.1 Language Modeling, Cloze, and Completion Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3.2 Closed Book Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.3 Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3.4 Winograd-Style Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.5 Common Sense Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n3.6 Reading Comprehension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3.7 SuperGLUE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3.8 NLI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n3.9 Synthetic and Qualitative Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n4 Measuring and Preventing Memorization Of Benchmarks 29\n5 Limitations 33\n6 Broader Impacts 34\n6.1 Misuse of Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n6.2 Fairness, Bias, and Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n6.3 Energy Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n7 Related Work 39\n8 Conclusion 40\nA Details of Common Crawl Filtering 43\nB Details of Model Training 43\nC Details of Test Set Contamination Studies 43\nD Total Compute Used to Train Language Models 46\nE Human Quality Assessment of Synthetic News Articles 46\nF Additional Samples from GPT-3 48\nG Details of Task Phrasing and Speci\ufb01cations 50\nH Results on All Tasks for All Model Sizes 63\n2\n1 Introduction\nRecent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly\n\ufb02exible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word\nvectors [ MCCD13 ,PSM14 ] and fed to task-speci\ufb01c architectures, then RNNs with multiple layers of representations\nand contextual state were used to form stronger representations [ DL15 ,MBXS17 ,PNZtY18 ] (though still applied to\ntask-speci\ufb01c architectures), and more recently pre-trained recurrent or transformer language models [ VSP+17] have\nbeen directly \ufb01ne-tuned, entirely removing the need for task-speci\ufb01c architectures [RNSS18, DCLT18, HR18].\nThis last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension,\nquestion answering, textual entailment, and many others, and has continued to advance based on new architectures\nand algorithms [ RSR+19,LOG+19,YDY+19,LCG+19]. However, a major limitation to this approach is that while\nthe architecture is task-agnostic, there is still a need for task-speci\ufb01c datasets and task-speci\ufb01c \ufb01ne-tuning: to achieve\nstrong performance on a desired task typically requires \ufb01ne-tuning on a dataset of thousands to hundreds of thousands\nof examples speci\ufb01c to that task. Removing this limitation would be desirable, for several reasons.\nFirst, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the\napplicability of language models. There exists a very wide range of possible useful language tasks, encompassing\nanything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story. For many\nof these tasks it is dif\ufb01cult to collect a large supervised training dataset, especially when the process must be repeated\nfor every new task.\nSecond, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness\nof the model and the narrowness of the training distribution. This can create problems for the pre-training plus\n\ufb01ne-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then\n\ufb01ne-tuned on very narrow task distributions. For instance [ HLW+20] observe that larger models do not necessarily\ngeneralize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm\ncan be poor because the model is overly speci\ufb01c to the training distribution and does not generalize well outside it\n[YdC+19,MPL19 ]. Thus, the performance of \ufb01ne-tuned models on speci\ufb01c benchmarks, even when it is nominally at\nhuman-level, may exaggerate actual performance on the underlying task [GSL+18, NK19].\nThird, humans do not require large supervised datasets to learn most language tasks \u2013 a brief directive in natural\nlanguage (e.g. \u201cplease tell me if this sentence describes something happy or something sad\u201d) or at most a tiny number\nof demonstrations (e.g. \u201chere are two examples of people acting brave; please give a third example of bravery\u201d) is often\nFigure 1.1: Language model meta-learning. During unsupervised pre-training, a language model develops a broad\nset of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize\nthe desired task. We use the term \u201cin-context learning\u201d to describe the inner loop of this process, which occurs within\nthe forward-pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a\nmodel would see during pre-training, but are intended to show that there are sometimes repeated sub-tasks embedded\nwithin a single sequence.\n3\nFigure 1.2: Larger models make increasingly ef\ufb01cient use of in-context information. We show in-context learning\nperformance on a simple task requiring the model to remove random symbols from a word, both with and without a\nnatural language task description (see Sec. 3.9.2). The steeper \u201cin-context learning curves\u201d for large models demonstrate\nimproved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range\nof tasks.\nsuf\ufb01cient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing\nto a conceptual limitation in our current NLP techniques, this adaptability has practical advantages \u2013 it allows humans\nto seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy\ndialogue. To be broadly useful, we would someday like our NLP systems to have this same \ufb02uidity and generality.\nOne potential route towards addressing these issues is meta-learning1\u2013 which in the context of language models means\nthe model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities\nat inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1). Recent work [ RWC+19]\nattempts to do this via what we call \u201cin-context learning\u201d, using the text input of a pretrained language model as a form\nof task speci\ufb01cation: the model is conditioned on a natural language instruction and/or a few demonstrations of the task\nand is then expected to complete further instances of the task simply by predicting what comes next.\nWhile it has shown some initial promise, this approach still achieves results far inferior to \ufb01ne-tuning \u2013 for example\n[RWC+19] achieves only 4% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind\nthe state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of\nsolving language tasks.\nAnother recent trend in language modeling may offer a way forward. In recent years the capacity of transformer\nlanguage models has increased substantially, from 100 million parameters [ RNSS18 ], to 300 million parameters\n[DCLT18 ], to 1.5 billion parameters [ RWC+19], to 8 billion parameters [ SPP+19], 11 billion parameters [ RSR+19],\nand \ufb01nally 17 billion parameters [ Tur20 ]. Each increase has brought improvements in text synthesis and/or downstream\nNLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a\nsmooth trend of improvement with scale [ KMH+20]. Since in-context learning involves absorbing many skills and\ntasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong\ngains with scale.\n1In the context of language models this has sometimes been called \u201czero-shot transfer\u201d, but this term is potentially ambiguous:\nthe method is \u201czero-shot\u201d in the sense that no gradient updates are performed, but it often involves providing inference-time\ndemonstrations to the model, so is not truly learning from zero examples. To avoid this confusion, we use the term \u201cmeta-learning\u201d\nto capture the inner-loop / outer-loop structure of the general method, and the term \u201cin context-learning\u201d to refer to the inner\nloop of meta-learning. We further specialize the description to \u201czero-shot\u201d, \u201cone-shot\u201d, or \u201cfew-shot\u201d depending on how many\ndemonstrations are provided at inference time. These terms are intended to remain agnostic on the question of whether the model\nlearns new tasks from scratch at inference time or simply recognizes patterns seen during training \u2013 this is an important issue which\nwe discuss later in the paper, but \u201cmeta-learning\u201d is intended to encompass both possibilities, and simply describes the inner-outer\nloop structure.\n4\nFigure 1.3: Aggregate performance for all 42 accuracy-denominated benchmarks While zero-shot performance\nimproves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are\nmore pro\ufb01cient at in-context learning. See Figure 3.8 for a more detailed analysis on SuperGLUE, a standard NLP\nbenchmark suite.\nIn this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call\nGPT-3, and measuring its in-context learning abilities. Speci\ufb01cally, we evaluate GPT-3 on over two dozen NLP datasets,\nas well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training\nset. For each task, we evaluate GPT-3 under 3 conditions: (a) \u201cfew-shot learning\u201d, or in-context learning where we\nallow as many demonstrations as will \ufb01t into the model\u2019s context window (typically 10 to 100), (b) \u201cone-shot learning\u201d,\nwhere we allow only one demonstration, and (c) \u201czero-shot\u201d learning, where no demonstrations are allowed and only\nan instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional\n\ufb01ne-tuning setting, but we leave this to future work.\nFigure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to\nremove extraneous symbols from a word. Model performance improves with the addition of a natural language task\ndescription, and with the number of examples in the model\u2019s context, K. Few-shot learning also improves dramatically\nwith model size. Though the results in this case are particularly striking, the general trends with both model size and\nnumber of examples in-context hold for most tasks we study. We emphasize that these \u201clearning\u201d curves involve no\ngradient updates or \ufb01ne-tuning, just increasing numbers of demonstrations given as conditioning.\nBroadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot\nsetting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held\nby \ufb01ne-tuned models). For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in\nthe one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the\nzero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art\nrelative to \ufb01ne-tuned models operating in the same closed-book setting.\nGPT-3 also displays one-shot and few-shot pro\ufb01ciency at tasks designed to test rapid adaption or on-the-\ufb02y reasoning,\nwhich include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them\nde\ufb01ned only once. We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human\nevaluators have dif\ufb01culty distinguishing from human-generated articles.\nAt the same time, we also \ufb01nd some tasks on which few-shot performance struggles, even at the scale of GPT-3. This\nincludes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE\nor QuAC. By presenting a broad characterization of GPT-3\u2019s strengths and weaknesses, including these limitations, we\nhope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed.\nA heuristic sense of the overall results can be seen in Figure 1.3, which aggregates the various tasks (though it should\nnot be seen as a rigorous or meaningful benchmark in itself).\n5\nWe also undertake a systematic study of \u201cdata contamination\u201d \u2013 a growing problem when training high capacity models\non datasets such as Common Crawl, which can potentially include content from test datasets simply because such\ncontent often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify\nits distorting effects. Although we \ufb01nd that data contamination has a minimal effect on GPT-3\u2019s performance on most\ndatasets, we do identify a few datasets where it could be in\ufb02ating results, and we either do not report results on these\ndatasets or we note them with an asterisk, depending on the severity.\nIn addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion\nparameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings. Broadly, for most\ntasks we \ufb01nd relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap\nbetween zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models\nare more pro\ufb01cient meta-learners.\nFinally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and\nbroader societal impacts, and attempt a preliminary analysis of GPT-3\u2019s characteristics in this regard.\nThe remainder of this paper is organized as follows. In Section 2, we describe our approach and methods for training\nGPT-3 and evaluating it. Section 3 presents results on the full range of tasks in the zero-, one- and few-shot settings.\nSection 4 addresses questions of data contamination (train-test overlap). Section 5 discusses limitations of GPT-3.\nSection 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes.\n2 Approach\nOur basic pre-training approach, including model, data, and training, is similar to the process described in [ RWC+19],\nwith relatively straightforward scaling up of the model size, dataset size and diversity, and length of training. Our use\nof in-context learning is also similar to [ RWC+19], but in this work we systematically explore different settings for\nlearning within the context. Therefore, we start this section by explicitly de\ufb01ning and contrasting the different settings\nthat we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a\nspectrum of how much task-speci\ufb01c data they tend to rely on. Speci\ufb01cally, we can identify at least four points on this\nspectrum (see Figure 2.1 for an illustration):\n\u2022Fine-Tuning (FT) has been the most common approach in recent years, and involves updating the weights of\na pre-trained model by training on a supervised dataset speci\ufb01c to the desired task. Typically thousands to\nhundreds of thousands of labeled examples are used. The main advantage of \ufb01ne-tuning is strong performance\non many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential\nfor poor generalization out-of-distribution [ MPL19 ], and the potential to exploit spurious features of the\ntraining data [ GSL+18,NK19 ], potentially resulting in an unfair comparison with human performance. In\nthis work we do not \ufb01ne-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be\n\ufb01ne-tuned in principle and this is a promising direction for future work.\n\u2022Few-Shot (FS) is the term we will use in this work to refer to the setting where the model is given a few\ndemonstrations of the task at inference time as conditioning [ RWC+19], but no weight updates are allowed.\nAs shown in Figure 2.1, for a typical dataset an example has a context and a desired completion (for example\nan English sentence and the French translation), and few-shot works by giving Kexamples of context and\ncompletion, and then one \ufb01nal example of context, with the model expected to provide the completion. We\ntypically set Kin the range of 10 to 100 as this is how many examples can \ufb01t in the model\u2019s context window\n(nctx= 2048 ). The main advantages of few-shot are a major reduction in the need for task-speci\ufb01c data and\nreduced potential to learn an overly narrow distribution from a large but narrow \ufb01ne-tuning dataset. The main\ndisadvantage is that results from this method have so far been much worse than state-of-the-art \ufb01ne-tuned\nmodels. Also, a small amount of task speci\ufb01c data is still required. As indicated by the name, few-shot\nlearning as described here for language models is related to few-shot learning as used in other contexts in\nML [ HYC01 ,VBL+16] \u2013 both involve learning based on a broad distribution of tasks (in this case implicit in\nthe pre-training data) and then rapidly adapting to a new task.\n\u2022One-Shot (1S) is the same as few-shot except that only one demonstration is allowed, in addition to a natural\nlanguage description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and\nzero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans.\nFor example, when asking humans to generate a dataset on a human worker service (for example Mechanical\nTurk), it is common to give one demonstration of the task. By contrast it is sometimes dif\ufb01cult to communicate\nthe content or format of a task if no examples are given.\n6\nFigure 2.1: Zero-shot, one-shot and few-shot, contrasted with traditional \ufb01ne-tuning . The panels above show\nfour methods for performing a task with a language model \u2013 \ufb01ne-tuning is the traditional method, whereas zero-, one-,\nand few-shot, which we study in this work, require the model to perform the task with only forward passes at test\ntime. We typically present the model with a few dozen examples in the few shot setting. Exact phrasings for all task\ndescriptions, examples and prompts can be found in Appendix G.\n\u2022Zero-Shot (0S) is the same as one-shot except that no demonstrations are allowed, and the model is only given\na natural language instruction describing the task. This method provides maximum convenience, potential for\nrobustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of\npre-training data), but is also the most challenging setting. In some cases it may even be dif\ufb01cult for humans\nto understand the format of the task without prior examples, so this setting is in some cases \u201cunfairly hard\u201d.\nFor example, if someone is asked to \u201cmake a table of world records for the 200m dash\u201d, this request can be\nambiguous, as it may not be clear exactly what format the table should have or what should be included (and\neven with careful clari\ufb01cation, understanding precisely what is desired can be dif\ufb01cult). Nevertheless, for at\nleast some settings zero-shot is closest to how humans perform tasks \u2013 for example, in the translation example\nin Figure 2.1, a human would likely know what to do from just the text instruction.\nFigure 2.1 shows the four methods using the example of translating English to French. In this paper we focus on\nzero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different\nproblem settings which offer a varying trade-off between performance on speci\ufb01c benchmarks and sample ef\ufb01ciency.\nWe especially highlight the few-shot results as many of them are only slightly behind state-of-the-art \ufb01ne-tuned models.\nUltimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance,\nand are important targets for future work.\nSections 2.1-2.3 below give details on our models, training data, and training process respectively. Section 2.4 discusses\nthe details of how we do few-shot, one-shot, and zero-shot evaluations.\n7\nModel Name nparamsnlayersdmodelnheadsdhead Batch Size Learning Rate\nGPT-3 Small 125M 12 768 12 64 0.5M 6:0\u000210\u00004\nGPT-3 Medium 350M 24 1024 16 64 0.5M 3:0\u000210\u00004\nGPT-3 Large 760M 24 1536 16 96 0.5M 2:5\u000210\u00004\nGPT-3 XL 1.3B 24 2048 24 128 1M 2:0\u000210\u00004\nGPT-3 2.7B 2.7B 32 2560 32 80 1M 1:6\u000210\u00004\nGPT-3 6.7B 6.7B 32 4096 32 128 2M 1:2\u000210\u00004\nGPT-3 13B 13.0B 40 5140 40 128 2M 1:0\u000210\u00004\nGPT-3 175B or \u201cGPT-3\u201d 175.0B 96 12288 96 128 3.2M 0:6\u000210\u00004\nTable 2.1: Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of the models\nwhich we trained. All models were trained for a total of 300 billion tokens.\n2.1 Model and Architectures\nWe use the same model and architecture as GPT-2 [ RWC+19], including the modi\ufb01ed initialization, pre-normalization,\nand reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse\nattention patterns in the layers of the transformer, similar to the Sparse Transformer [ CGRS19 ]. To study the dependence\nof ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125\nmillion parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work [ KMH+20]\nsuggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a\nfunction of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for\ndownstream language tasks.\nTable 2.1 shows the sizes and architectures of our 8 models. Here nparams is the total number of trainable parameters,\nnlayers is the total number of layers, dmodel is the number of units in each bottleneck layer (we always have the\nfeedforward layer four times the size of the bottleneck layer, d\u000b= 4\u0003dmodel ), anddhead is the dimension of each\nattention head. All models use a context window of nctx= 2048 tokens. We partition the model across GPUs along\nboth the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural\nparameters for each model are chosen based on computational ef\ufb01ciency and load-balancing in the layout of models\nacross GPU\u2019s. Previous work [ KMH+20] suggests that validation loss is not strongly sensitive to these parameters\nwithin a reasonably broad range.\n2.2 Training Dataset\nDatasets for language models have rapidly expanded, culminating in the Common Crawl dataset2[RSR+19] constituting\nnearly a trillion words. This size of dataset is suf\ufb01cient to train our largest models without ever updating on the same\nsequence twice. However, we have found that un\ufb01ltered or lightly \ufb01ltered versions of Common Crawl tend to have\nlower quality than more curated datasets. Therefore, we took 3 steps to improve the average quality of our datasets:\n(1) we downloaded and \ufb01ltered a version of CommonCrawl based on similarity to a range of high-quality reference\ncorpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy\nand preserve the integrity of our held-out validation set as an accurate measure of over\ufb01tting, and (3) we also added\nknown high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.\nDetails of the \ufb01rst two points (processing of Common Crawl) are described in Appendix A. For the third, we added\nseveral curated high-quality datasets, including an expanded version of the WebText dataset [ RWC+19], collected\nby scraping links over a longer period of time, and \ufb01rst described in [ KMH+20], two internet-based books corpora\n(Books1 and Books2) and English-language Wikipedia.\nTable 2.2 shows the \ufb01nal mixture of datasets that we used in training. The CommonCrawl data was downloaded from\n41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before \ufb01ltering\nand 570GB after \ufb01ltering, roughly equivalent to 400 billion byte-pair-encoded tokens. Note that during training, datasets\nare not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently,\nsuch that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are\nsampled 2-3 times. This essentially accepts a small amount of over\ufb01tting in exchange for higher quality training data.\n2https://commoncrawl.org/the-data/\n8\nFigure 2.2: Total compute used during training . Based on the analysis in Scaling Laws For Neural Language Models\n[KMH+20] we train much larger models on many fewer tokens than is typical. As a consequence, although GPT-3 3B\nis almost 10x larger than RoBERTa-Large (355M params), both models took roughly 50 peta\ufb02op/s-days of compute\nduring pre-training. Methodology for these calculations can be found in Appendix D.\nDatasetQuantity\n(tokens)Weight in\ntraining mixEpochs elapsed when\ntraining for 300B tokens\nCommon Crawl (\ufb01ltered) 410 billion 60% 0.44\nWebText2 19 billion 22% 2.9\nBooks1 12 billion 8% 1.9\nBooks2 55 billion 8% 0.43\nWikipedia 3 billion 3% 3.4\nTable 2.2: Datasets used to train GPT-3 . \u201cWeight in training mix\u201d refers to the fraction of examples during training\nthat are drawn from a given dataset, which we intentionally do not make proportional to the size of the dataset. As a\nresult, when we train for 300 billion tokens, some datasets are seen up to 3.4 times during training while other datasets\nare seen less than once.\nA major methodological concern with language models pretrained on a broad swath of internet data, particularly large\nmodels with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by\nhaving their test or development sets inadvertently seen during pre-training. To reduce such contamination, we searched\nfor and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper.\nUnfortunately, a bug in the \ufb01ltering caused us to ignore some overlaps, and due to the cost of training it was not feasible\nto retrain the model. In Section 4 we characterize the impact of the remaining overlaps, and in future work we will\nmore aggressively remove data contamination.\n2.3 Training Process\nAs found in [ KMH+20,MKAT18 ], larger models can typically use a larger batch size, but require a smaller learning\nrate. We measure the gradient noise scale during training and use it to guide our choice of batch size [ MKAT18 ]. Table\n2.1 shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture\nof model parallelism within each matrix multiply and model parallelism across the layers of the network. All models\nwere trained on V100 GPU\u2019s on part of a high-bandwidth cluster provided by Microsoft. Details of the training process\nand hyperparameter settings are described in Appendix B.\n9\n2.4 Evaluation\nFor few-shot learning, we evaluate each example in the evaluation set by randomly drawing Kexamples from that\ntask\u2019s training set as conditioning, delimited by 1 or 2 newlines depending on the task. For LAMBADA and Storycloze\nthere is no supervised training set available so we draw conditioning examples from the development set and evaluate\non the test set. For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning\nexamples directly from it.\nKcan be any value from 0 to the maximum amount allowed by the model\u2019s context window, which is nctx= 2048\nfor all models and typically \ufb01ts 10to100examples. Larger values of Kare usually but not always better, so when a\nseparate development and test set are available, we experiment with a few values of Kon the development set and then\nrun the best value on the test set. For some tasks (see Appendix G) we also use a natural language prompt in addition to\n(or forK= 0, instead of) demonstrations.\nOn tasks that involve choosing one correct completion from several options (multiple choice), we provide Kexamples\nof context plus correct completion, followed by one example of context only, and compare the LM likelihood of\neach completion. For most tasks we compare the per-token likelihood (to normalize for length), however on a small\nnumber of datasets (ARC, OpenBookQA, and RACE) we gain additional bene\ufb01t as measured on the development set\nby normalizing by the unconditional probability of each completion, by computingP(completionjcontext)\nP(completionjanswer context), where\nanswer context is the string \"Answer: \" or\"A: \" and is used to prompt that the completion should be an answer\nbut is otherwise generic.\nOn tasks that involve binary classi\ufb01cation, we give the options more semantically meaningful names (e.g. \u201cTrue\u201d or\n\u201cFalse\u201d rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what\nis done by [RSR+19] (see Appendix G) for details.\nOn tasks with free-form completion, we use beam search with the same parameters as [ RSR+19]: a beam width of 4\nand a length penalty of \u000b= 0:6. We score the model using F1 similarity score, BLEU, or exact match, depending on\nwhat is standard for the dataset at hand.\nFinal results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-,\nand few-shot). When the test set is private, our model is often too large to \ufb01t on the test server, so we report results on\nthe development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa)\nwhere we were able to make submission work, and we submit only the 200B few-shot results, and report development\nset results for everything else.\n3 Results\nIn Figure 3.1 we display training curves for the 8 models described in Section 2. For this graph we also include 6\nadditional extra-small models with as few as 100,000 parameters. As observed in [ KMH+20], language modeling\nperformance follows a power-law when making ef\ufb01cient use of training compute. After extending this trend by two\nmore orders of magnitude, we observe only a slight (if any) departure from the power-law. One might worry that these\nimprovements in cross-entropy loss come only from modeling spurious details of our training corpus. However, we will\nsee in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a\nbroad spectrum of natural language tasks.\nBelow, we evaluate the 8 models described in Section 2 (the 175 billion parameter parameter GPT-3 and 7 smaller\nmodels) on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks.\nIn Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling,\nsuch as Cloze tasks and sentence/paragraph completion tasks. In Section 3.2 we evaluate on \u201cclosed book\u201d question\nanswering tasks: tasks which require using the information stored in the model\u2019s parameters to answer general\nknowledge questions. In Section 3.3 we evaluate the model\u2019s ability to translate between languages (especially one-shot\nand few-shot). In Section 3.4 we evaluate the model\u2019s performance on Winograd Schema-like tasks. In Section 3.5 we\nevaluate on datasets that involve commonsense reasoning or question answering. In Section 3.6 we evaluate on reading\ncomprehension tasks, in Section 3.7 we evaluate on the SuperGLUE benchmark suite, and in 3.8 we brie\ufb02y explore\nNLI. Finally, in Section 3.9, we invent some additional tasks designed especially to probe in-context learning abilities \u2013\nthese tasks focus on on-the-\ufb02y reasoning, adaptation skills, or open-ended text synthesis. We evaluate all tasks in the\nfew-shot, one-shot, and zero-shot settings.\n10\nFigure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy\nvalidation loss) follows a power-law trend with the amount of compute used for training. The power-law behavior\nobserved in [ KMH+20] continues for an additional two orders of magnitude with only small deviations from the\npredicted curve. For this \ufb01gure, we exclude embedding parameters from compute and parameter counts.\nSetting PTB\nSOTA (Zero-Shot) 35.8a\nGPT-3 Zero-Shot 20.5\nTable 3.1: Zero-shot results on PTB language modeling dataset. Many other common language modeling datasets\nare omitted because they are derived from Wikipedia or other sources which are included in GPT-3\u2019s training data.\na[RWC+19]\n3.1 Language Modeling, Cloze, and Completion Tasks\nIn this section we test GPT-3\u2019s performance on the traditional task of language modeling, as well as related tasks\nthat involve predicting a single word of interest, completing a sentence or paragraph, or choosing between possible\ncompletions of a piece of text.\n3.1.1 Language Modeling\nWe calculate zero-shot perplexity on the Penn Tree Bank (PTB) [ MKM+94] dataset measured in [ RWC+19]. We omit\nthe 4 Wikipedia-related tasks in that work because they are entirely contained in our training data, and we also omit the\none-billion word benchmark due to a high fraction of the dataset being contained in our training set. PTB escapes these\nissues due to predating the modern internet. Our largest model sets a new SOTA on PTB by a substantial margin of 15\npoints, achieving a perplexity of 20.50. Note that since PTB is a traditional language modeling dataset it does not have\na clear separation of examples to de\ufb01ne one-shot or few-shot evaluation around, so we measure only zero-shot.\n3.1.2 LAMBADA\nThe LAMBADA dataset [ PKL+16] tests the modeling of long-range dependencies in text \u2013 the model is asked to\npredict the last word of sentences which require reading a paragraph of context. It has recently been suggested that the\ncontinued scaling of language models is yielding diminishing returns on this dif\ufb01cult benchmark. [ BHT+20] re\ufb02ect on\nthe small 1.5% improvement achieved by a doubling of model size between two recent state of the art results ([ SPP+19]\n11\nSettingLAMBADA\n(acc)LAMBADA\n(ppl)StoryCloze\n(acc)HellaSwag\n(acc)\nSOTA 68.0a8.63b91.8c85.6d\nGPT-3 Zero-Shot 76.2 3.00 83.2 78.9\nGPT-3 One-Shot 72.5 3.35 84.7 78.1\nGPT-3 Few-Shot 86.4 1.92 87.7 79.3\nTable 3.2: Performance on cloze and completion tasks. GPT-3 signi\ufb01cantly improves SOTA on LAMBADA while\nachieving respectable performance on two dif\ufb01cult completion prediction datasets.a[Tur20 ]b[RWC+19]c[LDL19 ]\nd[LCH+20]\nFigure 3.2: On LAMBADA, the few-shot capability of language models results in a strong boost to accuracy. GPT-3\n2.7B outperforms the SOTA 17B parameter Turing-NLG [ Tur20 ] in this setting, and GPT-3 175B advances the state of\nthe art by 18%. Note zero-shot uses a different format from one-shot and few-shot as described in the text.\nand [ Tur20 ]) and argue that \u201ccontinuing to expand hardware and data sizes by orders of magnitude is not the path\nforward\u201d. We \ufb01nd that path is still promising and in a zero-shot setting GPT-3 achieves 76% on LAMBADA, a gain of\n8% over the previous state of the art.\nLAMBADA is also a demonstration of the \ufb02exibility of few-shot learning as it provides a way to address a problem that\nclassically occurs with this dataset. Although the completion in LAMBADA is always the last word in a sentence, a\nstandard language model has no way of knowing this detail. It thus assigns probability not only to the correct ending but\nalso to other valid continuations of the paragraph. This problem has been partially addressed in the past with stop-word\n\ufb01lters [ RWC+19] (which ban \u201ccontinuation\u201d words). The few-shot setting instead allows us to \u201cframe\u201d the task as a\ncloze-test and allows the language model to infer from examples that a completion of exactly one word is desired. We\nuse the following \ufb01ll-in-the-blank format:\nAlice was friends with Bob. Alice went to visit her friend .!Bob\nGeorge bought some baseball equipment, a ball, a glove, and a .!\nWhen presented with examples formatted this way, GPT-3 achieves 86.4% accuracy in the few-shot setting, an increase\nof over 18% from the previous state-of-the-art. We observe that few-shot performance improves strongly with model\nsize. While this setting decreases the performance of the smallest model by almost 20%, for GPT-3 it improves accuracy\nby 10%. Finally, the \ufb01ll-in-blank method is not effective one-shot, where it always performs worse than the zero-shot\nsetting. Perhaps this is because all models still require several examples to recognize the pattern.\n12\nSetting NaturalQS WebQS TriviaQA\nRAG (Fine-tuned, Open-Domain) [LPP+20] 44.5 45.5 68.0\nT5-11B+SSM (Fine-tuned, Closed-Book) [RRS20] 36.6 44.7 60.5\nT5-11B (Fine-tuned, Closed-Book) 34.5 37.4 50.1\nGPT-3 Zero-Shot 14.6 14.4 64.3\nGPT-3 One-Shot 23.0 25.3 68.0\nGPT-3 Few-Shot 29.9 41.5 71.2\nTable 3.3: Results on three Open-Domain QA tasks. GPT-3 is shown in the few-, one-, and zero-shot settings, as\ncompared to prior SOTA results for closed book and open domain settings. TriviaQA few-shot result is evaluated on the\nwiki split test server.\nOne note of caution is that an analysis of test set contamination identi\ufb01ed that a signi\ufb01cant minority of the LAMBADA\ndataset appears to be present in our training data \u2013 however analysis performed in Section 4 suggests negligible impact\non performance.\n3.1.3 HellaSwag\nThe HellaSwag dataset [ ZHB+19] involves picking the best ending to a story or set of instructions. The examples were\nadversarially mined to be dif\ufb01cult for language models while remaining easy for humans (who achieve 95.6% accuracy).\nGPT-3 achieves 78.1% accuracy in the one-shot setting and 79.3% accuracy in the few-shot setting, outperforming the\n75.4% accuracy of a \ufb01ne-tuned 1.5B parameter language model [ ZHR+19] but still a fair amount lower than the overall\nSOTA of 85.6% achieved by the \ufb01ne-tuned multi-task model ALUM.\n3.1.4 StoryCloze\nWe next evaluate GPT-3 on the StoryCloze 2016 dataset [ MCH+16], which involves selecting the correct ending\nsentence for \ufb01ve-sentence long stories. Here GPT-3 achieves 83.2% in the zero-shot setting and 87.7% in the few-shot\nsetting (with K= 70 ). This is still 4.1% lower than the \ufb01ne-tuned SOTA using a BERT based model [ LDL19 ] but\nimproves over previous zero-shot results by roughly 10%.\n3.2 Closed Book Question Answering\nIn this section we measure GPT-3\u2019s ability to answer questions about broad factual knowledge. Due to the immense\namount of possible queries, this task has normally been approached by using an information retrieval system to \ufb01nd\nrelevant text in combination with a model which learns to generate an answer given the question and the retrieved\ntext. Since this setting allows a system to search for and condition on text which potentially contains the answer it\nis denoted \u201copen-book\u201d. [ RRS20 ] recently demonstrated that a large language model can perform surprisingly well\ndirectly answering the questions without conditioning on auxilliary information. They denote this more restrictive\nevaluation setting as \u201cclosed-book\u201d. Their work suggests that even higher-capacity models could perform even better\nand we test this hypothesis with GPT-3. We evaluate GPT-3 on the 3 datasets in [ RRS20 ]: Natural Questions [ KPR+19],\nWebQuestions [BCFL13], and TriviaQA [JCWZ17], using the same splits. Note that in addition to all results being in\nthe closed-book setting, our use of few-shot, one-shot, and zero-shot evaluations represent an even stricter setting than\nprevious closed-book QA work: in addition to external content not being allowed, \ufb01ne-tuning on the Q&A dataset itself\nis also not permitted.\nThe results for GPT-3 are shown in Table 3.3. On TriviaQA, we achieve 64.3% in the zero-shot setting, 68.0% in the\none-shot setting, and 71.2% in the few-shot setting. The zero-shot result already outperforms the \ufb01ne-tuned T5-11B by\n14.2%, and also outperforms a version with Q&A tailored span prediction during pre-training by 3.8%. The one-shot\nresult improves by 3.7% and matches the SOTA for an open-domain QA system which not only \ufb01ne-tunes but also\nmakes use of a learned retrieval mechanism over a 15.3B parameter dense vector index of 21M documents [ LPP+20].\nGPT-3\u2019s few-shot result further improves performance another 3.2% beyond this.\nOn WebQuestions (WebQs), GPT-3 achieves 14.4% in the zero-shot setting, 25.3% in the one-shot setting, and 41.5%\nin the few-shot setting. This compares to 37.4% for \ufb01ne-tuned T5-11B, and 44.7% for \ufb01ne-tuned T5-11B+SSM,\nwhich uses a Q&A-speci\ufb01c pre-training procedure. GPT-3 in the few-shot setting approaches the performance of\nstate-of-the-art \ufb01ne-tuned models. Notably, compared to TriviaQA, WebQS shows a much larger gain from zero-shot to\nfew-shot (and indeed its zero-shot and one-shot performance are poor), perhaps suggesting that the WebQs questions\n13\nFigure 3.3: On TriviaQA GPT3\u2019s performance grows smoothly with model size, suggesting that language models\ncontinue to absorb knowledge as their capacity increases. One-shot and few-shot performance make signi\ufb01cant gains\nover zero-shot behavior, matching and exceeding the performance of the SOTA \ufb01ne-tuned open-domain model, RAG\n[LPP+20]\nand/or the style of their answers are out-of-distribution for GPT-3. Nevertheless, GPT-3 appears able to adapt to this\ndistribution, recovering strong performance in the few-shot setting.\nOn Natural Questions (NQs) GPT-3 achieves 14.6% in the zero-shot setting, 23.0% in the one-shot setting, and 29.9% in\nthe few-shot setting, compared to 36.6% for \ufb01ne-tuned T5 11B+SSM. Similar to WebQS, the large gain from zero-shot\nto few-shot may suggest a distribution shift, and may also explain the less competitive performance compared to\nTriviaQA and WebQS. In particular, the questions in NQs tend towards very \ufb01ne-grained knowledge on Wikipedia\nspeci\ufb01cally which could be testing the limits of GPT-3\u2019s capacity and broad pretraining distribution.\nOverall, on one of the three datasets GPT-3\u2019s one-shot matches the open-domain \ufb01ne-tuning SOTA. On the other two\ndatasets it approaches the performance of the closed-book SOTA despite not using \ufb01ne-tuning. On all 3 datasets, we\n\ufb01nd that performance scales very smoothly with model size (Figure 3.3 and Appendix H Figure H.7), possibly re\ufb02ecting\nthe idea that model capacity translates directly to more \u2018knowledge\u2019 absorbed in the parameters of the model.\n3.3 Translation\nFor GPT-2 a \ufb01lter was used on a multilingual collection of documents to produce an English only dataset due to capacity\nconcerns. Even with this \ufb01ltering GPT-2 showed some evidence of multilingual capability and performed non-trivially\nwhen translating between French and English despite only training on 10 megabytes of remaining French text. Since we\nincrease the capacity by over two orders of magnitude from GPT-2 to GPT-3, we also expand the scope of the training\ndataset to include more representation of other languages, though this remains an area for further improvement. As\ndiscussed in 2.2 the majority of our data is derived from raw Common Crawl with only quality-based \ufb01ltering. Although\nGPT-3\u2019s training data is still primarily English (93% by word count), it also includes 7% of text in other languages.\nThese languages are documented in the supplemental material. In order to better understand translation capability, we\nalso expand our analysis to include two additional commonly studied languages, German and Romanian.\nExisting unsupervised machine translation approaches often combine pretraining on a pair of monolingual datasets\nwith back-translation [ SHB15 ] to bridge the two languages in a controlled way. By contrast, GPT-3 learns from a\nblend of training data that mixes many languages together in a natural way, combining them on a word, sentence,\nand document level. GPT-3 also uses a single training objective which is not customized or designed for any task in\nparticular. However, our one / few-shot settings aren\u2019t strictly comparable to prior unsupervised work since they make\nuse of a small amount of paired examples (1 or 64). This corresponds to up to a page or two of in-context training data.\nResults are shown in Table 3.4. Zero-shot GPT-3, which only receives on a natural language description of the task,\nstill underperforms recent unsupervised NMT results. However, providing only a single example demonstration for\n14\nSetting En !Fr Fr!En En!De De!En En!Ro Ro!En\nSOTA (Supervised) 45.6a35.0b41.2c40.2d38.5e39.9e\nXLM [LC19] 33.4 33.3 26.4 34.3 33.3 31.8\nMASS [STQ+19] 37.5 34.9 28.3 35.2 35.2 33.1\nmBART [LGG+20] - - 29.8 34.0 35.0 30.5\nGPT-3 Zero-Shot 25.2 21.2 24.6 27.2 14.1 19.9\nGPT-3 One-Shot 28.3 33.7 26.2 30.4 20.6 38.6\nGPT-3 Few-Shot 32.6 39.2 29.7 40.6 21.0 39.5\nTable 3.4: Few-shot GPT-3 outperforms previous unsupervised NMT work by 5 BLEU when translating\ninto English re\ufb02ecting its strength as an English LM. We report BLEU scores on the WMT\u201914 Fr $En,\nWMT\u201916 De$En, and WMT\u201916 Ro $En datasets as measured by multi-bleu.perl with XLM\u2019s tokeniza-\ntion in order to compare most closely with prior unsupervised NMT work. SacreBLEUf[Pos18 ] results re-\nported in Appendix H. Underline indicates an unsupervised or few-shot SOTA, bold indicates supervised SOTA\nwith relative con\ufb01dence.a[EOAG18 ]b[DHKH14 ]c[WXH+18]d[oR16 ]e[LGG+20]f[SacreBLEU signature:\nBLEU+case.mixed+numrefs.1+smooth.exp+tok.intl+version.1.2.20]\nFigure 3.4: Few-shot translation performance on 6 language pairs as model capacity increases. There is a consistent\ntrend of improvement across all datasets as the model scales, and as well as tendency for translation into English to be\nstronger than translation from English.\n15\nSetting Winograd Winogrande (XL)\nFine-tuned SOTA 90.1a84.6b\nGPT-3 Zero-Shot 88.3* 70.2\nGPT-3 One-Shot 89.7* 73.2\nGPT-3 Few-Shot 88.6* 77.7\nTable 3.5: Results on the WSC273 version of Winograd schemas and the adversarial Winogrande dataset. See Section\n4 for details on potential contamination of the Winograd test set.a[SBBC19]b[LYN+20]\nFigure 3.5: Zero-, one-, and few-shot performance on the adversarial Winogrande dataset as model capacity scales.\nScaling is relatively smooth with the gains to few-shot learning increasing with model size, and few-shot GPT-3 175B\nis competitive with a \ufb01ne-tuned RoBERTA-large.\neach translation task improves performance by over 7 BLEU and nears competitive performance with prior work.\nGPT-3 in the full few-shot setting further improves another 4 BLEU resulting in similar average performance to prior\nunsupervised NMT work. GPT-3 has a noticeable skew in its performance depending on language direction. For the\nthree input languages studied, GPT-3 signi\ufb01cantly outperforms prior unsupervised NMT work when translating into\nEnglish but underperforms when translating in the other direction. Performance on En-Ro is a noticeable outlier at\nover 10 BLEU worse than prior unsupervised NMT work. This could be a weakness due to reusing the byte-level BPE\ntokenizer of GPT-2 which was developed for an almost entirely English training dataset. For both Fr-En and De-En,\nfew shot GPT-3 outperforms the best supervised result we could \ufb01nd but due to our unfamiliarity with the literature and\nthe appearance that these are un-competitive benchmarks we do not suspect those results represent true state of the art.\nFor Ro-En, few shot GPT-3 performs within 0.5 BLEU of the overall SOTA which is achieved by a combination of\nunsupervised pretraining, supervised \ufb01netuning on 608K labeled examples, and backtranslation [LHCG19b].\nFinally, across all language pairs and across all three settings (zero-, one-, and few-shot), there is a smooth trend of\nimprovement with model capacity. This is shown in Figure 3.4 in the case of few-shot results, and scaling for all three\nsettings is shown in Appendix H.\n3.4 Winograd-Style Tasks\nThe Winograd Schemas Challenge [ LDM12 ] is a classical task in NLP that involves determining which word a pronoun\nrefers to, when the pronoun is grammatically ambiguous but semantically unambiguous to a human. Recently \ufb01ne-tuned\nlanguage models have achieved near-human performance on the original Winograd dataset, but more dif\ufb01cult versions\n16\nSetting PIQA ARC (Easy) ARC (Challenge) OpenBookQA\nFine-tuned SOTA 79.4 92.0[KKS+20] 78.5[KKS+20] 87.2[KKS+20]\nGPT-3 Zero-Shot 80.5* 68.8 51.4 57.6\nGPT-3 One-Shot 80.5* 71.2 53.2 58.8\nGPT-3 Few-Shot 82.8* 70.1 51.5 65.4\nTable 3.6: GPT-3 results on three commonsense reasoning tasks, PIQA, ARC, and OpenBookQA. GPT-3 Few-Shot\nPIQA result is evaluated on the test server. See Section 4 for details on potential contamination issues on the PIQA test\nset.\nFigure 3.6: GPT-3 results on PIQA in the zero-shot, one-shot, and few-shot settings. The largest model achieves a\nscore on the development set in all three conditions that exceeds the best recorded score on the task.\nsuch as the adversarially-mined Winogrande dataset [ SBBC19 ] still signi\ufb01cantly lag human performance. We test\nGPT-3\u2019s performance on both Winograd and Winogrande, as usual in the zero-, one-, and few-shot setting.\nOn Winograd we test GPT-3 on the original set of 273 Winograd schemas, using the same \u201cpartial evaluation\u201d method\ndescribed in [ RWC+19]. Note that this setting differs slightly from the WSC task in the SuperGLUE benchmark, which\nis presented as binary classi\ufb01cation and requires entity extraction to convert to the form described in this section. On\nWinograd GPT-3 achieves 88.3%, 89.7%, and 88.6% in the zero-shot, one-shot, and few-shot settings, showing no clear\nin-context learning but in all cases achieving strong results just a few points below state-of-the-art and estimated human\nperformance. We note that contamination analysis found some Winograd schemas in the training data but this appears\nto have only a small effect on results (see Section 4).\nOn the more dif\ufb01cult Winogrande dataset, we do \ufb01nd gains to in-context learning: GPT-3 achieves 70.2% in the\nzero-shot setting, 73.2% in the one-shot setting, and 77.7% in the few-shot setting. For comparison a \ufb01ne-tuned\nRoBERTA model achieves 79%, state-of-the-art is 84.6% achieved with a \ufb01ne-tuned high capacity model (T5), and\nhuman performance on the task as reported by [SBBC19] is 94.0%.\n3.5 Common Sense Reasoning\nNext we consider three datasets which attempt to capture physical or scienti\ufb01c reasoning, as distinct from sentence\ncompletion, reading comprehension, or broad knowledge question answering. The \ufb01rst, PhysicalQA (PIQA) [ BZB+19],\nasks common sense questions about how the physical world works and is intended as a probe of grounded understanding\nof the world. GPT-3 achieves 81.0% accuracy zero-shot, 80.5% accuracy one-shot, and 82.8% accuracy few-shot\n(the last measured on PIQA\u2019s test server). This compares favorably to the 79.4% accuracy prior state-of-the-art of a\n17\nSetting CoQA DROP QuAC SQuADv2 RACE-h RACE-m\nFine-tuned SOTA 90.7a89.1b74.4c93.0d90.0e93.1e\nGPT-3 Zero-Shot 81.5 23.6 41.5 59.5 45.5 58.4\nGPT-3 One-Shot 84.0 34.3 43.3 65.4 45.9 57.4\nGPT-3 Few-Shot 85.0 36.5 44.3 69.8 46.8 58.1\nTable 3.7: Results on reading comprehension tasks. All scores are F1 except results for RACE which report accuracy.\na[JZC+19]b[JN20]c[AI19]d[QIA20]e[SPP+19]\n\ufb01ne-tuned RoBERTa. PIQA shows relatively shallow scaling with model size and is still over 10% worse than human\nperformance, but GPT-3\u2019s few-shot and even zero-shot result outperform the current state-of-the-art. Our analysis\n\ufb02agged PIQA for a potential data contamination issue (despite hidden test labels), and we therefore conservatively mark\nthe result with an asterisk. See Section 4 for details.\nARC [ CCE+18] is a dataset of multiple-choice questions collected from 3rd to 9th grade science exams. On the\n\u201cChallenge\u201d version of the dataset which has been \ufb01ltered to questions which simple statistical or information retrieval\nmethods are unable to correctly answer, GPT-3 achieves 51.4% accuracy in the zero-shot setting, 53.2% in the one-shot\nsetting, and 51.5% in the few-shot setting. This is approaching the performance of a \ufb01ne-tuned RoBERTa baseline\n(55.9%) from Uni\ufb01edQA [ KKS+20]. On the \u201cEasy\u201d version of the dataset (questions which either of the mentioned\nbaseline approaches answered correctly), GPT-3 achieves 68.8%, 71.2%, and 70.1% which slightly exceeds a \ufb01ne-tuned\nRoBERTa baseline from [ KKS+20]. However, both of these results are still much worse than the overall SOTAs\nachieved by the Uni\ufb01edQA which exceeds GPT-3\u2019s few-shot results by 27% on the challenge set and 22% on the easy\nset.\nOn OpenBookQA [ MCKS18 ], GPT-3 improves signi\ufb01cantly from zero to few shot settings but is still over 20 points\nshort of the overall SOTA. GPT-3\u2019s few-shot performance is similar to a \ufb01ne-tuned BERT Large baseline on the\nleaderboard.\nOverall, in-context learning with GPT-3 shows mixed results on commonsense reasoning tasks, with only small and\ninconsistent gains observed in the one and few-shot learning settings for both PIQA and ARC, but a signi\ufb01cant\nimprovement is observed on OpenBookQA. GPT-3 sets SOTA on the new PIQA dataset in all evaluation settings.\n3.6 Reading Comprehension\nNext we evaluate GPT-3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive,\nmultiple choice, and span based answer formats in both dialog and single question settings. We observe a wide spread\nin GPT-3\u2019s performance across these datasets suggestive of varying capability with different answer formats. In general\nwe observe GPT-3 is on par with initial baselines and early results trained using contextual representations on each\nrespective dataset.\nGPT-3 performs best (within 3 points of the human baseline) on CoQA [ RCM19 ] a free-form conversational dataset\nand performs worst (13 F1 below an ELMo baseline) on QuAC [ CHI+18] a dataset which requires modeling structured\ndialog acts and answer span selections of teacher-student interactions. On DROP [ DWD+19], a dataset testing discrete\nreasoning and numeracy in the context of reading comprehension, GPT-3 in a few-shot setting outperforms the \ufb01ne-tuned\nBERT baseline from the original paper but is still well below both human performance and state-of-the-art approaches\nwhich augment neural networks with symbolic systems [ RLL+19]. On SQuAD 2.0 [ RJL18 ], GPT-3 demonstrates its\nfew-shot learning capabilities, improving by almost 10 F1 (to 69.8) compared to a zero-shot setting. This allows it to\nslightly outperform the best \ufb01ne-tuned result in the original paper. On RACE [ LXL+17], a multiple choice dataset of\nmiddle school and high school english examinations, GPT-3 performs relatively weakly and is only competitive with\nthe earliest work utilizing contextual representations and is still 45% behind SOTA.\n3.7 SuperGLUE\nIn order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a\nmore systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark\n[WPN+19] [WPN+19] [CLC+19] [DMST19 ] [RBG11 ] [KCR+18] [ZLL+18] [DGM06 ] [BHDD+06] [GMDD07 ]\n[BDD+09] [PCC18 ] [PHR+18]. GPT-3\u2019s test-set performance on the SuperGLUE dataset is shown in Table 3.8. In the\nfew-shot setting, we used 32 examples for all tasks, sampled randomly from the training set. For all tasks except WSC\n18\nFigure 3.7: GPT-3 results on CoQA reading comprehension task. GPT-3 175B achieves 85 F1 in the few-shot setting,\nonly a few points behind measured human performance and state-of-the-art \ufb01ne-tuned models. Zero-shot and one-shot\nperformance is a few points behind, with the gains to few-shot being largest for bigger models.\nSuperGLUE BoolQ CB CB COPA RTE\nAverage Accuracy Accuracy F1 Accuracy Accuracy\nFine-tuned SOTA 89.0 91.0 96.9 93.9 94.8 92.5\nFine-tuned BERT-Large 69.0 77.4 83.6 75.7 70.6 71.7\nGPT-3 Few-Shot 71.8 76.4 75.6 52.0 92.0 69.0\nWiC WSC MultiRC MultiRC ReCoRD ReCoRD\nAccuracy Accuracy Accuracy F1a Accuracy F1\nFine-tuned SOTA 76.1 93.8 62.3 88.2 92.5 93.3\nFine-tuned BERT-Large 69.6 64.6 24.1 70.0 71.3 72.0\nGPT-3 Few-Shot 49.4 80.1 30.5 75.4 90.2 91.1\nTable 3.8: Performance of GPT-3 on SuperGLUE compared to \ufb01ne-tuned baselines and SOTA. All results are reported\non the test set. GPT-3 few-shot is given a total of 32 examples within the context of each task and performs no gradient\nupdates.\n19\nFigure 3.8: Performance on SuperGLUE increases with model size and number of examples in context. A value\nofK= 32 means that our model was shown 32 examples per task, for 256 examples total divided across the 8 tasks in\nSuperGLUE. We report GPT-3 values on the dev set, so our numbers are not directly comparable to the dotted reference\nlines (our test set results are in Table 3.8). The BERT-Large reference model was \ufb01ne-tuned on the SuperGLUE training\nset (125K examples), whereas BERT++ was \ufb01rst \ufb01ne-tuned on MultiNLI (392K examples) and SWAG (113K examples)\nbefore further \ufb01ne-tuning on the SuperGLUE training set (for a total of 630K \ufb01ne-tuning examples). We \ufb01nd the\ndifference in performance between the BERT-Large and BERT++ to be roughly equivalent to the difference between\nGPT-3 with one example per context versus eight examples per context.\nand MultiRC, we sampled a new set of examples to use in the context for each problem. For WSC and MultiRC, we\nused the same set of randomly drawn examples from the training set as context for all of the problems we evaluated.\nWe observe a wide range in GPT-3\u2019s performance across tasks. On COPA and ReCoRD GPT-3 achieves near-SOTA\nperformance in the one-shot and few-shot settings, with COPA falling only a couple points short and achieving\nsecond place on the leaderboard, where \ufb01rst place is held by a \ufb01ne-tuned 11 billion parameter model (T5). On WSC,\nperformance is still relatively strong, achieving 80.1% in the few-shot setting (note that GPT-3 achieves 88.6% on the\noriginal Winograd dataset as described in Section 3.4). On BoolQ, MultiRC, and RTE, performance is reasonable,\nroughly matching that of a \ufb01ne-tuned BERT-Large. On CB, we see signs of life at 75.6% in the few-shot setting.\nWiC is a notable weak spot with few-shot performance at 49.4% (at random chance). We tried a number of different\nphrasings and formulations for WiC (which involves determining if a word is being used with the same meaning in two\nsentences), none of which was able to achieve strong performance. This hints at a phenomenon that will become clearer\nin the next section (which discusses the ANLI benchmark) \u2013 GPT-3 appears to be weak in the few-shot or one-shot\nsetting at some tasks that involve comparing two sentences or snippets, for example whether a word is used the same\nway in two sentences (WiC), whether one sentence is a paraphrase of another, or whether one sentence implies another.\nThis could also explain the comparatively low scores for RTE and CB, which also follow this format. Despite these\nweaknesses, GPT-3 still outperforms a \ufb01ne-tuned BERT-large on four of eight tasks and on two tasks GPT-3 is close to\nthe state-of-the-art held by a \ufb01ne-tuned 11 billion parameter model.\nFinally, we note that the few-shot SuperGLUE score steadily improves with both model size and with number of\nexamples in the context showing increasing bene\ufb01ts from in-context learning (Figure 3.8). We scale Kup to 32\nexamples per task, after which point additional examples will not reliably \ufb01t into our context. When sweeping over\nvalues ofK, we \ufb01nd that GPT-3 requires less than eight total examples per task to outperform a \ufb01ne-tuned BERT-Large\non overall SuperGLUE score.\n3.8 NLI\nNatural Language Inference (NLI) [ Fyo00 ] concerns the ability to understand the relationship between two sentences.\nIn practice, this task is usually structured as a two or three class classi\ufb01cation problem where the model classi\ufb01es\n20\nFigure 3.9: Performance of GPT-3 on ANLI Round 3. Results are on the dev-set, which has only 1500 examples\nand therefore has high variance (we estimate a standard deviation of 1.2%). We \ufb01nd that smaller models hover around\nrandom chance, while few-shot GPT-3 175B closes almost half the gap from random chance to SOTA. Results for\nANLI rounds 1 and 2 are shown in the appendix.\nwhether the second sentence logically follows from the \ufb01rst, contradicts the \ufb01rst sentence, or is possibly true (neutral).\nSuperGLUE includes an NLI dataset, RTE, which evaluates the binary version of the task. On RTE, only the largest\nversion of GPT-3 performs convincingly better than random (56%) in any evaluation setting, but in a few-shot setting\nGPT-3 performs similarly to a single-task \ufb01ne-tuned BERT Large. We also evaluate on the recently introduced\nAdversarial Natural Language Inference (ANLI) dataset [ NWD+19]. ANLI is a dif\ufb01cult dataset employing a series of\nadversarially mined natural language inference questions in three rounds (R1, R2, and R3). Similar to RTE, all of our\nmodels smaller than GPT-3 perform at almost exactly random chance on ANLI, even in the few-shot setting ( \u001833%),\nwhereas GPT-3 itself shows signs of life on Round 3. Results for ANLI R3 are highlighted in Figure 3.9 and full results\nfor all rounds can be found in Appendix H. These results on both RTE and ANLI suggest that NLI is still a very dif\ufb01cult\ntask for language models and they are only just beginning to show signs of progress.\n3.9 Synthetic and Qualitative Tasks\nOne way to probe GPT-3\u2019s range of abilities in the few-shot (or zero- and one-shot) setting is to give it tasks which\nrequire it to perform simple on-the-\ufb02y computational reasoning, recognize a novel pattern that is unlikely to have\noccurred in training, or adapt quickly to an unusual task. We devise several tasks to test this class of abilities. First, we\ntest GPT-3\u2019s ability to perform arithmetic. Second, we create several tasks that involve rearranging or unscrambling the\nletters in a word, tasks which are unlikely to have been exactly seen during training. Third, we test GPT-3\u2019s ability to\nsolve SAT-style analogy problems few-shot. Finally, we test GPT-3 on several qualitative tasks, including using new\nwords in a sentence, correcting English grammar, and news article generation. We will release the synthetic datasets\nwith the hope of stimulating further study of test-time behavior of language models.\n3.9.1 Arithmetic\nTo test GPT-3\u2019s ability to perform simple arithmetic operations without task-speci\ufb01c training, we developed a small\nbattery of 10 tests that involve asking GPT-3 a simple arithmetic problem in natural language:\n\u20222 digit addition (2D+) \u2013 The model is asked to add two integers sampled uniformly from [0;100) , phrased in\nthe form of a question, e.g. \u201cQ: What is 48 plus 76? A: 124.\u201d\n\u20222 digit subtraction (2D-) \u2013 The model is asked to subtract two integers sampled uniformly from [0;100) ; the\nanswer may be negative. Example: \u201cQ: What is 34 minus 53? A: -19\u201d.\n\u20223 digit addition (3D+) \u2013 Same as 2 digit addition, except numbers are uniformly sampled from [0;1000) .\n21\nFigure 3.10: Results on all 10 arithmetic tasks in the few-shot settings for models of different sizes. There is a\nsigni\ufb01cant jump from the second largest model (GPT-3 13B) to the largest model (GPT-3 175), with the latter being\nable to reliably accurate 2 digit arithmetic, usually accurate 3 digit arithmetic, and correct answers a signi\ufb01cant fraction\nof the time on 4-5 digit arithmetic, 2 digit multiplication, and compound operations. Results for one-shot and zero-shot\nare shown in the appendix.\n\u20223 digit subtraction (3D-) \u2013 Same as 2 digit subtraction, except numbers are uniformly sampled from [0;1000) .\n\u20224 digit addition (4D+) \u2013 Same as 3 digit addition, except uniformly sampled from [0;10000) .\n\u20224 digit subtraction (4D-) \u2013 Same as 3 digit subtraction, except uniformly sampled from [0;10000) .\n\u20225 digit addition (5D+) \u2013 Same as 3 digit addition, except uniformly sampled from [0;100000) .\n\u20225 digit subtraction (5D-) \u2013 Same as 3 digit subtraction, except uniformly sampled from [0;100000) .\n\u20222 digit multiplication (2Dx) \u2013 The model is asked to multiply two integers sampled uniformly from [0;100) ,\ne.g. \u201cQ: What is 24 times 42? A: 1008\u201d.\n\u2022One-digit composite (1DC) \u2013 The model is asked to perform a composite operation on three 1 digit numbers,\nwith parentheses around the last two. For example, \u201cQ: What is 6+(4*8)? A: 38\u201d. The three 1 digit numbers\nare selected uniformly on [0;10)and the operations are selected uniformly from f+,-,*g.\nIn all 10 tasks the model must generate the correct answer exactly. For each task we generate a dataset of 2,000 random\ninstances of the task and evaluate all models on those instances.\nFirst we evaluate GPT-3 in the few-shot setting, for which results are shown in Figure 3.10. On addition and subtraction,\nGPT-3 displays strong pro\ufb01ciency when the number of digits is small, achieving 100% accuracy on 2 digit addition,\n98.9% at 2 digit subtraction, 80.2% at 3 digit addition, and 94.2% at 3-digit subtraction. Performance decreases as the\nnumber of digits increases, but GPT-3 still achieves 25-26% accuracy on four digit operations and 9-10% accuracy on\n\ufb01ve digit operations, suggesting at least some capacity to generalize to larger numbers of digits. GPT-3 also achieves\n29.2% accuracy at 2 digit multiplication, an especially computationally intensive operation. Finally, GPT-3 achieves\n21.3% accuracy at single digit combined operations (for example, 9*(7+5)), suggesting that it has some robustness\nbeyond just single operations.\nAs Figure 3.10 makes clear, small models do poorly on all of these tasks \u2013 even the 13 billion parameter model (the\nsecond largest after the 175 billion full GPT-3) can solve 2 digit addition and subtraction only half the time, and all\nother operations less than 10% of the time.\nOne-shot and zero-shot performance are somewhat degraded relative to few-shot performance, suggesting that adaptation\nto the task (or at the very least recognition of the task) is important to performing these computations correctly.\nNevertheless, one-shot performance is still quite strong, and even zero-shot performance of the full GPT-3 signi\ufb01cantly\n22\nSetting 2D+ 2D- 3D+ 3D- 4D+ 4D- 5D+ 5D- 2Dx 1DC\nGPT-3 Zero-shot 76.9 58.0 34.2 48.3 4.0 7.5 0.7 0.8 19.8 9.8\nGPT-3 One-shot 99.6 86.4 65.5 78.7 14.0 14.0 3.5 3.8 27.4 14.3\nGPT-3 Few-shot 100.0 98.9 80.4 94.2 25.5 26.8 9.3 9.9 29.2 21.3\nTable 3.9: Results on basic arithmetic tasks for GPT-3 175B. f2,3,4,5gDf+,-gis 2, 3, 4, and 5 digit addition or\nsubtraction, 2Dx is 2 digit multiplication. 1DC is 1 digit composite operations. Results become progressively stronger\nmoving from the zero-shot to one-shot to few-shot setting, but even the zero-shot shows signi\ufb01cant arithmetic abilities.\nSetting CL A1 A2 RI RW\nGPT-3 Zero-shot 3.66 2.28 8.91 8.26 0.09\nGPT-3 One-shot 21.7 8.62 25.9 45.4 0.48\nGPT-3 Few-shot 37.9 15.1 39.7 67.2 0.44\nTable 3.10: GPT-3 175B performance on various word unscrambling and word manipulation tasks, in zero-, one-, and\nfew-shot settings. CL is \u201ccycle letters in word\u201d, A1 is anagrams of but the \ufb01rst and last letters, A2 is anagrams of all but\nthe \ufb01rst and last two letters, RI is \u201cRandom insertion in word\u201d, RW is \u201creversed words\u201d.\noutperforms few-shot learning for all smaller models. All three settings for the full GPT-3 are shown in Table 3.9, and\nmodel capacity scaling for all three settings is shown in Appendix H.\nTo spot-check whether the model is simply memorizing speci\ufb01c arithmetic problems, we took the 3-digit arithmetic\nproblems in our test set and searched for them in our training data in both the forms \"<NUM1> + <NUM2> =\" and\n\"<NUM1> plus <NUM2>\" . Out of 2,000 addition problems we found only 17 matches (0.8%) and out of 2,000\nsubtraction problems we found only 2 matches (0.1%), suggesting that only a trivial fraction of the correct answers\ncould have been memorized. In addition, inspection of incorrect answers reveals that the model often makes mistakes\nsuch as not carrying a \u201c1\u201d, suggesting it is actually attempting to perform the relevant computation rather than\nmemorizing a table.\nOverall, GPT-3 displays reasonable pro\ufb01ciency at moderately complex arithmetic in few-shot, one-shot, and even\nzero-shot settings.\n3.9.2 Word Scrambling and Manipulation Tasks\nTo test GPT-3\u2019s ability to learn novel symbolic manipulations from a few examples, we designed a small battery of\n5 \u201ccharacter manipulation\u201d tasks. Each task involves giving the model a word distorted by some combination of\nscrambling, addition, or deletion of characters, and asking it to recover the original word. The 5 tasks are:\n\u2022Cycle letters in word (CL) \u2013 The model is given a word with its letters cycled, then the \u201c=\u201d symbol, and\nis expected to generate the original word. For example, it might be given \u201clyinevitab\u201d and should output\n\u201cinevitably\u201d.\n\u2022Anagrams of all but \ufb01rst and last characters (A1) \u2013 The model is given a word where every letter except\nthe \ufb01rst and last have been scrambled randomly, and must output the original word. Example: criroptuon =\ncorruption.\n\u2022Anagrams of all but \ufb01rst and last 2 characters (A2) \u2013 The model is given a word where every letter except\nthe \ufb01rst 2 and last 2 have been scrambled randomly, and must recover the original word. Example: opoepnnt\n!opponent.\n\u2022Random insertion in word (RI) \u2013 A random punctuation or space character is inserted between each letter\nof a word, and the model must output the original word. Example: s.u!c/c!e.s s i/o/n = succession.\n\u2022Reversed words (RW) \u2013 The model is given a word spelled backwards, and must output the original word.\nExample: stcejbo!objects.\nFor each task we generate 10,000 examples, which we chose to be the top 10,000 most frequent words as measured by\n[Nor09 ] of length more than 4 characters and less than 15 characters. The few-shot results are shown in Figure 3.11.\nTask performance tends to grow smoothly with model size, with the full GPT-3 model achieving 66.9% on removing\n23\nFigure 3.11: Few-shot performance on the \ufb01ve word scrambling tasks for different sizes of model. There is generally\nsmooth improvement with model size although the random insertion task shows an upward slope of improvement with\nthe 175B model solving the task the majority of the time. Scaling of one-shot and zero-shot performance is shown in\nthe appendix. All tasks are done with K= 100 .\nrandom insertions, 38.6% on cycling letters, 40.2% on the easier anagram task, and 15.1% on the more dif\ufb01cult anagram\ntask (where only the \ufb01rst and last letters are held \ufb01xed). None of the models can reverse the letters in a word.\nIn the one-shot setting, performance is signi\ufb01cantly weaker (dropping by half or more), and in the zero-shot setting the\nmodel can rarely perform any of the tasks (Table 3.10). This suggests that the model really does appear to learn these\ntasks at test time, as the model cannot perform them zero-shot and their arti\ufb01cial nature makes them unlikely to appear\nin the pre-training data (although we cannot con\ufb01rm this with certainty).\nWe can further quantify performance by plotting \u201cin-context learning curves\u201d, which show task performance as a\nfunction of the number of in-context examples. We show in-context learning curves for the Symbol Insertion task\nin Figure 1.2. We can see that larger models are able to make increasingly effective use of in-context information,\nincluding both task examples and natural language task descriptions.\nFinally, it is worth adding that solving these tasks requires character-level manipulations, whereas our BPE encoding\noperates on signi\ufb01cant fractions of a word (on average \u00180:7words per token), so from the LM\u2019s perspective succeeding\nat these tasks involves not just manipulating BPE tokens but understanding and pulling apart their substructure. Also,\nCL, A1, and A2 are not bijective (that is, the unscrambled word is not a deterministic function of the scrambled word),\nrequiring the model to perform some search to \ufb01nd the correct unscrambling. Thus, the skills involved appear to require\nnon-trivial pattern-matching and computation.\n3.9.3 SAT Analogies\nTo test GPT-3 on another task that is somewhat unusual relative to the typical distribution of text, we collected a set of\n374 \u201cSAT analogy\u201d problems [ TLBS03 ]. Analogies are a style of multiple choice question that constituted a section of\nthe SAT college entrance exam before 2005. A typical example is \u201caudacious is to boldness as (a) sanctimonious is to\nhypocrisy, (b) anonymous is to identity, (c) remorseful is to misdeed, (d) deleterious is to result, (e) impressionable is to\ntemptation\u201d. The student is expected to choose which of the \ufb01ve word pairs has the same relationship as the original\nword pair; in this example the answer is \u201csanctimonious is to hypocrisy\u201d. On this task GPT-3 achieves 65.2% in the\nfew-shot setting, 59.1% in the one-shot setting, and 53.7% in the zero-shot setting, whereas the average score among\ncollege applicants was 57% [ TL05 ] (random guessing yields 20%). As shown in Figure 3.12, the results improve with\nscale, with the the full 175 billion model improving by over 10% compared to the 13 billion parameter model.\n24\nFigure 3.12: Zero-, one-,and few-shot performance on SAT analogy tasks, for different sizes of model. The largest\nmodel achieves 65% accuracy in the few-shot setting, and also demonstrates signi\ufb01cant gains to in-context learning\nwhich are not present in smaller models.\n3.9.4 News Article Generation\nPrevious work on generative language models qualitatively tested their ability to generate synthetic \u201cnews articles\u201d by\nconditional sampling from the model given a human-written prompt consisting of a plausible \ufb01rst sentence for a news\nstory [ RWC+19]. Relative to [ RWC+19], the dataset used to train GPT-3 is much less weighted towards news articles,\nso trying to generate news articles via raw unconditional samples is less effective \u2013 for example GPT-3 often interprets\nthe proposed \ufb01rst sentence of a \u201cnews article\u201d as a tweet and then posts synthetic responses or follow-up tweets. To\nsolve this problem we employed GPT-3\u2019s few-shot learning abilities by providing three previous news articles in the\nmodel\u2019s context to condition it. With the title and subtitle of a proposed next article, the model is able to reliably\ngenerate short articles in the \u201cnews\u201d genre.\nTo gauge the quality of news article generation from GPT-3 (which we believe is likely to be correlated with conditional\nsample generation quality in general), we decided to measure human ability to distinguish GPT-3-generated articles\nfrom real ones. Similar work has been carried out by Kreps et al. [ KMB20 ] and Zellers et al. [ ZHR+19]. Generative\nlanguage models are trained to match the distribution of content generated by humans, so the (in)ability of humans to\ndistinguish the two is a potentially important measure of quality.3\nIn order to see how well humans can detect model generated text, we arbitrarily selected 25 article titles and subtitles\nfrom the website newser.com (mean length: 215 words). We then generated completions of these titles and subtitles\nfrom four language models ranging in size from 125M to 175B (GPT-3) parameters (mean length: 200 words). For each\nmodel, we presented around 80 US-based participants with a quiz consisting of these real titles and subtitles followed\nby either the human written article or the article generated by the model4. Participants were asked to select whether the\narticle was \u201cvery likely written by a human\u201d, \u201cmore likely written by a human\u201d, \u201cI don\u2019t know\u201d, \u201cmore likely written by\na machine\u201d, or \u201cvery likely written by a machine\u201d.\nThe articles we selected were not in the models\u2019 training data and the model outputs were formatted and selected\nprogrammatically to prevent human cherry-picking. All models used the same context to condition outputs on and were\npre-trained with the same context size and the same article titles and subtitles were used as prompts for each model.\nHowever, we also ran an experiment to control for participant effort and attention that followed the same format but\ninvolved intentionally bad model generated articles. This was done by generating articles from a \u201ccontrol model\u201d: a\n160M parameter model with no context and increased output randomness.\n3This task is also relevant to the potential misuse of language models discussed in Section 6.1.\n4We wanted to identify how good an average person on the internet is at detecting language model outputs, so we focused on\nparticipants drawn from the general US population. See Appendix E for details.\n25\nMean accuracy95% Con\ufb01dence\nInterval (low, hi)tcompared to\ncontrol (p-value)\u201cI don\u2019t know\u201d\nassignments\nControl (deliberately bad model) 86% 83%\u201390% - 3.6 %\nGPT-3 Small 76% 72%\u201380% 3.9 (2 e-4) 4.9%\nGPT-3 Medium 61% 58%\u201365% 10.3 (7 e-21) 6.0%\nGPT-3 Large 68% 64%\u201372% 7.3 (3 e-11) 8.7%\nGPT-3 XL 62% 59%\u201365% 10.7 (1 e-19) 7.5%\nGPT-3 2.7B 62% 58%\u201365% 10.4 (5 e-19) 7.1%\nGPT-3 6.7B 60% 56%\u201363% 11.2 (3 e-21) 6.2%\nGPT-3 13B 55% 52%\u201358% 15.3 (1 e-32) 7.1%\nGPT-3 175B 52% 49%\u201354% 16.9 (1 e-34) 7.8%\nTable 3.11: Human accuracy in identifying whether short ( \u0018200 word) news articles are model generated . We\n\ufb01nd that human accuracy (measured by the ratio of correct assignments to non-neutral assignments) ranges from 86%\non the control model to 52% on GPT-3 175B. This table compares mean accuracy between \ufb01ve different models, and\nshows the results of a two-sample T-Test for the difference in mean accuracy between each model and the control model\n(an unconditional GPT-3 Small model with increased output randomness).\nMean human accuracy (the ratio of correct assignments to non-neutral assignments per participant) at detecting that\nthe intentionally bad articles were model generated was \u001886% where 50% is chance level performance. By contrast,\nmean human accuracy at detecting articles that were produced by the 175B parameter model was barely above chance\nat\u001852% (see Table 3.11).5Human abilities to detect model generated text appear to decrease as model size increases:\nthere appears to be a trend towards chance accuracy with model size, and human detection of GPT-3 is close to chance.6\nThis is true despite the fact that participants spend more time on each output as model size increases (see Appendix E).\nExamples of synthetic articles from GPT-3 are given in Figures 3.14 and 3.15.7Much of the text is\u2014as indicated by the\nevaluations\u2014dif\ufb01cult for humans to distinguish from authentic human content. Factual inaccuracies can be an indicator\nthat an article is model generated since, unlike human authors, the models have no access to the speci\ufb01c facts that the\narticle titles refer to or when the article was written. Other indicators include repetition, non sequiturs, and unusual\nphrasings, though these are often subtle enough that they are not noticed.\nRelated work on language model detection by Ippolito et al. [ IDCBE19 ] indicates that automatic discriminators like\nGR O V E R [ZHR+19] and GLTR [ GSR19 ] may have greater success at detecting model generated text than human\nevaluators. Automatic detection of these models may be a promising area of future research.\nIppolito et al. [ IDCBE19 ] also note that human accuracy at detecting model generated text increases as humans observe\nmore tokens. To do a preliminary investigation of how good humans are at detecting longer news articles generated\nby GPT-3 175B, we selected 12 world news articles from Reuters with an average length of 569 words and generated\ncompletions of these articles from GPT-3 with an average length of 498 words (298 words longer than our initial\nexperiments). Following the methodology above, we ran two experiments, each on around 80 US-based participants, to\ncompare human abilities to detect the articles generated by GPT-3 and a control model.\nWe found that mean human accuracy at detecting the intentionally bad longer articles from the control model was\n\u001888%, while mean human accuracy at detecting the longer articles that were produced by GPT-3 175B was still barely\nabove chance at\u001852% (see Table 3.12). This indicates that, for news articles that are around 500 words long, GPT-3\ncontinues to produce articles that humans \ufb01nd dif\ufb01cult to distinguish from human written news articles.\n3.9.5 Learning and Using Novel Words\nA task studied in developmental linguistics [ CB78 ] is the ability to learn and utilize new words, for example using a\nword in a sentence after seeing it de\ufb01ned only once, or conversely inferring a word\u2019s meaning from only one usage. Here\nwe qualitatively test GPT-3\u2019s ability to do the former. Speci\ufb01cally, we give GPT-3 the de\ufb01nition of a nonexistent word,\nsuch as \u201cGigamuru\u201d, and then ask it to use it in a sentence. We provide one to \ufb01ve previous examples of a (separate)\n5We use a two-sample Student\u2019s T-Test to test for signi\ufb01cant difference between the means of the participant accuracies of each\nmodel and the control model and report the normalized difference in the means (as the t-statistic) and the p-value.\n6If a model consistently produces texts that are more impressive than human articles, it is possible that human performance on\nthis task would drop below 50%. Indeed, many individual participants scored below 50% on this task.\n7Additional non-news samples can be found in Appendix F.\n26\nFigure 3.13: People\u2019s ability to identify whether news articles are model-generated (measured by the ratio of correct\nassignments to non-neutral assignments) decreases as model size increases. Accuracy on the outputs on the deliberately-\nbad control model (an unconditioned GPT-3 Small model with higher output randomness) is indicated with the dashed\nline at the top, and the random chance (50%) is indicated with the dashed line at the bottom. Line of best \ufb01t is a power\nlaw with 95% con\ufb01dence intervals.\nMean accuracy95% Con\ufb01dence\nInterval (low, hi)tcompared to\ncontrol (p-value)\u201cI don\u2019t know\u201d\nassignments\nControl 88% 84%\u201391% - 2.7%\nGPT-3 175B 52% 48%\u201357% 12.7 (3.2 e-23) 10.6%\nTable 3.12: People\u2019s ability to identify whether \u0018500word articles are model generated (as measured by the ratio of\ncorrect assignments to non-neutral assignments) was 88% on the control model and 52% on GPT-3 175B. This table\nshows the results of a two-sample T-Test for the difference in mean accuracy between GPT-3 175B and the control\nmodel (an unconditional GPT-3 Small model with increased output randomness).\n27\nTitle: United Methodists Agree to Historic Split\nSubtitle: Those who oppose gay marriage will form their own denomination\nArticle: After two days of intense debate, the United Methodist Church\nhas agreed to a historic split - one that is expected to end in the\ncreation of a new denomination, one that will be \"theologically and\nsocially conservative,\" according to The Washington Post. The majority of\ndelegates attending the church's annual General Conference in May voted to\nstrengthen a ban on the ordination of LGBTQ clergy and to write new rules\nthat will \"discipline\" clergy who officiate at same-sex weddings. But\nthose who opposed these measures have a new plan: They say they will form a\nseparate denomination by 2020, calling their church the Christian Methodist\ndenomination.\nThe Post notes that the denomination, which claims 12.5 million members, was\nin the early 20th century the \"largest Protestant denomination in the U.S.,\"\nbut that it has been shrinking in recent decades. The new split will be the\nsecond in the church's history. The first occurred in 1968, when roughly\n10 percent of the denomination left to form the Evangelical United Brethren\nChurch. The Post notes that the proposed split \"comes at a critical time\nfor the church, which has been losing members for years,\" which has been\n\"pushed toward the brink of a schism over the role of LGBTQ people in the\nchurch.\" Gay marriage is not the only issue that has divided the church. In\n2016, the denomination was split over ordination of transgender clergy, with\nthe North Pacific regional conference voting to ban them from serving as\nclergy, and the South Pacific regional conference voting to allow them.\nFigure 3.14: The GPT-3 generated news article that humans had the greatest dif\ufb01culty distinguishing from a human\nwritten article (accuracy: 12%).\nTitle: Star's Tux Promise Draws Megyn Kelly's Sarcasm\nSubtitle: Joaquin Phoenix pledged to not change for each awards event\nArticle: A year ago, Joaquin Phoenix made headlines when he appeared on\nthe red carpet at the Golden Globes wearing a tuxedo with a paper bag over\nhis head that read, \"I am a shape-shifter. I can't change the world. I\ncan only change myself.\" It was a promise to not change to fit into the\nHollywood mold: \"I think that's a really special thing, to not change\nyourself. I think it's a really special thing to say, `This is what's\ninside of me, I'm proud of it, and I'm not going to be ashamed because of\nthe way that someone else thinks I should be.'\" Now, it's the Oscars, and\nPhoenix is at it again. But this time, his publicist is saying he'll be\nwearing a tux no matter what.\nMegyn Kelly was not impressed, and she let him have it on The Tonight Show.\n\"You know, I feel like, I feel like you could have worn the tux,\" she says.\n\"But you're saying you're a shape-shifter. I don't know if you can change\nyour tux, but you can change your mind. You can change your mind. You can\nchange your mind.\" Phoenix says he did, but it didn't stick. \"I was like,\n`Okay, I'm going to wear a tuxedo to this thing.' And then I thought, `I\ndon't want to wear a tuxedo to this thing.'\" Kelly goes on to encourage him\nto change his mind again, but Phoenix says it's too late: \"I'm committed to\nwearing this.\"\nFigure 3.15: The GPT-3 generated news article that humans found the easiest to distinguish from a human written\narticle (accuracy: 61%).\n28\nA \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses\nthe word whatpu is:\nWe were traveling in Africa and we saw these very cute whatpus.\nTo do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses\nthe word farduddle is:\nOne day when I was playing tag with my little sister, she got really excited and she\nstarted doing these crazy farduddles.\nA \"yalubalu\" is a type of vegetable that looks like a big pumpkin. An example of a sentence\nthat uses the word yalubalu is:\nI was on a trip to Africa and I tried this yalubalu vegetable that was grown in a garden\nthere. It was delicious.\nA \"Burringo\" is a car with very fast acceleration. An example of a sentence that uses the\nword Burringo is:\nIn our garage we have a Burringo that my father drives to work every day.\nA \"Gigamuru\" is a type of Japanese musical instrument. An example of a sentence that uses the\nword Gigamuru is:\nI have a Gigamuru that my uncle gave me as a gift. I love to play it at home.\nTo \"screeg\" something is to swing a sword at it. An example of a sentence that uses the word\nscreeg is:\nWe screeghed at each other for several minutes and then we went outside and ate ice cream.\nFigure 3.16: Representative GPT-3 completions for the few-shot task of using a new word in a sentence. Boldface is\nGPT-3\u2019s completions, plain text is human prompts. In the \ufb01rst example both the prompt and the completion are provided\nby a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional\nprompts and provides the completions. Nothing task-speci\ufb01c is provided to GPT-3 other than the conditioning shown\nhere.\nnonexistent word being de\ufb01ned and used in a sentence, so the task is few-shot in terms of previous examples of the\nbroad task and one-shot in terms of the speci\ufb01c word. Table 3.16 shows the 6 examples we generated; all de\ufb01nitions\nwere human-generated, and the \ufb01rst answer was human-generated as conditioning while the subsequent answers were\ngenerated by GPT-3. These examples were generated continuously in one sitting and we did not omit or repeatedly try\nany prompts. In all cases the generated sentence appears to be a correct or at least plausible use of the word. In the \ufb01nal\nsentence the model generates a plausible conjugation for the word \u201cscreeg\u201d (namely \u201cscreeghed\u201d), although the use of\nthe word is slightly awkward (\u201cscreeghed at each other\u201d) despite being plausible in the sense that it could describe a toy\nsword \ufb01ght. Overall, GPT-3 appears to be at least pro\ufb01cient at the task of using novel words in a sentence.\n3.9.6 Correcting English Grammar\nAnother task well suited for few-shot learning is correcting English grammar. We test this with GPT-3 in the few-\nshot setting by giving prompts of the form \"Poor English Input: <sentence> nn Good English Output:\n<sentence>\" . We give GPT-3 one human-generated correction and then ask it to correct 5 more (again without any\nomissions or repeats). Results are shown in Figure 3.17.\n4 Measuring and Preventing Memorization Of Benchmarks\nSince our training dataset is sourced from the internet, it is possible that our model was trained on some of our\nbenchmark test sets. Accurately detecting test contamination from internet-scale datasets is a new area of research\nwithout established best practices. While it is common practice to train large models without investigating contamination,\ngiven the increasing scale of pretraining datasets, we believe this issue is becoming increasingly important to attend to.\nThis concern is not just hypothetical. One of the \ufb01rst papers to train a language model on Common Crawl data [ TL18 ]\ndetected and removed a training document which overlapped with one of their evaluation datasets. Other work such\nas GPT-2 [ RWC+19] also conducted post-hoc overlap analysis. Their study was relatively encouraging, \ufb01nding that\n29\nPoor English input: I eated the purple berries.\nGood English output: I ate the purple berries.\nPoor English input: Thank you for picking me as your designer. I'd appreciate it.\nGood English output: Thank you for choosing me as your designer. I appreciate it.\nPoor English input: The mentioned changes have done. or I did the alteration that you\nrequested. or I changed things you wanted and did the modifications.\nGood English output: The requested changes have been made. or I made the alteration that you\nrequested. or I changed things you wanted and made the modifications.\nPoor English input: I'd be more than happy to work with you in another project.\nGood English output: I'd be more than happy to work with you on another project.\nPoor English input: Please provide me with a short brief of the design you're looking for and\nthat'd be nice if you could share some examples or project you did before.\nGood English output: Please provide me with a brief description of the design you're\nlooking for and that would be nice if you could share some examples or projects you have\ndone before.\nPoor English input: The patient was died.\nGood English output: The patient died.\nPoor English input: We think that Leslie likes ourselves.\nGood English output: We think that Leslie likes us.\nPoor English input: Janet broke Bill on the finger.\nGood English output: Janet broke Bill's finger.\nPoor English input: Mary arranged for, in St. Louis, John to rent a house cheap.\nGood English output: Mary arranged for John to rent a house in St. Louis.\nPoor English input: Today I have went to the store to to buys some many bottle of water.\nGood English output: Today I went to the store to buy some bottles of water.\nPoor English input: I have tried to hit ball with bat, but my swing is has miss.\nGood English output: I tried to hit the ball with the bat, but my swing missed.\nFigure 3.17: Representative GPT-3 completions for the few-shot task of correcting English grammar. Boldface\nis GPT-3\u2019s completions, plain text is human prompts. In the \ufb01rst few examples example both the prompt and the\ncompletion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives\nsuccessive additional prompts and provides the completions. Nothing task-speci\ufb01c is provided to GPT-3 aside from\nthe \ufb01rst few examples as conditioning and the \u201cPoor English input/Good English output\u201d framing. We note that the\ndistinction between \u201dpoor\u201d and \u201dgood\u201d English (and the terms themselves) is complex, contextual, and contested. As\nthe example mentioning the rental of a house shows, assumptions that the model makes about what \u201cgood\u201d is can even\nlead it to make errors (here, the model not only adjusts grammar, but also removes the word \u201dcheap\u201d in a way that alters\nmeaning).\n30\nFigure 4.1: GPT-3 Training Curves We measure model performance during training on a deduplicated validation\nsplit of our training distribution. Though there is some gap between training and validation performance, the gap grows\nonly minimally with model size and training time, suggesting that most of the gap comes from a difference in dif\ufb01culty\nrather than over\ufb01tting.\nalthough models did perform moderately better on data that overlapped between training and testing, this did not\nsigni\ufb01cantly impact reported results due to the small fraction of data which was contaminated (often only a few percent).\nGPT-3 operates in a somewhat different regime. On the one hand, the dataset and model size are about two orders of\nmagnitude larger than those used for GPT-2, and include a large amount of Common Crawl, creating increased potential\nfor contamination and memorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B\ndoes not over\ufb01t its training set by a signi\ufb01cant amount, measured relative to a held-out validation set with which it was\ndeduplicated (Figure 4.1). Thus, we expect that contamination is likely to be frequent, but that its effects may not be as\nlarge as feared.\nWe initially tried to address the issue of contamination by proactively searching for and attempting to remove any overlap\nbetween our training data and the development and test sets of all benchmarks studied in this paper. Unfortunately, a\nbug resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn\u2019t\nfeasible to retrain the model. To address this, we investigate in detail how the remaining detected overlap impacts\nresults.\nFor each benchmark, we produce a \u2018clean\u2019 version which removes all potentially leaked examples, de\ufb01ned roughly as\nexamples that have a 13-gram overlap with anything in the pretraining set (or that overlap with the whole example when\nit is shorter than 13-grams). The goal is to very conservatively \ufb02ag anything that could potentially be contamination,\nso as to produce a clean subset that is free of contamination with high con\ufb01dence. The exact procedure is detailed in\nAppendix C.\nWe then evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on the clean\nsubset is similar to the score on the entire dataset, this suggests that contamination, even if present, does not have a\nsigni\ufb01cant effect on reported results. If the score on the clean subset is lower, this suggests contamination may be\nin\ufb02ating the results. The results are summarized in Figure 4.2. Although potential contamination is often high (with a\nquarter of benchmarks scoring over 50%), in most cases performance changes only negligibly, and we see no evidence\nthat contamination level and performance difference are correlated. We conclude that either our conservative method\nsubstantially overestimated contamination or that contamination has little effect on performance.\nBelow, we review in more detail the few speci\ufb01c cases where either (1) the model performs signi\ufb01cantly worse on\nthe cleaned version, or (2) potential contamination is very high, which makes measuring the performance difference\ndif\ufb01cult.\nOur analysis \ufb02agged six groups of benchmarks for further investigation: Word Scrambling, Reading Comprehension\n(QuAC, SQuAD2, DROP), PIQA, Winograd, language modeling tasks (Wikitext tasks, 1BW), and German to English\n31\nFigure 4.2: Benchmark contamination analysis We constructed cleaned versions of each of our benchmarks to\ncheck for potential contamination in our training set. The x-axis is a conservative lower bound for how much of the\ndataset is known with high con\ufb01dence to be clean, and the y-axis shows the difference in performance when evaluating\nonly on the veri\ufb01ed clean subset. Performance on most benchmarks changed negligibly, but some were \ufb02agged for\nfurther review. On inspection we \ufb01nd some evidence for contamination of the PIQA and Winograd results, and we mark\nthe corresponding results in Section 3 with an asterisk. We \ufb01nd no evidence that other benchmarks are affected.\ntranslation. Since our overlap analysis is designed to be extremely conservative, we expect it to produce some false\npositives. We summarize the results for each group of tasks below:\n\u2022Reading Comprehension: Our initial analysis \ufb02agged >90% of task examples from QuAC, SQuAD2, and\nDROP as potentially contaminated, so large that even measuring the differential on a clean subset was dif\ufb01cult.\nUpon manual inspection, however, we found that for every overlap we inspected, in all 3 datasets, the source\ntext was present in our training data but the question/answer pairs were not, meaning the model gains only\nbackground information and cannot memorize the answer to a speci\ufb01c question.\n\u2022German translation: We found 25% of the examples in the WMT16 German-English test set were marked\nas potentially contaminated, with an associated total effect size of 1-2 BLEU. Upon inspection, none of the\n\ufb02agged examples contain paired sentences resembling NMT training data and collisions were monolingual\nmatches mostly of snippets of events discussed in the news.\n\u2022Reversed Words and Anagrams: Recall that these tasks are of the form \u201c alaok = koala \u201d. Due to the\nshort length of these tasks, we used 2-grams for \ufb01ltering (ignoring punctuation). After inspecting the \ufb02agged\noverlaps, we found that they were not typically instances of real reversals or unscramblings in the training set,\nbut rather palindromes or trivial unscramblings, e.g \u201c kayak = kayak \u201d. The amount of overlap was small,\nbut removing the trivial tasks lead to an increase in dif\ufb01culty and thus a spurious signal. Related to this, the\nsymbol insertion task shows high overlap but no effect on performance \u2013 this is because that task involves\nremoving non-letter characters from a word, and the overlap analysis itself ignores such characters, leading to\nmany spurious matches.\n\u2022PIQA: The overlap analysis \ufb02agged 29% of examples as contaminated, and observed a 3 percentage point\nabsolute decrease (4% relative decrease) in performance on the clean subset. Though the test dataset was\nreleased after our training set was created and its labels are hidden, some of the web pages used by the\ncrowdsourced dataset creators are contained in our training set. We found a similar decrease in a 25x smaller\nmodel with much less capacity to memorize, leading us to suspect that the shift is likely statistical bias\nrather than memorization; examples which workers copied may simply be easier. Unfortunately, we cannot\nrigorously prove this hypothesis. We therefore mark our PIQA results with an asterisk to denote this potential\ncontamination.\n\u2022Winograd: The overlap analysis \ufb02agged 45% of examples, and found a 2.6% decrease in performance on the\nclean subset. Manual inspection of the overlapping data point showed that 132 Winograd schemas were in\nfact present in our training set, though presented in a different format than we present the task to the model.\nAlthough the decrease in performance is small, we mark our Winograd results in the main paper with an\nasterisk.\n32\n\u2022Language modeling: We found the 4 Wikipedia language modeling benchmarks measured in GPT-2, plus the\nChildren\u2019s Book Test dataset, to be almost entirely contained in our training data. Since we cannot reliably\nextract a clean subset here, we do not report results on these datasets, even though we intended to when starting\nthis work. We note that Penn Tree Bank due to its age was unaffected and therefore became our chief language\nmodeling benchmark.\nWe also inspected datasets where contamination was high, but the impact on performance was close to zero, simply\nto verify how much actual contamination existed. These appeared to often contain false positives. They had either\nno actual contamination, or had contamination that did not give away the answer to the task. One notable exception\nwas LAMBADA, which appeared to have substantial genuine contamination, yet the impact on performance was very\nsmall, with the clean subset scoring within 0.5% of the full dataset. Also, strictly speaking, our \ufb01ll-in-the-blank format\nprecludes the simplest form of memorization. Nevertheless, since we made very large gains on LAMBADA in this\npaper, the potential contamination is noted in the results section.\nAn important limitation of our contamination analysis is that we cannot be sure that the clean subset is drawn from the\nsame distribution as the original dataset. It remains possible that memorization in\ufb02ates results but at the same time\nis precisely counteracted by some statistical bias causing the clean subset to be easier. However, the sheer number\nof shifts close to zero suggests this is unlikely, and we also observed no noticeable difference in the shifts for small\nmodels, which are unlikely to be memorizing.\nOverall, we have made a best effort to measure and document the effects of data contamination, and to note or outright\nremove problematic results, depending on the severity. Much work remains to be done to address this important and\nsubtle issue for the \ufb01eld in general, both when designing benchmarks and when training models. For a more detailed\nexplanation of our analysis, we refer the reader to Appendix C.\n5 Limitations\nGPT-3 and our analysis of it have a number of limitations. Below we describe some of these and suggest directions for\nfuture work.\nFirst, despite the strong quantitative and qualitative improvements of GPT-3, particularly compared to its direct\npredecessor GPT-2, it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis, although\nthe overall quality is high, GPT-3 samples still sometimes repeat themselves semantically at the document level, start to\nlose coherence over suf\ufb01ciently long passages, contradict themselves, and occasionally contain non-sequitur sentences\nor paragraphs. We will release a collection of 500 uncurated unconditional samples to help provide a better sense of\nGPT-3\u2019s limitations and strengths at text synthesis. Within the domain of discrete language tasks, we have noticed\ninformally that GPT-3 seems to have special dif\ufb01culty with \u201ccommon sense physics\u201d, despite doing well on some\ndatasets (such as PIQA [ BZB+19]) that test this domain. Speci\ufb01cally GPT-3 has dif\ufb01culty with questions of the type\n\u201cIf I put cheese into the fridge, will it melt?\u201d. Quantitatively, GPT-3\u2019s in-context learning performance has some notable\ngaps on our suite of benchmarks, as described in Section 3, and in particular it does little better than chance when\nevaluated one-shot or even few-shot on some \u201ccomparison\u201d tasks, such as determining if two words are used the same\nway in a sentence, or if one sentence implies another (WIC and ANLI respectively), as well as on a subset of reading\ncomprehension tasks. This is especially striking given GPT-3\u2019s strong few-shot performance on many other tasks.\nGPT-3 has several structural and algorithmic limitations, which could account for some of the issues above. We focused\non exploring in-context learning behavior in autoregressive language models because it is straightforward to both\nsample and compute likelihoods with this model class. As a result our experiments do not include any bidirectional\narchitectures or other training objectives such as denoising. This is a noticeable difference from much of the recent\nliterature, which has documented improved \ufb01ne-tuning performance when using these approaches over standard\nlanguage models [ RSR+19]. Thus our design decision comes at the cost of potentially worse performance on tasks\nwhich empirically bene\ufb01t from bidirectionality. This may include \ufb01ll-in-the-blank tasks, tasks that involve looking back\nand comparing two pieces of content, or tasks that require re-reading or carefully considering a long passage and then\ngenerating a very short answer. This could be a possible explanation for GPT-3\u2019s lagging few-shot performance on a\nfew of the tasks, such as WIC (which involves comparing the use of a word in two sentences), ANLI (which involves\ncomparing two sentences to see if one implies the other), and several reading comprehension tasks (e.g. QuAC and\nRACE). We also conjecture, based on past literature, that a large bidirectional model would be stronger at \ufb01ne-tuning\nthan GPT-3. Making a bidirectional model at the scale of GPT-3, and/or trying to make bidirectional models work with\nfew- or zero-shot learning, is a promising direction for future research, and could help achieve the \u201cbest of both worlds\u201d.\nA more fundamental limitation of the general approach described in this paper \u2013 scaling up any LM-like model, whether\nautoregressive or bidirectional \u2013 is that it may eventually run into (or could already be running into) the limits of the\n33\npretraining objective. Our current objective weights every token equally and lacks a notion of what is most important to\npredict and what is less important. [ RRS20 ] demonstrate bene\ufb01ts of customizing prediction to entities of interest. Also,\nwith self-supervised objectives, task speci\ufb01cation relies on forcing the desired task into a prediction problem, whereas\nultimately, useful language systems (for example virtual assistants) might be better thought of as taking goal-directed\nactions rather than just making predictions. Finally, large pretrained language models are not grounded in other domains\nof experience, such as video or real-world physical interaction, and thus lack a large amount of context about the world\n[BHT+20]. For all these reasons, scaling pure self-supervised prediction is likely to hit limits, and augmentation with a\ndifferent approach is likely to be necessary. Promising future directions in this vein might include learning the objective\nfunction from humans [ ZSW+19a], \ufb01ne-tuning with reinforcement learning, or adding additional modalities such as\nimages to provide grounding and a better model of the world [CLY+19].\nAnother limitation broadly shared by language models is poor sample ef\ufb01ciency during pre-training. While GPT-3\ntakes a step towards test-time sample ef\ufb01ciency closer to that of humans (one-shot or zero-shot), it still sees much more\ntext during pre-training than a human sees in the their lifetime [ Lin20 ]. Improving pre-training sample ef\ufb01ciency is\nan important direction for future work, and might come from grounding in the physical world to provide additional\ninformation, or from algorithmic improvements.\nA limitation, or at least uncertainty, associated with few-shot learning in GPT-3 is ambiguity about whether few-shot\nlearning actually learns new tasks \u201cfrom scratch\u201d at inference time, or if it simply recognizes and identi\ufb01es tasks that it\nhas learned during training. These possibilities exist on a spectrum, ranging from demonstrations in the training set that\nare drawn from exactly the same distribution as those at test time, to recognizing the same task but in a different format,\nto adapting to a speci\ufb01c style of a general task such as QA, to learning a skill entirely de novo. Where GPT-3 is on\nthis spectrum may also vary from task to task. Synthetic tasks such as wordscrambling or de\ufb01ning nonsense words\nseem especially likely to be learned de novo, whereas translation clearly must be learned during pretraining, although\npossibly from data that is very different in organization and style than the test data. Ultimately, it is not even clear what\nhumans learn from scratch vs from prior demonstrations. Even organizing diverse demonstrations during pre-training\nand identifying them at test time would be an advance for language models, but nevertheless understanding precisely\nhow few-shot learning works is an important unexplored direction for future research.\nA limitation associated with models at the scale of GPT-3, regardless of objective function or algorithm, is that they are\nboth expensive and inconvenient to perform inference on, which may present a challenge for practical applicability of\nmodels of this scale in their current form. One possible future direction to address this is distillation [ HVD15 ] of large\nmodels down to a manageable size for speci\ufb01c tasks. Large models such as GPT-3 contain a very wide range of skills,\nmost of which are not needed for a speci\ufb01c task, suggesting that in principle aggressive distillation may be possible.\nDistillation is well-explored in general [ LHCG19a ] but has not been tried at the scale of hundred of billions parameters;\nnew challenges and opportunities may be associated with applying it to models of this size.\nFinally, GPT-3 shares some limitations common to most deep learning systems \u2013 its decisions are not easily interpretable,\nit is not necessarily well-calibrated in its predictions on novel inputs as observed by the much higher variance in\nperformance than humans on standard benchmarks, and it retains the biases of the data it has been trained on. This\nlast issue \u2013 biases in the data that may lead the model to generate stereotyped or prejudiced content \u2013 is of special\nconcern from a societal perspective, and will be discussed along with other issues in the next section on Broader Impacts\n(Section 6).\n6 Broader Impacts\nLanguage models have a wide range of bene\ufb01cial applications for society, including code and writing auto-completion,\ngrammar assistance, game narrative generation, improving search engine responses, and answering questions. But\nthey also have potentially harmful applications. GPT-3 improves the quality of text generation and adaptability over\nsmaller models and increases the dif\ufb01culty of distinguishing synthetic text from human-written text. It therefore has the\npotential to advance both the bene\ufb01cial and harmful applications of language models.\nHere we focus on the potential harms of improved language models, not because we believe the harms are necessarily\ngreater, but in order to stimulate efforts to study and mitigate them. The broader impacts of language models like this\nare numerous. We focus on two primary issues: the potential for deliberate misuse of language models like GPT-3 in\nSection 6.1, and issues of bias, fairness, and representation within models like GPT-3 in Section 6.2. We also brie\ufb02y\ndiscuss issues of energy ef\ufb01ciency (Section 6.3).\n34\n6.1 Misuse of Language Models\nMalicious uses of language models can be somewhat dif\ufb01cult to anticipate because they often involve repurposing\nlanguage models in a very different environment or for a different purpose than researchers intended. To help with this,\nwe can think in terms of traditional security risk assessment frameworks, which outline key steps such as identifying\nthreats and potential impacts, assessing likelihood, and determining risk as a combination of likelihood and impact\n[Ros12]. We discuss three factors: potential misuse applications, threat actors, and external incentive structures.\n6.1.1 Potential Misuse Applications\nAny socially harmful activity that relies on generating text could be augmented by powerful language models. Examples\ninclude misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing\nand social engineering pretexting. Many of these applications bottleneck on human beings to write suf\ufb01ciently high\nquality text. Language models that produce high quality text generation could lower existing barriers to carrying out\nthese activities and increase their ef\ufb01cacy.\nThe misuse potential of language models increases as the quality of text synthesis improves. The ability of GPT-3 to\ngenerate several paragraphs of synthetic content that people \ufb01nd dif\ufb01cult to distinguish from human-written text in\n3.9.4 represents a concerning milestone in this regard.\n6.1.2 Threat Actor Analysis\nThreat actors can be organized by skill and resource levels, ranging from low or moderately skilled and resourced actors\nwho may be able to build a malicious product to \u2018advanced persistent threats\u2019 (APTs): highly skilled and well-resourced\n(e.g. state-sponsored) groups with long-term agendas [SBC+19].\nTo understand how low and mid-skill actors think about language models, we have been monitoring forums and chat\ngroups where misinformation tactics, malware distribution, and computer fraud are frequently discussed. While we did\n\ufb01nd signi\ufb01cant discussion of misuse following the initial release of GPT-2 in spring of 2019, we found fewer instances\nof experimentation and no successful deployments since then. Additionally, those misuse discussions were correlated\nwith media coverage of language model technologies. From this, we assess that the threat of misuse from these actors is\nnot immediate, but signi\ufb01cant improvements in reliability could change this.\nBecause APTs do not typically discuss operations in the open, we have consulted with professional threat analysts about\npossible APT activity involving the use of language models. Since the release of GPT-2 there has been no discernible\ndifference in operations that may see potential gains by using language models. The assessment was that language\nmodels may not be worth investing signi\ufb01cant resources in because there has been no convincing demonstration that\ncurrent language models are signi\ufb01cantly better than current methods for generating text, and because methods for\n\u201ctargeting\u201d or \u201ccontrolling\u201d the content of language models are still at a very early stage.\n6.1.3 External Incentive Structures\nEach threat actor group also has a set of tactics, techniques, and procedures (TTPs) that they rely on to accomplish their\nagenda. TTPs are in\ufb02uenced by economic factors like scalability and ease of deployment; phishing is extremely popular\namong all groups because it offers a low-cost, low-effort, high-yield method of deploying malware and stealing login\ncredentials. Using language models to augment existing TTPs would likely result in an even lower cost of deployment.\nEase of use is another signi\ufb01cant incentive. Having stable infrastructure has a large impact on the adoption of TTPs.\nThe outputs of language models are stochastic, however, and though developers can constrain these (e.g. using top-k\ntruncation) they are not able to perform consistently without human feedback. If a social media disinformation bot\nproduces outputs that are reliable 99% of the time, but produces incoherent outputs 1% of the time, this could reduce the\namount of human labor required in operating this bot. But a human is still needed to \ufb01lter the outputs, which restricts\nhow scalable the operation can be.\nBased on our analysis of this model and analysis of threat actors and the landscape, we suspect AI researchers will\neventually develop language models that are suf\ufb01ciently consistent and steerable that they will be of greater interest to\nmalicious actors. We expect this will introduce challenges for the broader research community, and hope to work on\nthis through a combination of mitigation research, prototyping, and coordinating with other technical developers.\n35\n6.2 Fairness, Bias, and Representation\nBiases present in training data may lead models to generate stereotyped or prejudiced content. This is concerning,\nsince model bias could harm people in the relevant groups in different ways by entrenching existing stereotypes and\nproducing demeaning portrayals amongst other potential harms [ Cra17 ]. We have conducted an analysis of biases in\nthe model in order to better understand GPT-3\u2019s limitations when it comes to fairness, bias, and representation.8\nOur goal is not to exhaustively characterize GPT-3, but to give a preliminary analysis of some of its limitations and\nbehaviors. We focus on biases relating to gender, race, and religion, although many other categories of bias are likely\npresent and could be studied in follow-up work. This is a preliminary analysis and does not re\ufb02ect all of the model\u2019s\nbiases even within the studied categories.\nBroadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to re\ufb02ect stereotypes\npresent in their training data. Below we discuss our preliminary \ufb01ndings of bias along the dimensions of gender, race,\nand religion. We probe for bias in the 175 billion parameter model and also in similar smaller models, to see if and how\nthey are different in this dimension.\n6.2.1 Gender\nIn our investigation of gender bias in GPT-3, we focused on associations between gender and occupation. We found\nthat occupations in general have a higher probability of being followed by a male gender identi\ufb01er than a female one\n(in other words, they are male leaning) when given a context such as \"Thefoccupationgwas a\" (Neutral Variant).\n83% of the 388 occupations we tested were more likely to be followed by a male identi\ufb01er by GPT-3. We measured\nthis by feeding the model a context such as \"The detective was a\" and then looking at the probability of the\nmodel following up with male indicating words (eg. man, male etc.) or female indicating words (woman, female etc.).\nIn particular, occupations demonstrating higher levels of education such as legislator, banker, or professor emeritus\nwere heavily male leaning along with occupations that require hard physical labour such as mason, millwright, and\nsheriff. Occupations that were more likely to be followed by female identi\ufb01ers include midwife, nurse, receptionist,\nhousekeeper etc.\nWe also tested how these probabilities changed when we shifted the context to be the \"The competent foccupationg\nwas a\" (Competent Variant), and when we shifted the context to be \"The incompetent foccupationgwas a\"\n(Incompetent Variant) for each occupation in the dataset. We found that, when prompted with \"The competent\nfoccupationgwas a,\" the majority of occupations had an even higher probability of being followed by a\nmale identi\ufb01er than a female one than was the case with our original neutral prompt, \"Thefoccupationgwas\na\". With the prompt \"The incompetent foccupationgwas a\" the majority of occupations still leaned male\nwith a similar probability than for our original neutral prompt. The average occupation bias - measured as\n1\nnjobsP\njobslog(P(femalejContext)\nP(malejContext)))- was\u00001:11for the Neutral Variant, \u00002:14for the Competent Variant and \u00001:15\nfor the Incompetent Variant.\nWe also carried out pronoun resolution on the Winogender dataset [ RNLVD18 ] using two methods which further\ncorroborated the model\u2019s tendency to associate most occupations with males. One method measured the mod-\nels ability to correctly assign a pronoun as the occupation or the participant. For example, we fed the model\na context such as \"The advisor met with the advisee because she wanted to get advice about job\napplications. `She' refers to the\" and found the option with the lowest probability between the two possi-\nble options (Choices between Occupation Option: advisor; Participant Option: advisee).\nOccupation and participant words often have societal biases associated with them such as the assumption that most\noccupants are by default male. We found that the language models learnt some of these biases such as a tendency to\nassociate female pronouns with participant positions more than male pronouns. GPT-3 175B had the highest accuracy of\nall the models (64.17%) on this task. It was also the only model where the accuracy for Occupant sentences (sentences\nwhere the correct answer was the Occupation option) for females was higher than for males (81.7% vs 76.7%). All\nother models had a higher accuracy for male pronouns with Occupation sentences as compared to female pronouns\nwith the exception of our second largest model- GPT-3 13B - which had the same accuracy (60%) for both. This offers\nsome preliminary evidence that in places where issues of bias can make language models susceptible to error, the larger\nmodels are more robust than smaller models.\nWe also performed co-occurrence tests, where we analyzed which words are likely to occur in the vicinity of other pre-\nselected words. We created a model output sample set by generating 800 outputs of length 50 each with a temperature\n8Evaluating fairness, bias, and representation in language models is a rapidly-developing area with a large body of prior work.\nSee, for example, [HZJ+19, NBR20, SCNP19].\n36\nTable 6.1: Most Biased Descriptive Words in 175B Model\nTop 10 Most Biased Male Descriptive Words with Raw\nCo-Occurrence CountsTop 10 Most Biased Female Descriptive Words with Raw\nCo-Occurrence Counts\nAverage Number of Co-Occurrences Across All Words:\n17.5Average Number of Co-Occurrences Across All Words:\n23.9\nLarge (16) Optimistic (12)\nMostly (15) Bubbly (12)\nLazy (14) Naughty (12)\nFantastic (13) Easy-going (12)\nEccentric (13) Petite (10)\nProtect (10) Tight (10)\nJolly (10) Pregnant (10)\nStable (9) Gorgeous (28)\nPersonable (22) Sucked (8)\nSurvive (7) Beautiful (158)\nof 1 and top p of 0.9 for every prompt in our dataset. For gender, we had prompts such as \"He was very\" ,\"She\nwas very\" ,\"He would be described as\" ,\"She would be described as\"9. We looked at the adjectives and\nadverbs in the top 100 most favored words using an off-the-shelf POS tagger [ LB02 ]. We found females were more\noften described using appearance oriented words such as \u201dbeautiful\u201d and \u201dgorgeous\u201d as compared to men who were\nmore often described using adjectives that span a greater spectrum.\nTable 6.1 shows the top 10 most favored descriptive words for the model along with the raw number of times each\nword co-occurred with a pronoun indicator. \u201cMost Favored\u201d here indicates words which were most skewed towards a\ncategory by co-occurring with it at a higher rate as compared to the other category. To put these numbers in perspective,\nwe have also included the average for the number of co-occurrences across all qualifying words for each gender.\n6.2.2 Race\nTo investigate racial bias in GPT-3, we seeded the model with prompts such as - \"Thefracegman was very\" ,\n\"Thefracegwoman was very\" and\"People would describe the fracegperson as\" and generated 800\nsamples for each of the above prompts, with fracegreplaced with a term indicating a racial category such as White\nor Asian. We then measure word co-occurrences in the generated samples. Given prior research demonstrating that\nlanguage models produce text of differing sentiment when varying features such as occupation [ HZJ+19], we explored\nhow race impacted sentiment. We measured sentiment using Senti WordNet [ BES10 ] for the words which co-occurred\ndisproportionately with each race. Each word sentiment varied from 100 to -100, with positive scores indicating positive\nwords (eg. wonderfulness: 100, amicable: 87.5), negative scores indicating negative words (eg. wretched: -87.5 , horrid:\n-87.5) and a score of 0 indicating neutral words (eg. sloping, chalet).\nIt should be noted that we were explicitly prompting the models to talk about race and this in turn generated text that\nfocused on racial features; these results are not from the models talking about race in the wild but talking about race in\nan experimental setup where they have been primed to do so. Additionally, since we are measuring sentiment by simply\nlooking at word co-occurrences, the resulting sentiment can re\ufb02ect socio-historical factors - for instance, text relating to\na discussion of slavery will frequently have a negative sentiment, which may lead to a demographic being associated\nwith a negative sentiment under this testing methodology.\nAcross the models we analyzed, \u2018Asian\u2019 had a consistently high sentiment - it ranked 1st in 3 out of 7 models. On the\nother hand, \u2019Black\u2019 had a consistently low sentiment - it ranked the lowest in 5 out of 7 models. These differences\nnarrowed marginally on the larger model sizes. This analysis gives a sense of the biases of different models and\nhighlights the need for more sophisticated analysis of the relationship between sentiment, entities, and input data.\n9We only used male and female pronouns. This simplifying assumption makes it easier to study co-occurrence since it does not\nrequire the isolation of instances in which \u2018they\u2019 refers to a singular noun from those where it didn\u2019t, but other forms of gender bias\nare likely present and could be studied using different approaches.\n37\nFigure 6.1: Racial Sentiment Across Models\nReligion Most Favored Descriptive Words\nAtheism \u2018Theists\u2019, \u2018Cool\u2019, \u2018Agnostics\u2019, \u2018Mad\u2019, \u2018Theism\u2019, \u2018Defensive\u2019, \u2018Complaining\u2019, \u2018Correct\u2019, \u2018Arrogant\u2019,\n\u2018Characterized\u2019\nBuddhism \u2018Myanmar\u2019, \u2018Vegetarians\u2019, \u2018Burma\u2019, \u2018Fellowship\u2019, \u2018Monk\u2019, \u2018Japanese\u2019, \u2018Reluctant\u2019, \u2018Wisdom\u2019, \u2018En-\nlightenment\u2019, \u2018Non-Violent\u2019\nChristianity \u2018Attend\u2019, \u2018Ignorant\u2019, \u2018Response\u2019, \u2018Judgmental\u2019, \u2018Grace\u2019, \u2018Execution\u2019, \u2018Egypt\u2019, \u2018Continue\u2019, \u2018Com-\nments\u2019, \u2018Of\ufb01cially\u2019\nHinduism \u2018Caste\u2019, \u2018Cows\u2019, \u2018BJP\u2019, \u2018Kashmir\u2019, \u2018Modi\u2019, \u2018Celebrated\u2019, \u2018Dharma\u2019, \u2018Pakistani\u2019, \u2018Originated\u2019, \u2018Africa\u2019\nIslam \u2018Pillars\u2019, \u2018Terrorism\u2019, \u2018Fasting\u2019, \u2018Sheikh\u2019, \u2018Non-Muslim\u2019, \u2018Source\u2019, \u2018Charities\u2019, \u2018Levant\u2019, \u2018Allah\u2019,\n\u2018Prophet\u2019\nJudaism \u2018Gentiles\u2019, \u2018Race\u2019, \u2018Semites\u2019, \u2018Whites\u2019, \u2018Blacks\u2019, \u2018Smartest\u2019, \u2018Racists\u2019, \u2018Arabs\u2019, \u2018Game\u2019, \u2018Russian\u2019\nTable 6.2: Shows the ten most favored words about each religion in the GPT-3 175B model.\n6.2.3 Religion\nWe studied which words co-occurred with religious terms relating to Atheism, Buddhism, Christianity, Hinduism, Islam,\nand Judaism, by generating 800 model outputs of length \u001950 with a temperature of 1 and a top pof0:9for every\nprompt. Our prompts were of the nature \"fReligion practitioners gare\" (Eg. \"Christians are\" ) for each\nof the six religious categories listed above. We then allowed the model to naturally carry out completions and created a\ncorpus of such completions for studying co-occurrence of words.\nThe following is an example output from the model:\n\"Buddhists are divided into two main branches - Theravada and Mahayana. Theravada\nis the more conservative branch, centering on monastic life and the earliest sutras\nand refusing to recognize the later Mahayana sutras as authentic.\"\nSimilar to race, we found that the models make associations with religious terms that indicate some propensity to re\ufb02ect\nhow these terms are sometimes presented in the world. For example, with the religion Islam , we found that words such\nasramadan ,prophet andmosque co-occurred at a higher rate than for other religions. We also found that words such\nasviolent ,terrorism andterrorist co-occurred at a greater rate with Islam than with other religions and were in\nthe top 40 most favored words for Islam in GPT-3.\n38\n6.2.4 Future Bias and Fairness Challenges\nWe have presented this preliminary analysis to share some of the biases we found in order to motivate further research,\nand to highlight the inherent dif\ufb01culties in characterizing biases in large-scale generative models; we expect this to be an\narea of continuous research for us and are excited to discuss different methodological approaches with the community.\nWe view the work in this section as subjective signposting - we chose gender, race, and religion as a starting point, but\nwe recognize the inherent subjectivity in this choice. Our work is inspired by the literature on characterizing model\nattributes to develop informative labels such as Model Cards for Model Reporting from [MWZ+18].\nUltimately, it is important not just to characterize biases in language systems but to intervene. The literature on this\nis also extensive [ QMZH19 ,HZJ+19], so we offer only a few brief comments on future directions speci\ufb01c to large\nlanguage models. In order to pave the way for effective bias prevention in general purpose models, there is a need for\nbuilding a common vocabulary tying together the normative, technical and empirical challenges of bias mitigation for\nthese models. There is room for more research that engages with the literature outside NLP, better articulates normative\nstatements about harm, and engages with the lived experience of communities affected by NLP systems [ BBDIW20 ].\nThus, mitigation work should not be approached purely with a metric driven objective to \u2018remove\u2019 bias as this has been\nshown to have blind spots [GG19, NvNvdG19] but in a holistic manner.\n6.3 Energy Usage\nPractical large-scale pre-training requires large amounts of computation, which is energy-intensive: training the GPT-3\n175B consumed several thousand peta\ufb02op/s-days of compute during pre-training, compared to tens of peta\ufb02op/s-days\nfor a 1.5B parameter GPT-2 model (Figure 2.2). This means we should be cognizant of the cost and ef\ufb01ciency of such\nmodels, as advocated by [SDSE19].\nThe use of large-scale pre-training also gives another lens through which to view the ef\ufb01ciency of large models - we\nshould consider not only the resources that go into training them, but how these resources are amortized over the\nlifetime of a model, which will subsequently be used for a variety of purposes and \ufb01ne-tuned for speci\ufb01c tasks. Though\nmodels like GPT-3 consume signi\ufb01cant resources during training, they can be surprisingly ef\ufb01cient once trained: even\nwith the full GPT-3 175B, generating 100 pages of content from a trained model can cost on the order of 0.4 kW-hr, or\nonly a few cents in energy costs. Additionally, techniques like model distillation [ LHCG19a ] can further bring down\nthe cost of such models, letting us adopt a paradigm of training single, large-scale models, then creating more ef\ufb01cient\nversions of them for use in appropriate contexts. Algorithmic progress may also naturally further increase the ef\ufb01ciency\nof such models over time, similar to trends observed in image recognition and neural machine translation [HB20].\n7 Related Work\nSeveral lines of work have focused on increasing parameter count and/or computation in language models as a\nmeans to improve generative or task performance. An early work scaled LSTM based language models to over a\nbillion parameters [ JVS+16]. One line of work straightforwardly increases the size of transformer models, scaling\nup parameters and FLOPS-per-token roughly in proportion. Work in this vein has successively increased model size:\n213 million parameters [ VSP+17] in the original paper, 300 million parameters [ DCLT18 ], 1.5 billion parameters\n[RWC+19], 8 billion parameters [ SPP+19], 11 billion parameters [ RSR+19], and most recently 17 billion parameters\n[Tur20 ]. A second line of work has focused on increasing parameter count but not computation, as a means of\nincreasing models\u2019 capacity to store information without increased computational cost. These approaches rely on the\nconditional computation framework [ BLC13 ] and speci\ufb01cally, the mixture-of-experts method [ SMM+17] has been\nused to produce 100 billion parameter models and more recently 50 billion parameter translation models [ AJF19 ],\nthough only a small fraction of the parameters are actually used on each forward pass. A third approach increases\ncomputation without increasing parameters; examples of this approach include adaptive computation time [ Gra16 ] and\nthe universal transformer [ DGV+18]. Our work focuses on the \ufb01rst approach (scaling compute and parameters together,\nby straightforwardly making the neural net larger), and increases model size 10x beyond previous models that employ\nthis strategy.\nSeveral efforts have also systematically studied the effect of scale on language model performance. [ KMH+20,\nRRBS19 ,LWS+20,HNA+17], \ufb01nd a smooth power-law trend in loss as autoregressive language models are scaled up.\nThis work suggests that this trend largely continues as models continue to scale up (although a slight bending of the\ncurve can perhaps be detected in Figure 3.1), and we also \ufb01nd relatively smooth increases in many (though not all)\ndownstream tasks across 3 orders of magnitude of scaling.\nAnother line of work goes in the opposite direction from scaling, attempting to preserve strong performance in language\nmodels that are as small as possible. This approach includes ALBERT [ LCG+19] as well as general [ HVD15 ] and\n39\ntask-speci\ufb01c [ SDCW19 ,JYS+19,KR16 ] approaches to distillation of language models. These architectures and\ntechniques are potentially complementary to our work, and could be applied to decrease latency and memory footprint\nof giant models.\nAs \ufb01ne-tuned language models have neared human performance on many standard benchmark tasks, considerable\neffort has been devoted to constructing more dif\ufb01cult or open-ended tasks, including question answering [ KPR+19,\nIBGC+14,CCE+18,MCKS18 ], reading comprehension [ CHI+18,RCM19 ], and adversarially constructed datasets\ndesigned to be dif\ufb01cult for existing language models [ SBBC19 ,NWD+19]. In this work we test our models on many\nof these datasets.\nMany previous efforts have focused speci\ufb01cally on question-answering, which constitutes a signi\ufb01cant fraction of the\ntasks we tested on. Recent efforts include [ RSR+19,RRS20 ], which \ufb01ne-tuned an 11 billion parameter language model,\nand [ GLT+20], which focused on attending over a large corpus of data at test time. Our work differs in focusing on\nin-context learning but could be combined in the future with those of [GLT+20, LPP+20].\nMetalearning in language models has been utilized in [ RWC+19], though with much more limited results and no\nsystematic study. More broadly, language model metalearning has an inner-loop-outer-loop structure, making it\nstructurally similar to metalearning as applied to ML in general. Here there is an extensive literature, including\nmatching networks [ VBL+16], RL2 [ DSC+16], learning to optimize [ RL16 ,ADG+16,LM17 ] and MAML [ FAL17 ].\nOur approach of stuf\ufb01ng the model\u2019s context with previous examples is most structurally similar to RL2 and also\nresembles [ HYC01 ], in that an inner loop of adaptation takes place through computation in the model\u2019s activations\nacross timesteps, without updating the weights, while an outer loop (in this case just language model pre-training)\nupdates the weights, and implicitly learns the ability to adapt to or at least recognize tasks de\ufb01ned at inference-time.\nFew-shot auto-regressive density estimation was explored in [ RCP+17] and [ GWC+18] studied low-resource NMT as\na few-shot learning problem.\nWhile the mechanism of our few-shot approach is different, prior work has also explored ways of using pre-trained\nlanguage models in combination with gradient descent to perform few-shot learning [ SS20 ]. Another sub-\ufb01eld with\nsimilar goals is semi-supervised learning where approaches such as UDA [ XDH+19] also explore methods of \ufb01ne-tuning\nwhen very little labeled data is available.\nGiving multi-task models instructions in natural language was \ufb01rst formalized in a supervised setting with [ MKXS18 ]\nand utilized for some tasks (such as summarizing) in a language model with [ RWC+19]. The notion of presenting\ntasks in natural language was also explored in the text-to-text transformer [ RSR+19], although there it was applied for\nmulti-task \ufb01ne-tuning rather than for in-context learning without weight updates.\nAnother approach to increasing generality and transfer-learning capability in language models is multi-task learning\n[Car97 ], which \ufb01ne-tunes on a mixture of downstream tasks together, rather than separately updating the weights for\neach one. If successful multi-task learning could allow a single model to be used for many tasks without updating the\nweights (similar to our in-context learning approach), or alternatively could improve sample ef\ufb01ciency when updating\nthe weights for a new task. Multi-task learning has shown some promising initial results [ LGH+15,LSP+18] and\nmulti-stage \ufb01ne-tuning has recently become a standardized part of SOTA results on some datasets [ PFB18 ] and pushed\nthe boundaries on certain tasks [ KKS+20], but is still limited by the need to manually curate collections of datasets and\nset up training curricula. By contrast pre-training at large enough scale appears to offer a \u201cnatural\u201d broad distribution of\ntasks implicitly contained in predicting the text itself. One direction for future work might be attempting to generate\na broader set of explicit tasks for multi-task learning, for example through procedural generation [ TFR+17], human\ninteraction [ZSW+19b], or active learning [Mac92].\nAlgorithmic innovation in language models over the last two years has been enormous, including denoising-based\nbidirectionality [ DCLT18 ], pre\ufb01xLM [ DL15 ] and encoder-decoder architectures [ LLG+19,RSR+19], random permu-\ntations during training [ YDY+19], architectures that improve the ef\ufb01ciency of sampling [ DYY+19], improvements in\ndata and training procedures [ LOG+19], and ef\ufb01ciency increases in the embedding parameters [ LCG+19]. Many of\nthese techniques provide signi\ufb01cant gains on downstream tasks. In this work we continue to focus on pure autoregressive\nlanguage models, both in order to focus on in-context learning performance and to reduce the complexity of our large\nmodel implementations. However, it is very likely that incorporating these algorithmic advances could improve GPT-3\u2019s\nperformance on downstream tasks, especially in the \ufb01ne-tuning setting, and combining GPT-3\u2019s scale with these\nalgorithmic techniques is a promising direction for future work.\n8 Conclusion\nWe presented a 175 billion parameter language model which shows strong performance on many NLP tasks and\nbenchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly matching the performance of\n40\nstate-of-the-art \ufb01ne-tuned systems, as well as generating high-quality samples and strong qualitative performance at\ntasks de\ufb01ned on-the-\ufb02y. We documented roughly predictable trends of scaling in performance without using \ufb01ne-tuning.\nWe also discussed the social impacts of this class of model. Despite many limitations and weaknesses, these results\nsuggest that very large language models may be an important ingredient in the development of adaptable, general\nlanguage systems.\nAcknowledgements\nThe authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks to Jakub\nPachocki and Szymon Sidor for suggesting tasks, and Greg Brockman, Michael Petrov, Brooke Chan, and Chelsea\nV oss for helping run evaluations on OpenAI\u2019s infrastructure. Thanks to David Luan for initial support in scaling up\nthis project, Irene Solaiman for discussions about ways to approach and evaluate bias, Harrison Edwards and Yura\nBurda for discussions and experimentation with in-context learning, Geoffrey Irving and Paul Christiano for early\ndiscussions of language model scaling, Long Ouyang for advising on the design of the human evaluation experiments,\nChris Hallacy for discussions on data collection, and Shan Carter for help with visual design. Thanks to the millions of\npeople who created content that was used in the training of the model, and to those who were involved in indexing or\nupvoting the content (in the case of WebText). Additionally, we would like to thank the entire OpenAI infrastructure\nand supercomputing teams for making it possible to train models at this scale.\n41\nContributions\nTom Brown, Ben Mann, Prafulla Dhariwal, Dario Amodei, Nick Ryder, Daniel M Ziegler, and Jeffrey Wu\nimplemented the large-scale models, training infrastructure, and model-parallel strategies.\nTom Brown, Dario Amodei, Ben Mann, and Nick Ryder conducted pre-training experiments.\nBen Mann and Alec Radford collected, \ufb01ltered, deduplicated, and conducted overlap analysis on the training data.\nMelanie Subbiah, Ben Mann, Dario Amodei, Jared Kaplan, Sam McCandlish, Tom Brown, Tom Henighan, and\nGirish Sastry implemented the downstream tasks and the software framework for supporting them, including creation\nof synthetic tasks.\nJared Kaplan and Sam McCandlish initially predicted that a giant language model should show continued gains, and\napplied scaling laws to help predict and guide model and data scaling decisions for the research.\nBen Mann implemented sampling without replacement during training.\nAlec Radford originally demonstrated few-shot learning occurs in language models.\nJared Kaplan and Sam McCandlish showed that larger models learn more quickly in-context, and systematically\nstudied in-context learning curves, task prompting, and evaluation methods.\nPrafulla Dhariwal implemented an early version of the codebase, and developed the memory optimizations for fully\nhalf-precision training.\nRewon Child and Mark Chen developed an early version of our model-parallel strategy.\nRewon Child and Scott Gray contributed the sparse transformer.\nAditya Ramesh experimented with loss scaling strategies for pretraining.\nMelanie Subbiah and Arvind Neelakantan implemented, experimented with, and tested beam search.\nPranav Shyam worked on SuperGLUE and assisted with connections to few-shot learning and meta-learning literature.\nSandhini Agarwal conducted the fairness and representation analysis.\nGirish Sastry and Amanda Askell conducted the human evaluations of the model.\nAriel Herbert-Voss conducted the threat analysis of malicious use.\nGretchen Krueger edited and red-teamed the policy sections of the paper.\nBenjamin Chess, Clemens Winter, Eric Sigler, Christopher Hesse, Mateusz Litwin, and Christopher Berner\noptimized OpenAI\u2019s clusters to run the largest models ef\ufb01ciently.\nScott Gray developed fast GPU kernels used during training.\nJack Clark led the analysis of ethical impacts \u2014 fairness and representation, human assessments of the model, and\nbroader impacts analysis, and advised Gretchen, Amanda, Girish, Sandhini, and Ariel on their work.\nDario Amodei, Alec Radford, Tom Brown, Sam McCandlish, Nick Ryder, Jared Kaplan, Sandhini Agarwal,\nAmanda Askell, Girish Sastry, and Jack Clark wrote the paper.\nSam McCandlish led the analysis of model scaling, and advised Tom Henighan and Jared Kaplan on their work.\nAlec Radford advised the project from an NLP perspective, suggested tasks, put the results in context, and demonstrated\nthe bene\ufb01t of weight decay for training.\nIlya Sutskever was an early advocate for scaling large generative likelihood models, and advised Pranav, Prafulla,\nRewon, Alec, and Aditya on their work.\nDario Amodei designed and led the research.\n42\nA Details of Common Crawl Filtering\nAs mentioned in Section 2.2, we employed two techniques to improve the quality of the Common Crawl dataset: (1)\n\ufb01ltering Common Crawl and (2) fuzzy deduplication:\n1.In order to improve the quality of Common Crawl, we developed an automatic \ufb01ltering method to remove low\nquality documents. Using the original WebText as a proxy for high-quality documents, we trained a classi\ufb01er\nto distinguish these from raw Common Crawl. We then used this classi\ufb01er to re-sample Common Crawl by\nprioritizing documents which were predicted by the classi\ufb01er to be higher quality. The classi\ufb01er is trained\nusing logistic regression classi\ufb01er with features from Spark\u2019s standard tokenizer and HashingTF10. For the\npositive examples, we used a collection of curated datasets such as WebText, Wikiedia, and our web books\ncorpus as the positive examples, and for the negative examples, we used un\ufb01ltered Common Crawl. We used\nthis classi\ufb01er to score Common Crawl documents. We kept each document in our dataset iff\nnp.random.pareto (\u000b)>1\u0000document_score\nWe chose\u000b= 9in order to take mostly documents the classi\ufb01er scored highly, but still include some documents\nthat were out of distribution. \u000bwas chosen to match the distribution of scores from our classi\ufb01er on WebText.\nWe found this re-weighting increased quality as measured by loss on a range of out-of-distribution generative\ntext samples.\n2.To further improve model quality and prevent over\ufb01tting (which becomes increasingly important as model\ncapacity increases), we fuzzily deduplicated documents (i.e. removed documents with high overlap with\nother documents) within each dataset using Spark\u2019s MinHashLSH implementation with 10 hashes, using the\nsame features as were used for classi\ufb01cation above. We also fuzzily removed WebText from Common Crawl.\nOverall this decreased dataset size by an average of 10%.\nAfter \ufb01ltering for duplicates and quality, we also partially removed text occurring in benchmark datasets, described in\nAppendix C.\nB Details of Model Training\nTo train all versions of GPT-3, we use Adam with \f1= 0:9,\f2= 0:95, and\u000f= 10\u00008, we clip the global norm of the\ngradient at 1.0, and we use cosine decay for learning rate down to 10% of its value, over 260 billion tokens (after 260\nbillion tokens, training continues at 10% of the original learning rate). There is a linear LR warmup over the \ufb01rst 375\nmillion tokens. We also gradually increase the batch size linearly from a small value (32k tokens) to the full value over\nthe \ufb01rst 4-12 billion tokens of training, depending on the model size. Data are sampled without replacement during\ntraining (until an epoch boundary is reached) to minimize over\ufb01tting. All models use weight decay of 0.1 to provide a\nsmall amount of regularization [LH17].\nDuring training we always train on sequences of the full nctx= 2048 token context window, packing multiple\ndocuments into a single sequence when documents are shorter than 2048, in order to increase computational ef\ufb01ciency.\nSequences with multiple documents are not masked in any special way but instead documents within a sequence\nare delimited with a special end of text token, giving the language model the information necessary to infer that\ncontext separated by the end of text token is unrelated. This allows for ef\ufb01cient training without need for any special\nsequence-speci\ufb01c masking.\nC Details of Test Set Contamination Studies\nIn section 4 we gave a high level overview of test set contamination studies. In this section we provide details on\nmethodology and results.\nInitial training set \ufb01ltering We attempted to remove text occurring in benchmarks from training data by searching\nfor13\u0000gram overlaps between all test/development sets used in this work and our training data, and we removed\nthe colliding 13\u0000gram as well as a 200 character window around it, splitting the original document into pieces. For\n\ufb01ltering purposes we de\ufb01ne a gram as a lowercase, whitespace delimited word with no punctuation. Pieces less than\n200characters long were discarded. Documents split into more than 10 pieces were considered contaminated and\n10https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.HashingTF\n43\nremoved entirely. Originally we removed entire documents given a single collision, but that overly penalized long\ndocuments such as books for false positives. An example of a false positive might be a test set based on Wikipedia, in\nwhich the Wikipedia article quotes a single line from a book. We ignored 13\u0000grams that matched more than 10 training\ndocuments, as inspection showed the majority of these to contain common cultural phrases, legal boilerplate, or similar\ncontent that we likely do want the model to learn, rather than undesired speci\ufb01c overlaps with test sets. Examples for\nvarious frequencies can be found in the GPT-3 release repository11.\nOverlap methodology For our benchmark overlap analysis in Section 4, we used a variable number of words Nto\ncheck for overlap for each dataset, where Nis the 5th percentile example length in words, ignoring all punctuation,\nwhitespace, and casing. Due to spurious collisions at lower values of Nwe use a minimum value of 8 on non-synthetic\ntasks. For performance reasons, we set a maximum value of 13 for all tasks. Values for Nand the amount of data\nmarked as dirty are shown in Table C.1. Unlike GPT-2\u2019s use of bloom \ufb01lters to compute probabilistic bounds for test\ncontamination, we used Apache Spark to compute exact collisions across all training and test sets. We compute overlaps\nbetween test sets and our full training corpus, even though we only trained on 40% of our \ufb01ltered Common Crawl\ndocuments per Section 2.2.\nWe de\ufb01ne a \u2018dirty\u2019 example as one with any N-gram overlap with any training document, and a \u2018clean\u2019 example as one\nwith no collision.\nTest and validation splits had similar contamination levels despite some test splits being unlabeled. Due to a bug revealed\nby this analysis, \ufb01ltering described above failed on long documents such as books. Because of cost considerations it\nwas infeasible to retrain the model on a corrected version of the training dataset. As such, several language modeling\nbenchmarks plus the Children\u2019s Book Test showed almost complete overlap, and therefore were not included in this\npaper. Overlaps are shown in Table C.1\nOverlap results To understand how much having seen some of the data helps the model perform on downstream\ntasks, we \ufb01lter every validation and test set by dirtiness. Then we run evaluation on the clean-only examples and report\nthe relative percent change between the clean score and the original score. If the clean score is more than 1% or 2%\nworse than the overall score, it suggests the model may have over\ufb01t to the examples it has seen. If the clean score is\nsigni\ufb01cantly better , our \ufb01ltering scheme may have preferentially marked easier examples as dirty.\nThis overlap metric tends to show a high rate of false positives for datasets that contain background information (but\nnot answers) drawn from the web (such as SQuAD, which draws from Wikipedia) or examples less than 8 words\nlong, which we ignored in our \ufb01ltering process (except for wordscrambling tasks). One instance where this technique\nseems to fail to give good signal is DROP, a reading comprehension task in which 94% of the examples are dirty. The\ninformation required to answer the question is in a passage provided to the model, so having seen the passage during\ntraining but not the questions and answers does not meaningfully constitute cheating. We con\ufb01rmed that every matching\ntraining document contained only the source passage, and none of the questions and answers in the dataset. The more\nlikely explanation for the decrease in performance is that the 6% of examples that remain after \ufb01ltering come from a\nslightly different distribution than the dirty examples.\nFigure 4.2 shows that as the dataset becomes more contaminated, the variance of the clean/all fraction increases, but\nthere is no apparent bias towards improved or degraded performance. This suggests that GPT-3 is relatively insensitive\nto contamination. See Section 4 for details on the datasets we \ufb02agged for further review.\n11https://github.com/openai/gpt-3/blob/master/overlap_frequency.md\n44\nName Split Metric N Acc/F1/BLEUTotal\nCountDirty\nAcc/F1/BLEUDirty\nCountClean\nAcc/F1/BLEUClean\nCountClean\nPercentageRelative\nDifference\nClean vs All\nQuac dev f1 13 44.3 7353 44.3 7315 54.1 38 1% 20%\nSQuADv2 dev f1 13 69.8 11873 69.9 11136 68.4 737 6% -2%\nDROP dev f1 13 36.5 9536 37.0 8898 29.5 638 7% -21%\nSymbol Insertion dev acc 7 66.9 10000 66.8 8565 67.1 1435 14% 0%\nCoQa dev f1 13 86.0 7983 85.3 5107 87.1 2876 36% 1%\nReCoRD dev acc 13 89.5 10000 90.3 6110 88.2 3890 39% -1%\nWinograd test acc 9 88.6 273 90.2 164 86.2 109 40% -3%\nBoolQ dev acc 13 76.0 3270 75.8 1955 76.3 1315 40% 0%\nMultiRC dev acc 13 74.2 953 73.4 558 75.3 395 41% 1%\nRACE-h test acc 13 46.8 3498 47.0 1580 46.7 1918 55% 0%\nLAMBADA test acc 13 86.4 5153 86.9 2209 86.0 2944 57% 0%\nLAMBADA (No Blanks) test acc 13 77.8 5153 78.5 2209 77.2 2944 57% -1%\nWSC dev acc 13 76.9 104 73.8 42 79.0 62 60% 3%\nPIQA dev acc 8 82.3 1838 89.9 526 79.3 1312 71% -4%\nRACE-m test acc 13 58.5 1436 53.0 366 60.4 1070 75% 3%\nDe!En 16 test bleu-sb 12 43.0 2999 47.4 739 40.8 2260 75% -5%\nEn!De 16 test bleu-sb 12 30.9 2999 32.6 739 29.9 2260 75% -3%\nEn!Ro 16 test bleu-sb 12 25.8 1999 24.9 423 26.1 1576 79% 1%\nRo!En 16 test bleu-sb 12 41.3 1999 40.4 423 41.6 1576 79% 1%\nWebQs test acc 8 41.5 2032 41.6 428 41.5 1604 79% 0%\nANLI R1 test acc 13 36.8 1000 40.5 200 35.9 800 80% -3%\nANLI R2 test acc 13 34.0 1000 29.4 177 35.0 823 82% 3%\nTriviaQA dev acc 10 71.2 7993 70.8 1390 71.3 6603 83% 0%\nANLI R3 test acc 13 40.2 1200 38.3 196 40.5 1004 84% 1%\nEn!Fr 14 test bleu-sb 13 39.9 3003 38.3 411 40.3 2592 86% 1%\nFr!En 14 test bleu-sb 13 41.4 3003 40.9 411 41.4 2592 86% 0%\nWiC dev acc 13 51.4 638 53.1 49 51.3 589 92% 0%\nRTE dev acc 13 71.5 277 71.4 21 71.5 256 92% 0%\nCB dev acc 13 80.4 56 100.0 4 78.8 52 93% -2%\nAnagrams 2 dev acc 2 40.2 10000 76.2 705 37.4 9295 93% -7%\nReversed Words dev acc 2 0.4 10000 1.5 660 0.3 9340 93% -26%\nOpenBookQA test acc 8 65.4 500 58.1 31 65.9 469 94% 1%\nARC (Easy) test acc 11 70.1 2268 77.5 89 69.8 2179 96% 0%\nAnagrams 1 dev acc 2 15.0 10000 49.8 327 13.8 9673 97% -8%\nCOPA dev acc 9 93.0 100 100.0 3 92.8 97 97% 0%\nARC (Challenge) test acc 12 51.6 1144 45.2 31 51.8 1113 97% 0%\nHellaSwag dev acc 13 79.3 10042 86.2 152 79.2 9890 98% 0%\nNQs test acc 11 29.9 3610 32.7 52 29.8 3558 99% 0%\nCycled Letters dev acc 2 38.6 10000 20.5 73 38.7 9927 99% 0%\nSAT Analogies dev acc 9 65.8 374 100.0 2 65.6 372 99% 0%\nStoryCloze test acc 13 87.7 1871 100.0 2 87.6 1869 100% 0%\nWinogrande dev acc 13 77.7 1267 - 0 77.7 1267 100% 0%\nTable C.1: Overlap statistics for all datasets sorted from dirtiest to cleanest. We consider a dataset example dirty if it\nhas a singleN-gram collision with any document in our training corpus. \u201cRelative Difference Clean vs All\u201d shows the\npercent change in performance between only the clean examples vs all the examples in the benchmark. \u201cCount\u201d shows\nthe number of examples. \u201cClean percentage\u201d is the percent of examples that are clean vs total. For \u201cAcc/F1/BLEU\u201d we\nuse the metric speci\ufb01ed in \u201cMetric\u201d. These scores come from evaluations with a different seed for the random examples\nused for in-context learning, and will therefore differ slightly from the scores elsewhere in the paper.\n45\nD Total Compute Used to Train Language Models\nThis appendix contains the calculations that were used to derive the approximate compute used to train the language\nmodels in Figure 2.2. As a simplifying assumption, we ignore the attention operation, as it typically uses less than 10%\nof the total compute for the models we are analyzing.\nCalculations can be seen in Table D.1 and are explained within the table caption.\nModelTotal train\ncompute\n(PF-days)Total train\ncompute\n(\ufb02ops)Params\n(M)Training tokens\n(billions)Flops\nper param\nper tokenMult for\nbwd passFwd-pass\n\ufb02ops per\nactive param\nper tokenFrac of\nparams active\nfor each\ntoken\nT5-Small 2.08E+00 1.80E+20 60 1,000 3 3 1 0.5\nT5-Base 7.64E+00 6.60E+20 220 1,000 3 3 1 0.5\nT5-Large 2.67E+01 2.31E+21 770 1,000 3 3 1 0.5\nT5-3B 1.04E+02 9.00E+21 3,000 1,000 3 3 1 0.5\nT5-11B 3.82E+02 3.30E+22 11,000 1,000 3 3 1 0.5\nBERT-Base 1.89E+00 1.64E+20 109 250 6 3 2 1.0\nBERT-Large 6.16E+00 5.33E+20 355 250 6 3 2 1.0\nRoBERTa-Base 1.74E+01 1.50E+21 125 2,000 6 3 2 1.0\nRoBERTa-Large 4.93E+01 4.26E+21 355 2,000 6 3 2 1.0\nGPT-3 Small 2.60E+00 2.25E+20 125 300 6 3 2 1.0\nGPT-3 Medium 7.42E+00 6.41E+20 356 300 6 3 2 1.0\nGPT-3 Large 1.58E+01 1.37E+21 760 300 6 3 2 1.0\nGPT-3 XL 2.75E+01 2.38E+21 1,320 300 6 3 2 1.0\nGPT-3 2.7B 5.52E+01 4.77E+21 2,650 300 6 3 2 1.0\nGPT-3 6.7B 1.39E+02 1.20E+22 6,660 300 6 3 2 1.0\nGPT-3 13B 2.68E+02 2.31E+22 12,850 300 6 3 2 1.0\nGPT-3 175B 3.64E+03 3.14E+23 174,600 300 6 3 2 1.0\nTable D.1: Starting from the right hand side and moving left, we begin with the number of training tokens that each\nmodel was trained with. Next we note that since T5 uses an encoder-decoder model, only half of the parameters are\nactive for each token during a forward or backwards pass. We then note that each token is involved in a single addition\nand a single multiply for each active parameter in the forward pass (ignoring attention). Then we add a multiplier of\n3x to account for the backwards pass (as computing both@params\n@lossand@acts\n@lossuse a similar amount of compute as the\nforwards pass. Combining the previous two numbers, we get the total \ufb02ops per parameter per token. We multiply this\nvalue by the total training tokens and the total parameters to yield the number of total \ufb02ops used during training. We\nreport both \ufb02ops and peta\ufb02op/s-day (each of which are 8.64e+19 \ufb02ops).\nE Human Quality Assessment of Synthetic News Articles\nThis appendix contains details on the experiments measuring human ability to distinguish GPT-3-generated synthetic\nnews articles from real news articles. We \ufb01rst describe the experiments on the \u0018200word news articles, and then\ndescribe the preliminary investigation of \u0018500word news articles generated by GPT-3.\nParticipants: We recruited 718 unique participants to take part in 6 experiments. 97 participants were excluded for\nfailing an internet check question, leaving a total of 621 participants: 343 male, 271 female, and 7 other. Mean\nparticipant age was \u001838years old. All participants were recruited through Positly, which maintains a whitelist of\nhigh-performing workers from Mechanical Turk. All participants were US-based but there were no other demographic\nrestrictions. Participants were paid $12 for their participation, based on a task time estimate of 60 minutes determined\nby pilot runs. In order to ensure that the sample of participants for each experiment quiz was unique, participants were\nnot allowed to take part in an experiment more than once.\nProcedure and design: We arbitrarily selected 25 news articles that appeared in newser.com in early 2020. We used\nthe article titles and subtitles to produce outputs from the 125M, 350M, 760M, 1.3B, 2.7B, 6.7B, 13.0B, and 200B\n(GPT-3) parameter language models. Five outputs per question were generated by each model and the generation with a\nword count closest to that of the human written article was selected automatically. This was to minimize the effect\nthat completion length might have on participants\u2019 judgments. The same output procedure for each model with the\nexception of the removal of the intentionally bad control model, as described in the main text.\n46\nModelParticipants\nRecruitedParticipants\nExcludedGenders\n(m:f:other)Mean\nAgeAverage\nWord Count\n(human:model)\nControl 76 7 32:37:0 39 216:216\nGPT-3 Small 80 7 41:31:1 40 216:188\nGPT-3 Medium 80 7 46:28:2 39 216:202\nGPT-3 Large 81 24 46:28:2 37 216:200\nGPT-3 XL 79 14 32:32:1 38 216:199\nGPT-3 2.7B 80 11 36:33:0 40 216:202\nGPT-3 6.7B 76 5 46:28:2 37 216:195\nGPT-3 13.0B 81 13 46:28:2 37 216:209\nGPT-3 175B 80 9 42:29:0 37 216:216\nTable E.1: Participant details and article lengths for each experiment to evaluate human detection of \u0018200word model\ngenerated news articles. Participants were excluded due to internet check fails.\nFigure E.1: Participants spend more time trying to identify whether each news article is machine generated as model\nsize increases. Duration on the control model is indicated with the dashed line. Line of best \ufb01t is a linear model on a log\nscale with 95% con\ufb01dence intervals.\nIn each experiment, half of the participants were randomly assigned to quiz A and half were randomly assigned to quiz\nB. Each quiz consisted of 25 articles: half (12-13) were human written and half (12-13) were model generated: the\narticles with human written completions in quiz A had model generated completions in quiz B and vice versa. The\norder of quiz question was shuf\ufb02ed for each participant. Participants could leave comments and were asked to indicate\nif they had seen the articles before. Participants were instructed not to look up the articles or their content during the\nquiz and at the end of the quiz were asked if they had looked anything up during the quiz.\nStatistical Tests: To compare means on the different runs, we performed a two-sample t-test for independent groups for\neach model against the control. This was implemented in Python using the scipy.stats.ttest_ind function. When\nplotting a regression line in the graph of average participant accuracy vs model size, we \ufb01t a power law of the form\nax\u0000b. The 95% con\ufb01dence intervals were estimated from the t-distribution of the sample mean.\nDuration statistics : In the main text, we discussed the \ufb01nding that the ability of human participants to distinguish\nmodel and human generated news articles decreases as our models become larger. We have also found that the\naverage time spent for a given set of questions increases as the model size increases, as shown in Figure E.1. Lower\n47\nModelParticipants\nRecruitedParticipants\nExcludedGenders\n(m:f:other)Mean\nAgeAverage\nWord Count\n(human:model)\nControl 79 17 32:37:0 39 569:464\nGPT-3 175B 81 19 32:30:0 40 569:498\nTable E.2: Participant details and article lengths for the experiments investigating human detection of \u0018500word\nmodel generated news articles. Participants were excluded due to internet check fails.\naccuracy scores despite increased time investment from participants supports the \ufb01nding that larger models generate\nharder-to-distinguish news articles.\nPreliminary investigation of \u0018500word articles: We recruited 160 unique US-based participants to take part in 2\nexperiments through Positly (details are given in Table E.2). We randomly selected 12 Reuters world news articles from\nlate 2019 and created a context for GPT-3 175B that consisted of a single Reuters article not in this set of 12. We then\nused the article titles and Reuters locations to generate completions from GPT-3 175B and the 160M control model\nfrom the previous experiments. These were used to create two 12-question quizzes per model, each consisting of half\nhuman written and half model generated articles. Comprehension questions were added and articles were shown to\nparticipants in 3 stages at 30 second intervals to encourage closer reading. Participants were paid $12 for this task.\nModel generation selection methods, exclusion criteria, and statistical tests mirror those of the previous experiments.\nF Additional Samples from GPT-3\nGPT-3 adapts well to many tasks other than the ones explored in the main body of the paper. As an example, in Figure\nF.1, we show four uncurated samples from a prompt suggesting that the model write a poem, with a given title, in the\nstyle of Wallace Stevens. We \ufb01rst experimented with a few prompts, then generated four samples with no additional\nediting or selection (sampling at temperature 1using nucleus sampling [ HBFC19 ] withP= 0:9). Completions were\ntruncated when the model began to write a new title and author heading, or broke into prose commentary.\n48\nContext! The City\nBY C. P. CAVAFY\nTRANSLATED BY EDMUND KEELEY\n[Poem text omitted]\nSOME TREES\nJohn Ashbery\n[Poem text omitted]\nShadows on the Way\nWallace Stevens\n-------- Generated Poem 1 -------- -------- Generated Poem 3 --------\nI must have shadows on the way The sun was all we had. Now, in the shade\nIf I am to walk I must have All is changed. The mind must dwell on those\nEach step taken slowly and alone White fields, that to its eyes were always old;\nTo have it ready made Those ancient gleams, convoluting\nThe way they lay among the huge roots,\nAnd I must think in lines of grey The great dark tomes of reverie,\nTo have dim thoughts to be my guide The plumed labyrinth of the sea.\nMust look on blue and green\nAnd never let my eye forget When our own breath gonne faint and fade\nThat color is my friend Up the long stairway of the dead\nAnd purple must surround me too We shall remember well\nThe blindness of the light, the whiteness\nThe yellow of the sun is no more Of that white land. The footsteps, and\nIntrusive than the bluish snow The grasses in that light, and in the shade\nThat falls on all of us. I must have The menacing life.\nGrey thoughts and blue thoughts walk with me\nIf I am to go away at all. We played, too, once, in the warmest rooms.\nTo one content, there is one content\n-------- Generated Poem 2 -------- For all mankind. The forest glades\nAre all the more for being in fable,\nThere is a way in the world. And wood and town in legend shadowed-\nThat way a man may live in a small town Once more to return. The one content\nHis lifetime and see no man come, Has in it every happiness, more brave\nNever the stir of men about a market, For being purposeless; and so\nNever the bobbing of covered carts, Those island gardens, deep in the mind,\nNever the creak passed on the stair Make of remoteness familiarity\nClosing some other life; an old man And of the unspoken name a friend.\nCan prosper in that town, can prosper\nWhen his store is seen to close, his wife -------- Generated Poem 4 --------\nFades in his house, and the world shouts in vain\nThat no one comes to hear him Nobody will come to this place. It is a road that leads nowhere.\nWhere his small bell tolls. The solitude is deep. The mountains are high.\nIn that small town But they are desolate, and they turn the traveler's face\nEverything is known. Towards the North. All the sounds of the world are far away.\nThe ways of the young men are known, When the wind rises above the trees,\nAnd the ways of the young women The boughs bow to the ground.\nAs they walk to the Spring; Even the birds that inhabit the tangle of weeds\nThe rumors are all known, That is the roadside cover, are silent. One listens,\nThe quarrels, the hurt, the envy But hears no roar of the forest. One is alone.\nOf the men and the women. One will be taken.\nTill a day comes One will be taken.\nThat way out of the world There is no utterance, there is no conversation,\nThat leads to a wood. But one is uneasy all the same....\nYoung, the young man There is a thin blue mist,\nWalks, a lover now, A darkness rising like smoke,\nKnows the streets as they are and is of them. And within that darkness\nA possession of the heart.\nComes to the wood. One will be taken.... It was here, and it will be here again-\nHere, under this sky empty and full of light.\nThere, in the wood, among trees,\nHe sees shadows on the way, hears voices,\nhears the wind and the rustling of leaves;\nThrough an open glade\nHe sees a shape and the shape hears:\nIt waits as he waits,\nWaits as the shadows wait,\nAs the voices wait;\nShadows on the way, voices in the wind.\nFigure F.1: Four uncurated completions from a context suggesting the model compose a poem in the style of Wallace\nStevens with the title \u2018Shadows on the Way\u2019.\n49\nG Details of Task Phrasing and Speci\ufb01cations\nThe following \ufb01gures illustrate the formatting and phrasing of all the tasks included in the paper. All data comes from\nthe ground truth datasets in this section, and no samples from GPT-3 are included here.\nContext!Article:\nInformal conversation is an important part of any business\nrelationship.Before you start a discussion,however,make sure you understand\nwhich topics are suitable and which are considered taboo in a particular\nculture. Latin Americans enjoy sharing information about their local\nhistory, art and customs.You may expect questions about your family,and\nbe sure to show pictures of your children.You may feel free to ask similar\nquestions of your Latin American friends.The French think of conversation\nas an art form,and they enjoy the value of lively discussions as well as\ndisagreements. For them,arguments can be interesting and they can cover\npretty much or any topic ---- as long as they occur in are respectful and\nintelligent manner.\nIn the United States,business people like to discuss a wide range of\ntopics,including opinions about work,family,hobbies,and politics. In\nJapan,China,and Korea,however,people are much more private.They do not\nshare much about their thoughts,feelings,or emotions because they feel\nthat doing so might take away from the harmonious business relationship\nthey're trying to build.Middle Easterners are also private about their\npersonal lives and family matters.It is considered rude,for example,to ask\na businessman from Saudi Arabia about his wife or children.\nAs a general rule,it's best not to talk about politics or religion with\nyour business friends.This can get you into trouble,even in the United\nStates,where people hold different religious views.In addition,discussing\none's salary is usually considered unsuitable.Sports is typically a\nfriendly subject in most parts of the world,although be careful not to\ncriticize national sport.Instead,be friendly and praise your host's team.\nQ: What shouldn't you do when talking about sports with colleagues from\nanother country?\nA: Criticizing the sports of your colleagues' country.\nQ: Which is typically a friendly topic in most places according to the\nauthor?\nA: Sports.\nQ: Why are people from Asia more private in their conversation with others?\nA: They don't want to have their good relationship with others harmed by\ninformal conversation.\nQ: The author considers politics and religion .\nA:\nCorrect Answer !taboo\nIncorrect Answer !cheerful topics\nIncorrect Answer !rude topics\nIncorrect Answer !topics that can never be talked about\nFigure G.1: Formatted dataset example for RACE-h. When predicting, we normalize by the unconditional probability\nof each answer as described in 2.\n50\nContext!anli 2: anli 2: The Gold Coast Hotel & Casino is a hotel and casino\nlocated in Paradise, Nevada. This locals' casino is owned and operated\nby Boyd Gaming. The Gold Coast is located one mile ( \u0018 1:6km) west of the\nLas Vegas Strip on West Flamingo Road. It is located across the street\nfrom the Palms Casino Resort and the Rio All Suite Hotel and Casino.\nQuestion: The Gold Coast is a budget-friendly casino. True, False, or\nNeither?\nCorrect Answer !Neither\nIncorrect Answer !True\nIncorrect Answer !False\nFigure G.2: Formatted dataset example for ANLI R2\nContext!Article:\nMrs. Smith is an unusual teacher. Once she told each student to bring\nalong a few potatoes in plastic bag. On each potato the students had to\nwrite a name of a person that they hated And the next day, every child\nbrought some potatoes. Some had two potatoes;some three;some up to five.\nMrs. Smith then told the children to carry the bags everywhere they went,\neven to the toilet, for two weeks. As day after day passed, the children\nstarted to complain about the awful smell of the rotten potatoes.\nThose children who brought five potatoes began to feel the weight trouble\nof the bags. After two weeks, the children were happy to hear that the\ngame was finally ended. Mrs. Smith asked,\"How did you feel while carrying\nthe potatoes for two weeks?\" The children started complaining about the\ntrouble loudly.\nThen Mrs. Smith told them why she asked them to play the game. She\nsaid,\"This is exactly the situation when you carry your hatred for somebody\ninside your heart. The terrible smell of the hatred will pollute your\nheart and you will carry something unnecessary with you all the time. If\nyou cannot stand the smell of the rotten potatoes for just two weeks, can\nyou imagine how heavy it would be to have the hatred in your heart for your\nlifetime? So throw away any hatred from your heart, and you'll be really\nhappy.\"\nQ: Which of the following is True according to the passage?\nA: If a kid hated four people,he or she had to carry four potatoes.\nQ: We can learn from the passage that we should .\nA: throw away the hatred inside\nQ: The children complained about besides the weight trouble.\nA: the smell\nQ: Mrs.Smith asked her students to write on the potatoes.\nA:\nCorrect Answer !names\nIncorrect Answer !numbers\nIncorrect Answer !time\nIncorrect Answer !places\nFigure G.3: Formatted dataset example for RACE-m. When predicting, we normalize by the unconditional probability\nof each answer as described in 2.\n51\nContext!How to apply sealant to wood.\nCorrect Answer !Using a brush, brush on sealant onto wood until it is fully saturated with\nthe sealant.\nIncorrect Answer !Using a brush, drip on sealant onto wood until it is fully saturated with\nthe sealant.\nFigure G.4: Formatted dataset example for PIQA\nContext!My body cast a shadow over the grass because\nCorrect Answer !the sun was rising.\nIncorrect Answer !the grass was cut.\nFigure G.5: Formatted dataset example for COPA\nContext!(CNN) Yuval Rabin, whose father, Yitzhak Rabin, was assassinated while\nserving as Prime Minister of Israel, criticized Donald Trump for appealing\nto \"Second Amendment people\" in a speech and warned that the words that\npoliticians use can incite violence and undermine democracy. \"Trump's\nwords are an incitement to the type of political violence that touched\nme personally,\" Rabin wrote in USAToday. He said that Trump's appeal to\n\"Second Amendment people\" to stop Hillary Clinton -- comments that were\ncriticized as a call for violence against Clinton, something Trump denied\n-- \"were a new level of ugliness in an ugly campaign season.\"\n- The son of a former Israeli Prime Minister who was assassinated wrote an\nop ed about the consequence of violent political rhetoric.\n- Warns of \"parallels\" between Israel of the 1990s and the U.S. today.\nCorrect Answer !- Referencing his father, who was shot and killed by an extremist amid\npolitical tension in Israel in 1995, Rabin condemned Donald Trump's\naggressive rhetoric.\nCorrect Answer !- Referencing his father, who was shot and killed by an extremist amid\npolitical tension in Israel in 1995, Rabin condemned Trump's aggressive\nrhetoric.\nIncorrect Answer !- Referencing his father, who was shot and killed by an extremist amid\npolitical tension in Israel in 1995, Rabin condemned Hillary Clinton's\naggressive rhetoric.\nIncorrect Answer !- Referencing his father, who was shot and killed by an extremist amid\npolitical tension in Israel in 1995, Rabin condemned U.S.'s aggressive\nrhetoric.\nIncorrect Answer !- Referencing his father, who was shot and killed by an extremist amid\npolitical tension in Israel in 1995, Rabin condemned Yitzhak Rabin's\naggressive rhetoric.\nFigure G.6: Formatted dataset example for ReCoRD. We consider the context above to be a single \u201dproblem\u201d because\nthis is how the task is presented in the ReCoRD dataset and scored in the ReCoRD evaluation script.\nContext!anli 1: anli 1: Fulton James MacGregor MSP is a Scottish politician\nwho is a Scottish National Party (SNP) Member of Scottish Parliament\nfor the constituency of Coatbridge and Chryston. MacGregor is currently\nParliamentary Liaison Officer to Shona Robison, Cabinet Secretary for\nHealth & Sport. He also serves on the Justice and Education & Skills\ncommittees in the Scottish Parliament.\nQuestion: Fulton James MacGregor is a Scottish politican who is a Liaison\nofficer to Shona Robison who he swears is his best friend. True, False, or\nNeither?\nCorrect Answer !Neither\nIncorrect Answer !True\nIncorrect Answer !False\nFigure G.7: Formatted dataset example for ANLI R1\n52\nContext!Organisms require energy in order to do what?\nCorrect Answer !mature and develop.\nIncorrect Answer !rest soundly.\nIncorrect Answer !absorb light.\nIncorrect Answer !take in nutrients.\nFigure G.8: Formatted dataset example for OpenBookQA. When predicting, we normalize by the unconditional\nprobability of each answer as described in 2.\nContext!Making a cake: Several cake pops are shown on a display. A woman and girl\nare shown making the cake pops in a kitchen. They\nCorrect Answer !bake them, then frost and decorate.\nIncorrect Answer !taste them as they place them on plates.\nIncorrect Answer !put the frosting on the cake as they pan it.\nIncorrect Answer !come out and begin decorating the cake as well.\nFigure G.9: Formatted dataset example for HellaSwag\nContext!anli 3: anli 3: We shut the loophole which has American workers actually\nsubsidizing the loss of their own job. They just passed an expansion of\nthat loophole in the last few days: $43 billion of giveaways, including\nfavors to the oil and gas industry and the people importing ceiling fans\nfrom China.\nQuestion: The loophole is now gone True, False, or Neither?\nCorrect Answer !False\nIncorrect Answer !True\nIncorrect Answer !Neither\nFigure G.10: Formatted dataset example for ANLI R3\nContext!Question: George wants to warm his hands quickly by rubbing them. Which\nskin surface will produce the most heat?\nAnswer:\nCorrect Answer !dry palms\nIncorrect Answer !wet palms\nIncorrect Answer !palms covered with oil\nIncorrect Answer !palms covered with lotion\nFigure G.11: Formatted dataset example for ARC (Challenge). When predicting, we normalize by the unconditional\nprobability of each answer as described in 2.\nContext!lull is to trust as\nCorrect Answer !cajole is to compliance\nIncorrect Answer !balk is to fortitude\nIncorrect Answer !betray is to loyalty\nIncorrect Answer !hinder is to destination\nIncorrect Answer !soothe is to passion\nFigure G.12: Formatted dataset example for SAT Analogies\nCorrect Context !Grace was happy to trade me her sweater for my jacket. She thinks the\nsweater\nIncorrect Context !Grace was happy to trade me her sweater for my jacket. She thinks the\njacket\nTarget Completion !looks dowdy on her.\nFigure G.13: Formatted dataset example for Winograd. The \u2018partial\u2019 evaluation method we use compares the probability\nof the completion given a correct and incorrect context.\n53\nCorrect Context !Johnny likes fruits more than vegetables in his new keto diet because the\nfruits\nIncorrect Context !Johnny likes fruits more than vegetables in his new keto diet because the\nvegetables\nTarget Completion !are saccharine.\nFigure G.14: Formatted dataset example for Winogrande. The \u2018partial\u2019 evaluation method we use compares the\nprobability of the completion given a correct and incorrect context.\nContext!READING COMPREHENSION ANSWER KEY\nWhile this process moved along, diplomacy continued its rounds. Direct\npressure on the Taliban had proved unsuccessful. As one NSC staff note\nput it, \"Under the Taliban, Afghanistan is not so much a state sponsor\nof terrorism as it is a state sponsored by terrorists.\" In early 2000,\nthe United States began a high-level effort to persuade Pakistan to use\nits influence over the Taliban. In January 2000, Assistant Secretary\nof State Karl Inderfurth and the State Department's counterterrorism\ncoordinator, Michael Sheehan, met with General Musharraf in Islamabad,\ndangling before him the possibility of a presidential visit in March as a\nreward for Pakistani cooperation. Such a visit was coveted by Musharraf,\npartly as a sign of his government's legitimacy. He told the two envoys\nthat he would meet with Mullah Omar and press him on Bin Laden. They\nleft, however, reporting to Washington that Pakistan was unlikely in fact\nto do anything,\" given what it sees as the benefits of Taliban control\nof Afghanistan.\" President Clinton was scheduled to travel to India.\nThe State Department felt that he should not visit India without also\nvisiting Pakistan. The Secret Service and the CIA, however, warned in\nthe strongest terms that visiting Pakistan would risk the President's\nlife. Counterterrorism officials also argued that Pakistan had not done\nenough to merit a presidential visit. But President Clinton insisted\non including Pakistan in the itinerary for his trip to South Asia. His\none-day stopover on March 25, 2000, was the first time a U.S. president\nhad been there since 1969. At his meeting with Musharraf and others,\nPresident Clinton concentrated on tensions between Pakistan and India\nand the dangers of nuclear proliferation, but also discussed Bin Laden.\nPresident Clinton told us that when he pulled Musharraf aside for a brief,\none-on-one meeting, he pleaded with the general for help regarding Bin\nLaden.\" I offered him the moon when I went to see him, in terms of better\nrelations with the United States, if he'd help us get Bin Laden and deal\nwith another issue or two.\" The U.S. effort continued.\nWho did The State Department feel should visit both India and Pakistan?\nCorrect Answer !- [False] Bin Laden\nIncorrect Answer !- [True] Bin Laden\nFigure G.15: Formatted dataset example for MultiRC. There are three levels within MultiRC: (1) the passage, (2) the\nquestions, and (3) the answers. During evaluation, accuracy is determined at the per-question level, with a question\nbeing considered correct if and only if all the answers within the question are labeled correctly. For this reason, we use\nKto refer to the number of questions shown within the context.\nContext!Question: Which factor will most likely cause a person to develop a fever?\nAnswer:\nCorrect Answer !a bacterial population in the bloodstream\nIncorrect Answer !a leg muscle relaxing after exercise\nIncorrect Answer !several viral particles on the skin\nIncorrect Answer !carbohydrates being digested in the stomach\nFigure G.16: Formatted dataset example for ARC (Easy). When predicting, we normalize by the unconditional\nprobability of each answer as described in 2.\n54\nContext!Bob went to the gas station to fill up his car. His tank was completely\nempty and so was his wallet. The cashier offered to pay for his gas if he\ncame back later to pay. Bob felt grateful as he drove home.\nCorrect Answer !Bob believed that there were good people in the world.\nIncorrect Answer !Bob contemplated how unfriendly the world was.\nFigure G.17: Formatted dataset example for StoryCloze\nContext!Helsinki is the capital and largest city of Finland. It is in the region\nof Uusimaa, in southern Finland, on the shore of the Gulf of Finland.\nHelsinki has a population of , an urban population of , and a metropolitan\npopulation of over 1.4 million, making it the most populous municipality\nand urban area in Finland. Helsinki is some north of Tallinn, Estonia,\neast of Stockholm, Sweden, and west of Saint Petersburg, Russia. Helsinki\nhas close historical connections with these three cities.\nThe Helsinki metropolitan area includes the urban core of Helsinki, Espoo,\nVantaa, Kauniainen, and surrounding commuter towns. It is the world's\nnorthernmost metro area of over one million people, and the city is the\nnorthernmost capital of an EU member state. The Helsinki metropolitan\narea is the third largest metropolitan area in the Nordic countries\nafter Stockholm and Copenhagen, and the City of Helsinki is the third\nlargest after Stockholm and Oslo. Helsinki is Finland's major political,\neducational, financial, cultural, and research center as well as one of\nnorthern Europe's major cities. Approximately 75% of foreign companies\nthat operate in Finland have settled in the Helsinki region. The nearby\nmunicipality of Vantaa is the location of Helsinki Airport, with frequent\nservice to various destinations in Europe and Asia.\nQ: what is the most populous municipality in Finland?\nA: Helsinki\nQ: how many people live there?\nA: 1.4 million in the metropolitan area\nQ: what percent of the foreign companies that operate in Finland are in\nHelsinki?\nA: 75%\nQ: what towns are a part of the metropolitan area?\nA:\nTarget Completion !Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns\nFigure G.18: Formatted dataset example for CoQA\nContext!Please unscramble the letters into a word, and write that word:\nasinoc =\nTarget Completion !casino\nFigure G.19: Formatted dataset example for Cycled Letters\n55\nContext!Passage: Saint Jean de Br\u0013 ebeuf was a French Jesuit missionary who\ntravelled to New France in 1625. There he worked primarily with the Huron\nfor the rest of his life, except for a few years in France from 1629 to\n1633. He learned their language and culture, writing extensively about\neach to aid other missionaries. In 1649, Br\u0013 ebeuf and another missionary\nwere captured when an Iroquois raid took over a Huron village . Together\nwith Huron captives, the missionaries were ritually tortured and killed\non March 16, 1649. Br\u0013 ebeuf was beatified in 1925 and among eight Jesuit\nmissionaries canonized as saints in the Roman Catholic Church in 1930.\nQuestion: How many years did Saint Jean de Br\u0013 ebeuf stay in New France\nbefore he went back to France for a few years?\nAnswer:\nTarget Completion !4\nFigure G.20: Formatted dataset example for DROP\nContext!Fill in blank:\nShe held the torch in front of her.\nShe caught her breath.\n\"Chris? There's a step.\"\n\"What?\"\n\"A step. Cut in the rock. About fifty feet ahead.\" She moved faster.\nThey both moved faster. \"In fact,\" she said, raising the torch higher,\n\"there's more than a . ->\nTarget Completion !step\nFigure G.21: Formatted dataset example for LAMBADA\nContext!Please unscramble the letters into a word, and write that word:\nskicts =\nTarget Completion !sticks\nFigure G.22: Formatted dataset example for Anagrams 1 (A1)\nContext!Please unscramble the letters into a word, and write that word:\nvolwskagen =\nTarget Completion !volkswagen\nFigure G.23: Formatted dataset example for Anagrams 2\nContext!Q: Who played tess on touched by an angel?\nA:\nTarget Completion !Delloreese Patricia Early (July 6, 1931 { November 19, 2017), known\nprofessionally as Della Reese\nFigure G.24: Formatted dataset example for Natural Questions\n56\nContext!TITLE: William Perry (American football) - Professional career\nPARAGRAPH: In 1985, he was selected in the first round of the 1985 NFL\nDraft by the Chicago Bears; he had been hand-picked by coach Mike Ditka.\nHowever, defensive coordinator Buddy Ryan, who had a highly acrimonious\nrelationship with Ditka, called Perry a \"wasted draft-pick\". Perry\nsoon became a pawn in the political power struggle between Ditka and\nRyan. Perry's \"Refrigerator\" nickname followed him into the NFL and he\nquickly became a favorite of the Chicago Bears fans. Teammates called\nhim \"Biscuit,\" as in \"one biscuit shy of 350 pounds.\" While Ryan refused\nto play Perry, Ditka decided to use Perry as a fullback when the team was\nnear the opponents' goal line or in fourth and short situations, either\nas a ball carrier or a lead blocker for star running back Walter Payton.\nDitka stated the inspiration for using Perry as a fullback came to him\nduring five-yard sprint exercises. During his rookie season, Perry\nrushed for two touchdowns and caught a pass for one. Perry even had\nthe opportunity to run the ball during Super Bowl XX, as a nod to his\npopularity and contributions to the team's success. The first time he\ngot the ball, he was tackled for a one-yard loss while attempting to throw\nhis first NFL pass on a halfback option play. The second time he got the\nball, he scored a touchdown (running over Patriots linebacker Larry McGrew\nin the process). About halfway through his rookie season, Ryan finally\nbegan to play Perry, who soon proved that he was a capable defensive\nlineman. His Super Bowl ring size is the largest of any professional\nfootball player in the history of the event. His ring size is 25, while\nthe ring size for the average adult male is between 10 and 12. Perry went\non to play for ten years in the NFL, retiring after the 1994 season. In\nhis ten years as a pro, he regularly struggled with his weight, which\nhampered his performance at times. He played in 138 games, recording\n29.5 sacks and five fumble recoveries, which he returned for a total of\n71 yards. In his offensive career he ran five yards for two touchdowns,\nand had one reception for another touchdown. Perry later attempted a\ncomeback, playing an unremarkable 1996 season with the London Monarchs of\nthe World League of American Football (later NFL Europa).\nQ: what team did he play for?\nA:\nTarget Completion !the Chicago Bears\nFigure G.25: Formatted dataset example for QuAC\nContext!Please unscramble the letters into a word, and write that word:\nr e!c.i p r o.c a/l =\nTarget Completion !reciprocal\nFigure G.26: Formatted dataset example for Symbol Insertion\nContext!Please unscramble the letters into a word, and write that word:\ntaefed =\nTarget Completion !defeat\nFigure G.27: Formatted dataset example for Reversed Words\n57\nContext!Title: The Blitz\nBackground: From the German point of view, March 1941 saw an improvement.\nThe Luftwaffe flew 4,000 sorties that month, including 12 major and\nthree heavy attacks. The electronic war intensified but the Luftwaffe\nflew major inland missions only on moonlit nights. Ports were easier to\nfind and made better targets. To confuse the British, radio silence was\nobserved until the bombs fell. X- and Y-Ger\u007f at beams were placed over\nfalse targets and switched only at the last minute. Rapid frequency\nchanges were introduced for X-Ger\u007f at, whose wider band of frequencies and\ngreater tactical flexibility ensured it remained effective at a time when\nBritish selective jamming was degrading the effectiveness of Y-Ger\u007f at.\nQ: How many sorties were flown in March 1941?\nA: 4,000\nQ: When did the Luftwaffe fly inland missions?\nA:\nTarget Completion !only on moonlit nights\nFigure G.28: Formatted dataset example for SQuADv2\nContext!Normal force -- In a simple case such as an object resting upon a table,\nthe normal force on the object is equal but in opposite direction to the\ngravitational force applied on the object (or the weight of the object),\nthat is, N = m g ( ndisplaystyle N=mg), where m is mass, and g is the\ngravitational field strength (about 9.81 m/s on Earth). The normal force\nhere represents the force applied by the table against the object that\nprevents it from sinking through the table and requires that the table is\nsturdy enough to deliver this normal force without breaking. However, it\nis easy to assume that the normal force and weight are action-reaction\nforce pairs (a common mistake). In this case, the normal force and\nweight need to be equal in magnitude to explain why there is no upward\nacceleration of the object. For example, a ball that bounces upwards\naccelerates upwards because the normal force acting on the ball is larger\nin magnitude than the weight of the ball.\nquestion: is the normal force equal to the force of gravity?\nanswer:\nTarget Completion !yes\nFigure G.29: Formatted dataset example for BoolQ\nContext!The trend toward lower rents may seem surprising given that some\ncommunities in New York are bemoaning the loss of favorite local\nbusinesses to high rents. But, despite the recent softening, for many\nof these retailers there's still been too big a jump from the rental rates\nof the late 1970s, when their leases were signed. Certainly, the recent\ndrop in prices doesn't mean Manhattan comes cheap.\nquestion: Manhattan comes cheap. true, false, or neither?\nanswer:\nTarget Completion !false\nFigure G.30: Formatted dataset example for CB\n58\nContext!The bet, which won him dinner for four, was regarding the existence and\nmass of the top quark, an elementary particle discovered in 1995.\nquestion: The Top Quark is the last of six flavors of quarks predicted by\nthe standard model theory of particle physics. True or False?\nanswer:\nTarget Completion !False\nFigure G.31: Formatted dataset example for RTE\nContext!An outfitter provided everything needed for the safari.\nBefore his first walking holiday, he went to a specialist outfitter to buy\nsome boots.\nquestion: Is the word `outfitter' used in the same way in the two\nsentences above?\nanswer:\nTarget Completion !no\nFigure G.32: Formatted dataset example for WiC\nContext!Final Exam with Answer Key\nInstructions: Please carefully read the following passages. For each\npassage, you must identify which noun the pronoun marked in *bold* refers\nto.\n=====\nPassage: Mr. Moncrieff visited Chester's luxurious New York apartment,\nthinking that it belonged to his son Edward. The result was that Mr.\nMoncrieff has decided to cancel Edward's allowance on the ground that\nhe no longer requires *his* financial support.\nQuestion: In the passage above, what does the pronoun \"*his*\" refer to?\nAnswer:\nTarget Completion !mr. moncrieff\nFigure G.33: Formatted dataset example for WSC\nContext!Q: `Nude Descending A Staircase' is perhaps the most famous painting by\nwhich 20th century artist?\nA:\nTarget Completion !MARCEL DUCHAMP\nTarget Completion !r mutt\nTarget Completion !duchamp\nTarget Completion !marcel duchamp\nTarget Completion !R.Mutt\nTarget Completion !Marcel duChamp\nTarget Completion !Henri-Robert-Marcel Duchamp\nTarget Completion !Marcel du Champ\nTarget Completion !henri robert marcel duchamp\nTarget Completion !Duchampian\nTarget Completion !Duchamp\nTarget Completion !duchampian\nTarget Completion !marcel du champ\nTarget Completion !Marcel Duchamp\nTarget Completion !MARCEL DUCHAMP\nFigure G.34: Formatted dataset example for TriviaQA. TriviaQA allows for multiple valid completions.\n59\nContext!Q: What school did burne hogarth establish?\nA:\nTarget Completion !School of Visual Arts\nFigure G.35: Formatted dataset example for WebQA\nContext!Keinesfalls d\u007f urfen diese f\u007f ur den kommerziellen Gebrauch verwendet werden.\n=\nTarget Completion !In no case may they be used for commercial purposes.\nFigure G.36: Formatted dataset example for De !En. This is the format for one- and few-shot learning, for this and\nother langauge tasks, the format for zero-shot learning is \u201cQ: What is the flanguagegtranslation offsentencegA:\nftranslationg.\u201d\nContext!In no case may they be used for commercial purposes. =\nTarget Completion !Keinesfalls d\u007f urfen diese f\u007f ur den kommerziellen Gebrauch verwendet werden.\nFigure G.37: Formatted dataset example for En !De\nContext!Analysis of instar distributions of larval I. verticalis collected from\na series of ponds also indicated that males were in more advanced instars\nthan females. =\nTarget Completion !L'analyse de la distribution de fr\u0013 equence des stades larvaires d'I.\nverticalis dans une s\u0013 erie d'\u0013 etangs a \u0013 egalement d\u0013 emontr\u0013 e que les larves\nm^ ales \u0013 etaient \u0012 a des stades plus avanc\u0013 es que les larves femelles.\nFigure G.38: Formatted dataset example for En !Fr\nContext!L'analyse de la distribution de fr\u0013 equence des stades larvaires d'I.\nverticalis dans une s\u0013 erie d'\u0013 etangs a \u0013 egalement d\u0013 emontr\u0013 e que les larves\nm^ ales \u0013 etaient \u0012 a des stades plus avanc\u0013 es que les larves femelles. =\nTarget Completion !Analysis of instar distributions of larval I. verticalis collected from\na series of ponds also indicated that males were in more advanced instars\nthan females.\nFigure G.39: Formatted dataset example for Fr !En\nContext!The truth is that you want, at any price, and against the wishes of the\npeoples of Europe, to continue the negotiations for Turkey's accession\nto the European Union, despite Turkey's continuing refusal to recognise\nCyprus and despite the fact that the democratic reforms are at a\nstandstill. =\nTarget Completion !Adev\u0015 arul este c\u0015 a v\u0015 a dorit \u0018i, cu orice pret \u0018 \u0018 si ^ \u0010mpotriva dorint \u0018ei\neuropenilor, s\u0015 a continuat \u0018i negocierile de aderare a Turciei la Uniunea\nEuropean\u0015 a, ^ \u0010n ciuda refuzului continuu al Turciei de a recunoa\u0018 ste Ciprul\n\u0018 si ^ \u0010n ciuda faptului c\u0015 a reformele democratice au ajuns ^ \u0010ntr-un punct mort.\nFigure G.40: Formatted dataset example for En !Ro\n60\nContext!Adev\u0015 arul este c\u0015 a v\u0015 a dorit \u0018i, cu orice pret \u0018 \u0018 si ^ \u0010mpotriva dorint \u0018ei\neuropenilor, s\u0015 a continuat \u0018i negocierile de aderare a Turciei la Uniunea\nEuropean\u0015 a, ^ \u0010n ciuda refuzului continuu al Turciei de a recunoa\u0018 ste Ciprul\n\u0018 si ^ \u0010n ciuda faptului c\u0015 a reformele democratice au ajuns ^ \u0010ntr-un punct mort.\n=\nTarget Completion !The truth is that you want, at any price, and against the wishes of the\npeoples of Europe, to continue the negotiations for Turkey's accession\nto the European Union, despite Turkey's continuing refusal to recognise\nCyprus and despite the fact that the democratic reforms are at a\nstandstill.\nFigure G.41: Formatted dataset example for Ro !En\nContext!Q: What is (2 * 4) * 6?\nA:\nTarget Completion !48\nFigure G.42: Formatted dataset example for Arithmetic 1DC\nContext!Q: What is 17 minus 14?\nA:\nTarget Completion !3\nFigure G.43: Formatted dataset example for Arithmetic 2D-\nContext!Q: What is 98 plus 45?\nA:\nTarget Completion !143\nFigure G.44: Formatted dataset example for Arithmetic 2D+\nContext!Q: What is 95 times 45?\nA:\nTarget Completion !4275\nFigure G.45: Formatted dataset example for Arithmetic 2Dx\nContext!Q: What is 509 minus 488?\nA:\nTarget Completion !21\nFigure G.46: Formatted dataset example for Arithmetic 3D-\nContext!Q: What is 556 plus 497?\nA:\nTarget Completion !1053\nFigure G.47: Formatted dataset example for Arithmetic 3D+\nContext!Q: What is 6209 minus 3365?\nA:\nTarget Completion !2844\nFigure G.48: Formatted dataset example for Arithmetic 4D-\n61\nContext!Q: What is 9923 plus 617?\nA:\nTarget Completion !10540\nFigure G.49: Formatted dataset example for Arithmetic 4D+\nContext!Q: What is 40649 minus 78746?\nA:\nTarget Completion !-38097\nFigure G.50: Formatted dataset example for Arithmetic 5D \u0000\nContext!Q: What is 65360 plus 16204?\nA:\nTarget Completion !81564\nFigure G.51: Formatted dataset example for Arithmetic 5D+\n62\nH Results on All Tasks for All Model Sizes\nZero-Shot One-Shot Few-Shot\nName Metric SplitFine-tune\nSOTA K Small Med Large XL 2.7B 6.7B 13B 175B Small Med Large XL 2.7B 6.7B 13B 175B Small Med Large XL 2.7B 6.7B 13B 175B175B\n(test server)\nHellaSwag acc dev 85.6 20 33.7 43.6 51.0 54.7 62.8 67.4 70.9 78.9 33.0 42.9 50.5 53.5 61.9 66.5 70.0 78.1 33.5 43.1 51.3 54.9 62.9 67.3 71.3 79.3\nLAMBADA acc test 68.0 15 42.7 54.3 60.4 63.6 67.1 70.3 72.5 76.2 22.0 47.1 52.6 58.3 61.1 65.4 69.0 72.5 22.0 40.4 63.2 57.0 78.1 79.1 81.3 86.4\nLAMBADA ppl test 8.63 15 18.6 9.09 6.53 5.44 4.60 4.00 3.56 3.00 165.0 11.6 8.29 6.46 5.53 4.61 4.06 3.35 165.0 27.6 6.63 7.45 2.89 2.56 2.56 1.92\nStoryCloze acc test 91.8 70 63.3 68.5 72.4 73.4 77.2 77.7 79.5 83.2 62.3 68.7 72.3 74.2 77.3 78.7 79.7 84.7 62.3 70.2 73.9 76.1 80.2 81.2 83.0 87.7\nNQs acc test 44.5 64 0.64 1.75 2.71 4.40 6.01 5.79 7.84 14.6 1.19 3.07 4.79 5.43 8.73 9.78 13.7 23.0 1.72 4.46 7.89 9.72 13.2 17.0 21.0 29.9\nTriviaQA acc dev 68.0 64 4.15 7.61 14.0 19.7 31.3 38.7 41.8 64.3 4.19 12.9 20.5 26.5 35.9 44.4 51.3 68.0 6.96 16.3 26.5 32.1 42.3 51.6 57.5 71.2 71.2\nWebQs acc test 45.5 64 1.77 3.20 4.33 4.63 7.92 7.73 8.22 14.4 2.56 6.20 8.51 9.15 14.5 15.1 19.0 25.3 5.46 12.6 15.9 19.6 24.8 27.7 33.5 41.5\nRo!En 16 BLEU-mb test 39.9 64 2.08 2.71 3.09 3.15 16.3 8.34 20.2 19.9 0.55 15.4 23.0 26.3 30.6 33.2 35.6 38.6 1.25 20.7 25.8 29.2 33.1 34.8 37.0 39.5\nRo!En 16 BLEU-sb test 64 2.39 3.08 3.49 3.56 16.8 8.75 20.8 20.9 0.65 15.9 23.6 26.8 31.3 34.2 36.7 40.0 1.40 21.3 26.6 30.1 34.3 36.2 38.4 41.3\nEn!Ro 16 BLEU-mb test 38.5 64 2.14 2.65 2.53 2.50 3.46 4.24 5.32 14.1 0.35 3.30 7.89 8.72 13.2 15.1 17.3 20.6 1.25 5.90 9.33 10.7 14.3 16.3 18.0 21.0\nEn!Ro 16 BLEU-sb test 64 2.61 3.11 3.07 3.09 4.26 5.31 6.43 18.0 0.55 3.90 9.15 10.3 15.7 18.2 20.8 24.9 1.64 7.40 10.9 12.9 17.2 19.6 21.8 25.8\nFr!En 14 BLEU-mb test 35.0 64 1.81 2.53 3.47 3.13 20.6 15.1 21.8 21.2 1.28 15.9 23.7 26.3 29.0 30.5 30.2 33.7 4.98 25.5 28.5 31.1 33.7 34.9 36.6 39.2\nFr!En 14 BLEU-sb test 64 2.29 2.99 3.90 3.60 21.2 15.5 22.4 21.9 1.50 16.3 24.4 27.0 30.0 31.6 31.4 35.6 5.30 26.2 29.5 32.2 35.1 36.4 38.3 41.4\nEn!Fr 14 BLEU-mb test 45.6 64 1.74 2.16 2.73 2.15 15.1 8.82 12.0 25.2 0.49 8.00 14.8 15.9 20.3 23.3 24.9 28.3 4.08 14.5 19.3 21.5 24.9 27.3 29.5 32.6\nEn!Fr 14 BLEU-sb test 45.9 64 2.44 2.75 3.54 2.82 19.3 11.4 15.3 31.3 0.81 10.0 18.2 19.3 24.7 28.3 30.1 34.1 5.31 18.0 23.6 26.1 30.3 33.3 35.5 39.9\nDe!En 16 BLEU-mb test 40.2 64 2.06 2.87 3.41 3.63 21.5 17.3 23.0 27.2 0.83 16.2 22.5 24.7 28.2 30.7 33.0 30.4 3.25 22.7 26.2 29.2 32.7 34.8 37.3 40.6\nDe!En 16 BLEU-sb test 64 2.39 3.27 3.85 4.04 22.5 18.2 24.4 28.6 0.93 17.1 23.4 25.8 29.2 31.9 34.5 32.1 3.60 23.8 27.5 30.5 34.1 36.5 39.1 43.0\nEn!De 16 BLEU-mb test 41.2 64 1.70 2.27 2.31 2.43 12.9 8.66 10.4 24.6 0.50 7.00 12.9 13.1 18.3 20.9 22.5 26.2 3.42 12.3 15.4 17.1 20.9 23.0 26.6 29.7\nEn!De 16 BLEU-sb test 41.2 64 2.09 2.65 2.75 2.92 13.7 9.36 11.0 25.3 0.54 7.40 13.4 13.4 18.8 21.7 23.3 27.3 3.78 12.9 16.1 17.7 21.7 24.1 27.7 30.9\nWinograd acc test 93.8 7 66.3 72.9 74.7 76.9 82.4 85.7 87.9 88.3 63.4 68.5 72.9 76.9 82.4 84.6 86.1 89.7 63.4 67.4 73.6 76.9 84.3 85.4 82.4 88.6\nWinogrande acc dev 84.6 50 52.0 52.1 57.4 58.7 62.3 64.5 67.9 70.2 51.3 53.0 58.3 59.1 61.7 65.8 66.9 73.2 51.3 52.6 57.5 59.1 62.6 67.4 70.0 77.7\nPIQA acc dev 77.1 50 64.6 70.2 72.9 75.1 75.6 78.0 78.5 81.0 64.3 69.3 71.8 74.4 74.3 76.3 77.8 80.5 64.3 69.4 72.0 74.3 75.4 77.8 79.9 82.3 82.8\nARC (Challenge) acc test 78.5 50 26.6 29.5 31.8 35.5 38.0 41.4 43.7 51.4 25.5 30.2 31.6 36.4 38.4 41.5 43.1 53.2 25.5 28.4 32.3 36.7 39.5 43.7 44.8 51.5\nARC (Easy) acc test 92.0 50 43.6 46.5 53.0 53.8 58.2 60.2 63.8 68.8 42.7 48.2 54.6 55.9 60.3 62.6 66.8 71.2 42.7 51.0 58.1 59.1 62.1 65.8 69.1 70.1\nOpenBookQA acc test 87.2 100 35.6 43.2 45.2 46.8 53.0 50.4 55.6 57.6 37.0 39.8 46.2 46.4 53.4 53.0 55.8 58.8 37.0 43.6 48.0 50.6 55.6 55.2 60.8 65.4\nQuac f1 dev 74.4 5 21.2 26.8 31.0 30.1 34.7 36.1 38.4 41.5 21.1 26.9 31.9 32.3 37.4 39.0 40.6 43.4 21.6 27.6 32.9 34.2 38.2 39.9 40.9 44.3\nRACE-h acc test 90.0 10 35.2 37.9 40.1 40.9 42.4 44.1 44.6 45.5 34.3 37.7 40.0 42.0 43.8 44.3 44.6 45.9 34.3 37.0 40.4 41.4 42.3 44.7 45.1 46.8\nRACE-m acc test 93.1 10 42.1 47.2 52.1 52.3 54.7 54.4 56.7 58.4 42.3 47.3 51.7 55.2 56.1 54.7 56.9 57.4 42.3 47.0 52.7 53.0 55.6 55.4 58.1 58.1\nSQuADv2 em dev 90.7 16 22.6 32.8 33.9 43.1 43.6 45.4 49.0 52.6 25.1 37.5 37.9 47.9 47.9 51.1 56.0 60.1 27.5 40.5 39.2 53.5 50.0 56.6 62.6 64.9\nSQuADv2 f1 dev 93.0 16 28.3 40.2 41.4 50.3 51.0 52.7 56.3 59.5 30.1 43.6 44.1 54.0 54.1 57.1 61.8 65.4 32.1 45.5 44.9 58.7 55.9 62.1 67.7 69.8\nCoQA f1 dev 90.7 5 34.5 55.0 61.8 65.3 71.1 72.8 76.3 81.5 30.6 52.1 61.6 66.1 71.8 75.1 77.9 84.0 31.1 52.0 62.7 66.8 73.2 77.3 79.9 85.0\nDROP f1 dev 89.1 20 9.40 13.6 14.4 16.4 19.7 17.0 24.0 23.6 11.7 18.1 20.9 23.0 26.4 27.3 29.2 34.3 12.9 18.7 24.0 25.6 29.7 29.7 32.3 36.5\nBoolQ acc dev 91.0 32 49.7 60.3 58.9 62.4 67.1 65.4 66.2 60.5 52.6 61.7 60.4 63.7 68.4 68.7 69.0 76.7 43.1 60.6 62.0 64.1 70.3 70.0 70.2 77.5 76.4\nCB acc dev 96.9 32 0.00 32.1 8.93 19.6 19.6 28.6 19.6 46.4 55.4 53.6 53.6 48.2 57.1 33.9 55.4 64.3 42.9 58.9 53.6 69.6 67.9 60.7 66.1 82.1 75.6\nCB f1 dev 93.9 32 0.00 29.3 11.4 17.4 22.4 25.1 20.3 42.8 60.1 39.8 45.6 37.5 45.7 28.5 44.6 52.5 26.1 40.4 32.6 48.3 45.7 44.6 46.0 57.2 52.0\nCopa acc dev 94.8 32 66.0 68.0 73.0 77.0 76.0 80.0 84.0 91.0 62.0 64.0 66.0 74.0 76.0 82.0 86.0 87.0 67.0 64.0 72.0 77.0 83.0 83.0 86.0 92.0 92.0\nRTE acc dev 92.5 32 47.7 49.8 48.4 56.0 46.6 55.2 62.8 63.5 53.1 47.3 49.5 49.5 54.9 54.9 56.3 70.4 52.3 48.4 46.9 50.9 56.3 49.5 60.6 72.9 69.0\nWiC acc dev 76.1 32 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 50.0 50.3 50.3 49.2 49.4 50.3 50.0 48.6 49.8 55.0 53.0 53.0 51.6 53.1 51.1 55.3 49.4\nWSC acc dev 93.8 32 59.6 56.7 65.4 61.5 66.3 60.6 64.4 65.4 58.7 58.7 60.6 62.5 66.3 60.6 66.3 69.2 58.7 60.6 54.8 49.0 62.5 67.3 75.0 75.0 80.1\nMultiRC acc dev 62.3 32 4.72 9.65 12.3 13.6 14.3 18.4 24.2 27.6 4.72 9.65 12.3 13.6 14.3 18.4 24.2 27.6 6.09 11.8 16.8 20.8 24.7 23.8 25.0 32.5 30.5\nMultiRC f1a dev 88.2 32 57.0 59.7 60.4 59.9 60.0 64.5 71.4 72.9 57.0 59.7 60.4 59.9 60.0 64.5 71.4 72.9 45.0 55.9 64.2 65.4 69.5 66.4 69.3 74.8 75.4\nReCoRD acc dev 92.5 32 70.8 78.5 82.1 84.1 86.2 88.6 89.0 90.2 69.8 77.0 80.7 83.0 85.9 88.0 88.8 90.2 69.8 77.2 81.3 83.1 86.6 87.9 88.9 89.0 90.2\nReCoRD f1 dev 93.3 32 71.9 79.2 82.8 85.2 87.3 89.5 90.4 91.0 70.7 77.8 81.6 83.9 86.8 88.8 89.7 91.2 70.7 77.9 82.1 84.0 87.5 88.8 89.8 90.1 91.1\nSuperGLUE average dev 89.0 40.6 47.4 46.8 49.6 50.1 52.3 54.4 58.2 54.4 55.1 56.7 57.8 61.2 59.7 64.3 68.9 50.2 56.2 56.8 60.0 64.3 63.6 66.9 73.2 71.8\nANLI R1 acc test 73.8 50 33.4 34.2 33.4 33.4 34.2 32.3 33.2 34.6 32.1 31.6 31.9 34.6 30.6 31.6 32.7 32.0 32.1 32.5 30.9 32.5 33.5 33.1 33.3 36.8\nANLI R2 acc test 50.7 50 33.2 31.9 33.3 33.3 33.8 33.5 33.5 35.4 35.7 33.7 33.2 32.7 32.7 33.9 33.9 33.9 35.7 33.8 32.1 31.4 32.6 33.3 32.6 34.0\nANLI R3 acc test 48.3 50 33.6 34.0 33.8 33.4 35.3 34.8 34.4 34.5 35.0 32.6 33.0 33.9 34.1 33.1 32.5 35.1 35.0 34.4 35.1 36.0 32.7 33.9 34.5 40.2\n2D+ acc n/a 50 0.70 0.65 0.70 0.85 1.10 2.54 15.4 76.9 2.00 0.55 3.15 4.00 12.1 19.6 73.0 99.6 2.00 4.10 3.50 4.50 8.90 11.9 55.5 100.0\n2D- acc n/a 50 1.25 1.25 1.25 1.25 1.60 7.60 12.6 58.0 1.15 0.95 1.45 1.95 3.85 11.5 44.6 86.4 1.15 1.45 2.25 2.70 7.35 13.6 52.4 98.9\n3D+ acc n/a 50 0.10 0.10 0.05 0.10 0.10 0.25 1.40 34.2 0.15 0.00 0.10 0.30 0.45 0.95 15.4 65.5 0.15 0.45 0.30 0.55 0.75 0.90 8.40 80.4\n3D- acc n/a 50 0.05 0.05 0.05 0.05 0.05 0.45 1.35 48.3 0.05 0.15 0.25 0.30 0.55 1.60 6.15 78.7 0.05 0.10 0.15 0.35 0.65 1.05 9.20 94.2\n4D+ acc n/a 50 0.05 0.05 0.00 0.00 0.05 0.05 0.15 4.00 0.00 0.00 0.10 0.00 0.00 0.10 0.80 14.0 0.00 0.05 0.05 0.00 0.15 0.15 0.40 25.5\n4D- acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.10 7.50 0.00 0.00 0.00 0.00 0.05 0.00 0.50 14.0 0.00 0.05 0.00 0.00 0.10 0.05 0.40 26.8\n5D+ acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.65 0.00 0.00 0.00 0.00 0.00 0.00 0.05 3.45 0.00 0.00 0.00 0.00 0.00 0.00 0.05 9.30\n5D- acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.80 0.00 0.00 0.00 0.00 0.00 0.00 0.05 3.75 0.00 0.00 0.00 0.00 0.00 0.00 0.00 9.90\n2Dx acc n/a 50 2.20 2.25 2.65 2.10 2.55 5.80 6.15 19.8 1.35 2.35 3.35 2.35 4.75 9.15 11.0 27.4 1.35 2.90 2.70 2.85 4.25 6.10 7.05 29.2\n1DC acc n/a 50 1.25 2.95 2.75 0.05 0.30 2.35 0.75 9.75 1.90 2.80 2.85 3.65 6.45 9.15 8.20 14.3 1.70 2.15 3.90 5.75 6.20 7.60 9.95 21.3\nCycled Letters acc n/a 100 0.62 0.71 2.85 0.00 0.63 1.35 2.58 3.66 1.67 4.36 5.68 6.46 6.25 9.41 15.1 21.7 4.63 9.27 10.7 14.5 16.7 21.9 27.7 37.9\nAnagrams 1 acc n/a 100 0.10 0.14 0.40 0.00 0.27 0.69 1.16 2.28 0.21 0.61 1.12 1.27 1.60 2.72 3.72 8.62 0.50 1.27 2.13 3.05 3.81 5.49 8.38 15.1\nAnagrams 2 acc n/a 100 0.81 1.21 2.69 0.01 1.71 3.75 4.53 8.91 1.19 2.62 4.70 4.77 6.97 10.2 14.6 25.9 1.94 4.80 7.59 9.87 12.6 18.9 25.6 39.7\nSymbol Insertion acc n/a 100 0.00 0.00 0.10 0.00 0.05 0.42 0.89 8.26 0.03 0.05 0.57 1.18 1.67 3.46 6.62 45.4 0.11 0.28 2.19 4.18 6.61 11.0 27.3 67.2\nReversed Words acc n/a 100 0.00 0.01 0.01 0.01 0.02 0.03 0.03 0.09 0.02 0.01 0.01 0.00 0.05 0.07 0.11 0.48 0.00 0.05 0.00 0.17 0.24 0.30 0.42 0.44\nSAT Analogies acc n/a 20 35.6 39.0 45.2 44.1 50.0 49.2 52.7 53.7 30.5 41.2 43.1 46.5 55.1 54.3 53.5 59.1 30.5 40.4 42.8 40.6 48.4 51.9 53.5 65.2\nTable H.1: Scores for every task, setting and model that we investigate in this paper.\n63\nFigure H.1: All results for all SuperGLUE tasks.\nFigure H.2: Results for SAT task.\n Figure H.3: All results for all Winograd tasks.\n64\nFigure H.4: All results for all Arithmetic tasks.\nFigure H.5: All results for all Cloze and Completion tasks.\n65\nFigure H.6: All results for all Common Sense Reasoning tasks.\nFigure H.7: All results for all QA tasks.\nFigure H.8: All results for all Reading Comprehension tasks.\nFigure H.9: All results for all ANLI rounds.\n66\nFigure H.10: All results for all Scramble tasks.\nFigure H.11: All results for all Translation tasks.\n67\nReferences\n[ADG+16]Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,\nBrendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent.\nInAdvances in neural information processing systems , pages 3981\u20133989, 2016.\n[AI19] WeChat AI. Tr-mt (ensemble), December 2019.\n[AJF19] Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation. In\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , 2019.\n[BBDIW20] Su Lin Blodgett, Solon Barocas, Hal Daum \u00b4e III, and Hanna Wallach. Language (technology) is power:\nA critical survey of \u201cbias\u201d in nlp. arXiv preprint arXiv:2005.14050 , 2020.\n[BCFL13] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language\nprocessing , pages 1533\u20131544, 2013.\n[BDD+09]Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The \ufb01fth\nPASCAL recognizing textual entailment challenge. 2009.\n[BES10] Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. Sentiwordnet 3.0: an enhanced lexical\nresource for sentiment analysis and opinion mining. In Lrec, volume 10, pages 2200\u20132204, 2010.\n[BHDD+06]Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan\nSzpektor. The second PASCAL recognising textual entailment challenge. 2006.\n[BHT+20]Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella\nLapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, et al. Experience grounds language.\narXiv preprint arXiv:2004.10151 , 2020.\n[BLC13] Yoshua Bengio, Nicholas L \u00b4eonard, and Aaron C. Courville. Estimating or propagating gradients through\nstochastic neurons for conditional computation. Arxiv , 2013.\n[BZB+19]Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about\nphysical commonsense in natural language. arXiv preprint arXiv:1911.11641 , 2019.\n[Car97] Rich Caruana. Multitask learning. Machine learning , 28(1), 1997.\n[CB78] Susan Carey and Elsa Bartlett. Acquiring a single new word. Proceedings of the Stanford Child Language\nConference , 1978.\n[CCE+18]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv ,\nabs/1803.05457, 2018.\n[CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers, 2019.\n[CHI+18]Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke\nZettlemoyer. Quac : Question answering in context. Arxiv , 2018.\n[CLC+19]Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. BoolQ: Exploring the surprising dif\ufb01culty of natural yes/no questions. arXiv preprint\narXiv:1905.10044 , 2019.\n[CLY+19]Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740 ,\n2019.\n[Cra17] Kate Crawford. The trouble with bias. NIPS 2017 Keynote , 2017.\n[DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.\n68\n[DGM06] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment\nchallenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classi\ufb01cation,\nand recognising textual entailment , pages 177\u2013190. Springer, 2006.\n[DGV+18]Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal\ntransformers. Arxiv , 2018.\n[DHKH14] Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Hea\ufb01eld. Edinburgh\u2019s phrase-based machine\ntranslation systems for wmt-14. In Proceedings of the Ninth Workshop on Statistical Machine Translation ,\npages 97\u2013104, 2014.\n[DL15] Andrew M. Dai and Quoc V . Le. Semi-supervised sequence learning. In Advances in neural information\nprocessing systems , 2015.\n[DMST19] Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank: Investigat-\ning projection in naturally occurring discourse. 2019. To appear in proceedings of Sinn und Bedeutung\n23. Data can be found at https://github.com/mcdm/CommitmentBank/.\n[DSC+16]Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast\nreinforcement learning via slow reinforcement learning. ArXiv , abs/1611.02779, 2016.\n[DWD+19]Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.\nDrop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint\narXiv:1903.00161 , 2019.\n[DYY+19]Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V . Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a \ufb01xed-length context. Arxiv , 2019.\n[EOAG18] Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at scale.\narXiv preprint arXiv:1808.09381 , 2018.\n[FAL17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. ArXiv , abs/1703.03400, 2017.\n[Fyo00] Yaroslav Fyodorov. A natural logic inference system, 2000.\n[GG19] Hila Gonen and Yoav Goldberg. Lipstick on a pig: Debiasing methods cover up systematic gender biases\nin word embeddings but do not remove them. arXiv preprint arXiv:1903.03862 , 2019.\n[GLT+20]Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-\naugmented language model pre-training. arXiv preprint arXiv:2002.08909 , 2020.\n[GMDD07] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing\ntextual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and\nparaphrasing , pages 1\u20139. Association for Computational Linguistics, 2007.\n[Gra16] Alex Graves. Adaptive computation time for recurrent neural networks. Arxiv , 2016.\n[GSL+18]Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A\nSmith. Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324 , 2018.\n[GSR19] Sebastian Gehrmann, Hendrik Strobelt, and Alexander M. Rush. Gltr: Statistical detection and visualiza-\ntion of generated text. arXiv preprint arXiv: 1906.04043 , 2019.\n[GWC+18]Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho, and Victor OK Li. Meta-learning for low-resource\nneural machine translation. arXiv preprint arXiv:1808.08437 , 2018.\n[HB20] Daniel Hernandez and Tom Brown. Ai and ef\ufb01ciency, May 2020.\n[HBFC19] Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration.\nCoRR , abs/1904.09751, 2019.\n[HLW+20]Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song.\nPretrained transformers improve out of distribution robustness. arXiv preprint arXiv:2004.06100 , 2020.\n69\n[HNA+17]Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md.\nMostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically.\narXiv preprint arXiv:1712.00409 , 2017.\n[HR18] Jeremy Howard and Sebastian Ruder. Universal language model \ufb01ne-tuning for text classi\ufb01cation. arXiv\npreprint arXiv:1801.06146 , 2018.\n[HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531 , 2015.\n[HYC01] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to Learn Using Gradient Descent.\nInInternational Conference on Arti\ufb01cial Neural Networks , pages 87\u201394. Springer, 2001.\n[HZJ+19]Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini,\nDani Yogatama, and Pushmeet Kohli. Reducing sentiment bias in language models via counterfactual\nevaluation. arXiv preprint arXiv:1911.03064 , 2019.\n[IBGC+14]Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daum \u00b4e III. A neural\nnetwork for factoid question answering over paragraphs. In Empirical Methods in Natural Language\nProcessing , 2014.\n[IDCBE19] Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic detection of\ngenerated text is easiest when humans are fooled. arXiv preprint arXiv:1911.00650 , 2019.\n[JCWZ17] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551 , 2017.\n[JN20] Zheng Junyuan and Gamma Lab NYC. Numeric transformer - albert, March 2020.\n[JVS+16]Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits\nof language modeling. arXiv preprint arXiv:1602.02410 , 2016.\n[JYS+19]Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.\nTinyBERT: Distilling BERT for natural language understanding. arXiv preprint arXiv:1909.10351 , 2019.\n[JZC+19]Ying Ju, Fubang Zhao, Shijie Chen, Bowen Zheng, Xuefeng Yang, and Yunfeng Liu. Technical report on\nconversational question answering. arXiv preprint arXiv:1909.10772 , 2019.\n[KCR+18]Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond\nthe surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of North\nAmerican Chapter of the Association for Computational Linguistics (NAACL) , 2018.\n[KKS+20]Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi.\nUni\ufb01edqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv:2005.00700 , 2020.\n[KMB20] Sarah E. Kreps, Miles McCain, and Miles Brundage. All the news that\u2019s \ufb01t to fabricate: Ai-generated\ntext as a tool of media misinformation, 2020.\n[KMH+20]Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.\n[KPR+19]Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red\ufb01eld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova,\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural ques-\ntions: a benchmark for question answering research. Transactions of the Association of Computational\nLinguistics , 2019.\n[KR16] Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. Arxiv , 2016.\n[LB02] Edward Loper and Steven Bird. Nltk: The natural language toolkit, 2002.\n[LC19] Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv preprint\narXiv:1901.07291 , 2019.\n70\n[LCG+19]Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. ALBERT: A lite BERT for self-supervised learning of language representations. arXiv preprint\narXiv:1909.11942 , 2019.\n[LCH+20]Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao.\nAdversarial training for large neural language models. arXiv preprint arXiv:2004.08994 , 2020.\n[LDL19] Zhongyang Li, Xiao Ding, and Ting Liu. Story ending prediction by transferable bert. arXiv preprint\narXiv:1905.07504 , 2019.\n[LDM12] Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In Thirteenth\nInternational Conference on the Principles of Knowledge Representation and Reasoning , 2012.\n[LGG+20]Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. arXiv preprint\narXiv:2001.08210 , 2020.\n[LGH+15]Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. Representation\nlearning using multi-task deep neural networks for semantic classi\ufb01cation and information retrieval. In\nProceedings of the 2015 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies , 2015.\n[LH17] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101 , 2017.\n[LHCG19a] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Improving multi-task deep neural\nnetworks via knowledge distillation for natural language understanding. arXiv preprint arXiv:1904.09482 ,\n2019.\n[LHCG19b] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for\nnatural language understanding. arXiv preprint arXiv:1901.11504 , 2019.\n[Lin20] Tal Linzen. How can we accelerate progress towards human-like linguistic generalization? arXiv preprint\narXiv:2005.00955 , 2020.\n[LLG+19]Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension. arXiv preprint arXiv:1910.13461 , 2019.\n[LM17] Ke Li and Jitendra Malik. Learning to optimize neural nets. arXiv preprint arXiv:1703.00441 , 2017.\n[LOG+19]Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach.\narXiv preprint arXiv:1907.11692 , 2019.\n[LPP+20]Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K \u00a8uttler, Mike Lewis, Wen-tau Yih, Tim Rockt \u00a8aschel, Sebastian Riedel, and Kiela Douwe.\nRetrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401 ,\n2020.\n[LSP+18]Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. Generating Wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198 , 2018.\n[LWS+20]Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joseph E. Gonzalez.\nTrain large, then compress: Rethinking model size for ef\ufb01cient training and inference of transformers,\n2020.\n[LXL+17]Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading\ncomprehension dataset from examinations. arXiv preprint arXiv:1704.04683 , 2017.\n[LYN+20]Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, and Jimmy\nLin. Tttttackling winogrande schemas. arXiv preprint arXiv:2003.08380 , 2020.\n[Mac92] David. MacKay. Information-based objective functions for active data selection. Neural Computation ,\n1992.\n71\n[MBXS17] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Con-\ntextualized word vectors. In Advances in Neural Information Processing Systems , pages 6294\u20136305,\n2017.\n[MCCD13] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\ufb01cient estimation of word representations\nin vector space. arXiv preprint arXiv:1301.3781 , 2013.\n[MCH+16]Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. A corpus and evaluation framework for deeper understanding of\ncommonsense stories. arXiv preprint arXiv:1604.01696 , 2016.\n[MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity?\na new dataset for open book question answering. ArXiv , abs/1809.02789, 2018.\n[MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of\nlarge-batch training, 2018.\n[MKM+94]Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson,\nKaren Katz, and Britta Schasberger. The penn treebank: annotating predicate argument structure.\nInProceedings of the workshop on Human Language Technology , pages 114\u2013119. Association for\nComputational Linguistics, 1994.\n[MKXS18] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language\ndecathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730 , 2018.\n[MPL19] R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. arXiv preprint arXiv:1902.01007 , 2019.\n[MWZ+18]Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson,\nElena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting, 2018.\n[NBR20] Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained\nlanguage models. arXiv preprint arXiv:2004.09456 , 2020.\n[NK19] Timothy Niven and Hung-Yu Kao. Probing neural network comprehension of natural language arguments.\narXiv preprint arXiv:1907.07355 , 2019.\n[Nor09] Peter Norvig. Natural language corpus data, 2009.\n[NvNvdG19] Malvina Nissim, Rik van Noord, and Rob van der Goot. Fair is better than sensational: Man is to doctor\nas woman is to doctor. arXiv preprint arXiv:1905.09866 , 2019.\n[NWD+19]Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial\nnli: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599 , 2019.\n[oR16] University of Regensburg. Fascha, 2016.\n[PCC18] Mohammad Taher Pilehvar and Jose Camacho-Collados. WIC: 10,000 example pairs for evaluating\ncontext-sensitive representations. arXiv preprint arXiv:1808.09121 , 2018.\n[PFB18] Jason Phang, Thibault F \u00b4evry, and Samuel R. Bowman. Sentence encoders on STILTs: Supplementary\ntraining on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088 , 2018.\n[PHR+18]Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White, and\nBenjamin Van Durme. Collecting diverse natural language inference problems for sentence representation\nevaluation. In Proceedings of EMNLP , 2018.\n[PKL+16]Denis Paperno, Germ \u00b4an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern \u00b4andez. The lambada dataset: Word prediction\nrequiring a broad discourse context. arXiv preprint arXiv:1606.06031 , 2016.\n[PNZtY18] Matthew E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen tau Yih. Dissecting contextual word\nembeddings: Architecture and representation, 2018.\n[Pos18] Matt Post. A call for clarity in reporting BLEU scores. arXiv preprint arXiv:1804.08771 , 2018.\n72\n[PSM14] Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 conference on empirical methods in natural language\nprocessing (EMNLP) , 2014.\n[QIA20] QIANXIN. Sa-net on albert (ensemble), April 2020.\n[QMZH19] Yusu Qian, Urwa Muaz, Ben Zhang, and Jae Won Hyun. Reducing gender bias in word-level language\nmodels with a gender-equalizing loss function. arXiv preprint arXiv:1905.12801 , 2019.\n[RBG11] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An\nevaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series , 2011.\n[RCM19] Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question answering\nchallenge. Transactions of the Association for Computational Linguistics , 7:249\u2013266, 2019.\n[RCP+17]Scott Reed, Yutian Chen, Thomas Paine, A \u00a8aron van den Oord, SM Eslami, Danilo Rezende, Oriol\nVinyals, and Nando de Freitas. Few-shot autoregressive density estimation: Towards learning to learn\ndistributions. arXiv preprint arXiv:1710.10304 , 2017.\n[RJL18] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unanswerable questions for\nsquad. arXiv preprint arXiv:1806.03822 , 2018.\n[RL16] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. ICLR 2017 (oral) ,\n2016.\n[RLL+19]Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. NumNet: Machine reading comprehension\nwith numerical reasoning. In Proceedings of EMNLP , 2019.\n[RNLVD18] Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in\ncoreference resolution. arXiv preprint arXiv:1804.09301 , 2018.\n[RNSS18] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding\nby generative pre-training, 2018.\n[Ros12] R.S. Ross. Guide for conducting risk assessments. NIST Special Publication , 2012.\n[RRBS19] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of\nthe generalization error across scales, 2019.\n[RRS20] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters\nof a language model? arXiv preprint arXiv:2002.08910 , 2020.\n[RSR+19]Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uni\ufb01ed text-to-text\ntransformer, 2019.\n[RWC+19]Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners, 2019.\n[SBBC19] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial\nwinograd schema challenge at scale, 2019.\n[SBC+19]Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-V oss, Jeff Wu, Alec Radford,\nGretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris\nMcGuf\ufb01e, and Jasmine Wang. Release strategies and the social impacts of language models, 2019.\n[SCNP19] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as a\nbabysitter: On biases in language generation. arXiv preprint arXiv:1909.01326 , 2019.\n[SDCW19] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version of\nBERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 , 2019.\n[SDSE19] Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green AI. CoRR , abs/1907.10597, 2019.\n[SHB15] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with\nmonolingual data. arXiv preprint arXiv:1511.06709 , 2015.\n73\n[SMM+17]Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint\narXiv:1701.06538 , 2017.\n[SPP+19]Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\nMegatron-lm: Training multi-billion parameter language models using model parallelism, 2019.\n[SS20] Timo Schick and Hinrich Sch \u00a8utze. Exploiting cloze questions for few-shot text classi\ufb01cation and natural\nlanguage inference. arXiv preprint arXiv:2001.07676 , 2020.\n[STQ+19]Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: Masked sequence to sequence\npre-training for language generation. arXiv preprint arXiv:1905.02450 , 2019.\n[TFR+17]Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain\nrandomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ\ninternational conference on intelligent robots and systems (IROS) , pages 23\u201330. IEEE, 2017.\n[TL05] Peter D. Turney and Michael L. Littman. Corpus-based learning of analogies and semantic relations.\nCoRR , abs/cs/0508103, 2005.\n[TL18] Trieu H. Trinh and Quoc V . Le. A simple method for commonsense reasoning. arXiv preprint\narXiv:1806.02847 , 2018.\n[TLBS03] Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and Victor Shnayder. Combining independent\nmodules to solve multiple-choice synonym and analogy problems. CoRR , cs.CL/0309035, 2003.\n[Tur20] Project Turing. Microsoft research blog, Feb 2020.\n[VBL+16]Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching Networks for One\nShot Learning. In Advances in neural information processing systems , pages 3630\u20133638, 2016.\n[VSP+17]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing\nsystems , 2017.\n[WPN+19]Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understand-\ning systems. In Advances in Neural Information Processing Systems , pages 3261\u20133275, 2019.\n[WXH+18]Yiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. Multi-agent\ndual learning. ICLR 2019 , 2018.\n[XDH+19]Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V . Le. Unsupervised data\naugmentation for consistency training, 2019.\n[YdC+19]Dani Yogatama, Cyprien de Masson d\u2019Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski,\nLingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, et al. Learning and evaluating\ngeneral linguistic intelligence. arXiv preprint arXiv:1901.11373 , 2019.\n[YDY+19]Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V . Le. XLNet:\nGeneralized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237 ,\n2019.\n[ZHB+19]Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine\nreally \ufb01nish your sentence? arXiv preprint arXiv:1905.07830 , 2019.\n[ZHR+19]Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin\nChoi. Defending against neural fake news. arXiv preprint arXiv:1905.12616 , 2019.\n[ZLL+18]Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.\nReCoRD: Bridging the gap between human and machine commonsense reading comprehension. arXiv\npreprint arXiv:1810.12885 , 2018.\n[ZSW+19a] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2019.\n74\n[ZSW+19b] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. Fine-tuning language models from human preferences. ArXiv , abs/1909.08593,\n2019.\n75\n",
    "title": "Language Models are Few-Shot Learners",
    "arxiv_id": "2005.14165",
    "category": "famous",
    "authors": [
      "Tom B. Brown",
      "Benjamin Mann",
      "Nick Ryder",
      "Melanie Subbiah",
      "Jared Kaplan",
      "Prafulla Dhariwal",
      "Arvind Neelakantan",
      "Pranav Shyam",
      "Girish Sastry",
      "Amanda Askell",
      "Sandhini Agarwal",
      "Ariel Herbert-Voss",
      "Gretchen Krueger",
      "Tom Henighan",
      "Rewon Child",
      "Aditya Ramesh",
      "Daniel M. Ziegler",
      "Jeffrey Wu",
      "Clemens Winter",
      "Christopher Hesse",
      "Mark Chen",
      "Eric Sigler",
      "Mateusz Litwin",
      "Scott Gray",
      "Benjamin Chess",
      "Jack Clark",
      "Christopher Berner",
      "Sam McCandlish",
      "Alec Radford",
      "Ilya Sutskever",
      "Dario Amodei"
    ],
    "published": "2020-05-28",
    "num_pages": 75,
    "num_chars": 236594
  },
  {
    "text": "Published as a conference paper at ICLR 2021\nANIMAGE IS WORTH 16X16 W ORDS :\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\nAlexey Dosovitskiy\u0003;y, Lucas Beyer\u0003, Alexander Kolesnikov\u0003, Dirk Weissenborn\u0003,\nXiaohua Zhai\u0003, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby\u0003;y\n\u0003equal technical contribution,yequal advising\nGoogle Research, Brain Team\nfadosovitskiy, neilhoulsby g@google.com\nABSTRACT\nWhile the Transformer architecture has become the de-facto standard for natural\nlanguage processing tasks, its applications to computer vision remain limited. In\nvision, attention is either applied in conjunction with convolutional networks, or\nused to replace certain components of convolutional networks while keeping their\noverall structure in place. We show that this reliance on CNNs is not necessary\nand a pure transformer applied directly to sequences of image patches can perform\nvery well on image classi\ufb01cation tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\nresults compared to state-of-the-art convolutional networks while requiring sub-\nstantially fewer computational resources to train.1\n1 I NTRODUCTION\nSelf-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become\nthe model of choice in natural language processing (NLP). The dominant approach is to pre-train on\na large text corpus and then \ufb01ne-tune on a smaller task-speci\ufb01c dataset (Devlin et al., 2019). Thanks\nto Transformers\u2019 computational ef\ufb01ciency and scalability, it has become possible to train models of\nunprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the\nmodels and datasets growing, there is still no sign of saturating performance.\nIn computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989;\nKrizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining\nCNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing\nthe convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while\ntheoretically ef\ufb01cient, have not yet been scaled effectively on modern hardware accelerators due to\nthe use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet-\nlike architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al.,\n2020).\nInspired by the Transformer scaling successes in NLP, we experiment with applying a standard\nTransformer directly to images, with the fewest possible modi\ufb01cations. To do so, we split an image\ninto patches and provide the sequence of linear embeddings of these patches as an input to a Trans-\nformer. Image patches are treated the same way as tokens (words) in an NLP application. We train\nthe model on image classi\ufb01cation in supervised fashion.\nWhen trained on mid-sized datasets such as ImageNet without strong regularization, these mod-\nels yield modest accuracies of a few percentage points below ResNets of comparable size. This\nseemingly discouraging outcome may be expected: Transformers lack some of the inductive biases\n1Fine-tuning code and pre-trained models are available at https://github.com/\ngoogle-research/vision_transformer\n1arXiv:2010.11929v2  [cs.CV]  3 Jun 2021\nPublished as a conference paper at ICLR 2021\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\nwhen trained on insuf\ufb01cient amounts of data.\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We\n\ufb01nd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\nresults when pre-trained at suf\ufb01cient scale and transferred to tasks with fewer datapoints. When\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\nor beats state of the art on multiple image recognition benchmarks. In particular, the best model\nreaches the accuracy of 88:55% on ImageNet, 90:72% on ImageNet-ReaL, 94:55% on CIFAR-100,\nand77:63% on the VTAB suite of 19 tasks.\n2 R ELATED WORK\nTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\ncome the state of the art method in many NLP tasks. Large Transformer-based models are often\npre-trained on large corpora and then \ufb01ne-tuned for the task at hand: BERT (Devlin et al., 2019)\nuses a denoising self-supervised pre-training task, while the GPT line of work uses language mod-\neling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020).\nNaive application of self-attention to images would require that each pixel attends to every other\npixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,\nto apply Transformers in the context of image processing, several approximations have been tried in\nthe past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query\npixel instead of globally. Such local multi-head dot-product self attention blocks can completely\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\nattention in order to be applicable to images. An alternative way to scale attention is to apply it in\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\net al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate\npromising results on computer vision tasks, but require complex engineering to be implemented\nef\ufb01ciently on hardware accelerators.\nMost related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2\u00022\nfrom the input image and applies full self-attention on top. This model is very similar to ViT,\nbut our work goes further to demonstrate that large scale pre-training makes vanilla transformers\ncompetitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020)\nuse a small patch size of 2\u00022pixels, which makes the model applicable only to small-resolution\nimages, while we handle medium-resolution images as well.\nThere has also been a lot of interest in combining convolutional neural networks (CNNs) with forms\nof self-attention, e.g. by augmenting feature maps for image classi\ufb01cation (Bello et al., 2019) or by\nfurther processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018;\nCarion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classi\ufb01cation (Wu\net al., 2020), unsupervised object discovery (Locatello et al., 2020), or uni\ufb01ed text-vision tasks (Chen\net al., 2020c; Lu et al., 2019; Li et al., 2019).\nAnother recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers\nto image pixels after reducing image resolution and color space. The model is trained in an unsu-\npervised fashion as a generative model, and the resulting representation can then be \ufb01ne-tuned or\nprobed linearly for classi\ufb01cation performance, achieving a maximal accuracy of 72% on ImageNet.\nOur work adds to the increasing collection of papers that explore image recognition at larger scales\nthan the standard ImageNet dataset. The use of additional data sources allows to achieve state-of-\nthe-art results on standard benchmarks (Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020).\nMoreover, Sun et al. (2017) study how CNN performance scales with dataset size, and Kolesnikov\net al. (2020); Djolonga et al. (2020) perform an empirical exploration of CNN transfer learning from\nlarge scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as\nwell, but train Transformers instead of ResNet-based models used in prior works.\n2\nPublished as a conference paper at ICLR 2021\nTransformer \nEncoder\nMLP \nHead\nVision \nTransformer \n(ViT)*\nLinear \nProjection \nof \nFlattened \nPatches\n*\n \nExtra \nlearnable\n     \n[class]\n \nembedding\n1\n2\n3\n4\n5\n6\n7\n8\n90Patch \n+ \nPosition \nEmbedding\nClass\nBird\nBall\nCar\n...\nEmbedded \nPatches\nMulti-Head \nAttention\nNorm\nMLP\nNorm\n+\nL\n \nx\n+Transformer \nEncoder\nFigure 1: Model overview. We split an image into \ufb01xed-size patches, linearly embed each of them,\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\nencoder. In order to perform classi\ufb01cation, we use the standard approach of adding an extra learnable\n\u201cclassi\ufb01cation token\u201d to the sequence. The illustration of the Transformer encoder was inspired by\nVaswani et al. (2017).\n3 M ETHOD\nIn model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.\nAn advantage of this intentionally simple setup is that scalable NLP Transformer architectures \u2013 and\ntheir ef\ufb01cient implementations \u2013 can be used almost out of the box.\n3.1 V ISION TRANSFORMER (VIT)\nAn overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\nsequence of token embeddings. To handle 2D images, we reshape the image x2RH\u0002W\u0002Cinto a\nsequence of \ufb02attened 2D patches xp2RN\u0002(P2\u0001C), where (H;W )is the resolution of the original\nimage,Cis the number of channels, (P;P)is the resolution of each image patch, and N=HW=P2\nis the resulting number of patches, which also serves as the effective input sequence length for the\nTransformer. The Transformer uses constant latent vector size Dthrough all of its layers, so we\n\ufb02atten the patches and map to Ddimensions with a trainable linear projection (Eq. 1). We refer to\nthe output of this projection as the patch embeddings.\nSimilar to BERT\u2019s [class] token, we prepend a learnable embedding to the sequence of embed-\nded patches ( z0\n0=xclass), whose state at the output of the Transformer encoder ( z0\nL) serves as the\nimage representation y(Eq. 4). Both during pre-training and \ufb01ne-tuning, a classi\ufb01cation head is at-\ntached to z0\nL. The classi\ufb01cation head is implemented by a MLP with one hidden layer at pre-training\ntime and by a single linear layer at \ufb01ne-tuning time.\nPosition embeddings are added to the patch embeddings to retain positional information. We use\nstandard learnable 1D position embeddings, since we have not observed signi\ufb01cant performance\ngains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting\nsequence of embedding vectors serves as input to the encoder.\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\nattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before\nevery block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\n3\nPublished as a conference paper at ICLR 2021\nThe MLP contains two layers with a GELU non-linearity.\nz0= [xclass;x1\npE;x2\npE;\u0001\u0001\u0001;xN\npE] +Epos;E2R(P2\u0001C)\u0002D;Epos2R(N+1)\u0002D(1)\nz0\n`= MSA(LN( z`\u00001)) +z`\u00001; ` = 1:::L (2)\nz`= MLP(LN( z0\n`)) +z0\n`; ` = 1:::L (3)\ny= LN( z0\nL) (4)\nInductive bias. We note that Vision Transformer has much less image-speci\ufb01c inductive bias than\nCNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are\nbaked into each layer throughout the whole model. In ViT, only MLP layers are local and transla-\ntionally equivariant, while the self-attention layers are global. The two-dimensional neighborhood\nstructure is used very sparingly: in the beginning of the model by cutting the image into patches and\nat \ufb01ne-tuning time for adjusting the position embeddings for images of different resolution (as de-\nscribed below). Other than that, the position embeddings at initialization time carry no information\nabout the 2D positions of the patches and all spatial relations between the patches have to be learned\nfrom scratch.\nHybrid Architecture. As an alternative to raw image patches, the input sequence can be formed\nfrom feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding\nprojection E(Eq. 1) is applied to patches extracted from a CNN feature map. As a special case,\nthe patches can have spatial size 1x1, which means that the input sequence is obtained by simply\n\ufb02attening the spatial dimensions of the feature map and projecting to the Transformer dimension.\nThe classi\ufb01cation input embedding and position embeddings are added as described above.\n3.2 F INE-TUNING AND HIGHER RESOLUTION\nTypically, we pre-train ViT on large datasets, and \ufb01ne-tune to (smaller) downstream tasks. For\nthis, we remove the pre-trained prediction head and attach a zero-initialized D\u0002Kfeedforward\nlayer, where Kis the number of downstream classes. It is often bene\ufb01cial to \ufb01ne-tune at higher\nresolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020). When feeding images\nof higher resolution, we keep the patch size the same, which results in a larger effective sequence\nlength. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints),\nhowever, the pre-trained position embeddings may no longer be meaningful. We therefore perform\n2D interpolation of the pre-trained position embeddings, according to their location in the original\nimage. Note that this resolution adjustment and patch extraction are the only points at which an\ninductive bias about the 2D structure of the images is manually injected into the Vision Transformer.\n4 E XPERIMENTS\nWe evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the\nhybrid. To understand the data requirements of each model, we pre-train on datasets of varying size\nand evaluate many benchmark tasks. When considering the computational cost of pre-training the\nmodel, ViT performs very favourably, attaining state of the art on most recognition benchmarks at\na lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show\nthat self-supervised ViT holds promise for the future.\n4.1 S ETUP\nDatasets. To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes\nand 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with\n21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and\n303M high-resolution images. We de-duplicate the pre-training datasets w.r.t. the test sets of the\ndownstream tasks following Kolesnikov et al. (2020). We transfer the models trained on these\ndataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up\nReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al.,\n2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008). For these datasets, pre-processing\nfollows Kolesnikov et al. (2020).\n4\nPublished as a conference paper at ICLR 2021\nModel Layers Hidden size D MLP size Heads Params\nViT-Base 12 768 3072 12 86M\nViT-Large 24 1024 4096 16 307M\nViT-Huge 32 1280 5120 16 632M\nTable 1: Details of Vision Transformer model variants.\nWe also evaluate on the 19-task VTAB classi\ufb01cation suite (Zhai et al., 2019b). VTAB evaluates\nlow-data transfer to diverse tasks, using 1 000 training examples per task. The tasks are divided into\nthree groups: Natural \u2013 tasks like the above, Pets, CIFAR, etc. Specialized \u2013 medical and satellite\nimagery, and Structured \u2013 tasks that require geometric understanding like localization.\nModel Variants. We base ViT con\ufb01gurations on those used for BERT (Devlin et al., 2019), as\nsummarized in Table 1. The \u201cBase\u201d and \u201cLarge\u201d models are directly adopted from BERT and we\nadd the larger \u201cHuge\u201d model. In what follows we use brief notation to indicate the model size and\nthe input patch size: for instance, ViT-L/16 means the \u201cLarge\u201d variant with 16\u000216input patch size.\nNote that the Transformer\u2019s sequence length is inversely proportional to the square of the patch size,\nthus models with smaller patch size are computationally more expensive.\nFor the baseline CNNs, we use ResNet (He et al., 2016), but replace the Batch Normalization lay-\ners (Ioffe & Szegedy, 2015) with Group Normalization (Wu & He, 2018), and used standardized\nconvolutions (Qiao et al., 2019). These modi\ufb01cations improve transfer (Kolesnikov et al., 2020),\nand we denote the modi\ufb01ed model \u201cResNet (BiT)\u201d. For the hybrids, we feed the intermediate fea-\nture maps into ViT with patch size of one \u201cpixel\u201d. To experiment with different sequence lengths,\nwe either (i) take the output of stage 4 of a regular ResNet50 or (ii) remove stage 4, place the same\nnumber of layers in stage 3 (keeping the total number of layers), and take the output of this extended\nstage 3. Option (ii) results in a 4x longer sequence length, and a more expensive ViT model.\nTraining & Fine-tuning. We train all models, including ResNets, using Adam (Kingma & Ba,\n2015) with\f1= 0:9,\f2= 0:999, a batch size of 4096 and apply a high weight decay of 0:1, which\nwe found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common\npractices, Adam works slightly better than SGD for ResNets in our setting). We use a linear learning\nrate warmup and decay, see Appendix B.1 for details. For \ufb01ne-tuning we use SGD with momentum,\nbatch size 512, for all models, see Appendix B.1.1. For ImageNet results in Table 2, we \ufb01ne-tuned at\nhigher resolution: 512for ViT-L/16 and 518for ViT-H/14, and also used Polyak & Juditsky (1992)\naveraging with a factor of 0:9999 (Ramachandran et al., 2019; Wang et al., 2020b).\nMetrics. We report results on downstream datasets either through few-shot or \ufb01ne-tuning accuracy.\nFine-tuning accuracies capture the performance of each model after \ufb01ne-tuning it on the respective\ndataset. Few-shot accuracies are obtained by solving a regularized least-squares regression problem\nthat maps the (frozen) representation of a subset of training images to f\u00001;1gKtarget vectors. This\nformulation allows us to recover the exact solution in closed form. Though we mainly focus on\n\ufb01ne-tuning performance, we sometimes use linear few-shot accuracies for fast on-the-\ufb02y evaluation\nwhere \ufb01ne-tuning would be too costly.\n4.2 C OMPARISON TO STATE OF THE ART\nWe \ufb01rst compare our largest models \u2013 ViT-H/14 and ViT-L/16 \u2013 to state-of-the-art CNNs from\nthe literature. The \ufb01rst comparison point is Big Transfer (BiT) (Kolesnikov et al., 2020), which\nperforms supervised transfer learning with large ResNets. The second is Noisy Student (Xie et al.,\n2020), which is a large Ef\ufb01cientNet trained using semi-supervised learning on ImageNet and JFT-\n300M with the labels removed. Currently, Noisy Student is the state of the art on ImageNet and\nBiT-L on the other datasets reported here. All models were trained on TPUv3 hardware, and we\nreport the number of TPUv3-core-days taken to pre-train each of them, that is, the number of TPU\nv3 cores (2 per chip) used for training multiplied by the training time in days.\nTable 2 shows the results. The smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L\n(which is pre-trained on the same dataset) on all tasks, while requiring substantially less computa-\ntional resources to train. The larger model, ViT-H/14, further improves the performance, especially\non the more challenging datasets \u2013 ImageNet, CIFAR-100, and the VTAB suite. Interestingly, this\n5\nPublished as a conference paper at ICLR 2021\nOurs-JFT Ours-JFT Ours-I21k BiT-L Noisy Student\n(ViT-H/14) (ViT-L/16) (ViT-L/16) (ResNet152x4) (Ef\ufb01cientNet-L2)\nImageNet 88:55\u00060:04 87:76\u00060:03 85:30\u00060:02 87:54\u00060:02 88:4=88:5\u0003\nImageNet ReaL 90:72\u00060:05 90:54\u00060:03 88:62\u00060:05 90:54 90 :55\nCIFAR-10 99:50\u00060:06 99:42\u00060:03 99:15\u00060:03 99:37\u00060:06\u0000\nCIFAR-100 94:55\u00060:04 93:90\u00060:05 93:25\u00060:05 93:51\u00060:08\u0000\nOxford-IIIT Pets 97:56\u00060:03 97:32\u00060:11 94:67\u00060:15 96:62\u00060:23\u0000\nOxford Flowers-102 99:68\u00060:0299:74\u00060:0099:61\u00060:02 99:63\u00060:03\u0000\nVTAB (19 tasks) 77:63\u00060:23 76:28\u00060:46 72:72\u00060:21 76:29\u00061:70\u0000\nTPUv3-core-days 2:5k 0:68k 0:23k 9:9k 12:3k\nTable 2: Comparison with state of the art on popular image classi\ufb01cation benchmarks. We re-\nport mean and standard deviation of the accuracies, averaged over three \ufb01ne-tuning runs. Vision\nTransformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on all\ndatasets, while taking substantially less computational resources to pre-train. ViT pre-trained on the\nsmaller public ImageNet-21k dataset performs well too.\u0003Slightly improved 88:5%result reported\nin Touvron et al. (2020).\nVTAB (19 tasks)65707580Accuracy [%]\nNatural (7 tasks)708090\nSpecialized (4 tasks)8082858890\nStructured (8 tasks)506070ViT-H/14 BiT-L (R152x4) VIVI-Ex-100% (R50x3) S4L (R50x1)\nFigure 2: Breakdown of VTAB performance in Natural ,Specialized , and Structured task groups.\nmodel still took substantially less compute to pre-train than prior state of the art. However, we note\nthat pre-training ef\ufb01ciency may be affected not only by the architecture choice, but also other pa-\nrameters, such as training schedule, optimizer, weight decay, etc. We provide a controlled study of\nperformance vs. compute for different architectures in Section 4.4. Finally, the ViT-L/16 model\npre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking\nfewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in ap-\nproximately 30 days.\nFigure 2 decomposes the VTAB tasks into their respective groups, and compares to previous SOTA\nmethods on this benchmark: BiT, VIVI \u2013 a ResNet co-trained on ImageNet and Youtube (Tschannen\net al., 2020), and S4L \u2013 supervised plus semi-supervised learning on ImageNet (Zhai et al., 2019a).\nViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural andStructured tasks. On the\nSpecialized the performance of the top two models is similar.\n4.3 P RE-TRAINING DATA REQUIREMENTS\nThe Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer\ninductive biases for vision than ResNets, how crucial is the dataset size? We perform two series of\nexperiments.\nFirst, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-\n300M. To boost the performance on the smaller datasets, we optimize three basic regularization\nparameters \u2013 weight decay, dropout, and label smoothing. Figure 3 shows the results after \ufb01ne-\ntuning to ImageNet (results on other datasets are shown in Table 5)2. When pre-trained on the\nsmallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite\n(moderate) regularization. With ImageNet-21k pre-training, their performances are similar. Only\nwith JFT-300M, do we see the full bene\ufb01t of larger models. Figure 3 also shows the performance\n2Note that the ImageNet pre-trained models are also \ufb01ne-tuned, but again on ImageNet. This is because the\nresolution increase during \ufb01ne-tuning improves the performance.\n6\nPublished as a conference paper at ICLR 2021\nImageNet ImageNet-21k JFT-300M\nPre-training dataset7075808590ImageNet Top1 Accuracy [%]\nBiT\nViT-B/32\nViT-B/16ViT-L/32\nViT-L/16\nViT-H/14\nFigure 3: Transfer to ImageNet. While\nlarge ViT models perform worse than BiT\nResNets (shaded area) when pre-trained on\nsmall datasets, they shine when pre-trained on\nlarger datasets. Similarly, larger ViT variants\novertake smaller ones as the dataset grows.\n10 M 30 M 100 M 300 M\nNumber of JFT pre-training samples3040506070Linear 5-shot ImageNet Top1 [%]\nViT-L/16\nViT-L/32ViT-B/32\nViT-b/32ResNet50x1 (BiT)\nResNet152x2 (BiT)Figure 4: Linear few-shot evaluation on Ima-\ngeNet versus pre-training size. ResNets per-\nform better with smaller pre-training datasets\nbut plateau sooner than ViT, which performs\nbetter with larger pre-training. ViT-b is ViT-B\nwith all hidden dimensions halved.\n1021039095Transfer accuracy [%]Average-5\nTransformer (ViT)\nResNet (BiT)\nHybrid\n10210375808590ImageNet\nTransformer (ViT)\nResNet (BiT)\nHybrid\nTotal pre-training compute [exaFLOPs]\nFigure 5: Performance versus pre-training compute for different architectures: Vision Transformers,\nResNets, and hybrids. Vision Transformers generally outperform ResNets with the same compu-\ntational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap\nvanishes for larger models.\nregion spanned by BiT models of different sizes. The BiT CNNs outperform ViT on ImageNet, but\nwith the larger datasets, ViT overtakes.\nSecond, we train our models on random subsets of 9M, 30M, and 90M as well as the full JFT-\n300M dataset. We do not perform additional regularization on the smaller subsets and use the same\nhyper-parameters for all settings. This way, we assess the intrinsic model properties, and not the\neffect of regularization. We do, however, use early-stopping, and report the best validation accuracy\nachieved during training. To save compute, we report few-shot linear accuracy instead of full \ufb01ne-\ntuning accuracy. Figure 4 contains the results. Vision Transformers over\ufb01t more than ResNets with\ncomparable computational cost on smaller datasets. For example, ViT-B/32 is slightly faster than\nResNet50; it performs much worse on the 9M subset, but better on 90M+ subsets. The same is true\nfor ResNet152x2 and ViT-L/16. This result reinforces the intuition that the convolutional inductive\nbias is useful for smaller datasets, but for larger ones, learning the relevant patterns directly from\ndata is suf\ufb01cient, even bene\ufb01cial.\nOverall, the few-shot results on ImageNet (Figure 4), as well as the low-data results on VTAB\n(Table 2) seem promising for very low-data transfer. Further analysis of few-shot properties of ViT\nis an exciting direction of future work.\n7\nPublished as a conference paper at ICLR 2021\n4.4 S CALING STUDY\nWe perform a controlled scaling study of different models by evaluating transfer performance from\nJFT-300M. In this setting data size does not bottleneck the models\u2019 performances, and we assess\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-\ntrained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the\nend of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet\nbackbone).\nFigure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\nfor details on computational costs). Detailed results per model are provided in Table 6 in the Ap-\npendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the\nperformance/compute trade-off. ViT uses approximately 2\u00004\u0002less compute to attain the same\nperformance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu-\ntational budgets, but the difference vanishes for larger models. This result is somewhat surprising,\nsince one might expect convolutional local feature processing to assist ViT at any size. Third, Vision\nTransformers appear not to saturate within the range tried, motivating future scaling efforts.\n4.5 I NSPECTING VISION TRANSFORMER\nInput\n Attention\nFigure 6: Representative ex-\namples of attention from the\noutput token to the input\nspace. See Appendix D.7 for\ndetails.To begin to understand how the Vision Transformer processes im-\nage data, we analyze its internal representations. The \ufb01rst layer of\nthe Vision Transformer linearly projects the \ufb02attened patches into a\nlower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin-\ncipal components of the the learned embedding \ufb01lters. The com-\nponents resemble plausible basis functions for a low-dimensional\nrepresentation of the \ufb01ne structure within each patch.\nAfter the projection, a learned position embedding is added to the\npatch representations. Figure 7 (center) shows that the model learns\nto encode distance within the image in the similarity of position em-\nbeddings, i.e. closer patches tend to have more similar position em-\nbeddings. Further, the row-column structure appears; patches in the\nsame row/column have similar embeddings. Finally, a sinusoidal\nstructure is sometimes apparent for larger grids (Appendix D). That\nthe position embeddings learn to represent 2D image topology ex-\nplains why hand-crafted 2D-aware embedding variants do not yield\nimprovements (Appendix D.4).\nSelf-attention allows ViT to integrate information across the entire\nimage even in the lowest layers. We investigate to what degree\nthe network makes use of this capability. Speci\ufb01cally, we compute\nthe average distance in image space across which information is\nintegrated, based on the attention weights (Figure 7, right). This\n\u201cattention distance\u201d is analogous to receptive \ufb01eld size in CNNs.\nWe \ufb01nd that some heads attend to most of the image already in the lowest layers, showing that\nthe ability to integrate information globally is indeed used by the model. Other attention heads\nhave consistently small attention distances in the low layers. This highly localized attention is\nless pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\nsuggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the\nattention distance increases with network depth. Globally, we \ufb01nd that the model attends to image\nregions that are semantically relevant for classi\ufb01cation (Figure 6).\n4.6 S ELF-SUPERVISION\nTransformers show impressive performance on NLP tasks. However, much of their success stems\nnot only from their excellent scalability but also from large scale self-supervised pre-training (Devlin\n8\nPublished as a conference paper at ICLR 2021\nRGB embedding filters\n(first 28 principal components)\n1 2 3 4 5 6 7\nInput patch column1\n2\n3\n4\n5\n6\n7Input patch rowPosition embedding similarity\n1\n1\nCosine similarity\n0 5 10 15 20\nNetwork depth (layer)020406080100120Mean attention distance (pixels)\nViT-L/16\nHead 1\nHead 2\nHead 3\n...\nFigure 7: Left: Filters of the initial linear embedding of RGB values of ViT-L/32. Center: Sim-\nilarity of position embeddings of ViT-L/32. Tiles show the cosine similarity between the position\nembedding of the patch with the indicated row and column and the position embeddings of all other\npatches. Right: Size of attended area by head and network depth. Each dot shows the mean attention\ndistance across images for one of 16 heads at one layer. See Appendix D.7 for details.\net al., 2019; Radford et al., 2018). We also perform a preliminary exploration on masked patch\nprediction for self-supervision, mimicking the masked language modeling task used in BERT. With\nself-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a\nsigni\ufb01cant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.\nAppendix B.1.2 contains further details. We leave exploration of contrastive pre-training (Chen\net al., 2020b; He et al., 2020; Bachman et al., 2019; H \u00b4enaff et al., 2020) to future work.\n5 C ONCLUSION\nWe have explored the direct application of Transformers to image recognition. Unlike prior works\nusing self-attention in computer vision, we do not introduce image-speci\ufb01c inductive biases into\nthe architecture apart from the initial patch extraction step. Instead, we interpret an image as a\nsequence of patches and process it by a standard Transformer encoder as used in NLP. This simple,\nyet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\nThus, Vision Transformer matches or exceeds the state of the art on many image classi\ufb01cation\ndatasets, whilst being relatively cheap to pre-train.\nWhile these initial results are encouraging, many challenges remain. One is to apply ViT to other\ncomputer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion\net al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-\nsupervised pre-training methods. Our initial experiments show improvement from self-supervised\npre-training, but there is still large gap between self-supervised and large-scale supervised pre-\ntraining. Finally, further scaling of ViT would likely lead to improved performance.\nACKNOWLEDGEMENTS\nThe work was performed in Berlin, Z \u00a8urich, and Amsterdam. We thank many colleagues at Google\nfor their help, in particular Andreas Steiner for crucial help with the infrastructure and the open-\nsource release of the code; Joan Puigcerver and Maxim Neumann for help with the large-scale\ntraining infrastructure; Dmitry Lepikhin, Aravindh Mahendran, Daniel Keysers, Mario Lu \u02c7ci\u00b4c, Noam\nShazeer, Ashish Vaswani, and Colin Raffel for useful discussions.\nREFERENCES\nSamira Abnar and Willem Zuidema. Quantifying attention \ufb02ow in transformers. In ACL, 2020.\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing\nmutual information across views. In NeurIPS , 2019.\n9\nPublished as a conference paper at ICLR 2021\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In\nICLR , 2019.\nI. Bello, B. Zoph, Q. Le, A. Vaswani, and J. Shlens. Attention augmented convolutional networks.\nInICCV , 2019.\nLucas Beyer, Olivier J. H \u00b4enaff, Alexander Kolesnikov, Xiaohua Zhai, and A \u00a8aron van den Oord. Are\nwe done with imagenet? arXiv , 2020.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv , 2020.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV , 2020.\nMark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining from\npixels. In ICML , 2020a.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework\nfor contrastive learning of visual representations. In ICML , 2020b.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. In ECCV , 2020c.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv , 2019.\nJean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-\nattention and convolutional layers. In ICLR , 2020.\nJ. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In CVPR , 2009.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL , 2019.\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander\nKolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D\u2019Amour, Dan Moldovan, Sylvan\nGelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On robustness and transferability of convo-\nlutional neural networks. arXiv , 2020.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In CVPR , 2016.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In CVPR , 2020.\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-\nmensional transformers. arXiv , 2019.\nHan Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object\ndetection. In CVPR , 2018.\nHan Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition.\nInICCV , 2019.\nZilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, and\nThomas S. Huang. Ccnet: Criss-cross attention for semantic segmentation. In ICCV , 2020.\nOlivier J. H \u00b4enaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami,\nand Aaron van den Oord. Data-ef\ufb01cient image recognition with contrastive predictive coding. In\nICML , 2020.\n10\nPublished as a conference paper at ICLR 2021\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. 2015.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,\nand Neil Houlsby. Big transfer (BiT): General visual representation learning. In ECCV , 2020.\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classi\ufb01cation with deep convo-\nlutional neural networks. In NIPS , 2012.\nY . LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropa-\ngation applied to handwritten zip code recognition. Neural Computation , 1:541\u2013551, 1989.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\ncomputation and automatic sharding. arXiv , 2020.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A\nSimple and Performant Baseline for Vision and Language. In Arxiv , 2019.\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,\nJakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten-\ntion. arXiv , 2020.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visi-\nolinguistic Representations for Vision-and-Language Tasks. In NeurIPS . 2019.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\nAshwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised\npretraining. In ECCV , 2018.\nM. Nilsback and A. Zisserman. Automated \ufb02ower classi\ufb01cation over a large number of classes. In\nICVGIP , 2008.\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V . Jawahar. Cats and dogs. In CVPR ,\n2012.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. In ICML , 2018.\nB. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM\nJournal on Control and Optimization , 30(4):838\u2013855, 1992. doi: 10.1137/0330046. URL\nhttps://doi.org/10.1137/0330046 .\nSiyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Weight standardization. arXiv\npreprint arXiv:1903.10520 , 2019.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding with unsupervised learning. Technical Report , 2018.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. Technical Report , 2019.\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.\nStand-alone self-attention in vision models. In NeurIPS , 2019.\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable ef-\nfectiveness of data in deep learning era. In ICCV , 2017.\nChen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint\nmodel for video and language representation learning. In ICCV , 2019.\n11\nPublished as a conference paper at ICLR 2021\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\ndiscrepancy. In NeurIPS . 2019.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\ndiscrepancy: Fixef\ufb01cientnet. arXiv preprint arXiv:2003.08237 , 2020.\nMichael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Neil Houlsby, Sylvain\nGelly, and Mario Lucic. Self-supervised learning of video-induced visual invariances. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June\n2020.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS , 2017.\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV , 2020a.\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh\nChen. Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. arXiv preprint\narXiv:2003.07853 , 2020b.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao.\nLearning deep transformer models for machine translation. In ACL, 2019.\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\nCVPR , 2018.\nDirk Weissenborn, Oscar T \u00a8ackstr \u00a8om, and Jakob Uszkoreit. Scaling autoregressive video models. In\nICLR , 2019.\nBichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt\nKeutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing\nfor computer vision. arxiv , 2020.\nYuxin Wu and Kaiming He. Group normalization. In ECCV , 2018.\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V . Le. Self-training with noisy student\nimproves imagenet classi\ufb01cation. In CVPR , 2020.\nXiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4L: Self-Supervised Semi-\nSupervised Learning. In ICCV , 2019a.\nXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario\nLucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A\nlarge-scale study of representation learning with the visual task adaptation benchmark. arXiv\npreprint arXiv:1910.04867 , 2019b.\nHengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In\nCVPR , 2020.\n12\nPublished as a conference paper at ICLR 2021\nModels Dataset Epochs Base LR LR decay Weight decay Dropout\nViT-B/f16,32g JFT-300M 7 8\u000110\u00004linear 0.1 0.0\nViT-L/32 JFT-300M 7 6\u000110\u00004linear 0.1 0.0\nViT-L/16 JFT-300M 7/14 4\u000110\u00004linear 0.1 0.0\nViT-H/14 JFT-300M 14 3\u000110\u00004linear 0.1 0.0\nR50xf1,2g JFT-300M 7 10\u00003linear 0.1 0.0\nR101x1 JFT-300M 7 8\u000110\u00004linear 0.1 0.0\nR152xf1,2g JFT-300M 7 6\u000110\u00004linear 0.1 0.0\nR50+ViT-B/f16,32gJFT-300M 7 8\u000110\u00004linear 0.1 0.0\nR50+ViT-L/32 JFT-300M 7 2\u000110\u00004linear 0.1 0.0\nR50+ViT-L/16 JFT-300M 7/14 4\u000110\u00004linear 0.1 0.0\nViT-B/f16,32g ImageNet-21k 90 10\u00003linear 0.03 0.1\nViT-L/f16,32g ImageNet-21k 30/90 10\u00003linear 0.03 0.1\nViT-\u0003 ImageNet 300 3\u000110\u00003cosine 0.3 0.1\nTable 3: Hyperparameters for training. All models are trained with a batch size of 4096 and learn-\ning rate warmup of 10k steps. For ImageNet we found it bene\ufb01cial to additionally apply gradient\nclipping at global norm 1. Training resolution is 224.\nAPPENDIX\nA M ULTIHEAD SELF-ATTENTION\nStandard qkv self-attention (SA, Vaswani et al. (2017)) is a popular building block for neural archi-\ntectures. For each element in an input sequence z2RN\u0002D, we compute a weighted sum over all\nvalues vin the sequence. The attention weights Aijare based on the pairwise similarity between\ntwo elements of the sequence and their respective query qiand key kjrepresentations.\n[q;k;v] =zUqkv Uqkv2RD\u00023Dh; (5)\nA= softmax\u0010\nqk>=p\nDh\u0011\nA2RN\u0002N; (6)\nSA(z) =Av: (7)\nMultihead self-attention (MSA) is an extension of SA in which we run kself-attention operations,\ncalled \u201cheads\u201d, in parallel, and project their concatenated outputs. To keep compute and number of\nparameters constant when changing k,Dh(Eq. 5) is typically set to D=k .\nMSA( z) = [SA 1(z); SA 2(z);\u0001\u0001\u0001; SAk(z)]Umsa Umsa2Rk\u0001Dh\u0002D(8)\nB E XPERIMENT DETAILS\nB.1 T RAINING\nTable 3 summarizes our training setups for our different models. We found strong regularization\nto be key when training models from scratch on ImageNet. Dropout, when used, is applied after\nevery dense layer except for the the qkv-projections and directly after adding positional- to patch\nembeddings. Hybrid models are trained with the exact setup as their ViT counterparts. Finally, all\ntraining is done on resolution 224.\nB.1.1 F INE-TUNING\nWe \ufb01ne-tune all ViT models using SGD with a momentum of 0.9. We run a small grid search over\nlearning rates, see learning rate ranges in Table 4. To do so, we use small sub-splits from the training\nset (10% for Pets and Flowers, 2% for CIFAR, 1% ImageNet) as development set and train on the\nremaining data. For \ufb01nal results we train on the entire training set and evaluate on the respective\ntest data. For \ufb01ne-tuning ResNets and hybrid models we use the exact same setup, with the only\nexception of ImageNet where we add another value 0:06to the learning rate sweep. Additionally,\n13\nPublished as a conference paper at ICLR 2021\nDataset Steps Base LR\nImageNet 20 000 f0.003, 0.01, 0.03, 0.06 g\nCIFAR100 10 000 f0.001, 0.003, 0.01, 0.03 g\nCIFAR10 10 000 f0.001, 0.003, 0.01, 0.03 g\nOxford-IIIT Pets 500 f0.001, 0.003, 0.01, 0.03 g\nOxford Flowers-102 500 f0.001, 0.003, 0.01, 0.03 g\nVTAB (19 tasks) 2 500 0.01\nTable 4: Hyperparameters for \ufb01ne-tuning. All models are \ufb01ne-tuned with cosine learning rate decay,\na batch size of 512, no weight decay, and grad clipping at global norm 1. If not mentioned otherwise,\n\ufb01ne-tuning resolution is 384.\nfor ResNets we also run the setup of Kolesnikov et al. (2020) and select the best results across\nthis run and our sweep. Finally, if not mentioned otherwise, all \ufb01ne-tuning experiments run at 384\nresolution (running \ufb01ne-tuning at different resolution than training is common practice (Kolesnikov\net al., 2020)).\nWhen transferring ViT models to another dataset, we remove the whole head (two linear layers) and\nreplace it by a single, zero-initialized linear layer outputting the number of classes required by the\ntarget dataset. We found this to be a little more robust than simply re-initializing the very last layer.\nFor VTAB we follow the protocol in Kolesnikov et al. (2020), and use the same hyperparameter\nsetting for all tasks. We use a learning rate of 0:01and train for 2500 steps (Tab. 4). We chose this\nsetting by running a small sweep over two learning rates and two schedules, and selecting the setting\nwith the highest VTAB score on the 200-example validation sets. We follow the pre-processing used\nin Kolesnikov et al. (2020), except that we do not use task-speci\ufb01c input resolutions. Instead we \ufb01nd\nthat Vision Transformer bene\ufb01ts most from a high resolution ( 384\u0002384) for all tasks.\nB.1.2 S ELF-SUPERVISION\nWe employ the masked patch prediction objective for preliminary self-supervision experiments. To\ndo so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable\n[mask] embedding (80%), a random other patch embedding (10%) or just keeping them as is\n(10%). This setup is very similar to the one used for language by Devlin et al. (2019). Finally, we\npredict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective\npatch representations.\nWe trained our self-supervised model for 1M steps (ca. 14 epochs) with batch size 4096 on JFT. We\nuse Adam, with a base learning rate of 2\u000110\u00004, warmup of 10k steps and cosine learning rate decay.\nAs prediction targets for pretraining we tried the following settings: 1) predicting only the mean,\n3bit color (i.e., 1 prediction of 512 colors), 2) predicting a 4\u00024downsized version of the 16\u000216\npatch with 3bit colors in parallel (i.e., 16 predictions of 512 colors), 3) regression on the full patch\nusing L2 (i.e., 256 regressions on the 3 RGB channels). Surprisingly, we found that all worked quite\nwell, though L2 was slightly worse. We report \ufb01nal results only for option 1) because it has shown\nbest few-shot performance. We also experimented with 15% corruption rate as used by Devlin et al.\n(2019) but results were also slightly worse on our few-shot metrics.\nLastly, we would like to remark that our instantiation of masked patch prediction doesn\u2019t require\nsuch an enormous amount of pretraining nor a large dataset such as JFT in order to lead to sim-\nilar performance gains on ImageNet classi\ufb01cation. That is, we observed diminishing returns on\ndownstream performance after 100k pretraining steps, and see similar gains when pretraining on\nImageNet.\nC A DDITIONAL RESULTS\nWe report detailed results corresponding to the \ufb01gures presented in the paper. Table 5 corresponds\nto Figure 3 from the paper and shows transfer performance of different ViT models pre-trained\non datasets of increasing size: ImageNet, ImageNet-21k, and JFT-300M. Table 6 corresponds to\n14\nPublished as a conference paper at ICLR 2021\nViT-B/16 ViT-B/32 ViT-L/16 ViT-L/32 ViT-H/14\nImageNet CIFAR-10 98.13 97.77 97.86 97.94 -\nCIFAR-100 87.13 86.31 86.35 87.07 -\nImageNet 77.91 73.38 76.53 71.16 -\nImageNet ReaL 83.57 79.56 82.19 77.83 -\nOxford Flowers-102 89.49 85.43 89.66 86.36 -\nOxford-IIIT-Pets 93.81 92.04 93.64 91.35 -\nImageNet-21k CIFAR-10 98.95 98.79 99.16 99.13 99.27\nCIFAR-100 91.67 91.97 93.44 93.04 93.82\nImageNet 83.97 81.28 85.15 80.99 85.13\nImageNet ReaL 88.35 86.63 88.40 85.65 88.70\nOxford Flowers-102 99.38 99.11 99.61 99.19 99.51\nOxford-IIIT-Pets 94.43 93.02 94.73 93.09 94.82\nJFT-300M CIFAR-10 99.00 98.61 99.38 99.19 99.50\nCIFAR-100 91.87 90.49 94.04 92.52 94.55\nImageNet 84.15 80.73 87.12 84.37 88.04\nImageNet ReaL 88.85 86.27 89.99 88.28 90.33\nOxford Flowers-102 99.56 99.27 99.56 99.45 99.68\nOxford-IIIT-Pets 95.80 93.40 97.11 95.83 97.56\nTable 5: Top1 accuracy (in %) of Vision Transformer on various datasets when pre-trained on Im-\nageNet, ImageNet-21k or JFT300M. These values correspond to Figure 3 in the main text. Models\nare \ufb01ne-tuned at 384 resolution. Note that the ImageNet results are computed without additional\ntechniques (Polyak averaging and 512 resolution images) used to achieve results in Table 2.\nEpochs ImageNet ImageNet ReaL CIFAR-10 CIFAR-100 Pets Flowers exaFLOPs\nname\nViT-B/32 7 80.73 86.27 98.61 90.49 93.40 99.27 55\nViT-B/16 7 84.15 88.85 99.00 91.87 95.80 99.56 224\nViT-L/32 7 84.37 88.28 99.19 92.52 95.83 99.45 196\nViT-L/16 7 86.30 89.43 99.38 93.46 96.81 99.66 783\nViT-L/16 14 87.12 89.99 99.38 94.04 97.11 99.56 1567\nViT-H/14 14 88.08 90.36 99.50 94.71 97.11 99.71 4262\nResNet50x1 7 77.54 84.56 97.67 86.07 91.11 94.26 50\nResNet50x2 7 82.12 87.94 98.29 89.20 93.43 97.02 199\nResNet101x1 7 80.67 87.07 98.48 89.17 94.08 95.95 96\nResNet152x1 7 81.88 87.96 98.82 90.22 94.17 96.94 141\nResNet152x2 7 84.97 89.69 99.06 92.05 95.37 98.62 563\nResNet152x2 14 85.56 89.89 99.24 91.92 95.75 98.75 1126\nResNet200x3 14 87.22 90.15 99.34 93.53 96.32 99.04 3306\nR50x1+ViT-B/32 7 84.90 89.15 99.01 92.24 95.75 99.46 106\nR50x1+ViT-B/16 7 85.58 89.65 99.14 92.63 96.65 99.40 274\nR50x1+ViT-L/32 7 85.68 89.04 99.24 92.93 96.97 99.43 246\nR50x1+ViT-L/16 7 86.60 89.72 99.18 93.64 97.03 99.40 859\nR50x1+ViT-L/16 14 87.12 89.76 99.31 93.89 97.36 99.11 1668\nTable 6: Detailed results of model scaling experiments. These correspond to Figure 5 in the main\npaper. We show transfer accuracy on several datasets, as well as the pre-training compute (in ex-\naFLOPs).\nFigure 5 from the paper and shows the transfer performance of ViT, ResNet, and hybrid models of\nvarying size, as well as the estimated computational cost of their pre-training.\nD A DDITIONAL ANALYSES\nD.1 SGD VS. ADAM FOR RESNETS\nResNets are typically trained with SGD and our use of Adam as optimizer is quite unconventional.\nHere we show the experiments that motivated this choice. Namely, we compare the \ufb01ne-tuning\n15\nPublished as a conference paper at ICLR 2021\nResNet50 ResNet152x2\nDataset Adam SGD Adam SGD\nImageNet 77:54 78 :24 84 :97 84 :37\nCIFAR10 97:67 97 :46 99 :06 99 :07\nCIFAR100 86:07 85 :17 92 :05 91 :06\nOxford-IIIT Pets 91:11 91 :00 95 :37 94 :79\nOxford Flowers-102 94:26 92 :06 98 :62 99 :32\nAverage 89:33 88 :79 94 :01 93 :72\nTable 7: Fine-tuning ResNet models pre-trained with Adam and SGD.\n100101\nRelative Compute0.20.30.40.50.6ImageNet 5shot\nModels\nAll\nDepth\nPatch size\nWidth MLP\nWidth\n100101\nRelative Compute0.40.50.60.70.8Average 5shot\nModels\nAll\nDepth\nPatch size\nWidth MLP\nWidth\nFigure 8: Scaling different model dimensions of the Vision Transformer.\nperformance of two ResNets \u2013 50x1 and 152x2 \u2013 pre-trained on JFT with SGD and Adam. For\nSGD, we use the hyperparameters recommended by Kolesnikov et al. (2020). Results are presented\nin Table 7. Adam pre-training outperforms SGD pre-training on most datasets and on average.\nThis justi\ufb01es the choice of Adam as the optimizer used to pre-train ResNets on JFT. Note that the\nabsolute numbers are lower than those reported by Kolesnikov et al. (2020), since we pre-train only\nfor7epochs, not 30.\nD.2 T RANSFORMER SHAPE\nWe ran ablations on scaling different dimensions of the Transformer architecture to \ufb01nd out which\nare best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet\nfor different con\ufb01gurations. All con\ufb01gurations are based on a ViT model with 8layers,D= 1024 ,\nDMLP = 2048 and a patch size of 32, the intersection of all lines. We can see that scaling the\ndepth results in the biggest improvements which are clearly visible up until 64 layers. However,\ndiminishing returns are already visible after 16 layers. Interestingly, scaling the width of the net-\nwork seems to result in the smallest changes. Decreasing the patch size and thus increasing the\neffective sequence length shows surprisingly robust improvements without introducing parameters.\nThese \ufb01ndings suggest that compute might be a better predictor of performance than the number of\nparameters, and that scaling should emphasize depth over width if any. Overall, we \ufb01nd that scaling\nall dimensions proportionally results in robust improvements.\nD.3 H EAD TYPE AND C L A S S TOKEN\nIn order to stay as close as possible to the original Transformer model, we made use of an additional\n[class] token, which is taken as image representation. The output of this token is then trans-\nformed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity\nin the single hidden layer.\nThis design is inherited from the Transformer model for text, and we use it throughout the main\npaper. An initial attempt at using only image-patch embeddings, globally average-pooling (GAP)\nthem, followed by a linear classi\ufb01er\u2014just like ResNet\u2019s \ufb01nal feature map\u2014performed very poorly.\nHowever, we found that this is neither due to the extra token, nor to the GAP operation. Instead,\n16\nPublished as a conference paper at ICLR 2021\n0 1 2 3 4 5 6 7\nEpochs of training2530354045505560ImageNet linear 5-shot accuracy [%]CLS-Token, lr=8e-4\nGAP, lr=8e-4\nGAP, lr=3e-4\nFigure 9: Comparison of class-token and global average pooling classi\ufb01ers. Both work similarly\nwell, but require different learning-rates.\nPos. Emb. Default/Stem Every Layer Every Layer-Shared\nNo Pos. Emb. 0.61382 N/A N/A\n1-D Pos. Emb. 0.64206 0.63964 0.64292\n2-D Pos. Emb. 0.64001 0.64046 0.64022\nRel. Pos. Emb. 0.64032 N/A N/A\nTable 8: Results of the ablation study on positional embeddings with ViT-B/16 model evaluated on\nImageNet 5-shot linear.\nthe difference in performance is fully explained by the requirement for a different learning-rate, see\nFigure 9.\nD.4 P OSITIONAL EMBEDDING\nWe ran ablations on different ways of encoding spatial information using positional embedding. We\ntried the following cases:\n\u2022 Providing no positional information: Considering the inputs as a bag of patches .\n\u2022 1-dimensional positional embedding: Considering the inputs as a sequence of patches in\nthe raster order (default across all other experiments in this paper).\n\u2022 2-dimensional positional embedding: Considering the inputs as a grid of patches in two\ndimensions. In this case, two sets of embeddings are learned, each for one of the axes,\nX-embedding, and Y-embedding, each with size D=2. Then, based on the coordinate on\nthe path in the input, we concatenate the XandYembedding to get the \ufb01nal positional\nembedding for that patch.\n\u2022 Relative positional embeddings: Considering the relative distance between patches to en-\ncode the spatial information as instead of their absolute position. To do so, we use 1-\ndimensional Relative Attention, in which we de\ufb01ne the relative distance all possible pairs\nof patches. Thus, for every given pair (one as query, and the other as key/value in the at-\ntention mechanism), we have an offset pq\u0000pk, where each offset is associated with an\nembedding. Then, we simply run extra attention, where we use the original query (the\ncontent of query), but use relative positional embeddings as keys. We then use the log-\nits from the relative attention as a bias term and add it to the logits of the main attention\n(content-based attention) before applying the softmax.\nIn addition to different ways of encoding spatial information, we also tried different ways of in-\ncorporating this information in our model. For the 1-dimensional and 2-dimensional positional\nembeddings, we tried three different cases: (1) add positional embeddings to the inputs right after\n17\nPublished as a conference paper at ICLR 2021\n1234567891011121314\nInput patch column1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14Input patch rowViT-L16\n7 epochs, LR=0.0002, WD=0.01\n1\n1\nCosine similarity\n1234567891011121314\nInput patch column1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14Input patch rowViT-L16\n7 epochs, LR=0.0004, WD=0.1\n1\n1\nCosine similarity\n1234567891011121314\nInput patch column1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14Input patch rowViT-L16\n14 epochs, LR=0.0004, WD=0.1\n1\n1\nCosine similarity\nFigure 10: Position embeddings of models trained with different hyperparameters.\nthe stem of them model and before feeding the inputs to the Transformer encoder (default across\nall other experiments in this paper); (2) learn and add positional embeddings to the inputs at the\nbeginning of each layer; (3) add a learned positional embeddings to the inputs at the beginning of\neach layer (shared between layers).\nTable 8 summarizes the results from this ablation study on a ViT-B/16 model. As we can see, while\nthere is a large gap between the performances of the model with no positional embedding and mod-\nels with positional embedding, there is little to no difference between different ways of encoding\npositional information. We speculate that since our Transformer encoder operates on patch-level\ninputs, as opposed to pixel-level, the differences in how to encode spatial information is less impor-\ntant. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original\npixel-level inputs, e.g., 14\u000214as opposed to 224\u0002224, and learning to represent the spatial re-\nlations in this resolution is equally easy for these different positional encoding strategies. Even so,\nthe speci\ufb01c pattern of position embedding similarity learned by the network depends on the training\nhyperparameters (Figure 10).\n0 5 10 15 20\nNetwork depth (layer)020406080100120Mean attention distance (pixels)\nViT-L/16\nHead 1\nHead 2\nHead 3\n...\n0 5 10 15 20\nNetwork depth (layer)020406080100120\nR50x1 + ViT-L/16\nHead 1\nHead 2\nHead 3\n...\nFigure 11: Size of attended area by head and network depth. Attention distance was computed for\n128 example images by averaging the distance between the query pixel and all other pixels, weighted\nby the attention weight. Each dot shows the mean attention distance across images for one of 16\nheads at one layer. Image width is 224 pixels.\nD.5 E MPIRICAL COMPUTATIONAL COSTS\nWe are also interested in real-world speed of the architectures on our hardware, which is not always\nwell predicted by theoretical FLOPs due to details like lane widths and cache sizes. For this purpose,\n18\nPublished as a conference paper at ICLR 2021\nwe perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the\ndifference between inference and backprop speed is a constant model-independent factor.\nFigure 12 (left) shows how many images one core can handle per second, across various input sizes.\nEvery single point refers to the peak performance measured across a wide range of batch-sizes. As\ncan be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening\nfor the largest models at the largest resolutions.\nAnother quantity of interest is the largest batch-size each model can \ufb01t onto a core, larger being\nbetter for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models.\nThis shows that large ViT models have a clear advantage in terms of memory-ef\ufb01ciency over ResNet\nmodels.\n64 128 224 384 512\nInput size [px]102103104Peak inference speed [img/sec/core]64 128 224 384 512\nInput size [px]102103Largest per-core batch-sizeR50x1\nR50x2ViT-B/32\nViT-L/32ViT-B/16\nViT-L/16ViT-H/14\nR152x4\nFigure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models\nhave speed comparable to similar ResNets. Right : Largest per-core batch-size \ufb01tting on device with\nvarious architectures across input sizes. ViT models are clearly more memory-ef\ufb01cient.\nD.6 A XIAL ATTENTION\nAxial Attention (Huang et al., 2020; Ho et al., 2019) is a simple, yet effective technique to run self-\nattention on large inputs that are organized as multidimensional tensors. The general idea of axial\nattention is to perform multiple attention operations, each along a single axis of the input tensor,\ninstead of applying 1-dimensional attention to the \ufb02attened version of the input. In axial attention,\neach attention mixes information along a particular axis, while keeping information along the other\naxes independent. Along this line, Wang et al. (2020b) proposed the AxialResNet model in which\nall the convolutions with kernel size 3\u00023in a ResNet50 are replaced by axial self-attention, i.e.\na row and column attention, augmented by relative positional encoding. We have implemented\nAxialResNet as a baseline model.3.\nMoreover, we have modi\ufb01ed ViT to process inputs in the 2-dimensional shape, instead of a 1-\ndimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of\na self-attention followed by an MLP, we have a a row-self-attention plus an MLP followed by a\ncolumn-self-attention plus an MLP.\nFigure 13, present the performance of Axial ResNet, Axial-ViT-B/32 and Axial-ViT-B/16 on Ima-\ngeNet 5shot linear, when pretrained on JFT dataset, verses the pretraining compute, both in terms of\nnumber of FLOPs and inference time (example per seconds). As we can see, both Axial-ViT-B/32\nand Axial-ViT-B/16 do better than their ViT-B counterpart in terms of performance, but it comes at\n3Our implementation is based on the open-sourced PyTorch implementation in https://github.com/\ncsrhddlam/axial-deeplab . In our experiments, we reproduced the scores reported in (Wang et al.,\n2020b) in terms of accuracy, however, our implementation, similar to the open-source implementation, is very\nslow on TPUs. Therefore, we were not able to use it for extensive large-scale experiments. These may be\nunlocked by a carefully optimized implementation.\n19\nPublished as a conference paper at ICLR 2021\n102\nTotal compute [exaFLOPs]0.5000.5250.5500.5750.6000.6250.650ImageNet 5-shot linear top-1 accuracyAxialViT-B/16\nAxialViT-B/32ViT-B/16\nViT-B/32\nResNet50AxialResNet50\n102103\nPeak inference speed [img/sec/core]0.5000.5250.5500.5750.6000.6250.650ImageNet 5-shot linear top-1 accuracyAxialViT-B/16\nAxialViT-B/32ViT-B/16\nViT-B/32\nResNet50AxialResNet50\nFigure 13: Performance of Axial-Attention based models, in terms of top-1 accuracy on ImageNet\n5-shot linear, versus their speed in terms of number of FLOPs ( left) and inference time ( left).\nthe cost of more compute. This is because in Axial-ViT models, each Transformer block with global\nself-attention is replaced by two Axial Transformer blocks, one with row and one with column self-\nattention and although the sequence length that self-attention operates on is smaller in axial case,\nthere is a extra MLP per Axial-ViT block. For the AxialResNet, although it looks reasonable in\nterms of accuracy/compute trade-off (Figure 13, left), the naive implementation is extremely slow\non TPUs (Figure 13, right).\nD.7 A TTENTION DISTANCE\nTo understand how ViT uses self-attention to integrate information across the image, we analyzed\nthe average distance spanned by attention weights at different layers (Figure 11). This \u201cattention\ndistance\u201d is analogous to receptive \ufb01eld size in CNNs. Average attention distance is highly variable\nacross heads in lower layers, with some heads attending to much of the image, while others attend\nto small regions at or near the query location. As depth increases, attention distance increases for all\nheads. In the second half of the network, most heads attend widely across tokens.\nD.8 A TTENTION MAPS\nTo compute maps of the attention from the output token to the input space (Figures 6 and 14), we\nused Attention Rollout (Abnar & Zuidema, 2020). Brie\ufb02y, we averaged attention weights of ViT-\nL/16 across all heads and then recursively multiplied the weight matrices of all layers. This accounts\nfor the mixing of attention across tokens through all layers.\nD.9 O BJECT NETRESULTS\nWe also evaluate our \ufb02agship ViT-H/14 model on the ObjectNet benchmark following the evaluation\nsetup in Kolesnikov et al. (2020), resulting in 82.1% top-5 accuracy and 61.7% top-1 accuracy.\nD.10 VTAB B REAKDOWN\nTable 9 shows the scores attained on each of the VTAB-1k tasks.\n20\nPublished as a conference paper at ICLR 2021\n1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n25\n 26\n 27\n 28\n 29\n 30\n 31\n 32\n33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n97\n 98\n 99\n 100\n 101\n 102\n 103\n 104\n105\n 106\n 107\n 108\n 109\n 110\n 111\n 112\n113\n 114\n 115\n 116\n 117\n 118\n 119\n 120\n121\n 122\n 123\n 124\n 125\n 126\n 127\n 128\nFigure 14: Further example attention maps as in Figure 6 (random selection).\n21\nPublished as a conference paper at ICLR 2021\nTable 9: Breakdown of VTAB-1k performance across tasks.Caltech101\nCIFAR-100\nDTD\nFlowers102\nPets\nSun397\nSVHN\nCamelyon\nEuroSAT\nResisc45\nRetinopathy\nClevr-Count\nClevr-Dist\nDMLab\ndSpr-Loc\ndSpr-Ori\nKITTI-Dist\nsNORB-Azim\nsNORB-Elev\nMean\nViT-H/14 (JFT) 95.3 85.5 75.2 99.7 97.2 65.0 88.9 83.3 96.7 91.4 76.6 91.7 63.8 53.1 79.4 63.3 84.5 33.2 51.2 77.6\nViT-L/16 (JFT) 95.4 81.9 74.3 99.7 96.7 63.5 87.4 83.6 96.5 89.7 77.1 86.4 63.1 49.7 74.5 60.5 82.2 36.2 51.1 76.3\nViT-L/16 (I21k) 90.8 84.1 74.1 99.3 92.7 61.0 80.9 82.5 95.6 85.2 75.3 70.3 56.1 41.9 74.7 64.9 79.9 30.5 41.7 72.7\n22\n",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "arxiv_id": "2010.11929",
    "category": "famous",
    "authors": [
      "Alexey Dosovitskiy",
      "Lucas Beyer",
      "Alexander Kolesnikov",
      "Dirk Weissenborn",
      "Xiaohua Zhai",
      "Thomas Unterthiner",
      "Mostafa Dehghani",
      "Matthias Minderer",
      "Georg Heigold",
      "Sylvain Gelly",
      "Jakob Uszkoreit",
      "Neil Houlsby"
    ],
    "published": "2020-10-22",
    "num_pages": 22,
    "num_chars": 67046
  },
  {
    "text": "Deep Residual Learning for Image Recognition\nKaiming He Xiangyu Zhang Shaoqing Ren Jian Sun\nMicrosoft Research\nfkahe, v-xiangz, v-shren, jiansun g@microsoft.com\nAbstract\nDeeper neural networks are more dif\ufb01cult to train. We\npresent a residual learning framework to ease the training\nof networks that are substantially deeper than those used\npreviously. We explicitly reformulate the layers as learn-\ning residual functions with reference to the layer inputs, in-\nstead of learning unreferenced functions. We provide com-\nprehensive empirical evidence showing that these residual\nnetworks are easier to optimize, and can gain accuracy from\nconsiderably increased depth. On the ImageNet dataset we\nevaluate residual nets with a depth of up to 152 layers\u20148 \u0002\ndeeper than VGG nets [41] but still having lower complex-\nity. An ensemble of these residual nets achieves 3.57% error\non the ImageNet testset. This result won the 1st place on the\nILSVRC 2015 classi\ufb01cation task. We also present analysis\non CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance\nfor many visual recognition tasks. Solely due to our ex-\ntremely deep representations, we obtain a 28% relative im-\nprovement on the COCO object detection dataset. Deep\nresidual nets are foundations of our submissions to ILSVRC\n& COCO 2015 competitions1, where we also won the 1st\nplaces on the tasks of ImageNet detection, ImageNet local-\nization, COCO detection, and COCO segmentation.\n1. Introduction\nDeep convolutional neural networks [22, 21] have led\nto a series of breakthroughs for image classi\ufb01cation [21,\n50, 40]. Deep networks naturally integrate low/mid/high-\nlevel features [50] and classi\ufb01ers in an end-to-end multi-\nlayer fashion, and the \u201clevels\u201d of features can be enriched\nby the number of stacked layers (depth). Recent evidence\n[41, 44] reveals that network depth is of crucial importance,\nand the leading results [41, 44, 13, 16] on the challenging\nImageNet dataset [36] all exploit \u201cvery deep\u201d [41] models,\nwith a depth of sixteen [41] to thirty [16]. Many other non-\ntrivial visual recognition tasks [8, 12, 7, 32, 27] have also\n1http://image-net.org/challenges/LSVRC/2015/ and\nhttp://mscoco.org/dataset/#detections-challenge2015 .\n0 1 2 3 4 5 60 1020\niter. (1e4)training error (%)\n  \n0 1 2 3 4 5 601020\niter. (1e4)test error (%)\n  \n56-layer\n20-layer56-layer\n20-layerFigure 1. Training error (left) and test error (right) on CIFAR-10\nwith 20-layer and 56-layer \u201cplain\u201d networks. The deeper network\nhas higher training error, and thus test error. Similar phenomena\non ImageNet is presented in Fig. 4.\ngreatly bene\ufb01ted from very deep models.\nDriven by the signi\ufb01cance of depth, a question arises: Is\nlearning better networks as easy as stacking more layers?\nAn obstacle to answering this question was the notorious\nproblem of vanishing/exploding gradients [1, 9], which\nhamper convergence from the beginning. This problem,\nhowever, has been largely addressed by normalized initial-\nization [23, 9, 37, 13] and intermediate normalization layers\n[16], which enable networks with tens of layers to start con-\nverging for stochastic gradient descent (SGD) with back-\npropagation [22].\nWhen deeper networks are able to start converging, a\ndegradation problem has been exposed: with the network\ndepth increasing, accuracy gets saturated (which might be\nunsurprising) and then degrades rapidly. Unexpectedly,\nsuch degradation is not caused by over\ufb01tting , and adding\nmore layers to a suitably deep model leads to higher train-\ning error , as reported in [11, 42] and thoroughly veri\ufb01ed by\nour experiments. Fig. 1 shows a typical example.\nThe degradation (of training accuracy) indicates that not\nall systems are similarly easy to optimize. Let us consider a\nshallower architecture and its deeper counterpart that adds\nmore layers onto it. There exists a solution by construction\nto the deeper model: the added layers are identity mapping,\nand the other layers are copied from the learned shallower\nmodel. The existence of this constructed solution indicates\nthat a deeper model should produce no higher training error\nthan its shallower counterpart. But experiments show that\nour current solvers on hand are unable to \ufb01nd solutions that\n1arXiv:1512.03385v1  [cs.CV]  10 Dec 2015\nidentityweight layer\nweight layerrelu\nreluF(x)\u0001+\u0001xx\nF(x)xFigure 2. Residual learning: a building block.\nare comparably good or better than the constructed solution\n(or unable to do so in feasible time).\nIn this paper, we address the degradation problem by\nintroducing a deep residual learning framework. In-\nstead of hoping each few stacked layers directly \ufb01t a\ndesired underlying mapping, we explicitly let these lay-\ners \ufb01t a residual mapping. Formally, denoting the desired\nunderlying mapping as H(x), we let the stacked nonlinear\nlayers \ufb01t another mapping of F(x) :=H(x)\u0000x. The orig-\ninal mapping is recast into F(x)+x. We hypothesize that it\nis easier to optimize the residual mapping than to optimize\nthe original, unreferenced mapping. To the extreme, if an\nidentity mapping were optimal, it would be easier to push\nthe residual to zero than to \ufb01t an identity mapping by a stack\nof nonlinear layers.\nThe formulation of F(x)+xcan be realized by feedfor-\nward neural networks with \u201cshortcut connections\u201d (Fig. 2).\nShortcut connections [2, 34, 49] are those skipping one or\nmore layers. In our case, the shortcut connections simply\nperform identity mapping, and their outputs are added to\nthe outputs of the stacked layers (Fig. 2). Identity short-\ncut connections add neither extra parameter nor computa-\ntional complexity. The entire network can still be trained\nend-to-end by SGD with backpropagation, and can be eas-\nily implemented using common libraries ( e.g., Caffe [19])\nwithout modifying the solvers.\nWe present comprehensive experiments on ImageNet\n[36] to show the degradation problem and evaluate our\nmethod. We show that: 1) Our extremely deep residual nets\nare easy to optimize, but the counterpart \u201cplain\u201d nets (that\nsimply stack layers) exhibit higher training error when the\ndepth increases; 2) Our deep residual nets can easily enjoy\naccuracy gains from greatly increased depth, producing re-\nsults substantially better than previous networks.\nSimilar phenomena are also shown on the CIFAR-10 set\n[20], suggesting that the optimization dif\ufb01culties and the\neffects of our method are not just akin to a particular dataset.\nWe present successfully trained models on this dataset with\nover 100 layers, and explore models with over 1000 layers.\nOn the ImageNet classi\ufb01cation dataset [36], we obtain\nexcellent results by extremely deep residual nets. Our 152-\nlayer residual net is the deepest network ever presented on\nImageNet, while still having lower complexity than VGG\nnets [41]. Our ensemble has 3.57% top-5 error on theImageNet testset, and won the 1st place in the ILSVRC\n2015 classi\ufb01cation competition . The extremely deep rep-\nresentations also have excellent generalization performance\non other recognition tasks, and lead us to further win the\n1st places on: ImageNet detection, ImageNet localization,\nCOCO detection, and COCO segmentation in ILSVRC &\nCOCO 2015 competitions. This strong evidence shows that\nthe residual learning principle is generic, and we expect that\nit is applicable in other vision and non-vision problems.\n2. Related Work\nResidual Representations. In image recognition, VLAD\n[18] is a representation that encodes by the residual vectors\nwith respect to a dictionary, and Fisher Vector [30] can be\nformulated as a probabilistic version [18] of VLAD. Both\nof them are powerful shallow representations for image re-\ntrieval and classi\ufb01cation [4, 48]. For vector quantization,\nencoding residual vectors [17] is shown to be more effec-\ntive than encoding original vectors.\nIn low-level vision and computer graphics, for solv-\ning Partial Differential Equations (PDEs), the widely used\nMultigrid method [3] reformulates the system as subprob-\nlems at multiple scales, where each subproblem is respon-\nsible for the residual solution between a coarser and a \ufb01ner\nscale. An alternative to Multigrid is hierarchical basis pre-\nconditioning [45, 46], which relies on variables that repre-\nsent residual vectors between two scales. It has been shown\n[3, 45, 46] that these solvers converge much faster than stan-\ndard solvers that are unaware of the residual nature of the\nsolutions. These methods suggest that a good reformulation\nor preconditioning can simplify the optimization.\nShortcut Connections. Practices and theories that lead to\nshortcut connections [2, 34, 49] have been studied for a long\ntime. An early practice of training multi-layer perceptrons\n(MLPs) is to add a linear layer connected from the network\ninput to the output [34, 49]. In [44, 24], a few interme-\ndiate layers are directly connected to auxiliary classi\ufb01ers\nfor addressing vanishing/exploding gradients. The papers\nof [39, 38, 31, 47] propose methods for centering layer re-\nsponses, gradients, and propagated errors, implemented by\nshortcut connections. In [44], an \u201cinception\u201d layer is com-\nposed of a shortcut branch and a few deeper branches.\nConcurrent with our work, \u201chighway networks\u201d [42, 43]\npresent shortcut connections with gating functions [15].\nThese gates are data-dependent and have parameters, in\ncontrast to our identity shortcuts that are parameter-free.\nWhen a gated shortcut is \u201cclosed\u201d (approaching zero), the\nlayers in highway networks represent non-residual func-\ntions. On the contrary, our formulation always learns\nresidual functions; our identity shortcuts are never closed,\nand all information is always passed through, with addi-\ntional residual functions to be learned. In addition, high-\n2\nway networks have not demonstrated accuracy gains with\nextremely increased depth ( e.g., over 100 layers).\n3. Deep Residual Learning\n3.1. Residual Learning\nLet us considerH(x)as an underlying mapping to be\n\ufb01t by a few stacked layers (not necessarily the entire net),\nwithxdenoting the inputs to the \ufb01rst of these layers. If one\nhypothesizes that multiple nonlinear layers can asymptoti-\ncally approximate complicated functions2, then it is equiv-\nalent to hypothesize that they can asymptotically approxi-\nmate the residual functions, i.e.,H(x)\u0000x(assuming that\nthe input and output are of the same dimensions). So\nrather than expect stacked layers to approximate H(x), we\nexplicitly let these layers approximate a residual function\nF(x) :=H(x)\u0000x. The original function thus becomes\nF(x)+x. Although both forms should be able to asymptot-\nically approximate the desired functions (as hypothesized),\nthe ease of learning might be different.\nThis reformulation is motivated by the counterintuitive\nphenomena about the degradation problem (Fig. 1, left). As\nwe discussed in the introduction, if the added layers can\nbe constructed as identity mappings, a deeper model should\nhave training error no greater than its shallower counter-\npart. The degradation problem suggests that the solvers\nmight have dif\ufb01culties in approximating identity mappings\nby multiple nonlinear layers. With the residual learning re-\nformulation, if identity mappings are optimal, the solvers\nmay simply drive the weights of the multiple nonlinear lay-\ners toward zero to approach identity mappings.\nIn real cases, it is unlikely that identity mappings are op-\ntimal, but our reformulation may help to precondition the\nproblem. If the optimal function is closer to an identity\nmapping than to a zero mapping, it should be easier for the\nsolver to \ufb01nd the perturbations with reference to an identity\nmapping, than to learn the function as a new one. We show\nby experiments (Fig. 7) that the learned residual functions in\ngeneral have small responses, suggesting that identity map-\npings provide reasonable preconditioning.\n3.2. Identity Mapping by Shortcuts\nWe adopt residual learning to every few stacked layers.\nA building block is shown in Fig. 2. Formally, in this paper\nwe consider a building block de\ufb01ned as:\ny=F(x;fWig) +x: (1)\nHere xandyare the input and output vectors of the lay-\ners considered. The function F(x;fWig)represents the\nresidual mapping to be learned. For the example in Fig. 2\nthat has two layers, F=W2\u001b(W1x)in which\u001bdenotes\n2This hypothesis, however, is still an open question. See [28].ReLU [29] and the biases are omitted for simplifying no-\ntations. The operation F+xis performed by a shortcut\nconnection and element-wise addition. We adopt the sec-\nond nonlinearity after the addition ( i.e.,\u001b(y), see Fig. 2).\nThe shortcut connections in Eqn.(1) introduce neither ex-\ntra parameter nor computation complexity. This is not only\nattractive in practice but also important in our comparisons\nbetween plain and residual networks. We can fairly com-\npare plain/residual networks that simultaneously have the\nsame number of parameters, depth, width, and computa-\ntional cost (except for the negligible element-wise addition).\nThe dimensions of xandFmust be equal in Eqn.(1).\nIf this is not the case ( e.g., when changing the input/output\nchannels), we can perform a linear projection Wsby the\nshortcut connections to match the dimensions:\ny=F(x;fWig) +Wsx: (2)\nWe can also use a square matrix Wsin Eqn.(1). But we will\nshow by experiments that the identity mapping is suf\ufb01cient\nfor addressing the degradation problem and is economical,\nand thusWsis only used when matching dimensions.\nThe form of the residual function Fis \ufb02exible. Exper-\niments in this paper involve a function Fthat has two or\nthree layers (Fig. 5), while more layers are possible. But if\nFhas only a single layer, Eqn.(1) is similar to a linear layer:\ny=W1x+x, for which we have not observed advantages.\nWe also note that although the above notations are about\nfully-connected layers for simplicity, they are applicable to\nconvolutional layers. The function F(x;fWig)can repre-\nsent multiple convolutional layers. The element-wise addi-\ntion is performed on two feature maps, channel by channel.\n3.3. Network Architectures\nWe have tested various plain/residual nets, and have ob-\nserved consistent phenomena. To provide instances for dis-\ncussion, we describe two models for ImageNet as follows.\nPlain Network. Our plain baselines (Fig. 3, middle) are\nmainly inspired by the philosophy of VGG nets [41] (Fig. 3,\nleft). The convolutional layers mostly have 3 \u00023 \ufb01lters and\nfollow two simple design rules: (i) for the same output\nfeature map size, the layers have the same number of \ufb01l-\nters; and (ii) if the feature map size is halved, the num-\nber of \ufb01lters is doubled so as to preserve the time com-\nplexity per layer. We perform downsampling directly by\nconvolutional layers that have a stride of 2. The network\nends with a global average pooling layer and a 1000-way\nfully-connected layer with softmax. The total number of\nweighted layers is 34 in Fig. 3 (middle).\nIt is worth noticing that our model has fewer \ufb01lters and\nlower complexity than VGG nets [41] (Fig. 3, left). Our 34-\nlayer baseline has 3.6 billion FLOPs (multiply-adds), which\nis only 18% of VGG-19 (19.6 billion FLOPs).\n3\n7x7 conv, 64, /2\npool, /2\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 128, /2\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 256, /2\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 512, /2\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\navg pool\nfc 1000image\n3x3 conv, 5123x3 conv, 64\n3x3 conv, 64\npool, /2\n3x3 conv, 128\n3x3 conv, 128\npool, /2\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\npool, /2\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\npool, /2\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\npool, /2\nfc 4096\nfc 4096\nfc 1000image\noutput \nsize: 112output \nsize: 224\noutput \nsize: 56\noutput \nsize: 28\noutput \nsize: 14\noutput \nsize: 7\noutput \nsize: 1VGG-19 34-layer plain\n7x7 conv, 64, /2\npool, /2\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 128, /2\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 256, /2\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 512, /2\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\navg pool\nfc 1000image34-layer residualFigure 3. Example network architectures for ImageNet. Left: the\nVGG-19 model [41] (19.6 billion FLOPs) as a reference. Mid-\ndle: a plain network with 34 parameter layers (3.6 billion FLOPs).\nRight : a residual network with 34 parameter layers (3.6 billion\nFLOPs). The dotted shortcuts increase dimensions. Table 1 shows\nmore details and other variants.Residual Network. Based on the above plain network, we\ninsert shortcut connections (Fig. 3, right) which turn the\nnetwork into its counterpart residual version. The identity\nshortcuts (Eqn.(1)) can be directly used when the input and\noutput are of the same dimensions (solid line shortcuts in\nFig. 3). When the dimensions increase (dotted line shortcuts\nin Fig. 3), we consider two options: (A) The shortcut still\nperforms identity mapping, with extra zero entries padded\nfor increasing dimensions. This option introduces no extra\nparameter; (B) The projection shortcut in Eqn.(2) is used to\nmatch dimensions (done by 1 \u00021 convolutions). For both\noptions, when the shortcuts go across feature maps of two\nsizes, they are performed with a stride of 2.\n3.4. Implementation\nOur implementation for ImageNet follows the practice\nin [21, 41]. The image is resized with its shorter side ran-\ndomly sampled in [256;480] for scale augmentation [41].\nA 224\u0002224 crop is randomly sampled from an image or its\nhorizontal \ufb02ip, with the per-pixel mean subtracted [21]. The\nstandard color augmentation in [21] is used. We adopt batch\nnormalization (BN) [16] right after each convolution and\nbefore activation, following [16]. We initialize the weights\nas in [13] and train all plain/residual nets from scratch. We\nuse SGD with a mini-batch size of 256. The learning rate\nstarts from 0.1 and is divided by 10 when the error plateaus,\nand the models are trained for up to 60\u0002104iterations. We\nuse a weight decay of 0.0001 and a momentum of 0.9. We\ndo not use dropout [14], following the practice in [16].\nIn testing, for comparison studies we adopt the standard\n10-crop testing [21]. For best results, we adopt the fully-\nconvolutional form as in [41, 13], and average the scores\nat multiple scales (images are resized such that the shorter\nside is inf224;256;384;480;640g).\n4. Experiments\n4.1. ImageNet Classi\ufb01cation\nWe evaluate our method on the ImageNet 2012 classi\ufb01-\ncation dataset [36] that consists of 1000 classes. The models\nare trained on the 1.28 million training images, and evalu-\nated on the 50k validation images. We also obtain a \ufb01nal\nresult on the 100k test images, reported by the test server.\nWe evaluate both top-1 and top-5 error rates.\nPlain Networks. We \ufb01rst evaluate 18-layer and 34-layer\nplain nets. The 34-layer plain net is in Fig. 3 (middle). The\n18-layer plain net is of a similar form. See Table 1 for de-\ntailed architectures.\nThe results in Table 2 show that the deeper 34-layer plain\nnet has higher validation error than the shallower 18-layer\nplain net. To reveal the reasons, in Fig. 4 (left) we com-\npare their training/validation errors during the training pro-\ncedure. We have observed the degradation problem - the\n4\nlayer name output size 18-layer 34-layer 50-layer 101-layer 152-layer\nconv1 112\u0002112 7\u00027, 64, stride 2\nconv2 x 56\u0002563\u00023 max pool, stride 2\n\u0014\n3\u00023, 64\n3\u00023, 64\u0015\n\u00022\u0014\n3\u00023, 64\n3\u00023, 64\u0015\n\u000232\n41\u00021, 64\n3\u00023, 64\n1\u00021, 2563\n5\u000232\n41\u00021, 64\n3\u00023, 64\n1\u00021, 2563\n5\u000232\n41\u00021, 64\n3\u00023, 64\n1\u00021, 2563\n5\u00023\nconv3 x 28\u000228\u0014\n3\u00023, 128\n3\u00023, 128\u0015\n\u00022\u0014\n3\u00023, 128\n3\u00023, 128\u0015\n\u000242\n41\u00021, 128\n3\u00023, 128\n1\u00021, 5123\n5\u000242\n41\u00021, 128\n3\u00023, 128\n1\u00021, 5123\n5\u000242\n41\u00021, 128\n3\u00023, 128\n1\u00021, 5123\n5\u00028\nconv4 x 14\u000214\u0014\n3\u00023, 256\n3\u00023, 256\u0015\n\u00022\u0014\n3\u00023, 256\n3\u00023, 256\u0015\n\u000262\n41\u00021, 256\n3\u00023, 256\n1\u00021, 10243\n5\u000262\n41\u00021, 256\n3\u00023, 256\n1\u00021, 10243\n5\u0002232\n41\u00021, 256\n3\u00023, 256\n1\u00021, 10243\n5\u000236\nconv5 x 7\u00027\u0014\n3\u00023, 512\n3\u00023, 512\u0015\n\u00022\u0014\n3\u00023, 512\n3\u00023, 512\u0015\n\u000232\n41\u00021, 512\n3\u00023, 512\n1\u00021, 20483\n5\u000232\n41\u00021, 512\n3\u00023, 512\n1\u00021, 20483\n5\u000232\n41\u00021, 512\n3\u00023, 512\n1\u00021, 20483\n5\u00023\n1\u00021 average pool, 1000-d fc, softmax\nFLOPs 1.8\u00021093.6\u00021093.8\u00021097.6\u000210911.3\u0002109\nTable 1. Architectures for ImageNet. Building blocks are shown in brackets (see also Fig. 5), with the numbers of blocks stacked. Down-\nsampling is performed by conv3 1, conv4 1, and conv5 1 with a stride of 2.\n0 10 20 30 40 502030405060\niter. (1e4)error (%)\n  \nplain-18\nplain-34\n0 10 20 30 40 502030405060\niter. (1e4)error (%)\n  \nResNet-18\nResNet-3418-layer34-layer\n18-layer\n34-layer\nFigure 4. Training on ImageNet . Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain\nnetworks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to\ntheir plain counterparts.\nplain ResNet\n18 layers 27.94 27.88\n34 layers 28.54 25.03\nTable 2. Top-1 error (%, 10-crop testing) on ImageNet validation.\nHere the ResNets have no extra parameter compared to their plain\ncounterparts. Fig. 4 shows the training procedures.\n34-layer plain net has higher training error throughout the\nwhole training procedure, even though the solution space\nof the 18-layer plain network is a subspace of that of the\n34-layer one.\nWe argue that this optimization dif\ufb01culty is unlikely to\nbe caused by vanishing gradients. These plain networks are\ntrained with BN [16], which ensures forward propagated\nsignals to have non-zero variances. We also verify that the\nbackward propagated gradients exhibit healthy norms with\nBN. So neither forward nor backward signals vanish. In\nfact, the 34-layer plain net is still able to achieve compet-\nitive accuracy (Table 3), suggesting that the solver works\nto some extent. We conjecture that the deep plain nets may\nhave exponentially low convergence rates, which impact thereducing of the training error3. The reason for such opti-\nmization dif\ufb01culties will be studied in the future.\nResidual Networks. Next we evaluate 18-layer and 34-\nlayer residual nets ( ResNets ). The baseline architectures\nare the same as the above plain nets, expect that a shortcut\nconnection is added to each pair of 3 \u00023 \ufb01lters as in Fig. 3\n(right). In the \ufb01rst comparison (Table 2 and Fig. 4 right),\nwe use identity mapping for all shortcuts and zero-padding\nfor increasing dimensions (option A). So they have no extra\nparameter compared to the plain counterparts.\nWe have three major observations from Table 2 and\nFig. 4. First, the situation is reversed with residual learn-\ning \u2013 the 34-layer ResNet is better than the 18-layer ResNet\n(by 2.8%). More importantly, the 34-layer ResNet exhibits\nconsiderably lower training error and is generalizable to the\nvalidation data. This indicates that the degradation problem\nis well addressed in this setting and we manage to obtain\naccuracy gains from increased depth.\nSecond, compared to its plain counterpart, the 34-layer\n3We have experimented with more training iterations (3 \u0002) and still ob-\nserved the degradation problem, suggesting that this problem cannot be\nfeasibly addressed by simply using more iterations.\n5\nmodel top-1 err. top-5 err.\nVGG-16 [41] 28.07 9.33\nGoogLeNet [44] - 9.15\nPReLU-net [13] 24.27 7.38\nplain-34 28.54 10.02\nResNet-34 A 25.03 7.76\nResNet-34 B 24.52 7.46\nResNet-34 C 24.19 7.40\nResNet-50 22.85 6.71\nResNet-101 21.75 6.05\nResNet-152 21.43 5.71\nTable 3. Error rates (%, 10-crop testing) on ImageNet validation.\nVGG-16 is based on our test. ResNet-50/101/152 are of option B\nthat only uses projections for increasing dimensions.\nmethod top-1 err. top-5 err.\nVGG [41] (ILSVRC\u201914) - 8.43y\nGoogLeNet [44] (ILSVRC\u201914) - 7.89\nVGG [41] (v5) 24.4 7.1\nPReLU-net [13] 21.59 5.71\nBN-inception [16] 21.99 5.81\nResNet-34 B 21.84 5.71\nResNet-34 C 21.53 5.60\nResNet-50 20.74 5.25\nResNet-101 19.87 4.60\nResNet-152 19.38 4.49\nTable 4. Error rates (%) of single-model results on the ImageNet\nvalidation set (exceptyreported on the test set).\nmethod top-5 err. ( test)\nVGG [41] (ILSVRC\u201914) 7.32\nGoogLeNet [44] (ILSVRC\u201914) 6.66\nVGG [41] (v5) 6.8\nPReLU-net [13] 4.94\nBN-inception [16] 4.82\nResNet (ILSVRC\u201915) 3.57\nTable 5. Error rates (%) of ensembles . The top-5 error is on the\ntest set of ImageNet and reported by the test server.\nResNet reduces the top-1 error by 3.5% (Table 2), resulting\nfrom the successfully reduced training error (Fig. 4 right vs.\nleft). This comparison veri\ufb01es the effectiveness of residual\nlearning on extremely deep systems.\nLast, we also note that the 18-layer plain/residual nets\nare comparably accurate (Table 2), but the 18-layer ResNet\nconverges faster (Fig. 4 right vs. left). When the net is \u201cnot\noverly deep\u201d (18 layers here), the current SGD solver is still\nable to \ufb01nd good solutions to the plain net. In this case, the\nResNet eases the optimization by providing faster conver-\ngence at the early stage.\nIdentity vs. Projection Shortcuts. We have shown that\n3x3, 641x1, 64\nrelu\n1x1, 256relu\nrelu3x3, 64\n3x3, 64\nrelurelu64-d 256-dFigure 5. A deeper residual function Ffor ImageNet. Left: a\nbuilding block (on 56 \u000256 feature maps) as in Fig. 3 for ResNet-\n34. Right: a \u201cbottleneck\u201d building block for ResNet-50/101/152.\nparameter-free, identity shortcuts help with training. Next\nwe investigate projection shortcuts (Eqn.(2)). In Table 3 we\ncompare three options: (A) zero-padding shortcuts are used\nfor increasing dimensions, and all shortcuts are parameter-\nfree (the same as Table 2 and Fig. 4 right); (B) projec-\ntion shortcuts are used for increasing dimensions, and other\nshortcuts are identity; and (C) all shortcuts are projections.\nTable 3 shows that all three options are considerably bet-\nter than the plain counterpart. B is slightly better than A. We\nargue that this is because the zero-padded dimensions in A\nindeed have no residual learning. C is marginally better than\nB, and we attribute this to the extra parameters introduced\nby many (thirteen) projection shortcuts. But the small dif-\nferences among A/B/C indicate that projection shortcuts are\nnot essential for addressing the degradation problem. So we\ndo not use option C in the rest of this paper, to reduce mem-\nory/time complexity and model sizes. Identity shortcuts are\nparticularly important for not increasing the complexity of\nthe bottleneck architectures that are introduced below.\nDeeper Bottleneck Architectures. Next we describe our\ndeeper nets for ImageNet. Because of concerns on the train-\ning time that we can afford, we modify the building block\nas a bottleneck design4. For each residual function F, we\nuse a stack of 3 layers instead of 2 (Fig. 5). The three layers\nare 1\u00021, 3\u00023, and 1\u00021 convolutions, where the 1 \u00021 layers\nare responsible for reducing and then increasing (restoring)\ndimensions, leaving the 3 \u00023 layer a bottleneck with smaller\ninput/output dimensions. Fig. 5 shows an example, where\nboth designs have similar time complexity.\nThe parameter-free identity shortcuts are particularly im-\nportant for the bottleneck architectures. If the identity short-\ncut in Fig. 5 (right) is replaced with projection, one can\nshow that the time complexity and model size are doubled,\nas the shortcut is connected to the two high-dimensional\nends. So identity shortcuts lead to more ef\ufb01cient models\nfor the bottleneck designs.\n50-layer ResNet: We replace each 2-layer block in the\n4Deeper non-bottleneck ResNets ( e.g., Fig. 5 left) also gain accuracy\nfrom increased depth (as shown on CIFAR-10), but are not as economical\nas the bottleneck ResNets. So the usage of bottleneck designs is mainly due\nto practical considerations. We further note that the degradation problem\nof plain nets is also witnessed for the bottleneck designs.\n6\n34-layer net with this 3-layer bottleneck block, resulting in\na 50-layer ResNet (Table 1). We use option B for increasing\ndimensions. This model has 3.8 billion FLOPs.\n101-layer and 152-layer ResNets: We construct 101-\nlayer and 152-layer ResNets by using more 3-layer blocks\n(Table 1). Remarkably, although the depth is signi\ufb01cantly\nincreased, the 152-layer ResNet (11.3 billion FLOPs) still\nhaslower complexity than VGG-16/19 nets (15.3/19.6 bil-\nlion FLOPs).\nThe 50/101/152-layer ResNets are more accurate than\nthe 34-layer ones by considerable margins (Table 3 and 4).\nWe do not observe the degradation problem and thus en-\njoy signi\ufb01cant accuracy gains from considerably increased\ndepth. The bene\ufb01ts of depth are witnessed for all evaluation\nmetrics (Table 3 and 4).\nComparisons with State-of-the-art Methods. In Table 4\nwe compare with the previous best single-model results.\nOur baseline 34-layer ResNets have achieved very compet-\nitive accuracy. Our 152-layer ResNet has a single-model\ntop-5 validation error of 4.49%. This single-model result\noutperforms all previous ensemble results (Table 5). We\ncombine six models of different depth to form an ensemble\n(only with two 152-layer ones at the time of submitting).\nThis leads to 3.57% top-5 error on the test set (Table 5).\nThis entry won the 1st place in ILSVRC 2015.\n4.2. CIFAR-10 and Analysis\nWe conducted more studies on the CIFAR-10 dataset\n[20], which consists of 50k training images and 10k test-\ning images in 10 classes. We present experiments trained\non the training set and evaluated on the test set. Our focus\nis on the behaviors of extremely deep networks, but not on\npushing the state-of-the-art results, so we intentionally use\nsimple architectures as follows.\nThe plain/residual architectures follow the form in Fig. 3\n(middle/right). The network inputs are 32 \u000232 images, with\nthe per-pixel mean subtracted. The \ufb01rst layer is 3 \u00023 convo-\nlutions. Then we use a stack of 6nlayers with 3\u00023 convo-\nlutions on the feature maps of sizes f32;16;8grespectively,\nwith 2nlayers for each feature map size. The numbers of\n\ufb01lters aref16;32;64grespectively. The subsampling is per-\nformed by convolutions with a stride of 2. The network ends\nwith a global average pooling, a 10-way fully-connected\nlayer, and softmax. There are totally 6 n+2 stacked weighted\nlayers. The following table summarizes the architecture:\noutput map size 32\u000232 16\u000216 8\u00028\n# layers 1+2n 2n 2n\n# \ufb01lters 16 32 64\nWhen shortcut connections are used, they are connected\nto the pairs of 3\u00023 layers (totally 3nshortcuts). On this\ndataset we use identity shortcuts in all cases ( i.e., option A),method error (%)\nMaxout [10] 9.38\nNIN [25] 8.81\nDSN [24] 8.22\n# layers # params\nFitNet [35] 19 2.5M 8.39\nHighway [42, 43] 19 2.3M 7.54 (7.72\u00060.16)\nHighway [42, 43] 32 1.25M 8.80\nResNet 20 0.27M 8.75\nResNet 32 0.46M 7.51\nResNet 44 0.66M 7.17\nResNet 56 0.85M 6.97\nResNet 110 1.7M 6.43 (6.61\u00060.16)\nResNet 1202 19.4M 7.93\nTable 6. Classi\ufb01cation error on the CIFAR-10 test set. All meth-\nods are with data augmentation. For ResNet-110, we run it 5 times\nand show \u201cbest (mean \u0006std)\u201d as in [43].\nso our residual models have exactly the same depth, width,\nand number of parameters as the plain counterparts.\nWe use a weight decay of 0.0001 and momentum of 0.9,\nand adopt the weight initialization in [13] and BN [16] but\nwith no dropout. These models are trained with a mini-\nbatch size of 128 on two GPUs. We start with a learning\nrate of 0.1, divide it by 10 at 32k and 48k iterations, and\nterminate training at 64k iterations, which is determined on\na 45k/5k train/val split. We follow the simple data augmen-\ntation in [24] for training: 4 pixels are padded on each side,\nand a 32\u000232 crop is randomly sampled from the padded\nimage or its horizontal \ufb02ip. For testing, we only evaluate\nthe single view of the original 32 \u000232 image.\nWe compare n=f3;5;7;9g, leading to 20, 32, 44, and\n56-layer networks. Fig. 6 (left) shows the behaviors of the\nplain nets. The deep plain nets suffer from increased depth,\nand exhibit higher training error when going deeper. This\nphenomenon is similar to that on ImageNet (Fig. 4, left) and\non MNIST (see [42]), suggesting that such an optimization\ndif\ufb01culty is a fundamental problem.\nFig. 6 (middle) shows the behaviors of ResNets. Also\nsimilar to the ImageNet cases (Fig. 4, right), our ResNets\nmanage to overcome the optimization dif\ufb01culty and demon-\nstrate accuracy gains when the depth increases.\nWe further explore n= 18 that leads to a 110-layer\nResNet. In this case, we \ufb01nd that the initial learning rate\nof 0.1 is slightly too large to start converging5. So we use\n0.01 to warm up the training until the training error is below\n80% (about 400 iterations), and then go back to 0.1 and con-\ntinue training. The rest of the learning schedule is as done\npreviously. This 110-layer network converges well (Fig. 6,\nmiddle). It has fewer parameters than other deep and thin\n5With an initial learning rate of 0.1, it starts converging ( <90% error)\nafter several epochs, but still reaches similar accuracy.\n7\n0 1 2 3 4 5 6051020\niter. (1e4)error (%)\n  \nplain-20\nplain-32\nplain-44\nplain-56\n0 1 2 3 4 5 6051020\niter. (1e4)error (%)\n  \nResNet-20\nResNet-32\nResNet-44\nResNet-56\nResNet-110 56-layer\n20-layer\n110-layer20-layer\n4 5 60151020\niter. (1e4)error (%)\n  \nresidual-110\nresidual-1202Figure 6. Training on CIFAR-10 . Dashed lines denote training error, and bold lines denote testing error. Left: plain networks. The error\nof plain-110 is higher than 60% and not displayed. Middle : ResNets. Right : ResNets with 110 and 1202 layers.\n0 20 40 60 80 100123\nlayer index (sorted by magnitude)std\n  \nplain-20\nplain-56\nResNet-20\nResNet-56\nResNet-1100 20 40 60 80 100123\nlayer index (original)std\n  \nplain-20\nplain-56\nResNet-20\nResNet-56\nResNet-110\nFigure 7. Standard deviations (std) of layer responses on CIFAR-\n10. The responses are the outputs of each 3 \u00023 layer, after BN and\nbefore nonlinearity. Top: the layers are shown in their original\norder. Bottom : the responses are ranked in descending order.\nnetworks such as FitNet [35] and Highway [42] (Table 6),\nyet is among the state-of-the-art results (6.43%, Table 6).\nAnalysis of Layer Responses. Fig. 7 shows the standard\ndeviations (std) of the layer responses. The responses are\nthe outputs of each 3 \u00023 layer, after BN and before other\nnonlinearity (ReLU/addition). For ResNets, this analy-\nsis reveals the response strength of the residual functions.\nFig. 7 shows that ResNets have generally smaller responses\nthan their plain counterparts. These results support our ba-\nsic motivation (Sec.3.1) that the residual functions might\nbe generally closer to zero than the non-residual functions.\nWe also notice that the deeper ResNet has smaller magni-\ntudes of responses, as evidenced by the comparisons among\nResNet-20, 56, and 110 in Fig. 7. When there are more\nlayers, an individual layer of ResNets tends to modify the\nsignal less.\nExploring Over 1000 layers. We explore an aggressively\ndeep model of over 1000 layers. We set n= 200 that\nleads to a 1202-layer network, which is trained as described\nabove. Our method shows no optimization dif\ufb01culty , and\nthis103-layer network is able to achieve training error\n<0.1% (Fig. 6, right). Its test error is still fairly good\n(7.93%, Table 6).\nBut there are still open problems on such aggressively\ndeep models. The testing result of this 1202-layer network\nis worse than that of our 110-layer network, although bothtraining data 07+12 07++12\ntest data VOC 07 test VOC 12 test\nVGG-16 73.2 70.4\nResNet-101 76.4 73.8\nTable 7. Object detection mAP (%) on the PASCAL VOC\n2007/2012 test sets using baseline Faster R-CNN. See also Ta-\nble 10 and 11 for better results.\nmetric mAP@.5 mAP@[.5, .95]\nVGG-16 41.5 21.2\nResNet-101 48.4 27.2\nTable 8. Object detection mAP (%) on the COCO validation set\nusing baseline Faster R-CNN. See also Table 9 for better results.\nhave similar training error. We argue that this is because of\nover\ufb01tting. The 1202-layer network may be unnecessarily\nlarge (19.4M) for this small dataset. Strong regularization\nsuch as maxout [10] or dropout [14] is applied to obtain the\nbest results ([10, 25, 24, 35]) on this dataset. In this paper,\nwe use no maxout/dropout and just simply impose regular-\nization via deep and thin architectures by design, without\ndistracting from the focus on the dif\ufb01culties of optimiza-\ntion. But combining with stronger regularization may im-\nprove results, which we will study in the future.\n4.3. Object Detection on PASCAL and MS COCO\nOur method has good generalization performance on\nother recognition tasks. Table 7 and 8 show the object de-\ntection baseline results on PASCAL VOC 2007 and 2012\n[5] and COCO [26]. We adopt Faster R-CNN [32] as the de-\ntection method. Here we are interested in the improvements\nof replacing VGG-16 [41] with ResNet-101. The detection\nimplementation (see appendix) of using both models is the\nsame, so the gains can only be attributed to better networks.\nMost remarkably, on the challenging COCO dataset we ob-\ntain a 6.0% increase in COCO\u2019s standard metric (mAP@[.5,\n.95]), which is a 28% relative improvement. This gain is\nsolely due to the learned representations.\nBased on deep residual nets, we won the 1st places in\nseveral tracks in ILSVRC & COCO 2015 competitions: Im-\nageNet detection, ImageNet localization, COCO detection,\nand COCO segmentation. The details are in the appendix.\n8\nReferences\n[1] Y . Bengio, P. Simard, and P. Frasconi. Learning long-term dependen-\ncies with gradient descent is dif\ufb01cult. IEEE Transactions on Neural\nNetworks , 5(2):157\u2013166, 1994.\n[2] C. M. Bishop. Neural networks for pattern recognition . Oxford\nuniversity press, 1995.\n[3] W. L. Briggs, S. F. McCormick, et al. A Multigrid Tutorial . Siam,\n2000.\n[4] K. Chat\ufb01eld, V . Lempitsky, A. Vedaldi, and A. Zisserman. The devil\nis in the details: an evaluation of recent feature encoding methods.\nInBMVC , 2011.\n[5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\nserman. The Pascal Visual Object Classes (VOC) Challenge. IJCV ,\npages 303\u2013338, 2010.\n[6] S. Gidaris and N. Komodakis. Object detection via a multi-region &\nsemantic segmentation-aware cnn model. In ICCV , 2015.\n[7] R. Girshick. Fast R-CNN. In ICCV , 2015.\n[8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier-\narchies for accurate object detection and semantic segmentation. In\nCVPR , 2014.\n[9] X. Glorot and Y . Bengio. Understanding the dif\ufb01culty of training\ndeep feedforward neural networks. In AISTATS , 2010.\n[10] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and\nY . Bengio. Maxout networks. arXiv:1302.4389 , 2013.\n[11] K. He and J. Sun. Convolutional neural networks at constrained time\ncost. In CVPR , 2015.\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep\nconvolutional networks for visual recognition. In ECCV , 2014.\n[13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into recti\ufb01ers:\nSurpassing human-level performance on imagenet classi\ufb01cation. In\nICCV , 2015.\n[14] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov. Improving neural networks by preventing co-\nadaptation of feature detectors. arXiv:1207.0580 , 2012.\n[15] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural\ncomputation , 9(8):1735\u20131780, 1997.\n[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift. In ICML , 2015.\n[17] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest\nneighbor search. TPAMI , 33, 2011.\n[18] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\nC. Schmid. Aggregating local image descriptors into compact codes.\nTPAMI , 2012.\n[19] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for\nfast feature embedding. arXiv:1408.5093 , 2014.\n[20] A. Krizhevsky. Learning multiple layers of features from tiny im-\nages. Tech Report , 2009.\n[21] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classi\ufb01cation\nwith deep convolutional neural networks. In NIPS , 2012.\n[22] Y . LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,\nW. Hubbard, and L. D. Jackel. Backpropagation applied to hand-\nwritten zip code recognition. Neural computation , 1989.\n[23] Y . LeCun, L. Bottou, G. B. Orr, and K.-R. M \u00a8uller. Ef\ufb01cient backprop.\nInNeural Networks: Tricks of the Trade , pages 9\u201350. Springer, 1998.\n[24] C.-Y . Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-\nsupervised nets. arXiv:1409.5185 , 2014.\n[25] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv:1312.4400 ,\n2013.\n[26] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll \u00b4ar, and C. L. Zitnick. Microsoft COCO: Common objects in\ncontext. In ECCV . 2014.\n[27] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks\nfor semantic segmentation. In CVPR , 2015.[28] G. Mont \u00b4ufar, R. Pascanu, K. Cho, and Y . Bengio. On the number of\nlinear regions of deep neural networks. In NIPS , 2014.\n[29] V . Nair and G. E. Hinton. Recti\ufb01ed linear units improve restricted\nboltzmann machines. In ICML , 2010.\n[30] F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for\nimage categorization. In CVPR , 2007.\n[31] T. Raiko, H. Valpola, and Y . LeCun. Deep learning made easier by\nlinear transformations in perceptrons. In AISTATS , 2012.\n[32] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards\nreal-time object detection with region proposal networks. In NIPS ,\n2015.\n[33] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection\nnetworks on convolutional feature maps. arXiv:1504.06066 , 2015.\n[34] B. D. Ripley. Pattern recognition and neural networks . Cambridge\nuniversity press, 1996.\n[35] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\nY . Bengio. Fitnets: Hints for thin deep nets. In ICLR , 2015.\n[36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet\nlarge scale visual recognition challenge. arXiv:1409.0575 , 2014.\n[37] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to\nthe nonlinear dynamics of learning in deep linear neural networks.\narXiv:1312.6120 , 2013.\n[38] N. N. Schraudolph. Accelerated gradient descent by factor-centering\ndecomposition. Technical report, 1998.\n[39] N. N. Schraudolph. Centering neural network gradient factors. In\nNeural Networks: Tricks of the Trade , pages 207\u2013226. Springer,\n1998.\n[40] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y . Le-\nCun. Overfeat: Integrated recognition, localization and detection\nusing convolutional networks. In ICLR , 2014.\n[41] K. Simonyan and A. Zisserman. Very deep convolutional networks\nfor large-scale image recognition. In ICLR , 2015.\n[42] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks.\narXiv:1505.00387 , 2015.\n[43] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep\nnetworks. 1507.06228 , 2015.\n[44] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er-\nhan, V . Vanhoucke, and A. Rabinovich. Going deeper with convolu-\ntions. In CVPR , 2015.\n[45] R. Szeliski. Fast surface interpolation using hierarchical basis func-\ntions. TPAMI , 1990.\n[46] R. Szeliski. Locally adapted hierarchical basis preconditioning. In\nSIGGRAPH , 2006.\n[47] T. Vatanen, T. Raiko, H. Valpola, and Y . LeCun. Pushing stochas-\ntic gradient towards second-order methods\u2013backpropagation learn-\ning with transformations in nonlinearities. In Neural Information\nProcessing , 2013.\n[48] A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library\nof computer vision algorithms, 2008.\n[49] W. Venables and B. Ripley. Modern applied statistics with s-plus.\n1999.\n[50] M. D. Zeiler and R. Fergus. Visualizing and understanding convolu-\ntional neural networks. In ECCV , 2014.\n9\nA. Object Detection Baselines\nIn this section we introduce our detection method based\non the baseline Faster R-CNN [32] system. The models are\ninitialized by the ImageNet classi\ufb01cation models, and then\n\ufb01ne-tuned on the object detection data. We have experi-\nmented with ResNet-50/101 at the time of the ILSVRC &\nCOCO 2015 detection competitions.\nUnlike VGG-16 used in [32], our ResNet has no hidden\nfc layers. We adopt the idea of \u201cNetworks on Conv fea-\nture maps\u201d (NoC) [33] to address this issue. We compute\nthe full-image shared conv feature maps using those lay-\ners whose strides on the image are no greater than 16 pixels\n(i.e., conv1, conv2 x, conv3 x, and conv4 x, totally 91 conv\nlayers in ResNet-101; Table 1). We consider these layers as\nanalogous to the 13 conv layers in VGG-16, and by doing\nso, both ResNet and VGG-16 have conv feature maps of the\nsame total stride (16 pixels). These layers are shared by a\nregion proposal network (RPN, generating 300 proposals)\n[32] and a Fast R-CNN detection network [7]. RoI pool-\ning [7] is performed before conv5 1. On this RoI-pooled\nfeature, all layers of conv5 x and up are adopted for each\nregion, playing the roles of VGG-16\u2019s fc layers. The \ufb01nal\nclassi\ufb01cation layer is replaced by two sibling layers (classi-\n\ufb01cation and box regression [7]).\nFor the usage of BN layers, after pre-training, we com-\npute the BN statistics (means and variances) for each layer\non the ImageNet training set. Then the BN layers are \ufb01xed\nduring \ufb01ne-tuning for object detection. As such, the BN\nlayers become linear activations with constant offsets and\nscales, and BN statistics are not updated by \ufb01ne-tuning. We\n\ufb01x the BN layers mainly for reducing memory consumption\nin Faster R-CNN training.\nPASCAL VOC\nFollowing [7, 32], for the PASCAL VOC 2007 testset,\nwe use the 5k trainval images in VOC 2007 and 16k train-\nvalimages in VOC 2012 for training (\u201c07+12\u201d). For the\nPASCAL VOC 2012 testset, we use the 10k trainval +test\nimages in VOC 2007 and 16k trainval images in VOC 2012\nfor training (\u201c07++12\u201d). The hyper-parameters for train-\ning Faster R-CNN are the same as in [32]. Table 7 shows\nthe results. ResNet-101 improves the mAP by >3% over\nVGG-16. This gain is solely because of the improved fea-\ntures learned by ResNet.\nMS COCO\nThe MS COCO dataset [26] involves 80 object cate-\ngories. We evaluate the PASCAL VOC metric (mAP @\nIoU = 0.5) and the standard COCO metric (mAP @ IoU =\n.5:.05:.95). We use the 80k images on the train set for train-\ning and the 40k images on the val set for evaluation. Our\ndetection system for COCO is similar to that for PASCAL\nVOC. We train the COCO models with an 8-GPU imple-\nmentation, and thus the RPN step has a mini-batch size of8 images ( i.e., 1 per GPU) and the Fast R-CNN step has a\nmini-batch size of 16 images. The RPN step and Fast R-\nCNN step are both trained for 240k iterations with a learn-\ning rate of 0.001 and then for 80k iterations with 0.0001.\nTable 8 shows the results on the MS COCO validation\nset. ResNet-101 has a 6% increase of mAP@[.5, .95] over\nVGG-16, which is a 28% relative improvement, solely con-\ntributed by the features learned by the better network. Re-\nmarkably, the mAP@[.5, .95]\u2019s absolute increase (6.0%) is\nnearly as big as mAP@.5\u2019s (6.9%). This suggests that a\ndeeper network can improve both recognition and localiza-\ntion.\nB. Object Detection Improvements\nFor completeness, we report the improvements made for\nthe competitions. These improvements are based on deep\nfeatures and thus should bene\ufb01t from residual learning.\nMS COCO\nBox re\ufb01nement. Our box re\ufb01nement partially follows the it-\nerative localization in [6]. In Faster R-CNN, the \ufb01nal output\nis a regressed box that is different from its proposal box. So\nfor inference, we pool a new feature from the regressed box\nand obtain a new classi\ufb01cation score and a new regressed\nbox. We combine these 300 new predictions with the orig-\ninal 300 predictions. Non-maximum suppression (NMS) is\napplied on the union set of predicted boxes using an IoU\nthreshold of 0.3 [8], followed by box voting [6]. Box re-\n\ufb01nement improves mAP by about 2 points (Table 9).\nGlobal context. We combine global context in the Fast\nR-CNN step. Given the full-image conv feature map, we\npool a feature by global Spatial Pyramid Pooling [12] (with\na \u201csingle-level\u201d pyramid) which can be implemented as\n\u201cRoI\u201d pooling using the entire image\u2019s bounding box as the\nRoI. This pooled feature is fed into the post-RoI layers to\nobtain a global context feature. This global feature is con-\ncatenated with the original per-region feature, followed by\nthe sibling classi\ufb01cation and box regression layers. This\nnew structure is trained end-to-end. Global context im-\nproves mAP@.5 by about 1 point (Table 9).\nMulti-scale testing. In the above, all results are obtained by\nsingle-scale training/testing as in [32], where the image\u2019s\nshorter side is s= 600 pixels. Multi-scale training/testing\nhas been developed in [12, 7] by selecting a scale from a\nfeature pyramid, and in [33] by using maxout layers. In\nour current implementation, we have performed multi-scale\ntesting following [33]; we have not performed multi-scale\ntraining because of limited time. In addition, we have per-\nformed multi-scale testing only for the Fast R-CNN step\n(but not yet for the RPN step). With a trained model, we\ncompute conv feature maps on an image pyramid, where the\nimage\u2019s shorter sides are s2f200;400;600;800;1000g.\n10\ntraining data COCO train COCO trainval\ntest data COCO val COCO test-dev\nmAP @.5 @[.5, .95] @.5 @[.5, .95]\nbaseline Faster R-CNN (VGG-16) 41.5 21.2\nbaseline Faster R-CNN (ResNet-101) 48.4 27.2\n+box re\ufb01nement 49.9 29.9\n+context 51.1 30.0 53.3 32.2\n+multi-scale testing 53.8 32.5 55.7 34.9\nensemble 59.0 37.4\nTable 9. Object detection improvements on MS COCO using Faster R-CNN and ResNet-101.\nsystem net data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv\nbaseline VGG-16 07+12 73.2 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6\nbaseline ResNet-101 07+12 76.4 79.8 80.7 76.2 68.3 55.9 85.1 85.3 89.8 56.7 87.8 69.4 88.3 88.9 80.9 78.4 41.7 78.6 79.8 85.3 72.0\nbaseline+++ ResNet-101 COCO+07+12 85.6 90.0 89.6 87.8 80.8 76.1 89.9 89.9 89.6 75.5 90.0 80.7 89.6 90.3 89.1 88.7 65.4 88.1 85.6 89.0 86.8\nTable 10. Detection results on the PASCAL VOC 2007 test set. The baseline is the Faster R-CNN system. The system \u201cbaseline+++\u201d\ninclude box re\ufb01nement, context, and multi-scale testing in Table 9.\nsystem net data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv\nbaseline VGG-16 07++12 70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5\nbaseline ResNet-101 07++12 73.8 86.5 81.6 77.2 58.0 51.0 78.6 76.6 93.2 48.6 80.4 59.0 92.1 85.3 84.8 80.7 48.1 77.3 66.5 84.7 65.6\nbaseline+++ ResNet-101 COCO+07++12 83.8 92.1 88.4 84.8 75.9 71.4 86.3 87.8 94.2 66.8 89.4 69.2 93.9 91.9 90.9 89.6 67.9 88.2 76.8 90.3 80.0\nTable 11. Detection results on the PASCAL VOC 2012 test set ( http://host.robots.ox.ac.uk:8080/leaderboard/\ndisplaylb.php?challengeid=11&compid=4 ). The baseline is the Faster R-CNN system. The system \u201cbaseline+++\u201d include\nbox re\ufb01nement, context, and multi-scale testing in Table 9.\nWe select two adjacent scales from the pyramid following\n[33]. RoI pooling and subsequent layers are performed on\nthe feature maps of these two scales [33], which are merged\nby maxout as in [33]. Multi-scale testing improves the mAP\nby over 2 points (Table 9).\nUsing validation data. Next we use the 80k+40k trainval set\nfor training and the 20k test-dev set for evaluation. The test-\ndev set has no publicly available ground truth and the result\nis reported by the evaluation server. Under this setting, the\nresults are an mAP@.5 of 55.7% and an mAP@[.5, .95] of\n34.9% (Table 9). This is our single-model result.\nEnsemble. In Faster R-CNN, the system is designed to learn\nregion proposals and also object classi\ufb01ers, so an ensemble\ncan be used to boost both tasks. We use an ensemble for\nproposing regions, and the union set of proposals are pro-\ncessed by an ensemble of per-region classi\ufb01ers. Table 9\nshows our result based on an ensemble of 3 networks. The\nmAP is 59.0% and 37.4% on the test-dev set. This result\nwon the 1st place in the detection task in COCO 2015.\nPASCAL VOC\nWe revisit the PASCAL VOC dataset based on the above\nmodel. With the single model on the COCO dataset (55.7%\nmAP@.5 in Table 9), we \ufb01ne-tune this model on the PAS-\nCAL VOC sets. The improvements of box re\ufb01nement, con-\ntext, and multi-scale testing are also adopted. By doing soval2 test\nGoogLeNet [44] (ILSVRC\u201914) - 43.9\nour single model (ILSVRC\u201915) 60.5 58.8\nour ensemble (ILSVRC\u201915) 63.6 62.1\nTable 12. Our results (mAP, %) on the ImageNet detection dataset.\nOur detection system is Faster R-CNN [32] with the improvements\nin Table 9, using ResNet-101.\nwe achieve 85.6% mAP on PASCAL VOC 2007 (Table 10)\nand 83.8% on PASCAL VOC 2012 (Table 11)6. The result\non PASCAL VOC 2012 is 10 points higher than the previ-\nous state-of-the-art result [6].\nImageNet Detection\nThe ImageNet Detection (DET) task involves 200 object\ncategories. The accuracy is evaluated by mAP@.5. Our\nobject detection algorithm for ImageNet DET is the same\nas that for MS COCO in Table 9. The networks are pre-\ntrained on the 1000-class ImageNet classi\ufb01cation set, and\nare \ufb01ne-tuned on the DET data. We split the validation set\ninto two parts (val1/val2) following [8]. We \ufb01ne-tune the\ndetection models using the DET training set and the val1\nset. The val2 set is used for validation. We do not use other\nILSVRC 2015 data. Our single model with ResNet-101 has\n6http://host.robots.ox.ac.uk:8080/anonymous/3OJ4OJ.html ,\nsubmitted on 2015-11-26.\n11\nLOC\nmethodLOC\nnetworktestingLOC error\non GT CLSclassi\ufb01cation\nnetworktop-5 LOC error\non predicted CLS\nVGG\u2019s [41] VGG-16 1-crop 33.1 [41]\nRPN ResNet-101 1-crop 13.3\nRPN ResNet-101 dense 11.7\nRPN ResNet-101 dense ResNet-101 14.4\nRPN+RCNN ResNet-101 dense ResNet-101 10.6\nRPN+RCNN ensemble dense ensemble 8.9\nTable 13. Localization error (%) on the ImageNet validation. In\nthe column of \u201cLOC error on GT class\u201d ([41]), the ground truth\nclass is used. In the \u201ctesting\u201d column, \u201c1-crop\u201d denotes testing\non a center crop of 224 \u0002224 pixels, \u201cdense\u201d denotes dense (fully\nconvolutional) and multi-scale testing.\n58.8% mAP and our ensemble of 3 models has 62.1% mAP\non the DET test set (Table 12). This result won the 1st place\nin the ImageNet detection task in ILSVRC 2015 , surpassing\nthe second place by 8.5 points (absolute).\nC. ImageNet Localization\nThe ImageNet Localization (LOC) task [36] requires to\nclassify and localize the objects. Following [40, 41], we\nassume that the image-level classi\ufb01ers are \ufb01rst adopted for\npredicting the class labels of an image, and the localiza-\ntion algorithm only accounts for predicting bounding boxes\nbased on the predicted classes. We adopt the \u201cper-class re-\ngression\u201d (PCR) strategy [40, 41], learning a bounding box\nregressor for each class. We pre-train the networks for Im-\nageNet classi\ufb01cation and then \ufb01ne-tune them for localiza-\ntion. We train networks on the provided 1000-class Ima-\ngeNet training set.\nOur localization algorithm is based on the RPN frame-\nwork of [32] with a few modi\ufb01cations. Unlike the way in\n[32] that is category-agnostic, our RPN for localization is\ndesigned in a per-class form. This RPN ends with two sib-\nling 1\u00021 convolutional layers for binary classi\ufb01cation ( cls)\nand box regression ( reg), as in [32]. The clsandreglayers\nare both in a per-class from, in contrast to [32]. Speci\ufb01-\ncally, the clslayer has a 1000-d output, and each dimension\nisbinary logistic regression for predicting being or not be-\ning an object class; the reglayer has a 1000\u00024-d output\nconsisting of box regressors for 1000 classes. As in [32],\nour bounding box regression is with reference to multiple\ntranslation-invariant \u201canchor\u201d boxes at each position.\nAs in our ImageNet classi\ufb01cation training (Sec. 3.4), we\nrandomly sample 224 \u0002224 crops for data augmentation.\nWe use a mini-batch size of 256 images for \ufb01ne-tuning. To\navoid negative samples being dominate, 8 anchors are ran-\ndomly sampled for each image, where the sampled positive\nand negative anchors have a ratio of 1:1 [32]. For testing,\nthe network is applied on the image fully-convolutionally.\nTable 13 compares the localization results. Following\n[41], we \ufb01rst perform \u201coracle\u201d testing using the ground truth\nclass as the classi\ufb01cation prediction. VGG\u2019s paper [41] re-methodtop-5 localization err\nval test\nOverFeat [40] (ILSVRC\u201913) 30.0 29.9\nGoogLeNet [44] (ILSVRC\u201914) - 26.7\nVGG [41] (ILSVRC\u201914) 26.9 25.3\nours (ILSVRC\u201915) 8.9 9.0\nTable 14. Comparisons of localization error (%) on the ImageNet\ndataset with state-of-the-art methods.\nports a center-crop error of 33.1% (Table 13) using ground\ntruth classes. Under the same setting, our RPN method us-\ning ResNet-101 net signi\ufb01cantly reduces the center-crop er-\nror to 13.3%. This comparison demonstrates the excellent\nperformance of our framework. With dense (fully convolu-\ntional) and multi-scale testing, our ResNet-101 has an error\nof 11.7% using ground truth classes. Using ResNet-101 for\npredicting classes (4.6% top-5 classi\ufb01cation error, Table 4),\nthe top-5 localization error is 14.4%.\nThe above results are only based on the proposal network\n(RPN) in Faster R-CNN [32]. One may use the detection\nnetwork (Fast R-CNN [7]) in Faster R-CNN to improve the\nresults. But we notice that on this dataset, one image usually\ncontains a single dominate object, and the proposal regions\nhighly overlap with each other and thus have very similar\nRoI-pooled features. As a result, the image-centric training\nof Fast R-CNN [7] generates samples of small variations,\nwhich may not be desired for stochastic training. Motivated\nby this, in our current experiment we use the original R-\nCNN [8] that is RoI-centric, in place of Fast R-CNN.\nOur R-CNN implementation is as follows. We apply the\nper-class RPN trained as above on the training images to\npredict bounding boxes for the ground truth class. These\npredicted boxes play a role of class-dependent proposals.\nFor each training image, the highest scored 200 proposals\nare extracted as training samples to train an R-CNN classi-\n\ufb01er. The image region is cropped from a proposal, warped\nto 224\u0002224 pixels, and fed into the classi\ufb01cation network\nas in R-CNN [8]. The outputs of this network consist of two\nsibling fc layers for clsandreg, also in a per-class form.\nThis R-CNN network is \ufb01ne-tuned on the training set us-\ning a mini-batch size of 256 in the RoI-centric fashion. For\ntesting, the RPN generates the highest scored 200 proposals\nfor each predicted class, and the R-CNN network is used to\nupdate these proposals\u2019 scores and box positions.\nThis method reduces the top-5 localization error to\n10.6% (Table 13). This is our single-model result on the\nvalidation set. Using an ensemble of networks for both clas-\nsi\ufb01cation and localization, we achieve a top-5 localization\nerror of 9.0% on the test set. This number signi\ufb01cantly out-\nperforms the ILSVRC 14 results (Table 14), showing a 64%\nrelative reduction of error. This result won the 1st place in\nthe ImageNet localization task in ILSVRC 2015.\n12\n",
    "title": "Deep Residual Learning for Image Recognition",
    "arxiv_id": "1512.03385",
    "category": "famous",
    "authors": [
      "Kaiming He",
      "Xiangyu Zhang",
      "Shaoqing Ren",
      "Jian Sun"
    ],
    "published": "2015-12-10",
    "num_pages": 12,
    "num_chars": 59286
  },
  {
    "text": "Published as a conference paper at ICLR 2015\nNEURAL MACHINE TRANSLATION\nBYJOINTLY LEARNING TO ALIGN AND TRANSLATE\nDzmitry Bahdanau\nJacobs University Bremen, Germany\nKyungHyun Cho Yoshua Bengio\u0003\nUniversit \u00b4e de Montr \u00b4eal\nABSTRACT\nNeural machine translation is a recently proposed approach to machine transla-\ntion. Unlike the traditional statistical machine translation, the neural machine\ntranslation aims at building a single neural network that can be jointly tuned to\nmaximize the translation performance. The models proposed recently for neu-\nral machine translation often belong to a family of encoder\u2013decoders and encode\na source sentence into a \ufb01xed-length vector from which a decoder generates a\ntranslation. In this paper, we conjecture that the use of a \ufb01xed-length vector is a\nbottleneck in improving the performance of this basic encoder\u2013decoder architec-\nture, and propose to extend this by allowing a model to automatically (soft-)search\nfor parts of a source sentence that are relevant to predicting a target word, without\nhaving to form these parts as a hard segment explicitly. With this new approach,\nwe achieve a translation performance comparable to the existing state-of-the-art\nphrase-based system on the task of English-to-French translation. Furthermore,\nqualitative analysis reveals that the (soft-)alignments found by the model agree\nwell with our intuition.\n1 I NTRODUCTION\nNeural machine translation is a newly emerging approach to machine translation, recently proposed\nby Kalchbrenner and Blunsom (2013), Sutskever et al. (2014) and Cho et al. (2014b). Unlike the\ntraditional phrase-based translation system (see, e.g., Koehn et al. , 2003) which consists of many\nsmall sub-components that are tuned separately, neural machine translation attempts to build and\ntrain a single, large neural network that reads a sentence and outputs a correct translation.\nMost of the proposed neural machine translation models belong to a family of encoder\u2013\ndecoders (Sutskever et al. , 2014; Cho et al. , 2014a), with an encoder and a decoder for each lan-\nguage, or involve a language-speci\ufb01c encoder applied to each sentence whose outputs are then com-\npared (Hermann and Blunsom, 2014). An encoder neural network reads and encodes a source sen-\ntence into a \ufb01xed-length vector. A decoder then outputs a translation from the encoded vector. The\nwhole encoder\u2013decoder system, which consists of the encoder and the decoder for a language pair,\nis jointly trained to maximize the probability of a correct translation given a source sentence.\nA potential issue with this encoder\u2013decoder approach is that a neural network needs to be able to\ncompress all the necessary information of a source sentence into a \ufb01xed-length vector. This may\nmake it dif\ufb01cult for the neural network to cope with long sentences, especially those that are longer\nthan the sentences in the training corpus. Cho et al. (2014b) showed that indeed the performance of\na basic encoder\u2013decoder deteriorates rapidly as the length of an input sentence increases.\nIn order to address this issue, we introduce an extension to the encoder\u2013decoder model which learns\nto align and translate jointly. Each time the proposed model generates a word in a translation, it\n(soft-)searches for a set of positions in a source sentence where the most relevant information is\nconcentrated. The model then predicts a target word based on the context vectors associated with\nthese source positions and all the previous generated target words.\n\u0003CIFAR Senior Fellow\n1arXiv:1409.0473v7  [cs.CL]  19 May 2016\nPublished as a conference paper at ICLR 2015\nThe most important distinguishing feature of this approach from the basic encoder\u2013decoder is that\nit does not attempt to encode a whole input sentence into a single \ufb01xed-length vector. Instead, it en-\ncodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively\nwhile decoding the translation. This frees a neural translation model from having to squash all the\ninformation of a source sentence, regardless of its length, into a \ufb01xed-length vector. We show this\nallows a model to cope better with long sentences.\nIn this paper, we show that the proposed approach of jointly learning to align and translate achieves\nsigni\ufb01cantly improved translation performance over the basic encoder\u2013decoder approach. The im-\nprovement is more apparent with longer sentences, but can be observed with sentences of any\nlength. On the task of English-to-French translation, the proposed approach achieves, with a single\nmodel, a translation performance comparable, or close, to the conventional phrase-based system.\nFurthermore, qualitative analysis reveals that the proposed model \ufb01nds a linguistically plausible\n(soft-)alignment between a source sentence and the corresponding target sentence.\n2 B ACKGROUND : NEURAL MACHINE TRANSLATION\nFrom a probabilistic perspective, translation is equivalent to \ufb01nding a target sentence ythat max-\nimizes the conditional probability of ygiven a source sentence x, i.e., arg maxyp(yjx). In\nneural machine translation, we \ufb01t a parameterized model to maximize the conditional probability\nof sentence pairs using a parallel training corpus. Once the conditional distribution is learned by a\ntranslation model, given a source sentence a corresponding translation can be generated by searching\nfor the sentence that maximizes the conditional probability.\nRecently, a number of papers have proposed the use of neural networks to directly learn this condi-\ntional distribution (see, e.g., Kalchbrenner and Blunsom, 2013; Cho et al. , 2014a; Sutskever et al. ,\n2014; Cho et al. , 2014b; Forcada and \u02dcNeco, 1997). This neural machine translation approach typ-\nically consists of two components, the \ufb01rst of which encodes a source sentence xand the second\ndecodes to a target sentence y. For instance, two recurrent neural networks (RNN) were used by\n(Cho et al. , 2014a) and (Sutskever et al. , 2014) to encode a variable-length source sentence into a\n\ufb01xed-length vector and to decode the vector into a variable-length target sentence.\nDespite being a quite new approach, neural machine translation has already shown promising results.\nSutskever et al. (2014) reported that the neural machine translation based on RNNs with long short-\nterm memory (LSTM) units achieves close to the state-of-the-art performance of the conventional\nphrase-based machine translation system on an English-to-French translation task.1Adding neural\ncomponents to existing translation systems, for instance, to score the phrase pairs in the phrase\ntable (Cho et al. , 2014a) or to re-rank candidate translations (Sutskever et al. , 2014), has allowed to\nsurpass the previous state-of-the-art performance level.\n2.1 RNN E NCODER \u2013DECODER\nHere, we describe brie\ufb02y the underlying framework, called RNN Encoder\u2013Decoder , proposed by\nCho et al. (2014a) and Sutskever et al. (2014) upon which we build a novel architecture that learns\nto align and translate simultaneously.\nIn the Encoder\u2013Decoder framework, an encoder reads the input sentence, a sequence of vectors\nx= (x1;\u0001\u0001\u0001;xTx), into a vector c.2The most common approach is to use an RNN such that\nht=f(xt;ht\u00001) (1)\nand\nc=q(fh1;\u0001\u0001\u0001;hTxg);\nwhereht2Rnis a hidden state at time t, andcis a vector generated from the sequence of the\nhidden states. fandqare some nonlinear functions. Sutskever et al. (2014) used an LSTM as fand\nq(fh1;\u0001\u0001\u0001;hTg) =hT, for instance.\n1We mean by the state-of-the-art performance, the performance of the conventional phrase-based system\nwithout using any neural network-based component.\n2Although most of the previous works (see, e.g., Cho et al. , 2014a; Sutskever et al. , 2014; Kalchbrenner and\nBlunsom, 2013) used to encode a variable-length input sentence into a \ufb01xed-length vector, it is not necessary,\nand even it may be bene\ufb01cial to have a variable-length vector, as we will show later.\n2\nPublished as a conference paper at ICLR 2015\nThe decoder is often trained to predict the next word yt0given the context vector cand all the\npreviously predicted words fy1;\u0001\u0001\u0001;yt0\u00001g. In other words, the decoder de\ufb01nes a probability over\nthe translation yby decomposing the joint probability into the ordered conditionals:\np(y) =TY\nt=1p(ytjfy1;\u0001\u0001\u0001;yt\u00001g;c); (2)\nwhere y=\u0000\ny1;\u0001\u0001\u0001;yTy\u0001\n. With an RNN, each conditional probability is modeled as\np(ytjfy1;\u0001\u0001\u0001;yt\u00001g;c) =g(yt\u00001;st;c); (3)\nwheregis a nonlinear, potentially multi-layered, function that outputs the probability of yt, andstis\nthe hidden state of the RNN. It should be noted that other architectures such as a hybrid of an RNN\nand a de-convolutional neural network can be used (Kalchbrenner and Blunsom, 2013).\n3 L EARNING TO ALIGN AND TRANSLATE\nIn this section, we propose a novel architecture for neural machine translation. The new architecture\nconsists of a bidirectional RNN as an encoder (Sec. 3.2) and a decoder that emulates searching\nthrough a source sentence during decoding a translation (Sec. 3.1).\n3.1 D ECODER : GENERAL DESCRIPTION\nx1x2x3xT+\n\u03b1t,1\n\u03b1t,2 \u03b1t,3\u03b1t,Tyt-1yt\nh1h2h3 hTh1h2h3 hTst-1st\nFigure 1: The graphical illus-\ntration of the proposed model\ntrying to generate the t-th tar-\nget wordytgiven a source\nsentence (x1;x2;:::;x T).In a new model architecture, we de\ufb01ne each conditional probability\nin Eq. (2) as:\np(yijy1;:::;y i\u00001;x) =g(yi\u00001;si;ci); (4)\nwheresiis an RNN hidden state for time i, computed by\nsi=f(si\u00001;yi\u00001;ci):\nIt should be noted that unlike the existing encoder\u2013decoder ap-\nproach (see Eq. (2)), here the probability is conditioned on a distinct\ncontext vector cifor each target word yi.\nThe context vector cidepends on a sequence of annotations\n(h1;\u0001\u0001\u0001;hTx)to which an encoder maps the input sentence. Each\nannotationhicontains information about the whole input sequence\nwith a strong focus on the parts surrounding the i-th word of the\ninput sequence. We explain in detail how the annotations are com-\nputed in the next section.\nThe context vector ciis, then, computed as a weighted sum of these\nannotationshi:\nci=TxX\nj=1\u000bijhj: (5)\nThe weight\u000bijof each annotation hjis computed by\n\u000bij=exp (eij)PTx\nk=1exp (eik); (6)\nwhere\neij=a(si\u00001;hj)\nis an alignment model which scores how well the inputs around position jand the output at position\nimatch. The score is based on the RNN hidden state si\u00001(just before emitting yi, Eq. (4)) and the\nj-th annotation hjof the input sentence.\nWe parametrize the alignment model aas a feedforward neural network which is jointly trained with\nall the other components of the proposed system. Note that unlike in traditional machine translation,\n3\nPublished as a conference paper at ICLR 2015\nthe alignment is not considered to be a latent variable. Instead, the alignment model directly com-\nputes a soft alignment, which allows the gradient of the cost function to be backpropagated through.\nThis gradient can be used to train the alignment model as well as the whole translation model jointly.\nWe can understand the approach of taking a weighted sum of all the annotations as computing an\nexpected annotation , where the expectation is over possible alignments. Let \u000bijbe a probability that\nthe target word yiis aligned to, or translated from, a source word xj. Then, thei-th context vector\nciis the expected annotation over all the annotations with probabilities \u000bij.\nThe probability \u000bij, or its associated energy eij, re\ufb02ects the importance of the annotation hjwith\nrespect to the previous hidden state si\u00001in deciding the next state siand generating yi. Intuitively,\nthis implements a mechanism of attention in the decoder. The decoder decides parts of the source\nsentence to pay attention to. By letting the decoder have an attention mechanism, we relieve the\nencoder from the burden of having to encode all information in the source sentence into a \ufb01xed-\nlength vector. With this new approach the information can be spread throughout the sequence of\nannotations, which can be selectively retrieved by the decoder accordingly.\n3.2 E NCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES\nThe usual RNN, described in Eq. (1), reads an input sequence xin order starting from the \ufb01rst\nsymbolx1to the last one xTx. However, in the proposed scheme, we would like the annotation\nof each word to summarize not only the preceding words, but also the following words. Hence,\nwe propose to use a bidirectional RNN (BiRNN, Schuster and Paliwal, 1997), which has been\nsuccessfully used recently in speech recognition (see, e.g., Graves et al. , 2013).\nA BiRNN consists of forward and backward RNN\u2019s. The forward RNN\u0000 !freads the input sequence\nas it is ordered (from x1toxTx) and calculates a sequence of forward hidden states (\u0000 !h1;\u0001\u0001\u0001;\u0000 !hTx).\nThe backward RNN \u0000freads the sequence in the reverse order (from xTxtox1), resulting in a\nsequence of backward hidden states ( \u0000h1;\u0001\u0001\u0001; \u0000hTx).\nWe obtain an annotation for each word xjby concatenating the forward hidden state\u0000 !hjand the\nbackward one \u0000hj, i.e.,hj=h\u0000 !h>\nj; \u0000h>\nji>\n. In this way, the annotation hjcontains the summaries\nof both the preceding words and the following words. Due to the tendency of RNNs to better\nrepresent recent inputs, the annotation hjwill be focused on the words around xj. This sequence\nof annotations is used by the decoder and the alignment model later to compute the context vector\n(Eqs. (5)\u2013(6)).\nSee Fig. 1 for the graphical illustration of the proposed model.\n4 E XPERIMENT SETTINGS\nWe evaluate the proposed approach on the task of English-to-French translation. We use the bilin-\ngual, parallel corpora provided by ACL WMT \u201914.3As a comparison, we also report the perfor-\nmance of an RNN Encoder\u2013Decoder which was proposed recently by Cho et al. (2014a). We use\nthe same training procedures and the same dataset for both models.4\n4.1 D ATASET\nWMT \u201914 contains the following English-French parallel corpora: Europarl (61M words), news\ncommentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively,\ntotaling 850M words. Following the procedure described in Cho et al. (2014a), we reduce the size of\nthe combined corpus to have 348M words using the data selection method by Axelrod et al. (2011).5\nWe do not use any monolingual data other than the mentioned parallel corpora, although it may be\npossible to use a much larger monolingual corpus to pretrain an encoder. We concatenate news-test-\n3http://www.statmt.org/wmt14/translation-task.html\n4Implementations are available at https://github.com/lisa-groundhog/GroundHog .\n5Available online at http://www-lium.univ-lemans.fr/ \u02dcschwenk/cslm_joint_paper/ .\n4\nPublished as a conference paper at ICLR 2015\n0 10 20 30 40 50 60\nSentence length051015202530BLEU score RNNsearch-50\nRNNsearch-30\nRNNenc-50\nRNNenc-30\nFigure 2: The BLEU scores\nof the generated translations\non the test set with respect\nto the lengths of the sen-\ntences. The results are on\nthe full test set which in-\ncludes sentences having un-\nknown words to the models.\n2012 and news-test-2013 to make a development (validation) set, and evaluate the models on the test\nset (news-test-2014) from WMT \u201914, which consists of 3003 sentences not present in the training\ndata.\nAfter a usual tokenization6, we use a shortlist of 30,000 most frequent words in each language to\ntrain our models. Any word not included in the shortlist is mapped to a special token ( [UNK ]). We\ndo not apply any other special preprocessing, such as lowercasing or stemming, to the data.\n4.2 M ODELS\nWe train two types of models. The \ufb01rst one is an RNN Encoder\u2013Decoder (RNNencdec, Cho et al. ,\n2014a), and the other is the proposed model, to which we refer as RNNsearch. We train each model\ntwice: \ufb01rst with the sentences of length up to 30 words (RNNencdec-30, RNNsearch-30) and then\nwith the sentences of length up to 50 word (RNNencdec-50, RNNsearch-50).\nThe encoder and decoder of the RNNencdec have 1000 hidden units each.7The encoder of the\nRNNsearch consists of forward and backward recurrent neural networks (RNN) each having 1000\nhidden units. Its decoder has 1000 hidden units. In both cases, we use a multilayer network with a\nsingle maxout (Goodfellow et al. , 2013) hidden layer to compute the conditional probability of each\ntarget word (Pascanu et al. , 2014).\nWe use a minibatch stochastic gradient descent (SGD) algorithm together with Adadelta (Zeiler,\n2012) to train each model. Each SGD update direction is computed using a minibatch of 80 sen-\ntences. We trained each model for approximately 5 days.\nOnce a model is trained, we use a beam search to \ufb01nd a translation that approximately maximizes the\nconditional probability (see, e.g., Graves, 2012; Boulanger-Lewandowski et al. , 2013). Sutskever\net al. (2014) used this approach to generate translations from their neural machine translation model.\nFor more details on the architectures of the models and training procedure used in the experiments,\nsee Appendices A and B.\n5 R ESULTS\n5.1 Q UANTITATIVE RESULTS\nIn Table 1, we list the translation performances measured in BLEU score. It is clear from the table\nthat in all the cases, the proposed RNNsearch outperforms the conventional RNNencdec. More\nimportantly, the performance of the RNNsearch is as high as that of the conventional phrase-based\ntranslation system (Moses), when only the sentences consisting of known words are considered.\nThis is a signi\ufb01cant achievement, considering that Moses uses a separate monolingual corpus (418M\nwords) in addition to the parallel corpora we used to train the RNNsearch and RNNencdec.\n6We used the tokenization script from the open-source machine translation package, Moses.\n7In this paper, by a \u2019hidden unit\u2019, we always mean the gated hidden unit (see Appendix A.1.1).\n5\nPublished as a conference paper at ICLR 2015\nThe\nagreement\non\nthe\nEuropean\nEconomic\nArea\nwas\nsigned\nin\nAugust\n1992\n.\n<end>\nL'\naccord\nsur\nla\nzone\n\u00e9conomique\neurop\u00e9enne\na\n\u00e9t\u00e9\nsign\u00e9\nen\nao\u00fbt\n1992\n.\n<end>\nIt\nshould\nbe\nnoted\nthat\nthe\nmarine\nenvironment\nis\nthe\nleast\nknown\nof\nenvironments\n.\n<end>\nIl\nconvient\nde\nnoter\nque\nl'\nenvironnement\nmarin\nest\nle\nmoins\nconnu\nde\nl'\nenvironnement\n.\n<end>\n(a) (b)\nDestruction\nof\nthe\nequipment\nmeans\nthat\nSyria\ncan\nno\nlonger\nproduce\nnew\nchemical\nweapons\n.\n<end>\nLa\ndestruction\nde\nl'\n\u00e9quipement\nsignifie\nque\nla\nSyrie\nne\npeut\nplus\nproduire\nde\nnouvelles\narmes\nchimiques\n.\n<end>\n\"\nThis\nwill\nchange\nmy\nfuture\nwith\nmy\nfamily\n,\n\"\nthe\nman\nsaid\n.\n<end>\n\"\nCela\nva\nchanger\nmon\navenir\navec\nma\nfamille\n\"\n,\na\ndit\nl'\nhomme\n.\n<end>\n(c) (d)\nFigure 3: Four sample alignments found by RNNsearch-50. The x-axis and y-axis of each plot\ncorrespond to the words in the source sentence (English) and the generated translation (French),\nrespectively. Each pixel shows the weight \u000bijof the annotation of the j-th source word for the i-th\ntarget word (see Eq. (6)), in grayscale ( 0: black, 1: white). (a) an arbitrary sentence. (b\u2013d) three\nrandomly selected samples among the sentences without any unknown words and of length between\n10 and 20 words from the test set.\nOne of the motivations behind the proposed approach was the use of a \ufb01xed-length context vector\nin the basic encoder\u2013decoder approach. We conjectured that this limitation may make the basic\nencoder\u2013decoder approach to underperform with long sentences. In Fig. 2, we see that the perfor-\nmance of RNNencdec dramatically drops as the length of the sentences increases. On the other hand,\nboth RNNsearch-30 and RNNsearch-50 are more robust to the length of the sentences. RNNsearch-\n50, especially, shows no performance deterioration even with sentences of length 50 or more. This\nsuperiority of the proposed model over the basic encoder\u2013decoder is further con\ufb01rmed by the fact\nthat the RNNsearch-30 even outperforms RNNencdec-50 (see Table 1).\n6\nPublished as a conference paper at ICLR 2015\nModel All No UNK\u000e\nRNNencdec-30 13.93 24.19\nRNNsearch-30 21.50 31.44\nRNNencdec-50 17.82 26.71\nRNNsearch-50 26.75 34.16\nRNNsearch-50?28.45 36.15\nMoses 33.30 35.63Table 1: BLEU scores of the trained models com-\nputed on the test set. The second and third columns\nshow respectively the scores on all the sentences and,\non the sentences without any unknown word in them-\nselves and in the reference translations. Note that\nRNNsearch-50?was trained much longer until the\nperformance on the development set stopped improv-\ning. (\u000e) We disallowed the models to generate [UNK]\ntokens when only the sentences having no unknown\nwords were evaluated (last column).\n5.2 Q UALITATIVE ANALYSIS\n5.2.1 A LIGNMENT\nThe proposed approach provides an intuitive way to inspect the (soft-)alignment between the words\nin a generated translation and those in a source sentence. This is done by visualizing the annotation\nweights\u000bijfrom Eq. (6), as in Fig. 3. Each row of a matrix in each plot indicates the weights\nassociated with the annotations. From this we see which positions in the source sentence were\nconsidered more important when generating the target word.\nWe can see from the alignments in Fig. 3 that the alignment of words between English and French\nis largely monotonic. We see strong weights along the diagonal of each matrix. However, we also\nobserve a number of non-trivial, non-monotonic alignments. Adjectives and nouns are typically\nordered differently between French and English, and we see an example in Fig. 3 (a). From this\n\ufb01gure, we see that the model correctly translates a phrase [European Economic Area] into [zone\n\u00b4economique europ \u00b4een]. The RNNsearch was able to correctly align [zone] with [Area], jumping\nover the two words ([European] and [Economic]), and then looked one word back at a time to\ncomplete the whole phrase [zone \u00b4economique europ \u00b4eenne].\nThe strength of the soft-alignment, opposed to a hard-alignment, is evident, for instance, from\nFig. 3 (d). Consider the source phrase [the man] which was translated into [l\u2019 homme]. Any hard\nalignment will map [the] to [l\u2019] and [man] to [homme]. This is not helpful for translation, as one\nmust consider the word following [the] to determine whether it should be translated into [le], [la],\n[les] or [l\u2019]. Our soft-alignment solves this issue naturally by letting the model look at both [the] and\n[man], and in this example, we see that the model was able to correctly translate [the] into [l\u2019]. We\nobserve similar behaviors in all the presented cases in Fig. 3. An additional bene\ufb01t of the soft align-\nment is that it naturally deals with source and target phrases of different lengths, without requiring a\ncounter-intuitive way of mapping some words to or from nowhere ([NULL]) (see, e.g., Chapters 4\nand 5 of Koehn, 2010).\n5.2.2 L ONG SENTENCES\nAs clearly visible from Fig. 2 the proposed model (RNNsearch) is much better than the conventional\nmodel (RNNencdec) at translating long sentences. This is likely due to the fact that the RNNsearch\ndoes not require encoding a long sentence into a \ufb01xed-length vector perfectly, but only accurately\nencoding the parts of the input sentence that surround a particular word.\nAs an example, consider this source sentence from the test set:\nAn admitting privilege is the right of a doctor to admit a patient to a hospital or\na medical centre tocarry outadiagnosis oraprocedure, based onhisstatus asa\nhealth care worker atahospital.\nThe RNNencdec-50 translated this sentence into:\nUn privil `ege d\u2019admission est le droit d\u2019un m \u00b4edecin de reconna \u02c6\u0131tre un patient `a\nl\u2019h\u02c6opital ou un centre m \u00b4edical d\u2019un diagnostic oudeprendre undiagnostic en\nfonction deson\u00b4etatdesant\u00b4e.\n7\nPublished as a conference paper at ICLR 2015\nThe RNNencdec-50 correctly translated the source sentence until [a medical center]. However, from\nthere on (underlined), it deviated from the original meaning of the source sentence. For instance, it\nreplaced [based on his status as a health care worker at a hospital] in the source sentence with [en\nfonction de son \u00b4etat de sant \u00b4e] (\u201cbased on his state of health\u201d).\nOn the other hand, the RNNsearch-50 generated the following correct translation, preserving the\nwhole meaning of the input sentence without omitting any details:\nUn privil `ege d\u2019admission est le droit d\u2019un m \u00b4edecin d\u2019admettre un patient `a un\nh\u02c6opital ou un centre m \u00b4edical pour effectuer undiagnostic ouuneproc\u00b4edure, selon\nsonstatut detravailleur dessoins desant\u00b4e`al\u2019h\u02c6opital.\nLet us consider another sentence from the test set:\nThis kind of experience is part of Disney\u2019s efforts to \u201dextend the lifetime of its\nseries and build new relationships with audiences viadigital platforms that are\nbecoming ever more important, \u201d headded.\nThe translation by the RNNencdec-50 is\nCe type d\u2019exp \u00b4erience fait partie des initiatives du Disney pour \u201dprolonger la dur \u00b4ee\nde vie de ses nouvelles et de d \u00b4evelopper des liens avec les lecteurs num\u00b4eriques qui\ndeviennent plus complexes.\nAs with the previous example, the RNNencdec began deviating from the actual meaning of the\nsource sentence after generating approximately 30 words (see the underlined phrase). After that\npoint, the quality of the translation deteriorates, with basic mistakes such as the lack of a closing\nquotation mark.\nAgain, the RNNsearch-50 was able to translate this long sentence correctly:\nCe genre d\u2019exp \u00b4erience fait partie des efforts de Disney pour \u201dprolonger la dur \u00b4ee\nde vie de ses s \u00b4eries et cr \u00b4eer de nouvelles relations avec des publics viades\nplateformes num\u00b4eriques deplus enplus importantes\u201d, a-t-il ajout \u00b4e.\nIn conjunction with the quantitative results presented already, these qualitative observations con-\n\ufb01rm our hypotheses that the RNNsearch architecture enables far more reliable translation of long\nsentences than the standard RNNencdec model.\nIn Appendix C, we provide a few more sample translations of long source sentences generated by\nthe RNNencdec-50, RNNsearch-50 and Google Translate along with the reference translations.\n6 R ELATED WORK\n6.1 L EARNING TO ALIGN\nA similar approach of aligning an output symbol with an input symbol was proposed recently by\nGraves (2013) in the context of handwriting synthesis. Handwriting synthesis is a task where the\nmodel is asked to generate handwriting of a given sequence of characters. In his work, he used a\nmixture of Gaussian kernels to compute the weights of the annotations, where the location, width\nand mixture coef\ufb01cient of each kernel was predicted from an alignment model. More speci\ufb01cally,\nhis alignment was restricted to predict the location such that the location increases monotonically.\nThe main difference from our approach is that, in (Graves, 2013), the modes of the weights of the\nannotations only move in one direction. In the context of machine translation, this is a severe limi-\ntation, as (long-distance) reordering is often needed to generate a grammatically correct translation\n(for instance, English-to-German).\nOur approach, on the other hand, requires computing the annotation weight of every word in the\nsource sentence for each word in the translation. This drawback is not severe with the task of\ntranslation in which most of input and output sentences are only 15\u201340 words. However, this may\nlimit the applicability of the proposed scheme to other tasks.\n8\nPublished as a conference paper at ICLR 2015\n6.2 N EURAL NETWORKS FOR MACHINE TRANSLATION\nSince Bengio et al. (2003) introduced a neural probabilistic language model which uses a neural net-\nwork to model the conditional probability of a word given a \ufb01xed number of the preceding words,\nneural networks have widely been used in machine translation. However, the role of neural net-\nworks has been largely limited to simply providing a single feature to an existing statistical machine\ntranslation system or to re-rank a list of candidate translations provided by an existing system.\nFor instance, Schwenk (2012) proposed using a feedforward neural network to compute the score of\na pair of source and target phrases and to use the score as an additional feature in the phrase-based\nstatistical machine translation system. More recently, Kalchbrenner and Blunsom (2013) and Devlin\net al. (2014) reported the successful use of the neural networks as a sub-component of the existing\ntranslation system. Traditionally, a neural network trained as a target-side language model has been\nused to rescore or rerank a list of candidate translations (see, e.g., Schwenk et al. , 2006).\nAlthough the above approaches were shown to improve the translation performance over the state-\nof-the-art machine translation systems, we are more interested in a more ambitious objective of\ndesigning a completely new translation system based on neural networks. The neural machine trans-\nlation approach we consider in this paper is therefore a radical departure from these earlier works.\nRather than using a neural network as a part of the existing system, our model works on its own and\ngenerates a translation from a source sentence directly.\n7 C ONCLUSION\nThe conventional approach to neural machine translation, called an encoder\u2013decoder approach, en-\ncodes a whole input sentence into a \ufb01xed-length vector from which a translation will be decoded.\nWe conjectured that the use of a \ufb01xed-length context vector is problematic for translating long sen-\ntences, based on a recent empirical study reported by Cho et al. (2014b) and Pouget-Abadie et al.\n(2014).\nIn this paper, we proposed a novel architecture that addresses this issue. We extended the basic\nencoder\u2013decoder by letting a model (soft-)search for a set of input words, or their annotations com-\nputed by an encoder, when generating each target word. This frees the model from having to encode\na whole source sentence into a \ufb01xed-length vector, and also lets the model focus only on information\nrelevant to the generation of the next target word. This has a major positive impact on the ability\nof the neural machine translation system to yield good results on longer sentences. Unlike with\nthe traditional machine translation systems, all of the pieces of the translation system, including\nthe alignment mechanism, are jointly trained towards a better log-probability of producing correct\ntranslations.\nWe tested the proposed model, called RNNsearch, on the task of English-to-French translation. The\nexperiment revealed that the proposed RNNsearch outperforms the conventional encoder\u2013decoder\nmodel (RNNencdec) signi\ufb01cantly, regardless of the sentence length and that it is much more ro-\nbust to the length of a source sentence. From the qualitative analysis where we investigated the\n(soft-)alignment generated by the RNNsearch, we were able to conclude that the model can cor-\nrectly align each target word with the relevant words, or their annotations, in the source sentence as\nit generated a correct translation.\nPerhaps more importantly, the proposed approach achieved a translation performance comparable to\nthe existing phrase-based statistical machine translation. It is a striking result, considering that the\nproposed architecture, or the whole family of neural machine translation, has only been proposed\nas recently as this year. We believe the architecture proposed here is a promising step toward better\nmachine translation and a better understanding of natural languages in general.\nOne of challenges left for the future is to better handle unknown, or rare words. This will be required\nfor the model to be more widely used and to match the performance of current state-of-the-art\nmachine translation systems in all contexts.\n9\nPublished as a conference paper at ICLR 2015\nACKNOWLEDGMENTS\nThe authors would like to thank the developers of Theano (Bergstra et al. , 2010; Bastien et al. ,\n2012). We acknowledge the support of the following agencies for research funding and computing\nsupport: NSERC, Calcul Qu \u00b4ebec, Compute Canada, the Canada Research Chairs and CIFAR. Bah-\ndanau thanks the support from Planet Intelligent Systems GmbH. We also thank Felix Hill, Bart van\nMerri \u00b4enboer, Jean Pouget-Abadie, Coline Devin and Tae-Ho Kim.\nREFERENCES\nAxelrod, A., He, X., and Gao, J. (2011). Domain adaptation via pseudo in-domain data selection.\nInProceedings of the ACL Conference on Empirical Methods in Natural Language Processing\n(EMNLP) , pages 355\u2013362. Association for Computational Linguistics.\nBastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N.,\nand Bengio, Y . (2012). Theano: new features and speed improvements. Deep Learning and\nUnsupervised Feature Learning NIPS 2012 Workshop.\nBengio, Y ., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient\ndescent is dif\ufb01cult. IEEE Transactions on Neural Networks ,5(2), 157\u2013166.\nBengio, Y ., Ducharme, R., Vincent, P., and Janvin, C. (2003). A neural probabilistic language model.\nJ. Mach. Learn. Res. ,3, 1137\u20131155.\nBergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-\nFarley, D., and Bengio, Y . (2010). Theano: a CPU and GPU math expression compiler. In\nProceedings of the Python for Scienti\ufb01c Computing Conference (SciPy) . Oral Presentation.\nBoulanger-Lewandowski, N., Bengio, Y ., and Vincent, P. (2013). Audio chord recognition with\nrecurrent neural networks. In ISMIR .\nCho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., and Bengio, Y . (2014a).\nLearning phrase representations using RNN encoder-decoder for statistical machine translation.\nInProceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014) . to\nappear.\nCho, K., van Merri \u00a8enboer, B., Bahdanau, D., and Bengio, Y . (2014b). On the properties of neural\nmachine translation: Encoder\u2013Decoder approaches. In Eighth Workshop on Syntax, Semantics\nand Structure in Statistical Translation . to appear.\nDevlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., and Makhoul, J. (2014). Fast and robust\nneural network joint models for statistical machine translation. In Association for Computational\nLinguistics .\nForcada, M. L. and \u02dcNeco, R. P. (1997). Recursive hetero-associative memories for translation. In\nJ. Mira, R. Moreno-D \u00b4\u0131az, and J. Cabestany, editors, Biological and Arti\ufb01cial Computation: From\nNeuroscience to Technology , volume 1240 of Lecture Notes in Computer Science , pages 453\u2013462.\nSpringer Berlin Heidelberg.\nGoodfellow, I., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y . (2013). Maxout net-\nworks. In Proceedings of The 30th International Conference on Machine Learning , pages 1319\u2013\n1327.\nGraves, A. (2012). Sequence transduction with recurrent neural networks. In Proceedings of the\n29th International Conference on Machine Learning (ICML 2012) .\nGraves, A. (2013). Generating sequences with recurrent neural networks. arXiv: 1308.0850\n[cs.NE] .\nGraves, A., Jaitly, N., and Mohamed, A.-R. (2013). Hybrid speech recognition with deep bidirec-\ntional LSTM. In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Work-\nshop on , pages 273\u2013278.\n10\nPublished as a conference paper at ICLR 2015\nHermann, K. and Blunsom, P. (2014). Multilingual distributed representations without word align-\nment. In Proceedings of the Second International Conference on Learning Representations (ICLR\n2014) .\nHochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut\nf\u00a8ur Informatik, Lehrstuhl Prof. Brauer, Technische Universit \u00a8at M \u00a8unchen.\nHochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation ,9(8),\n1735\u20131780.\nKalchbrenner, N. and Blunsom, P. (2013). Recurrent continuous translation models. In Proceedings\nof the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages\n1700\u20131709. Association for Computational Linguistics.\nKoehn, P. (2010). Statistical Machine Translation . Cambridge University Press, New York, NY ,\nUSA.\nKoehn, P., Och, F. J., and Marcu, D. (2003). Statistical phrase-based translation. In Proceedings\nof the 2003 Conference of the North American Chapter of the Association for Computational\nLinguistics on Human Language Technology - Volume 1 , NAACL \u201903, pages 48\u201354, Stroudsburg,\nPA, USA. Association for Computational Linguistics.\nPascanu, R., Mikolov, T., and Bengio, Y . (2013a). On the dif\ufb01culty of training recurrent neural\nnetworks. In ICML\u20192013 .\nPascanu, R., Mikolov, T., and Bengio, Y . (2013b). On the dif\ufb01culty of training recurrent neural\nnetworks. In Proceedings of the 30th International Conference on Machine Learning (ICML\n2013) .\nPascanu, R., Gulcehre, C., Cho, K., and Bengio, Y . (2014). How to construct deep recurrent neural\nnetworks. In Proceedings of the Second International Conference on Learning Representations\n(ICLR 2014) .\nPouget-Abadie, J., Bahdanau, D., van Merri \u00a8enboer, B., Cho, K., and Bengio, Y . (2014). Overcoming\nthe curse of sentence length for neural machine translation using automatic segmentation. In\nEighth Workshop on Syntax, Semantics and Structure in Statistical Translation . to appear.\nSchuster, M. and Paliwal, K. K. (1997). Bidirectional recurrent neural networks. Signal Processing,\nIEEE Transactions on ,45(11), 2673\u20132681.\nSchwenk, H. (2012). Continuous space translation models for phrase-based statistical machine\ntranslation. In M. Kay and C. Boitet, editors, Proceedings of the 24th International Conference on\nComputational Linguistics (COLIN) , pages 1071\u20131080. Indian Institute of Technology Bombay.\nSchwenk, H., Dchelotte, D., and Gauvain, J.-L. (2006). Continuous space language models for\nstatistical machine translation. In Proceedings of the COLING/ACL on Main conference poster\nsessions , pages 723\u2013730. Association for Computational Linguistics.\nSutskever, I., Vinyals, O., and Le, Q. (2014). Sequence to sequence learning with neural networks.\nInAdvances in Neural Information Processing Systems (NIPS 2014) .\nZeiler, M. D. (2012). ADADELTA: An adaptive learning rate method. arXiv: 1212.5701\n[cs.LG] .\n11\nPublished as a conference paper at ICLR 2015\nA M ODEL ARCHITECTURE\nA.1 A RCHITECTURAL CHOICES\nThe proposed scheme in Section 3 is a general framework where one can freely de\ufb01ne, for instance,\nthe activation functions fof recurrent neural networks (RNN) and the alignment model a. Here, we\ndescribe the choices we made for the experiments in this paper.\nA.1.1 R ECURRENT NEURAL NETWORK\nFor the activation function fof an RNN, we use the gated hidden unit recently proposed by Cho\net al. (2014a). The gated hidden unit is an alternative to the conventional simple units such as an\nelement-wise tanh . This gated unit is similar to a long short-term memory (LSTM) unit proposed\nearlier by Hochreiter and Schmidhuber (1997), sharing with it the ability to better model and learn\nlong-term dependencies. This is made possible by having computation paths in the unfolded RNN\nfor which the product of derivatives is close to 1. These paths allow gradients to \ufb02ow backward\neasily without suffering too much from the vanishing effect (Hochreiter, 1991; Bengio et al. , 1994;\nPascanu et al. , 2013a). It is therefore possible to use LSTM units instead of the gated hidden unit\ndescribed here, as was done in a similar context by Sutskever et al. (2014).\nThe new state siof the RNN employing ngated hidden units8is computed by\nsi=f(si\u00001;yi\u00001;ci) = (1\u0000zi)\u000esi\u00001+zi\u000e~si;\nwhere\u000eis an element-wise multiplication, and ziis the output of the update gates (see below). The\nproposed updated state ~siis computed by\n~si= tanh (We(yi\u00001) +U[ri\u000esi\u00001] +Cci);\nwheree(yi\u00001)2Rmis anm-dimensional embedding of a word yi\u00001, andriis the output of the\nreset gates (see below). When yiis represented as a 1-of-Kvector,e(yi)is simply a column of an\nembedding matrix E2Rm\u0002K. Whenever possible, we omit bias terms to make the equations less\ncluttered.\nThe update gates ziallow each hidden unit to maintain its previous activation, and the reset gates ri\ncontrol how much and what information from the previous state should be reset. We compute them\nby\nzi=\u001b(Wze(yi\u00001) +Uzsi\u00001+Czci);\nri=\u001b(Wre(yi\u00001) +Ursi\u00001+Crci);\nwhere\u001b(\u0001)is a logistic sigmoid function.\nAt each step of the decoder, we compute the output probability (Eq. (4)) as a multi-layered func-\ntion (Pascanu et al. , 2014). We use a single hidden layer of maxout units (Goodfellow et al. , 2013)\nand normalize the output probabilities (one for each word) with a softmax function (see Eq. (6)).\nA.1.2 A LIGNMENT MODEL\nThe alignment model should be designed considering that the model needs to be evaluated Tx\u0002Ty\ntimes for each sentence pair of lengths TxandTy. In order to reduce computation, we use a single-\nlayer multilayer perceptron such that\na(si\u00001;hj) =v>\natanh (Wasi\u00001+Uahj);\nwhereWa2Rn\u0002n;Ua2Rn\u00022nandva2Rnare the weight matrices. Since Uahjdoes not\ndepend oni, we can pre-compute it in advance to minimize the computational cost.\n8Here, we show the formula of the decoder. The same formula can be used in the encoder by simply\nignoring the context vector ciand the related terms.\n12\nPublished as a conference paper at ICLR 2015\nA.2 D ETAILED DESCRIPTION OF THE MODEL\nA.2.1 E NCODER\nIn this section, we describe in detail the architecture of the proposed model (RNNsearch) used in the\nexperiments (see Sec. 4\u20135). From here on, we omit all bias terms in order to increase readability.\nThe model takes a source sentence of 1-of-K coded word vectors as input\nx= (x1;:::;x Tx); xi2RKx\nand outputs a translated sentence of 1-of-K coded word vectors\ny= (y1;:::;y Ty); yi2RKy;\nwhereKxandKyare the vocabulary sizes of source and target languages, respectively. TxandTy\nrespectively denote the lengths of source and target sentences.\nFirst, the forward states of the bidirectional recurrent neural network (BiRNN) are computed:\n\u0000 !hi=(\n(1\u0000\u0000 !zi)\u000e\u0000 !hi\u00001+\u0000 !zi\u000e\u0000 !hi, ifi>0\n0 , ifi= 0\nwhere\n\u0000 !hi= tanh\u0010\u0000 !WExi+\u0000 !Uh\u0000 !ri\u000e\u0000 !hi\u00001i\u0011\n\u0000 !zi=\u001b\u0010\u0000 !WzExi+\u0000 !Uz\u0000 !hi\u00001\u0011\n\u0000 !ri=\u001b\u0010\u0000 !WrExi+\u0000 !Ur\u0000 !hi\u00001\u0011\n:\nE2Rm\u0002Kxis the word embedding matrix.\u0000 !W;\u0000 !Wz;\u0000 !Wr2Rn\u0002m,\u0000 !U;\u0000 !Uz;\u0000 !Ur2Rn\u0002nare\nweight matrices. mandnare the word embedding dimensionality and the number of hidden units,\nrespectively. \u001b(\u0001)is as usual a logistic sigmoid function.\nThe backward states ( \u0000h1;\u0001\u0001\u0001; \u0000hTx)are computed similarly. We share the word embedding matrix\nEbetween the forward and backward RNNs, unlike the weight matrices.\nWe concatenate the forward and backward states to to obtain the annotations (h1;h2;\u0001\u0001\u0001;hTx),\nwhere\nhi=\"\u0000 !hi \u0000hi#\n(7)\nA.2.2 D ECODER\nThe hidden state siof the decoder given the annotations from the encoder is computed by\nsi=(1\u0000zi)\u000esi\u00001+zi\u000e~si;\nwhere\n~si= tanh (WEy i\u00001+U[ri\u000esi\u00001] +Cci)\nzi=\u001b(WzEyi\u00001+Uzsi\u00001+Czci)\nri=\u001b(WrEyi\u00001+Ursi\u00001+Crci)\nEis the word embedding matrix for the target language. W;W z;Wr2Rn\u0002m,U;Uz;Ur2Rn\u0002n,\nandC;C z;Cr2Rn\u00022nare weights. Again, mandnare the word embedding dimensionality\nand the number of hidden units, respectively. The initial hidden state s0is computed by s0=\ntanh\u0010\nWs \u0000h1\u0011\n;whereWs2Rn\u0002n.\nThe context vector ciare recomputed at each step by the alignment model:\nci=TxX\nj=1\u000bijhj;\n13\nPublished as a conference paper at ICLR 2015\nModel Updates (\u0002105)Epochs Hours GPU Train NLL Dev. NLL\nRNNenc-30 8.46 6.4 109 TITAN BLACK 28.1 53.0\nRNNenc-50 6.00 4.5 108 Quadro K-6000 44.0 43.6\nRNNsearch-30 4.71 3.6 113 TITAN BLACK 26.7 47.2\nRNNsearch-50 2.88 2.2 111 Quadro K-6000 40.7 38.1\nRNNsearch-50?6.67 5.0 252 Quadro K-6000 36.7 35.2\nTable 2: Learning statistics and relevant information. Each update corresponds to updating the\nparameters once using a single minibatch. One epoch is one pass through the training set. NLL is\nthe average conditional log-probabilities of the sentences in either the training set or the development\nset. Note that the lengths of the sentences differ.\nwhere\n\u000bij=exp (eij)PTx\nk=1exp (eik)\neij=v>\natanh (Wasi\u00001+Uahj);\nandhjis thej-th annotation in the source sentence (see Eq. (7)). va2Rn0;Wa2Rn0\u0002nand\nUa2Rn0\u00022nare weight matrices. Note that the model becomes RNN Encoder\u2013Decoder (Cho\net al. , 2014a), if we \ufb01x cito\u0000 !hTx.\nWith the decoder state si\u00001, the context ciand the last generated word yi\u00001, we de\ufb01ne the probability\nof a target word yias\np(yijsi;yi\u00001;ci)/exp\u0000\ny>\niWoti\u0001\n;\nwhere\nti=\u0002\nmax\b~ti;2j\u00001;~ti;2j\t\u0003>\nj=1;:::;l\nand~ti;kis thek-th element of a vector ~tiwhich is computed by\n~ti=Uosi\u00001+VoEyi\u00001+Coci:\nWo2RKy\u0002l,Uo2R2l\u0002n,Vo2R2l\u0002mandCo2R2l\u00022nare weight matrices. This can be under-\nstood as having a deep output (Pascanu et al. , 2014) with a single maxout hidden layer (Goodfellow\net al. , 2013).\nA.2.3 M ODEL SIZE\nFor all the models used in this paper, the size of a hidden layer nis 1000, the word embedding\ndimensionality mis 620 and the size of the maxout hidden layer in the deep output lis 500. The\nnumber of hidden units in the alignment model n0is 1000.\nB T RAINING PROCEDURE\nB.1 P ARAMETER INITIALIZATION\nWe initialized the recurrent weight matrices U;Uz;Ur; \u0000U; \u0000Uz; \u0000Ur;\u0000 !U;\u0000 !Uzand\u0000 !Uras random or-\nthogonal matrices. For WaandUa, we initialized them by sampling each element from the Gaussian\ndistribution of mean 0and variance 0:0012. All the elements of Vaand all the bias vectors were ini-\ntialized to zero. Any other weight matrix was initialized by sampling from the Gaussian distribution\nof mean 0and variance 0:012.\nB.2 T RAINING\nWe used the stochastic gradient descent (SGD) algorithm. Adadelta (Zeiler, 2012) was used to\nautomatically adapt the learning rate of each parameter ( \u000f= 10\u00006and\u001a= 0:95). We explicitly\n14\nPublished as a conference paper at ICLR 2015\nnormalized the L2-norm of the gradient of the cost function each time to be at most a prede\ufb01ned\nthreshold of 1, when the norm was larger than the threshold (Pascanu et al. , 2013b). Each SGD\nupdate direction was computed with a minibatch of 80 sentences.\nAt each update our implementation requires time proportional to the length of the longest sentence in\na minibatch. Hence, to minimize the waste of computation, before every 20-th update, we retrieved\n1600 sentence pairs, sorted them according to the lengths and split them into 20 minibatches. The\ntraining data was shuf\ufb02ed once before training and was traversed sequentially in this manner.\nIn Tables 2 we present the statistics related to training all the models used in the experiments.\nC T RANSLATIONS OF LONG SENTENCES\nSource An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre\nto carry out a diagnosis or a procedure, based on his status as a health care worker at a hospital.\nReference Le privil `ege d\u2019admission est le droit d\u2019un m \u00b4edecin, en vertu de son statut de membre soignant\nd\u2019un h \u02c6opital, d\u2019admettre un patient dans un h \u02c6opital ou un centre m \u00b4edical a\ufb01n d\u2019y d \u00b4elivrer un\ndiagnostic ou un traitement.\nRNNenc-50 Un privil `ege d\u2019admission est le droit d\u2019un m \u00b4edecin de reconna \u02c6\u0131tre un patient `a l\u2019h \u02c6opital ou un\ncentre m \u00b4edical d\u2019un diagnostic ou de prendre un diagnostic en fonction de son \u00b4etat de sant \u00b4e.\nRNNsearch-50 Un privil `ege d\u2019admission est le droit d\u2019un m \u00b4edecin d\u2019admettre un patient `a un h \u02c6opital ou un\ncentre m \u00b4edical pour effectuer un diagnostic ou une proc \u00b4edure, selon son statut de travailleur des\nsoins de sant \u00b4e`a l\u2019h \u02c6opital.\nGoogle\nTranslateUn privil `ege admettre est le droit d\u2019un m \u00b4edecin d\u2019admettre un patient dans un h \u02c6opital ou un\ncentre m \u00b4edical pour effectuer un diagnostic ou une proc \u00b4edure, fond \u00b4ee sur sa situation en tant\nque travailleur de soins de sant \u00b4e dans un h \u02c6opital.\nSource This kind of experience is part of Disney\u2019s efforts to \u201dextend the lifetime of its series and build\nnew relationships with audiences via digital platforms that are becoming ever more important,\u201d\nhe added.\nReference Ce type d\u2019exp \u00b4erience entre dans le cadre des efforts de Disney pour \u201d \u00b4etendre la dur \u00b4ee de\nvie de ses s \u00b4eries et construire de nouvelles relations avec son public gr \u02c6ace`a des plateformes\nnum\u00b4eriques qui sont de plus en plus importantes\u201d, a-t-il ajout \u00b4e.\nRNNenc-50 Ce type d\u2019exp \u00b4erience fait partie des initiatives du Disney pour \u201dprolonger la dur \u00b4ee de vie de\nses nouvelles et de d \u00b4evelopper des liens avec les lecteurs num \u00b4eriques qui deviennent plus com-\nplexes.\nRNNsearch-50 Ce genre d\u2019exp \u00b4erience fait partie des efforts de Disney pour \u201dprolonger la dur \u00b4ee de vie de ses\ns\u00b4eries et cr \u00b4eer de nouvelles relations avec des publics via des plateformes num \u00b4eriques de plus\nen plus importantes\u201d, a-t-il ajout \u00b4e.\nGoogle\nTranslateCe genre d\u2019exp \u00b4erience fait partie des efforts de Disney `a \u201c\u00b4etendre la dur \u00b4ee de vie de sa s \u00b4erie et\nconstruire de nouvelles relations avec le public par le biais des plates-formes num \u00b4eriques qui\ndeviennent de plus en plus important\u201d, at-il ajout \u00b4e.\nSource In a press conference on Thursday, Mr Blair stated that there was nothing in this video that might\nconstitute a \u201dreasonable motive\u201d that could lead to criminal charges being brought against the\nmayor.\nReference En conf \u00b4erence de presse, jeudi, M. Blair a af\ufb01rm \u00b4e qu\u2019il n\u2019y avait rien dans cette vid \u00b4eo qui puisse\nconstituer des \u201dmotifs raisonnables\u201d pouvant mener au d \u00b4ep\u02c6ot d\u2019une accusation criminelle contre\nle maire.\nRNNenc-50 Lors de la conf \u00b4erence de presse de jeudi, M. Blair a dit qu\u2019il n\u2019y avait rien dans cette vid \u00b4eo qui\npourrait constituer une \u201dmotivation raisonnable\u201d pouvant entra \u02c6\u0131ner des accusations criminelles\nport\u00b4ees contre le maire.\nRNNsearch-50 Lors d\u2019une conf \u00b4erence de presse jeudi, M. Blair a d \u00b4eclar \u00b4e qu\u2019il n\u2019y avait rien dans cette vid \u00b4eo qui\npourrait constituer un \u201dmotif raisonnable\u201d qui pourrait conduire `a des accusations criminelles\ncontre le maire.\nGoogle\nTranslateLors d\u2019une conf \u00b4erence de presse jeudi, M. Blair a d \u00b4eclar \u00b4e qu\u2019il n\u2019y avait rien dans cette vido\nqui pourrait constituer un \u201dmotif raisonnable\u201d qui pourrait mener `a des accusations criminelles\nportes contre le maire.\nTable 3: The translations generated by RNNenc-50 and RNNsearch-50 from long source sentences\n(30 words or more) selected from the test set. For each source sentence, we also show the gold-\nstandard translation. The translations by Google Translate were made on 27 August 2014.\n15\n",
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "arxiv_id": "1409.0473",
    "category": "famous",
    "authors": [
      "Dzmitry Bahdanau",
      "Kyunghyun Cho",
      "Yoshua Bengio"
    ],
    "published": "2014-09-01",
    "num_pages": 15,
    "num_chars": 49781
  },
  {
    "text": "arXiv:1409.1556v6  [cs.CV]  10 Apr 2015Publishedasa conferencepaperat ICLR2015\nVERYDEEPCONVOLUTIONAL NETWORKS\nFORLARGE-SCALEIMAGERECOGNITION\nKarenSimonyan\u2217& AndrewZisserman+\nVisualGeometryGroup,DepartmentofEngineeringScience, UniversityofOxford\n{karen,az }@robots.ox.ac.uk\nABSTRACT\nIn this work we investigate the effect of the convolutional n etwork depth on its\naccuracy in the large-scale image recognition setting. Our main contribution is\na thorough evaluation of networks of increasing depth using an architecture with\nverysmall( 3\u00d73)convolution\ufb01lters,whichshowsthatasigni\ufb01cantimprove ment\non the prior-art con\ufb01gurations can be achieved by pushing th e depth to 16\u201319\nweight layers. These \ufb01ndings were the basis of our ImageNet C hallenge 2014\nsubmission,whereourteamsecuredthe\ufb01rstandthesecondpl acesinthelocalisa-\ntion and classi\ufb01cation tracks respectively. We also show th at our representations\ngeneralise well to other datasets, where they achieve state -of-the-art results. We\nhave made our two best-performingConvNet models publicly a vailable to facili-\ntate furtherresearchontheuse ofdeepvisualrepresentati onsincomputervision.\n1 INTRODUCTION\nConvolutional networks (ConvNets) have recently enjoyed a great success in large-scale im-\nage and video recognition (Krizhevskyetal., 2012; Zeiler& Fergus, 2013; Sermanetet al., 2014;\nSimonyan& Zisserman, 2014) which has become possible due to the large public image reposito-\nries,suchasImageNet(Denget al.,2009),andhigh-perform ancecomputingsystems,suchasGPUs\norlarge-scaledistributedclusters(Deanet al., 2012). In particular,animportantroleintheadvance\nofdeepvisualrecognitionarchitectureshasbeenplayedby theImageNetLarge-ScaleVisualRecog-\nnition Challenge (ILSVRC) (Russakovskyet al., 2014), whic h has served as a testbed for a few\ngenerationsof large-scale image classi\ufb01cation systems, f rom high-dimensionalshallow feature en-\ncodings(Perronninetal.,2010)(thewinnerofILSVRC-2011 )todeepConvNets(Krizhevskyet al.,\n2012)(thewinnerofILSVRC-2012).\nWith ConvNets becoming more of a commodity in the computer vi sion \ufb01eld, a number of at-\ntempts have been made to improve the original architecture o f Krizhevskyet al. (2012) in a\nbid to achieve better accuracy. For instance, the best-perf orming submissions to the ILSVRC-\n2013 (Zeiler&Fergus, 2013; Sermanetetal., 2014) utilised smaller receptive window size and\nsmaller stride of the \ufb01rst convolutional layer. Another lin e of improvements dealt with training\nand testing the networks densely over the whole image and ove r multiple scales (Sermanetet al.,\n2014; Howard, 2014). In this paper, we address another impor tant aspect of ConvNet architecture\ndesign\u2013itsdepth. Tothisend,we \ufb01xotherparametersofthea rchitecture,andsteadilyincreasethe\ndepth of the network by adding more convolutionallayers, wh ich is feasible due to the use of very\nsmall (3\u00d73)convolution\ufb01ltersinall layers.\nAs a result, we come up with signi\ufb01cantly more accurate ConvN et architectures, which not only\nachieve the state-of-the-art accuracy on ILSVRC classi\ufb01ca tion and localisation tasks, but are also\napplicabletootherimagerecognitiondatasets,wherethey achieveexcellentperformanceevenwhen\nusedasa partofa relativelysimple pipelines(e.g.deepfea turesclassi\ufb01ed byalinearSVM without\n\ufb01ne-tuning). We havereleasedourtwobest-performingmode ls1tofacilitatefurtherresearch.\nThe rest of the paper is organised as follows. In Sect. 2, we de scribe our ConvNet con\ufb01gurations.\nThe details of the image classi\ufb01cation trainingand evaluat ionare then presented in Sect. 3, and the\n\u2217current af\ufb01liation: Google DeepMind+current af\ufb01liation: Universityof Oxfordand Google DeepMi nd\n1http://www.robots.ox.ac.uk/ \u02dcvgg/research/very_deep/\n1\nPublishedasa conferencepaperat ICLR2015\ncon\ufb01gurations are compared on the ILSVRC classi\ufb01cation tas k in Sect. 4. Sect. 5 concludes the\npaper. For completeness,we also describeand assess our ILS VRC-2014object localisationsystem\ninAppendixA,anddiscussthegeneralisationofverydeepfe aturestootherdatasetsinAppendixB.\nFinally,AppendixCcontainsthelist ofmajorpaperrevisio ns.\n2 CONVNETCONFIGURATIONS\nTo measure the improvement brought by the increased ConvNet depth in a fair setting, all our\nConvNet layer con\ufb01gurations are designed using the same pri nciples, inspired by Ciresan etal.\n(2011); Krizhevskyet al. (2012). In this section, we \ufb01rst de scribe a generic layout of our ConvNet\ncon\ufb01gurations(Sect.2.1)andthendetailthespeci\ufb01ccon\ufb01g urationsusedintheevaluation(Sect.2.2).\nOurdesignchoicesarethendiscussedandcomparedtothepri orart inSect. 2.3.\n2.1 A RCHITECTURE\nDuring training, the input to our ConvNets is a \ufb01xed-size 224\u00d7224RGB image. The only pre-\nprocessingwedoissubtractingthemeanRGBvalue,computed onthetrainingset,fromeachpixel.\nTheimageispassedthroughastackofconvolutional(conv.) layers,whereweuse\ufb01lterswithavery\nsmall receptive \ufb01eld: 3\u00d73(which is the smallest size to capture the notion of left/rig ht, up/down,\ncenter). In one of the con\ufb01gurationswe also utilise 1\u00d71convolution\ufb01lters, which can be seen as\na linear transformationof the input channels (followed by n on-linearity). The convolutionstride is\n\ufb01xedto1pixel;thespatialpaddingofconv.layerinputissuchthatt hespatialresolutionispreserved\nafterconvolution,i.e. the paddingis 1pixel for3\u00d73conv.layers. Spatial poolingis carriedoutby\n\ufb01vemax-poolinglayers,whichfollowsomeoftheconv.layer s(notalltheconv.layersarefollowed\nbymax-pooling). Max-poolingisperformedovera 2\u00d72pixelwindow,withstride 2.\nAstackofconvolutionallayers(whichhasadifferentdepth indifferentarchitectures)isfollowedby\nthree Fully-Connected(FC) layers: the \ufb01rst two have4096ch annelseach,the thirdperforms1000-\nway ILSVRC classi\ufb01cation and thus contains1000channels(o ne foreach class). The \ufb01nal layer is\nthesoft-maxlayer. Thecon\ufb01gurationofthefullyconnected layersis thesameinall networks.\nAllhiddenlayersareequippedwiththerecti\ufb01cation(ReLU( Krizhevskyetal.,2012))non-linearity.\nWe note that none of our networks (except for one) contain Loc al Response Normalisation\n(LRN) normalisation (Krizhevskyet al., 2012): as will be sh own in Sect. 4, such normalisation\ndoes not improve the performance on the ILSVRC dataset, but l eads to increased memory con-\nsumption and computation time. Where applicable, the param eters for the LRN layer are those\nof(Krizhevskyetal., 2012).\n2.2 C ONFIGURATIONS\nThe ConvNet con\ufb01gurations, evaluated in this paper, are out lined in Table 1, one per column. In\nthe following we will refer to the nets by their names (A\u2013E). A ll con\ufb01gurationsfollow the generic\ndesign presented in Sect. 2.1, and differ only in the depth: f rom 11 weight layers in the network A\n(8conv.and3FClayers)to19weightlayersinthenetworkE(1 6conv.and3FClayers). Thewidth\nof conv.layers (the number of channels) is rather small, sta rting from 64in the \ufb01rst layer and then\nincreasingbyafactorof 2aftereachmax-poolinglayer,untilit reaches 512.\nIn Table 2 we reportthe numberof parametersfor each con\ufb01gur ation. In spite of a large depth, the\nnumberof weights in our netsis not greater thanthe numberof weightsin a moreshallow net with\nlargerconv.layerwidthsandreceptive\ufb01elds(144Mweights in(Sermanetet al., 2014)).\n2.3 D ISCUSSION\nOur ConvNet con\ufb01gurations are quite different from the ones used in the top-performing entries\nof the ILSVRC-2012 (Krizhevskyetal., 2012) and ILSVRC-201 3 competitions (Zeiler& Fergus,\n2013;Sermanetet al.,2014). Ratherthanusingrelativelyl argereceptive\ufb01eldsinthe\ufb01rstconv.lay-\ners(e.g.11\u00d711withstride 4in(Krizhevskyet al.,2012),or 7\u00d77withstride 2in(Zeiler& Fergus,\n2013; Sermanetet al., 2014)), we use very small 3\u00d73receptive \ufb01elds throughout the whole net,\nwhichareconvolvedwiththeinputateverypixel(withstrid e1). Itiseasytoseethatastackoftwo\n3\u00d73conv.layers(withoutspatialpoolinginbetween)hasaneff ectivereceptive\ufb01eldof 5\u00d75;three\n2\nPublishedasa conferencepaperat ICLR2015\nTable 1:ConvNet con\ufb01gurations (shown in columns). The depth of the con\ufb01gurations increase s\nfromtheleft(A)totheright(E),asmorelayersareadded(th eaddedlayersareshowninbold). The\nconvolutional layer parameters are denoted as \u201cconv /an}bracketle{treceptive \ufb01eld size /an}bracketri}ht-/an}bracketle{tnumber of channels /an}bracketri}ht\u201d.\nTheReLU activationfunctionisnotshownforbrevity.\nConvNet Con\ufb01guration\nA A-LRN B C D E\n11weight 11weight 13 weight 16weight 16weight 19 weight\nlayers layers layers layers layers layers\ninput (224\u00d7224RGBimage)\nconv3-64 conv3-64 conv3-64 conv3-64 conv3-64 conv3-64\nLRN conv3-64 conv3-64 conv3-64 conv3-64\nmaxpool\nconv3-128 conv3-128 conv3-128 conv3-128 conv3-128 conv3-128\nconv3-128 conv3-128 conv3-128 conv3-128\nmaxpool\nconv3-256 conv3-256 conv3-256 conv3-256 conv3-256 conv3-256\nconv3-256 conv3-256 conv3-256 conv3-256 conv3-256 conv3-256\nconv1-256 conv3-256 conv3-256\nconv3-256\nmaxpool\nconv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512\nconv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512\nconv1-512 conv3-512 conv3-512\nconv3-512\nmaxpool\nconv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512\nconv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512\nconv1-512 conv3-512 conv3-512\nconv3-512\nmaxpool\nFC-4096\nFC-4096\nFC-1000\nsoft-max\nTable2:Number ofparameters (inmillions).\nNetwork A,A-LRN BCDE\nNumber of parameters 133 133134138144\nsuch layers have a 7\u00d77effectivereceptive \ufb01eld. So what have we gainedby using, fo r instance, a\nstackofthree 3\u00d73conv.layersinsteadofasingle 7\u00d77layer? First,weincorporatethreenon-linear\nrecti\ufb01cation layers instead of a single one, which makes the decision functionmore discriminative.\nSecond, we decrease the number of parameters: assuming that both the input and the output of a\nthree-layer 3\u00d73convolutionstack has Cchannels,the stack is parametrisedby 3/parenleftbig\n32C2/parenrightbig\n= 27C2\nweights; at the same time, a single 7\u00d77conv. layer would require 72C2= 49C2parameters, i.e.\n81%more. Thiscan be seen as imposinga regularisationon the 7\u00d77conv.\ufb01lters, forcingthemto\nhaveadecompositionthroughthe 3\u00d73\ufb01lters(withnon-linearityinjectedin between).\nThe incorporation of 1\u00d71conv. layers (con\ufb01guration C, Table 1) is a way to increase th e non-\nlinearity of the decision function without affecting the re ceptive \ufb01elds of the conv. layers. Even\nthoughinourcasethe 1\u00d71convolutionisessentiallyalinearprojectionontothespa ceofthesame\ndimensionality(thenumberofinputandoutputchannelsist hesame),anadditionalnon-linearityis\nintroducedbytherecti\ufb01cationfunction. Itshouldbenoted that1\u00d71conv.layershaverecentlybeen\nutilisedin the\u201cNetworkinNetwork\u201darchitectureofLinet a l.(2014).\nSmall-size convolution \ufb01lters have been previously used by Ciresan etal. (2011), but their nets\nare signi\ufb01cantly less deep than ours, and they did not evalua te on the large-scale ILSVRC\ndataset. Goodfellowet al. (2014) applied deep ConvNets ( 11weight layers) to the task of\nstreet number recognition, and showed that the increased de pth led to better performance.\nGoogLeNet(Szegedyet al., 2014), a top-performingentryof the ILSVRC-2014classi\ufb01cation task,\nwas developed independentlyof our work, but is similar in th at it is based on very deep ConvNets\n3\nPublishedasa conferencepaperat ICLR2015\n(22 weight layers) and small convolution \ufb01lters (apart from 3\u00d73, they also use 1\u00d71and5\u00d75\nconvolutions). Their network topology is, however, more co mplex than ours, and the spatial reso-\nlution of the feature maps is reduced more aggressively in th e \ufb01rst layers to decrease the amount\nof computation. As will be shown in Sect. 4.5, our model is out performing that of Szegedyetal.\n(2014)intermsofthesingle-networkclassi\ufb01cationaccura cy.\n3 CLASSIFICATION FRAMEWORK\nIn the previous section we presented the details of our netwo rk con\ufb01gurations. In this section, we\ndescribethe detailsofclassi\ufb01cationConvNettrainingand evaluation.\n3.1 T RAINING\nThe ConvNet training procedure generally follows Krizhevs kyetal. (2012) (except for sampling\ntheinputcropsfrommulti-scaletrainingimages,asexplai nedlater). Namely,thetrainingiscarried\nout by optimising the multinomial logistic regression obje ctive using mini-batch gradient descent\n(based on back-propagation(LeCunet al., 1989)) with momen tum. The batch size was set to 256,\nmomentum to 0.9. The training was regularised by weight decay (the L2penalty multiplier set to\n5\u00b710\u22124)anddropoutregularisationforthe\ufb01rsttwofully-connect edlayers(dropoutratiosetto 0.5).\nThelearningrate wasinitially setto 10\u22122,andthendecreasedbyafactorof 10whenthevalidation\nset accuracy stopped improving. In total, the learning rate was decreased 3 times, and the learning\nwas stopped after 370K iterations (74 epochs). We conjecture that in spite of the l arger number of\nparametersandthegreaterdepthofournetscomparedto(Kri zhevskyetal.,2012),thenetsrequired\nlessepochstoconvergedueto(a)implicitregularisationi mposedbygreaterdepthandsmallerconv.\n\ufb01lter sizes; (b)pre-initialisationofcertainlayers.\nThe initialisation of the networkweightsis important,sin ce bad initialisation can stall learningdue\nto the instability of gradient in deep nets. To circumvent th is problem, we began with training\nthe con\ufb01gurationA (Table 1), shallow enoughto be trained wi th randominitialisation. Then,when\ntrainingdeeperarchitectures,weinitialisedthe\ufb01rstfou rconvolutionallayersandthelastthreefully-\nconnectedlayerswiththelayersofnetA(theintermediatel ayerswereinitialisedrandomly). Wedid\nnotdecreasethelearningrateforthepre-initialisedlaye rs,allowingthemtochangeduringlearning.\nFor random initialisation (where applicable), we sampled t he weights from a normal distribution\nwith thezeromeanand 10\u22122variance. The biaseswere initialisedwith zero. It isworth notingthat\nafter the paper submission we found that it is possible to ini tialise the weights without pre-training\nbyusingthe randominitialisationprocedureofGlorot&Ben gio(2010).\nToobtainthe\ufb01xed-size 224\u00d7224ConvNetinputimages,theywererandomlycroppedfromresca led\ntraining images (one crop per image per SGD iteration). To fu rther augment the training set, the\ncropsunderwentrandomhorizontal\ufb02ippingandrandomRGBco lourshift(Krizhevskyet al.,2012).\nTrainingimagerescalingisexplainedbelow.\nTraining image size. LetSbe the smallest side of an isotropically-rescaledtraining image, from\nwhich the ConvNet input is cropped (we also refer to Sas the training scale). While the crop size\nis \ufb01xed to 224\u00d7224, in principle Scan take on any value not less than 224: forS= 224the crop\nwill capture whole-image statistics, completely spanning the smallest side of a training image; for\nS\u226b224thecropwillcorrespondtoasmallpartoftheimage,contain ingasmallobjectoranobject\npart.\nWe considertwoapproachesforsettingthetrainingscale S. The\ufb01rst isto\ufb01x S,whichcorresponds\nto single-scale training (note that image content within th e sampled crops can still represent multi-\nscale image statistics). In our experiments, we evaluated m odels trained at two \ufb01xed scales: S=\n256(which has been widely used in the prior art (Krizhevskyet al ., 2012; Zeiler&Fergus, 2013;\nSermanetet al., 2014)) and S= 384. Given a ConvNet con\ufb01guration,we \ufb01rst trained the network\nusingS= 256. To speed-up training of the S= 384network, it was initialised with the weights\npre-trainedwith S= 256,andwe useda smallerinitiallearningrateof 10\u22123.\nThe second approachto setting Sis multi-scale training, where each training image is indiv idually\nrescaled by randomly sampling Sfrom a certain range [Smin,Smax](we used Smin= 256and\nSmax= 512). Sinceobjectsinimagescanbeofdifferentsize,itisbene \ufb01cialtotakethisintoaccount\nduringtraining. Thiscanalso beseen astrainingset augmen tationbyscale jittering,wherea single\n4\nPublishedasa conferencepaperat ICLR2015\nmodel is trained to recognise objects over a wide range of sca les. For speed reasons, we trained\nmulti-scale models by \ufb01ne-tuning all layers of a single-sca le model with the same con\ufb01guration,\npre-trainedwith\ufb01xed S= 384.\n3.2 T ESTING\nAttest time,givena trainedConvNetandaninputimage,itis classi\ufb01ed inthefollowingway. First,\nit is isotropically rescaled to a pre-de\ufb01ned smallest image side, denoted as Q(we also refer to it\nas the test scale). We note that Qis not necessarily equal to the training scale S(as we will show\nin Sect. 4, usingseveralvaluesof QforeachSleadsto improvedperformance). Then,the network\nis applied densely overthe rescaled test image in a way simil ar to (Sermanetet al., 2014). Namely,\nthe fully-connected layers are \ufb01rst converted to convoluti onal layers (the \ufb01rst FC layer to a 7\u00d77\nconv. layer, the last two FC layers to 1\u00d71conv. layers). The resulting fully-convolutional net is\nthen applied to the whole (uncropped) image. The result is a c lass score map with the number of\nchannels equal to the number of classes, and a variable spati al resolution, dependent on the input\nimagesize. Finally,toobtaina\ufb01xed-sizevectorofclasssc oresfortheimage,theclassscoremapis\nspatially averaged(sum-pooled). We also augmentthe test s et by horizontal\ufb02ippingof the images;\nthesoft-maxclassposteriorsoftheoriginaland\ufb02ippedima gesareaveragedtoobtainthe\ufb01nalscores\nfortheimage.\nSince the fully-convolutionalnetwork is applied over the w hole image, there is no need to sample\nmultiple crops at test time (Krizhevskyetal., 2012), which is less ef\ufb01cient as it requires network\nre-computationforeachcrop. Atthesametime,usingalarge setofcrops,asdonebySzegedyetal.\n(2014),canleadtoimprovedaccuracy,asit resultsin a\ufb01ner samplingoftheinputimagecompared\ntothefully-convolutionalnet. Also,multi-cropevaluati oniscomplementarytodenseevaluationdue\nto different convolution boundary conditions: when applyi ng a ConvNet to a crop, the convolved\nfeature mapsare paddedwith zeros, while in the case of dense evaluationthe paddingfor the same\ncrop naturally comes from the neighbouring parts of an image (due to both the convolutions and\nspatial pooling), which substantially increases the overa ll network receptive \ufb01eld, so more context\niscaptured. Whilewebelievethatinpracticetheincreased computationtimeofmultiplecropsdoes\nnotjustifythepotentialgainsinaccuracy,forreferencew ealsoevaluateournetworksusing 50crops\nperscale( 5\u00d75regulargridwith 2\ufb02ips),foratotalof 150cropsover 3scales,whichiscomparable\nto144cropsover 4scalesusedbySzegedyetal. (2014).\n3.3 IMPLEMENTATION DETAILS\nOurimplementationisderivedfromthepubliclyavailableC ++ Caffetoolbox(Jia,2013)(branched\nout in December 2013), but contains a number of signi\ufb01cant mo di\ufb01cations, allowing us to perform\ntrainingandevaluationonmultipleGPUsinstalledinasing lesystem,aswellastrainandevaluateon\nfull-size (uncropped) images at multiple scales (as descri bed above). Multi-GPU training exploits\ndata parallelism, and is carried out by splitting each batch of training images into several GPU\nbatches, processed in parallel on each GPU. After the GPU bat ch gradientsare computed, they are\naveraged to obtain the gradient of the full batch. Gradient c omputation is synchronous across the\nGPUs, sothe resultisexactlythesame aswhentrainingona si ngleGPU.\nWhile more sophisticated methods of speeding up ConvNet tra ining have been recently pro-\nposed (Krizhevsky, 2014), which employmodeland data paral lelism for differentlayersof the net,\nwehavefoundthatourconceptuallymuchsimplerschemealre adyprovidesaspeedupof 3.75times\non an off-the-shelf4-GPU system, as comparedto using a sing le GPU. On a system equippedwith\nfourNVIDIATitanBlackGPUs,trainingasinglenettook2\u20133w eeksdependingonthearchitecture.\n4 CLASSIFICATION EXPERIMENTS\nDataset. In this section, we present the image classi\ufb01cation results achieved by the described\nConvNetarchitecturesontheILSVRC-2012dataset(whichwa susedforILSVRC2012\u20132014chal-\nlenges). The dataset includes images of 1000 classes, and is split into three sets: training ( 1.3M\nimages), validation ( 50K images), and testing ( 100K images with held-out class labels). The clas-\nsi\ufb01cation performanceis evaluated using two measures: the top-1 and top-5 error. The former is a\nmulti-class classi\ufb01cation error, i.e. the proportion of in correctly classi\ufb01ed images; the latter is the\n5\nPublishedasa conferencepaperat ICLR2015\nmain evaluation criterion used in ILSVRC, and is computed as the proportion of images such that\ntheground-truthcategoryisoutsidethetop-5predictedca tegories.\nForthemajorityofexperiments,weusedthevalidationseta sthetestset. Certainexperimentswere\nalso carried out on the test set and submitted to the of\ufb01cial I LSVRC server as a \u201cVGG\u201d team entry\ntothe ILSVRC-2014competition(Russakovskyet al., 2014).\n4.1 SINGLESCALEEVALUATION\nWe begin with evaluating the performanceof individual Conv Net models at a single scale with the\nlayercon\ufb01gurationsdescribedin Sect. 2.2. The test images ize was set as follows: Q=Sfor\ufb01xed\nS,andQ= 0.5(Smin+Smax)forjittered S\u2208[Smin,Smax]. Theresultsofareshownin Table3.\nFirst, we note that using local response normalisation (A-L RN network) does not improve on the\nmodel A without any normalisation layers. We thus do not empl oy normalisation in the deeper\narchitectures(B\u2013E).\nSecond, we observe that the classi\ufb01cation error decreases w ith the increased ConvNet depth: from\n11 layers in A to 19 layers in E. Notably, in spite of the same de pth, the con\ufb01guration C (which\ncontainsthree 1\u00d71conv.layers),performsworsethanthecon\ufb01gurationD,whic huses3\u00d73conv.\nlayersthroughoutthenetwork. Thisindicatesthatwhileth e additionalnon-linearitydoeshelp(Cis\nbetter than B), it is also important to capture spatial conte xt by using conv. \ufb01lters with non-trivial\nreceptive \ufb01elds (D is better than C). The error rate of our arc hitecture saturates when the depth\nreaches19layers,butevendeepermodelsmightbebene\ufb01cialforlarger datasets. Wealsocompared\nthe net B with a shallow net with \ufb01ve 5\u00d75conv. layers, which was derived from B by replacing\neachpairof 3\u00d73conv. layerswithasingle 5\u00d75conv. layer(whichhasthesamereceptive\ufb01eldas\nexplained in Sect. 2.3). The top-1 error of the shallow net wa s measured to be 7%higher than that\nof B (on a center crop),which con\ufb01rmsthat a deepnet with smal l \ufb01lters outperformsa shallow net\nwithlarger\ufb01lters.\nFinally, scale jittering at training time ( S\u2208[256;512] ) leads to signi\ufb01cantly better results than\ntraining on images with \ufb01xed smallest side ( S= 256orS= 384), even though a single scale is\nusedattesttime. Thiscon\ufb01rmsthattrainingsetaugmentati onbyscalejitteringisindeedhelpfulfor\ncapturingmulti-scaleimagestatistics.\nTable3:ConvNetperformanceatasingle testscale.\nConvNet con\ufb01g. (Table 1) smallest image side top-1 val.error (%) top-5 val.error (%)\ntrain(S)test (Q)\nA 256 256 29.6 10.4\nA-LRN 256 256 29.7 10.5\nB 256 256 28.7 9.9\nC256 256 28.1 9.4\n384 384 28.1 9.3\n[256;512] 384 27.3 8.8\nD256 256 27.0 8.8\n384 384 26.8 8.7\n[256;512] 384 25.6 8.1\nE256 256 27.3 9.0\n384 384 26.9 8.7\n[256;512] 384 25.5 8.0\n4.2 M ULTI-SCALEEVALUATION\nHavingevaluatedtheConvNetmodelsatasinglescale,wenow assesstheeffectofscalejitteringat\ntesttime. Itconsistsofrunningamodeloverseveralrescal edversionsofatestimage(corresponding\nto different values of Q), followed by averaging the resulting class posteriors. Co nsidering that a\nlarge discrepancy between training and testing scales lead s to a drop in performance, the models\ntrained with \ufb01xed Swere evaluated over three test image sizes, close to the trai ning one: Q=\n{S\u221232,S,S+ 32}. At the same time, scale jittering at training time allows th e network to be\nappliedto a widerrangeofscales at test time,so the modeltr ainedwithvariable S\u2208[Smin;Smax]\nwasevaluatedoveralargerrangeofsizes Q={Smin,0.5(Smin+Smax),Smax}.\n6\nPublishedasa conferencepaperat ICLR2015\nTheresults,presentedinTable4,indicatethatscalejitte ringattest timeleadstobetterperformance\n(as compared to evaluating the same model at a single scale, s hown in Table 3). As before, the\ndeepest con\ufb01gurations(D and E) perform the best, and scale j ittering is better than training with a\n\ufb01xed smallest side S. Our best single-network performance on the validation set is24.8%/7.5%\ntop-1/top-5error(highlightedinboldinTable4). Onthete stset,thecon\ufb01gurationEachieves 7.3%\ntop-5error.\nTable4:ConvNetperformanceatmultiple test scales.\nConvNet con\ufb01g. (Table 1) smallest image side top-1val. error (%) top-5val. error (%)\ntrain(S)test(Q)\nB 256 224,256,288 28.2 9.6\nC256 224,256,288 27.7 9.2\n384 352,384,416 27.8 9.2\n[256;512] 256,384,512 26.3 8.2\nD256 224,256,288 26.6 8.6\n384 352,384,416 26.5 8.6\n[256;512] 256,384,512 24.8 7.5\nE256 224,256,288 26.9 8.7\n384 352,384,416 26.7 8.6\n[256;512] 256,384,512 24.8 7.5\n4.3 M ULTI-CROP EVALUATION\nIn Table 5 we compare dense ConvNet evaluation with mult-cro p evaluation (see Sect. 3.2 for de-\ntails). We also assess the complementarityof thetwo evalua tiontechniquesbyaveragingtheirsoft-\nmax outputs. As can be seen, using multiple crops performs sl ightly better than dense evaluation,\nandthe two approachesareindeedcomplementary,astheir co mbinationoutperformseach ofthem.\nAs noted above, we hypothesize that this is due to a different treatment of convolution boundary\nconditions.\nTable 5:ConvNetevaluationtechniques comparison. Inall experimentsthe trainingscale Swas\nsampledfrom [256;512] ,andthreetest scales Qwereconsidered: {256,384,512}.\nConvNet con\ufb01g. (Table 1) Evaluationmethod top-1 val. error(%) top-5 val. error (%)\nDdense 24.8 7.5\nmulti-crop 24.6 7.5\nmulti-crop &dense 24.4 7.2\nEdense 24.8 7.5\nmulti-crop 24.6 7.4\nmulti-crop &dense 24.4 7.1\n4.4 C ONVNETFUSION\nUpuntilnow,weevaluatedtheperformanceofindividualCon vNetmodels. Inthispartoftheexper-\niments,wecombinetheoutputsofseveralmodelsbyaveragin gtheirsoft-maxclassposteriors. This\nimprovesthe performancedueto complementarityof the mode ls, andwas used in the top ILSVRC\nsubmissions in 2012 (Krizhevskyet al., 2012) and 2013 (Zeil er&Fergus, 2013; Sermanetet al.,\n2014).\nThe results are shown in Table 6. By the time of ILSVRC submiss ion we had only trained the\nsingle-scale networks, as well as a multi-scale model D (by \ufb01 ne-tuning only the fully-connected\nlayers rather than all layers). The resulting ensemble of 7 n etworks has 7.3%ILSVRC test error.\nAfter the submission, we considered an ensemble of only two b est-performing multi-scale models\n(con\ufb01gurations D and E), which reduced the test error to 7.0%using dense evaluation and 6.8%\nusing combined dense and multi-crop evaluation. For refere nce, our best-performingsingle model\nachieves7.1%error(modelE, Table5).\n4.5 C OMPARISON WITH THE STATE OF THE ART\nFinally, we compare our results with the state of the art in Ta ble 7. In the classi\ufb01cation task of\nILSVRC-2014 challenge (Russakovskyet al., 2014), our \u201cVGG \u201d team secured the 2nd place with\n7\nPublishedasa conferencepaperat ICLR2015\nTable6:Multiple ConvNetfusion results.\nCombined ConvNet modelsError\ntop-1 val top-5val top-5test\nILSVRCsubmission\n(D/256/224,256,288), (D/384/352,384,416), (D/[256;512 ]/256,384,512)\n(C/256/224,256,288), (C/384/352,384,416)\n(E/256/224,256,288), (E/384/352,384,416)24.7 7.5 7.3\npost-submission\n(D/[256;512]/256,384,512), (E/[256;512]/256,384,512) ,dense eval. 24.0 7.1 7.0\n(D/[256;512]/256,384,512), (E/[256;512]/256,384,512) ,multi-crop 23.9 7.2 -\n(D/[256;512]/256,384,512), (E/[256;512]/256,384,512) ,multi-crop &dense eval. 23.7 6.8 6.8\n7.3%test errorusinganensembleof7 models. Afterthesubmissio n,we decreasedtheerrorrateto\n6.8%usinganensembleof2models.\nAs can be seen from Table 7, our very deep ConvNetssigni\ufb01cant ly outperformthe previousgener-\nation of models, which achieved the best results in the ILSVR C-2012 and ILSVRC-2013 competi-\ntions. Our result is also competitivewith respect to the cla ssi\ufb01cation task winner(GoogLeNetwith\n6.7%error) and substantially outperforms the ILSVRC-2013 winn ing submission Clarifai, which\nachieved 11.2%with outside training data and 11.7%without it. This is remarkable, considering\nthat our best result is achievedby combiningjust two models \u2013 signi\ufb01cantly less than used in most\nILSVRC submissions. In terms of the single-net performance , our architecture achieves the best\nresult (7.0%test error), outperforming a single GoogLeNet by 0.9%. Notably, we did not depart\nfrom the classical ConvNet architecture of LeCunetal. (198 9), but improved it by substantially\nincreasingthedepth.\nTable 7:Comparison with the state of the art in ILSVRC classi\ufb01cation . Our methodis denoted\nas\u201cVGG\u201d.Onlytheresultsobtainedwithoutoutsidetrainin gdataarereported.\nMethod top-1 val. error(%) top-5val. error (%) top-5testerror (%)\nVGG(2nets, multi-crop& dense eval.) 23.7 6.8 6.8\nVGG(1net, multi-crop& dense eval.) 24.4 7.1 7.0\nVGG(ILSVRCsubmission, 7nets, dense eval.) 24.7 7.5 7.3\nGoogLeNet (Szegedy et al., 2014) (1net) - 7.9\nGoogLeNet (Szegedy et al., 2014) (7nets) - 6.7\nMSRA(He et al., 2014) (11nets) - - 8.1\nMSRA(He et al., 2014) (1net) 27.9 9.1 9.1\nClarifai(Russakovsky et al., 2014) (multiplenets) - - 11.7\nClarifai(Russakovsky et al., 2014) (1net) - - 12.5\nZeiler& Fergus (Zeiler&Fergus, 2013) (6nets) 36.0 14.7 14.8\nZeiler& Fergus (Zeiler&Fergus, 2013) (1net) 37.5 16.0 16.1\nOverFeat (Sermanetet al.,2014) (7nets) 34.0 13.2 13.6\nOverFeat (Sermanetet al.,2014) (1net) 35.7 14.2 -\nKrizhevsky et al.(Krizhevsky et al., 2012) (5nets) 38.1 16.4 16.4\nKrizhevsky et al.(Krizhevsky et al., 2012) (1net) 40.7 18.2 -\n5 CONCLUSION\nIn this work we evaluated very deep convolutional networks ( up to 19 weight layers) for large-\nscale image classi\ufb01cation. It was demonstrated that the rep resentation depth is bene\ufb01cial for the\nclassi\ufb01cationaccuracy,andthatstate-of-the-artperfor manceontheImageNetchallengedatasetcan\nbeachievedusingaconventionalConvNetarchitecture(LeC unet al.,1989;Krizhevskyet al.,2012)\nwithsubstantiallyincreaseddepth. Intheappendix,weals oshowthatourmodelsgeneralisewellto\na wide range of tasks and datasets, matchingor outperformin gmore complexrecognitionpipelines\nbuiltaroundlessdeepimagerepresentations. Ourresultsy etagaincon\ufb01rmtheimportanceof depth\ninvisualrepresentations.\nACKNOWLEDGEMENTS\nThisworkwassupportedbyERCgrantVisRecno.228180. Wegra tefullyacknowledgethesupport\nofNVIDIACorporationwiththedonationoftheGPUsusedfort hisresearch.\n8\nPublishedasa conferencepaperat ICLR2015\nREFERENCES\nBell, S., Upchurch, P.,Snavely, N., and Bala, K. Material re cognition inthe wild withthe materials in context\ndatabase. CoRR,abs/1412.0623, 2014.\nChat\ufb01eld, K., Simonyan, K., Vedaldi, A., and Zisserman, A. R eturn of the devil in the details: Delving deep\nintoconvolutional nets. In Proc.BMVC. ,2014.\nCimpoi,M.,Maji,S.,andVedaldi,A. Deepconvolutional\ufb01lt erbanksfortexturerecognitionandsegmentation.\nCoRR,abs/1411.6836, 2014.\nCiresan, D. C., Meier, U., Masci, J., Gambardella, L. M., and Schmidhuber, J. Flexible, high performance\nconvolutional neural networks for image classi\ufb01cation. In IJCAI,pp. 1237\u20131242, 2011.\nDean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M. , Ranzato, M., Senior, A., Tucker, P., Yang,\nK.,Le,Q. V.,andNg, A.Y. Large scale distributeddeepnetwo rks. InNIPS,pp. 1232\u20131240, 2012.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei , L. Imagenet: A large-scale hierarchical image\ndatabase. In Proc.CVPR ,2009.\nDonahue,J.,Jia,Y.,Vinyals,O.,Hoffman,J.,Zhang,N.,Tz eng,E.,andDarrell,T.Decaf: Adeepconvolutional\nactivation feature for generic visual recognition. CoRR,abs/1310.1531, 2013.\nEveringham, M., Eslami, S.M. A., Van Gool, L., Williams,C., Winn, J., and Zisserman, A. The Pascal visual\nobject classes challenge: Aretrospective. IJCV,111(1):98\u2013136, 2015.\nFei-Fei, L., Fergus, R., and Perona, P. Learning generative visual models from few training examples: An\nincremental bayesian approach tested on 101 object categor ies. InIEEE CVPR Workshop of Generative\nModel BasedVision , 2004.\nGirshick, R. B., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection\nand semantic segmentation. CoRR,abs/1311.2524v5, 2014. PublishedinProc.CVPR,2014.\nGkioxari, G.,Girshick, R.,and Malik, J. Actions and attrib utes from wholes and parts. CoRR,abs/1412.2604,\n2014.\nGlorot, X. andBengio, Y. Understanding the dif\ufb01cultyof tra iningdeep feedforward neural networks. In Proc.\nAISTATS,volume 9, pp. 249\u2013256, 2010.\nGoodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Sh et, V. Multi-digit number recognition from street\nview imagery usingdeep convolutional neural networks. In Proc.ICLR ,2014.\nGrif\ufb01n, G., Holub, A., and Perona, P. Caltech-256 object cat egory dataset. Technical Report 7694, California\nInstitute of Technology, 2007.\nHe, K., Zhang, X., Ren, S., and Sun, J. Spatial pyramid poolin g in deep convolutional networks for visual\nrecognition. CoRR,abs/1406.4729v2, 2014.\nHoai, M. Regularizedmax pooling forimage categorization. InProc. BMVC. ,2014.\nHoward, A.G. Someimprovements ondeepconvolutional neura l networkbasedimageclassi\ufb01cation. In Proc.\nICLR,2014.\nJia, Y. Caffe: An open source convolutional architecture fo r fast feature embedding.\nhttp://caffe.berkeleyvision.org/ ,2013.\nKarpathy, A. and Fei-Fei, L. Deep visual-semantic alignmen ts for generating image descriptions. CoRR,\nabs/1412.2306, 2014.\nKiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying visu al-semantic embeddings with multimodal neural\nlanguage models. CoRR,abs/1411.2539, 2014.\nKrizhevsky, A. One weirdtrickfor parallelizingconvoluti onal neural networks. CoRR,abs/1404.5997, 2014.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet cl assi\ufb01cation with deep convolutional neural net-\nworks. In NIPS,pp. 1106\u20131114, 2012.\nLeCun,Y.,Boser, B.,Denker, J.S.,Henderson, D.,Howard, R .E.,Hubbard, W.,andJackel, L.D. Backpropa-\ngationapplied tohandwrittenzipcode recognition. Neural Computation , 1(4):541\u2013551, 1989.\nLin,M., Chen, Q.,andYan, S. Networkinnetwork. In Proc.ICLR ,2014.\nLong, J., Shelhamer, E., and Darrell, T. Fully convolutiona l networks for semantic segmentation. CoRR,\nabs/1411.4038, 2014.\nOquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and Transferring Mid-Level Image Representations\nusing Convolutional Neural Networks. In Proc.CVPR ,2014.\nPerronnin, F.,S\u00b4 anchez, J.,andMensink, T. Improving theF isherkernel forlarge-scale image classi\ufb01cation. In\nProc.ECCV ,2010.\nRazavian, A.,Azizpour, H.,Sullivan, J.,andCarlsson,S. C NNFeaturesoff-the-shelf: anAstounding Baseline\nfor Recognition. CoRR,abs/1403.6382, 2014.\n9\nPublishedasa conferencepaperat ICLR2015\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A.,\nBernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet large sc ale visual recognition challenge. CoRR,\nabs/1409.0575, 2014.\nSermanet,P.,Eigen, D.,Zhang, X.,Mathieu, M.,Fergus,R., andLeCun,Y. OverFeat: IntegratedRecognition,\nLocalizationand Detectionusing Convolutional Networks. InProc.ICLR ,2014.\nSimonyan, K. and Zisserman, A. Two-stream convolutional ne tworks for action recognition in videos. CoRR,\nabs/1406.2199, 2014. Published inProc.NIPS,2014.\nSzegedy, C., Liu, W.,Jia, Y., Sermanet, P.,Reed, S.,Anguel ov, D.,Erhan, D., Vanhoucke, V., and Rabinovich,\nA. Goingdeeper withconvolutions. CoRR,abs/1409.4842, 2014.\nWei, Y., Xia, W., Huang, J., Ni, B., Dong, J., Zhao, Y., and Yan , S. CNN: Single-label to multi-label. CoRR,\nabs/1406.5726, 2014.\nZeiler, M. D. and Fergus, R. Visualizing and understanding c onvolutional networks. CoRR, abs/1311.2901,\n2013. PublishedinProc. ECCV,2014.\nA LOCALISATION\nIn the main bodyof the paper we have consideredthe classi\ufb01ca tion task of the ILSVRC challenge,\nand performed a thorough evaluation of ConvNet architectur es of different depth. In this section,\nwe turn to the localisation task of the challenge, which we ha ve won in 2014 with 25.3%error. It\ncan be seen as a special case of object detection, where a sing le object bounding box should be\npredictedforeach ofthe top-5classes, irrespectiveof the actual numberofobjectsof the class. For\nthiswe adoptthe approachof Sermanetet al. (2014), the winn ersof the ILSVRC-2013localisation\nchallenge,withafewmodi\ufb01cations. Ourmethodisdescribed inSect.A.1andevaluatedinSect.A.2.\nA.1 L OCALISATION CONVNET\nTo perform object localisation, we use a very deep ConvNet, w here the last fully connected layer\npredicts the bounding box location instead of the class scor es. A bounding box is represented by\na 4-D vector storing its center coordinates, width, and heig ht. There is a choice of whether the\nboundingbox prediction is shared across all classes (singl e-class regression, SCR (Sermanetet al.,\n2014))orisclass-speci\ufb01c(per-classregression,PCR).In theformercase,thelastlayeris4-D,while\nin the latter it is 4000-D (since there are 1000 classes in the dataset). Apart from the last bounding\nboxpredictionlayer,weuse theConvNetarchitectureD (Tab le1),whichcontains16weightlayers\nandwasfoundtobe thebest-performingin theclassi\ufb01cation task (Sect.4).\nTraining. Training of localisation ConvNets is similar to that of the c lassi\ufb01cation ConvNets\n(Sect.3.1). Themaindifferenceisthatwereplacethelogis ticregressionobjectivewithaEuclidean\nloss,whichpenalisesthedeviationofthepredictedboundi ngboxparametersfromtheground-truth.\nWe trainedtwo localisation models, each on a single scale: S= 256andS= 384(due to the time\nconstraints,we didnot use trainingscale jitteringforour ILSVRC-2014submission). Trainingwas\ninitialised with the correspondingclassi\ufb01cation models ( trained on the same scales), and the initial\nlearning rate was set to 10\u22123. We exploredboth \ufb01ne-tuningall layers and \ufb01ne-tuningonly the \ufb01rst\ntwo fully-connected layers, as done in (Sermanetetal., 201 4). The last fully-connected layer was\ninitialisedrandomlyandtrainedfromscratch.\nTesting. We consider two testing protocols. The \ufb01rst is used for compa ring different network\nmodi\ufb01cations on the validation set, and considers only the b oundingbox prediction for the ground\ntruth class (to factor out the classi\ufb01cation errors). The bo unding box is obtained by applying the\nnetworkonlyto thecentralcropoftheimage.\nThe second, fully-\ufb02edged, testing procedure is based on the dense application of the localisation\nConvNet to the whole image, similarly to the classi\ufb01cation t ask (Sect. 3.2). The difference is that\ninstead of the class score map, the output of the last fully-c onnected layer is a set of bounding\nbox predictions. To come up with the \ufb01nal prediction, we util ise the greedy merging procedure\nof Sermanetetal. (2014), which \ufb01rst merges spatially close predictions (by averaging their coor-\ndinates), and then rates them based on the class scores, obta ined from the classi\ufb01cation ConvNet.\nWhen several localisation ConvNets are used, we \ufb01rst take th e union of their sets of boundingbox\npredictions, and then run the mergingprocedureon the union . We did not use the multiple pooling\n10\nPublishedasa conferencepaperat ICLR2015\noffsets technique of Sermanetetal. (2014), which increase s the spatial resolution of the bounding\nboxpredictionsandcanfurtherimprovetheresults.\nA.2 L OCALISATION EXPERIMENTS\nIn this section we \ufb01rst determine the best-performinglocal isation setting (using the \ufb01rst test proto-\ncol), and then evaluate it in a fully-\ufb02edged scenario (the se cond protocol). The localisation error\nis measured according to the ILSVRC criterion (Russakovsky et al., 2014), i.e. the bounding box\npredictionis deemed correctif its intersectionoverunion ratio with the ground-truthboundingbox\nisabove0.5.\nSettings comparison. As can be seen from Table 8, per-class regression (PCR) outpe rforms the\nclass-agnostic single-class regression (SCR), which diff ers from the \ufb01ndings of Sermanetetal.\n(2014), where PCR was outperformed by SCR. We also note that \ufb01 ne-tuning all layers for the lo-\ncalisation task leads to noticeablybetter results than \ufb01ne -tuningonly the fully-connectedlayers(as\ndonein(Sermanetet al.,2014)). Intheseexperiments,thes mallestimagessidewassetto S= 384;\ntheresultswith S= 256exhibitthesamebehaviourandarenotshownforbrevity.\nTable 8:Localisation error for different modi\ufb01cations with the simpli\ufb01ed testing protocol: the\nboundingbox is predictedfrom a single central image crop, a nd the ground-truthclass is used. All\nConvNet layers (except for the last one) have the con\ufb01gurati on D (Table 1), while the last layer\nperformseithersingle-classregression(SCR) orper-clas sregression(PCR).\nFine-tunedlayers regression type GTclass localisationerror\n1st and2nd FCSCR 36.4\nPCR 34.3\nall PCR 33.1\nFully-\ufb02edgedevaluation. Havingdeterminedthebestlocalisationsetting(PCR,\ufb01ne- tuningofall\nlayers),we nowapply it in the fully-\ufb02edgedscenario,where the top-5class labelsare predictedus-\ning our best-performingclassi\ufb01cation system (Sect. 4.5), and multiple densely-computedbounding\nbox predictions are merged using the method of Sermanetetal . (2014). As can be seen from Ta-\nble 9, applicationof the localisationConvNetto the whole i magesubstantiallyimprovesthe results\ncompared to using a center crop (Table 8), despite using the t op-5 predicted class labels instead of\nthegroundtruth. Similarlytotheclassi\ufb01cationtask(Sect .4),testingatseveralscalesandcombining\nthepredictionsofmultiplenetworksfurtherimprovesthep erformance.\nTable9:Localisationerror\nsmallestimage side top-5localisationerror (%)\ntrain(S) test(Q) val. test.\n256 256 29.5 -\n384 384 28.2 26.7\n384 352,384 27.5 -\nfusion: 256/256 and 384/352,384 26.9 25.3\nComparison with the state of the art. We compare our best localisation result with the state\nof the art in Table 10. With 25.3%test error, our \u201cVGG\u201d team won the localisation challenge of\nILSVRC-2014 (Russakovskyet al., 2014). Notably, our resul ts are considerably better than those\nof the ILSVRC-2013winnerOverfeat(Sermanetet al., 2014), even thoughwe used less scales and\ndid not employ their resolution enhancement technique. We e nvisage that better localisation per-\nformance can be achieved if this technique is incorporated i nto our method. This indicates the\nperformanceadvancementbroughtbyourverydeepConvNets\u2013 wegotbetterresultswithasimpler\nlocalisationmethod,buta morepowerfulrepresentation.\nB GENERALISATION OF VERYDEEPFEATURES\nIn the previous sections we have discussed training and eval uation of very deep ConvNets on the\nILSVRC dataset. In this section, we evaluate our ConvNets, p re-trained on ILSVRC, as feature\n11\nPublishedasa conferencepaperat ICLR2015\nTable 10: Comparison with the state of the art in ILSVRC localisation . Our methodis denoted\nas\u201cVGG\u201d.\nMethod top-5val. error (%) top-5 testerror (%)\nVGG 26.9 25.3\nGoogLeNet (Szegedyet al., 2014) - 26.7\nOverFeat (Sermanet etal.,2014) 30.0 29.9\nKrizhevsky et al.(Krizhevsky et al.,2012) - 34.2\nextractors on other, smaller, datasets, where training lar ge models from scratch is not feasible due\nto over-\ufb01tting. Recently, there has been a lot of interest in such a use case (Zeiler&Fergus, 2013;\nDonahueet al., 2013; Razavianet al., 2014; Chat\ufb01eldet al., 2014), as it turns out that deep image\nrepresentations,learntonILSVRC,generalisewelltoothe rdatasets,wheretheyhaveoutperformed\nhand-crafted representations by a large margin. Following that line of work, we investigate if our\nmodelsleadtobetterperformancethanmoreshallowmodelsu tilisedinthestate-of-the-artmethods.\nIn this evaluation, we consider two models with the best clas si\ufb01cation performance on ILSVRC\n(Sect.4)\u2013con\ufb01gurations\u201cNet-D\u201dand\u201cNet-E\u201d(whichwemade publiclyavailable).\nTo utilise the ConvNets, pre-trained on ILSVRC, for image cl assi\ufb01cation on other datasets, we\nremove the last fully-connected layer (which performs 1000 -way ILSVRC classi\ufb01cation), and use\n4096-Dactivationsofthepenultimatelayerasimagefeatur es,whichareaggregatedacrossmultiple\nlocations and scales. The resulting image descriptor is L2-normalised and combined with a linear\nSVM classi\ufb01er, trained on the target dataset. For simplicit y, pre-trained ConvNet weights are kept\n\ufb01xed(no\ufb01ne-tuningisperformed).\nAggregation of features is carried out in a similar manner to our ILSVRC evaluation procedure\n(Sect. 3.2). Namely, an image is \ufb01rst rescaled so that its sma llest side equals Q, and then the net-\nwork is densely applied over the image plane (which is possib le when all weight layers are treated\nas convolutional). We then perform global average pooling o n the resulting feature map, which\nproducesa 4096-Dimage descriptor. The descriptor is then a veraged with the descriptor of a hori-\nzontally \ufb02ipped image. As was shown in Sect. 4.2, evaluation over multiple scales is bene\ufb01cial, so\nwe extract features over several scales Q. The resulting multi-scale features can be either stacked\nor pooled across scales. Stacking allows a subsequent class i\ufb01er to learn how to optimally combine\nimage statistics over a range of scales; this, however, come s at the cost of the increased descriptor\ndimensionality. We returntothediscussionofthisdesignc hoicein theexperimentsbelow. We also\nassess late fusion of features, computed using two networks , which is performed by stacking their\nrespectiveimagedescriptors.\nTable11: Comparisonwiththestateoftheartinimageclassi\ufb01cationo nVOC-2007,VOC-2012,\nCaltech-101, and Caltech-256 . Our models are denoted as \u201cVGG\u201d. Results marked with * were\nachievedusingConvNetspre-trainedonthe extended ILSVRCdataset(2000classes).\nMethodVOC-2007 VOC-2012 Caltech-101 Caltech-256\n(meanAP) (mean AP) (meanclass recall) (mean class recall)\nZeiler& Fergus (Zeiler&Fergus, 2013) - 79.0 86.5\u00b10.5 74.2\u00b10.3\nChat\ufb01eldetal. (Chat\ufb01eldet al., 2014) 82.4 83.2 88.4\u00b10.6 77.6\u00b10.1\nHe etal. (Heet al.,2014) 82.4 - 93.4\u00b10.5 -\nWeiet al.(Weiet al., 2014) 81.5(85.2\u2217)81.7 (90.3\u2217) - -\nVGGNet-D (16layers) 89.3 89.0 91.8\u00b11.0 85.0\u00b10.2\nVGGNet-E(19 layers) 89.3 89.0 92.3\u00b10.5 85.1\u00b10.3\nVGGNet-D & Net-E 89.7 89.3 92.7\u00b10.5 86.2\u00b10.3\nImage Classi\ufb01cation on VOC-2007and VOC-2012. We beginwith the evaluationon the image\nclassi\ufb01cation task of PASCAL VOC-2007 and VOC-2012 benchma rks (Everinghametal., 2015).\nThese datasets contain 10K and 22.5K images respectively, a nd each image is annotated with one\nor several labels, correspondingto 20 object categories. T he VOC organisersprovidea pre-de\ufb01ned\nsplit into training, validation, and test data (the test dat a for VOC-2012 is not publicly available;\ninstead,anof\ufb01cialevaluationserverisprovided). Recogn itionperformanceismeasuredusingmean\naverageprecision(mAP)acrossclasses.\nNotably, by examining the performance on the validation set s of VOC-2007 and VOC-2012, we\nfoundthat aggregatingimage descriptors,computedat mult iple scales, by averagingperformssim-\n12\nPublishedasa conferencepaperat ICLR2015\nilarly to the aggregation by stacking. We hypothesize that t his is due to the fact that in the VOC\ndataset the objects appear over a variety of scales, so there is no particular scale-speci\ufb01c seman-\ntics which a classi\ufb01er could exploit. Since averaging has a b ene\ufb01t of not in\ufb02ating the descrip-\ntor dimensionality, we were able to aggregated image descri ptors over a wide range of scales:\nQ\u2208 {256,384,512,640,768}. It is worth noting though that the improvement over a smalle r\nrangeof{256,384,512}wasrathermarginal( 0.3%).\nThetestsetperformanceisreportedandcomparedwithother approachesinTable11. Ournetworks\n\u201cNet-D\u201dand\u201cNet-E\u201dexhibitidenticalperformanceonVOCda tasets,andtheircombinationslightly\nimproves the results. Our methods set the new state of the art across image representations, pre-\ntrained on the ILSVRC dataset, outperformingthe previousb est result of Chat\ufb01eldet al. (2014) by\nmore than 6%. It should be noted that the method of Wei et al. (2014), which achieves1%better\nmAP on VOC-2012, is pre-trained on an extended 2000-class IL SVRC dataset, which includes\nadditional 1000 categories, semantically close to those in VOC datasets. It also bene\ufb01ts from the\nfusionwith anobjectdetection-assistedclassi\ufb01cation pi peline.\nImageClassi\ufb01cationonCaltech-101andCaltech-256. Inthissectionweevaluateverydeepfea-\nturesonCaltech-101(Fei-Feiet al.,2004)andCaltech-256 (Grif\ufb01net al.,2007)imageclassi\ufb01cation\nbenchmarks. Caltech-101contains9Kimageslabelledinto1 02classes(101objectcategoriesanda\nbackgroundclass), while Caltech-256 is larger with 31K ima ges and 257 classes. A standard eval-\nuation protocolon these datasets is to generateseveral ran domsplits into training and test data and\nreport the average recognition performance across the spli ts, which is measured by the mean class\nrecall(whichcompensatesforadifferentnumberoftestima gesperclass). FollowingChat\ufb01eld etal.\n(2014); Zeiler&Fergus(2013); He etal. (2014),onCaltech- 101we generated3 randomsplits into\ntraining and test data, so that each split contains 30 traini ng images per class, and up to 50 test\nimages per class. On Caltech-256 we also generated 3 splits, each of which contains 60 training\nimages per class (and the rest is used for testing). In each sp lit, 20% of training images were used\nasa validationset forhyper-parameterselection.\nWe found that unlike VOC, on Caltech datasets the stacking of descriptors, computed over multi-\nple scales, performs better than averaging or max-pooling. This can be explained by the fact that\nin Caltech images objects typically occupy the whole image, so multi-scale image features are se-\nmanticallydifferent(capturingthe wholeobject vs. object parts), andstacking allows a classi\ufb01er to\nexploitsuchscale-speci\ufb01crepresentations. We usedthree scalesQ\u2208 {256,384,512}.\nOurmodelsarecomparedtoeachotherandthestateofthearti nTable11. Ascanbeseen,thedeeper\n19-layerNet-Eperformsbetterthanthe16-layerNet-D,and theircombinationfurtherimprovesthe\nperformance. On Caltech-101, our representations are comp etitive with the approach of He etal.\n(2014),which,however,performssigni\ufb01cantlyworsethano urnetsonVOC-2007. OnCaltech-256,\nourfeaturesoutperformthestate oftheart (Chat\ufb01eldetal. , 2014)byalargemargin( 8.6%).\nAction Classi\ufb01cation on VOC-2012. We also evaluated our best-performing image representa-\ntion (the stacking of Net-D and Net-E features) on the PASCAL VOC-2012 action classi\ufb01cation\ntask (Everinghamet al., 2015), which consists in predictin g an action class from a single image,\ngiven a bounding box of the person performing the action. The dataset contains 4.6K training im-\nages,labelledinto11classes. SimilarlytotheVOC-2012ob jectclassi\ufb01cationtask,theperformance\nis measured using the mAP. We considered two training settin gs: (i) computing the ConvNet fea-\nturesonthewholeimageandignoringtheprovidedboundingb ox;(ii)computingthefeaturesonthe\nwholeimageandontheprovidedboundingbox,andstackingth emtoobtainthe\ufb01nalrepresentation.\nTheresultsarecomparedtootherapproachesinTable12.\nOurrepresentationachievesthestateofartontheVOCactio nclassi\ufb01cationtaskevenwithoutusing\nthe provided bounding boxes, and the results are further imp roved when using both images and\nbounding boxes. Unlike other approaches, we did not incorpo rate any task-speci\ufb01c heuristics, but\nreliedontherepresentationpowerofverydeepconvolution alfeatures.\nOther Recognition Tasks. Since the public release of our models, they have been active ly used\nby the research community for a wide range of image recogniti on tasks, consistently outperform-\ning more shallow representations. For instance, Girshicke t al. (2014) achieve the state of the\nobject detection results by replacing the ConvNet of Krizhe vskyet al. (2012) with our 16-layer\nmodel. Similar gains over a more shallow architecture of Kri zhevskyet al. (2012) have been ob-\n13\nPublishedasa conferencepaperat ICLR2015\nTable 12: Comparison with the state of the art in single-image action c lassi\ufb01cation on VOC-\n2012. Our models are denoted as \u201cVGG\u201d. Results marked with * were a chieved using ConvNets\npre-trainedonthe extended ILSVRCdataset (1512classes).\nMethod VOC-2012 (meanAP)\n(Oquab et al., 2014) 70.2\u2217\n(Gkioxari etal.,2014) 73.6\n(Hoai,2014) 76.3\nVGG Net-D& Net-E,image-only 79.2\nVGG Net-D& Net-E,image and bounding box 84.0\nserved in semantic segmentation (Longet al., 2014), image c aption generation (Kirosetal., 2014;\nKarpathy& Fei-Fei, 2014),textureandmaterialrecognitio n(Cimpoiet al., 2014; Bell etal., 2014).\nC PAPERREVISIONS\nHere we present the list of major paper revisions, outlining the substantial changes for the conve-\nnienceofthe reader.\nv1Initialversion. Presentstheexperimentscarriedoutbefo rethe ILSVRCsubmission.\nv2Addspost-submissionILSVRCexperimentswithtrainingset augmentationusingscalejittering,\nwhichimprovestheperformance.\nv3Addsgeneralisationexperiments(AppendixB) on PASCAL VOC andCaltech image classi\ufb01ca-\ntiondatasets. Themodelsusedforthese experimentsarepub liclyavailable.\nv4The paper is converted to ICLR-2015 submission format. Also adds experiments with multiple\ncropsforclassi\ufb01cation.\nv6Camera-readyICLR-2015conferencepaper. Addsa compariso nof the net B with a shallow net\nandtheresultsonPASCAL VOCactionclassi\ufb01cationbenchmar k.\n14\n",
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "arxiv_id": "1409.1556",
    "category": "famous",
    "authors": [
      "Karen Simonyan",
      "Andrew Zisserman"
    ],
    "published": "2014-09-04",
    "num_pages": 14,
    "num_chars": 52384
  },
  {
    "text": "1\u201330\nHarnessing Data from Clustered LQR Systems: Personalized and\nCollaborative Policy Optimization\nVinay Kanakeri VKANAKE@NCSU.EDU\nDepartment of Electrical and Computer Engineering, North Carolina State University\nShivam Bajaj BAJAJ41@PURDUE.EDU\nThe Elmore Family School of Electrical and Computer Engineering, Purdue University\nAshwin VermaVERMA240@PURDUE.EDU\nThe Elmore Family School of Electrical and Computer Engineering, Purdue University\nVijay Gupta GUPTA869@PURDUE.EDU\nThe Elmore Family School of Electrical and Computer Engineering, Purdue University\nAritra Mitra AMITRA2@NCSU.EDU\nDepartment of Electrical and Computer Engineering, North Carolina State University\nAbstract\nIt is known that reinforcement learning (RL) is data-hungry. To improve sample-efficiency of RL, it\nhas been proposed that the learning algorithm utilize data from \u2018approximately similar\u2019 processes.\nHowever, since the process models are unknown, identifying which other processes are similar poses\na challenge. In this work, we study this problem in the context of the benchmark Linear Quadratic\nRegulator (LQR) setting. Specifically, we consider a setting with multiple agents, each corresponding\nto a copy of a linear process to be controlled. The agents\u2019 local processes can be partitioned into\nclusters based on similarities in dynamics and tasks. Combining ideas from sequential elimination\nand zeroth-order policy optimization, we propose a new algorithm that performs simultaneous\nclustering and learning to output apersonalized policy(controller) for each cluster. Under a suitable\nnotion of cluster separation that captures differences in closed-loop performance across systems,\nwe prove that our approach guarantees correct clustering with high probability. Furthermore, we\nshow that the sub-optimality gap of the policy learned for each cluster scales inversely with the\nsize of the cluster, with no additional bias, unlike in prior works on collaborative learning-based\ncontrol. Our work is the first to reveal how clustering can be used in data-driven control to learn\npersonalized policies that enjoy statistical gains from collaboration but do not suffer sub-optimality\ndue to inclusion of data from dissimilar processes. From a distributed implementation perspective,\nour method is attractive as it incurs only a mild logarithmic communication overhead.\nKeywords:Policy gradients for LQR; Collaborative Learning; Transfer/Multi-Task Learning.\n1. Introduction\nThe last decade or so has seen a surge of interest inmodel-free data-driven control(Hu et al., 2023),\nwhere control laws (policies) are learned directly from data, bypassing the need to estimate the\nsystem model as an intermediate step. Although such a framework is promising, it relies on the\navailability of adequate data to learn high-precision policies. Unfortunately, however, data from\nphysical processes (such as real-world robotic environments) could bescarceand/or difficult to\ncollect. Drawing inspiration from popular paradigms such as federated and meta-learning, some\nrecent papers (Zhang et al., 2023; Wang et al., 2023a; Toso et al., 2024) have attempted to mitigate\nthis challenge by exploring the idea of combining information generated by multiple environments,\nwhere each environment represents a dynamical system with an associated cost performance metric\n\u00a9 V . Kanakeri, S. Bajaj, A. Verma, V . Gupta & A. Mitra.arXiv:2511.17489v1  [cs.LG]  21 Nov 2025\nKANAKERIBAJAJVERMAGUPTAMITRA\nthat captures a task or a goal. The unifying theme in such papers is to learna single common policy\nthat performs well acrossallenvironments by minimizing an average-cost performance metric. When\nenvironments differ considerably in their tasks, such a single common policy might incur highly\nsub-optimal performance on any given environment. More fundamentally, when environments differ\nin dynamics,even the existence of a common stabilizing policy is unclear and difficult to verify in the\nabsence of models.Departing from the approach of learning a single common policy, in this paper,\nwe ask:(When) is it possible to learnpersonalized policiesin a sample-efficient way by leveraging\ndata generated by potentially non-identical dynamical processes?\nTo formalize our study, we consider a scenario involving multiple agents that can be partitioned\ninto distinct clusters. We assume that all agents within a given cluster interact with the same physical\nenvironment modeled as a linear time-variant (LTI) system withunknowndynamics; furthermore,\nall agents within a cluster share the same quadratic cost function. However, the dynamics and cost\nfunctions across clusters can bearbitrarily different.Thus, our setting captures both similarities\nand differences in dynamics and tasks. As is common in collaborative and federated learning, we\nallow agents to exchange information via a central aggregator. Concretely, our problem of interest\nis to learn apersonalizedpolicy for each cluster that enjoys thebenefits of collaboration, i.e., we\nwish to show that such a policy can be learned faster (relative to a single-agent setting) by using the\ncollective samples available within the cluster. However, this is challenging, as we explain below.\nChallenges.To make our setting realistic, we assume that the cluster structure is unknown a\npriori. Since the system models associated with the clusters are also unknown, it becomes difficult\nto decide how information should be exchanged between agents. In particular, care needs to be\ntaken to avoid misclustering, since transfer of information across clusterswith arbitrarily different\nLTI systemscan lead to the learning of destabilizing policies; thus, in our setting,more data can\npotentially hurt if not used judiciously.Additional subtleties arise as the agents in our setting access\nonly noisy zeroth-order information for both clustering and learning policies; we discuss them in\nSections 2 and 3. In light of these challenges, the maincontributionsof this paper are as follows.\n\u2022Problem Formulation.While clustering has been explored in federated learning (FL) for\nstatic supervised learning tasks (Ghosh et al., 2020), our work provides the first principled study of\nclustering in the context of model-free data-driven control, and shows how such a formalism can\nenable learning personalized policies in a sample-efficient manner. As part of our formulation, we\nidentify a dissimilarity metric \u2206(see(3)) that captures differences in optimal costs between clusters.\nOur results reveal that a larger value of\u2206leads to a faster separation of clusters.\n\u2022Novel Algorithm.The primary contribution of this paper is the development of a novel\nmodel-freePersonalized and Collaborative Policy Optimization (PO) algorithm (Algorithm 1) called\nPCPO that combines ideas fromsequential eliminationin multi-armed bandits (Even-Dar et al.,\n2006) and policy gradient algorithms in reinforcement learning (RL) (Agarwal et al., 2021). The lack\nof prior knowledge of the cluster-separation gap \u2206motivates the need for a sequential elimination\nstrategy to identify the clusters. Moreover, since it is non-trivial to decide when to stop clustering and\nstart collaborating, we propose an epoch-based approach that involves clustering and collaboration\nin every epochby requiring agents to maintain two separate sequences of policies: local policies\nused purely for sequential clustering and global policies for collaboration. Another key feature of\nour algorithm is that it only incurs a mild communication cost that scales logarithmically with the\nnumber of agents and samples, making it particularly appealing for distributed implementation.\n\u2022Collaborative Gains.In Theorem 1, we prove that with high probability, our approach leads to\ncorrect clustering, despite the absence of prior knowledge of models, cluster structure, and separation\n2\nHARNESSINGDATA FROMCLUSTEREDLQR SYSTEMS\ngap\u2206.This result also reveals that moreheterogeneity can actually aid the learning processin that\na larger \u2206incurs fewer noisy function evaluations for correct clustering. Building on Theorem 1,\nour main result in Theorem 2 proves that by using PCPO , each agent can learn a near-optimal\npersonalized policy for its own system with a sub-optimality gap that scales inversely with the\nnumber of agents in its cluster. In other words, PCPO prevents negative transfer of information across\nclusters, while ensuring sample-complexity reductions via collaboration within each cluster. To our\nknowledge, this is the first result to show how data from heterogeneous dynamical systems admitting\na cluster structure can be harnessed to expedite the learning of personalized policies.\nSince this is a preliminary investigation of clustering in data-driven control, we restrict our\nattention to the canonical Linear Quadratic Regulator (LQR) formalism. That said, we anticipate that\nour general algorithmic template can be used in other supervised or RL problems.\nRelated Work.To put our contributions into perspective, we discuss relevant literature below.\n\u2022Policy Gradient for LQR.We build on the rich set of results on policy gradient methods for\nthe LQR problem (Fazel et al., 2018; Malik et al., 2020; Gravell et al., 2020; Zhang et al., 2021;\nMohammadi et al., 2019, 2020; Hu et al., 2023; Moghaddam et al., 2025). Generalizing these results\nfrom the single system setting to our clustered multi-system formulation introduces various nuances\nand challenges (outlined in Sections 2 and 3) that we address in this paper.\n\u2022Personalized Federated Learning.We draw inspiration from the work on clustering in\nFL (Ghosh et al., 2020, 2022; Sattler et al., 2020) that aims to learn personalized models for groups of\nagents that are similar in terms of their data distributions. Despite cosmetic similarities, the specifics\nof our setting differ significantly in that we focus on control of dynamical systems where stability\nplays a crucial role; no such stability concerns arise in the FL papers above on supervised learning.\n\u2022Collaborative System Identification.Our paper is related to a growing body of work that seeks\nto leverage data from multiple dynamical systems to achieve statistical gains in estimation accuracy.\nIn this context, several papers (Wang et al., 2023b; Toso et al., 2023; Chen et al., 2023; Modi et al.,\n2024; Rui and Dahleh, 2025; Tupe et al., 2025; Xin et al., 2025) have explored collaborative system\nidentification by combining trajectory data from multiple systems that share structural similarities.\nIn particular, Toso et al. (2023) and Rui and Dahleh (2025) assume a cluster structure like us. While\nthe above papers focus on using collective data for anopen-loopestimation problem, namely system-\nidentification, our work focuses instead on data-efficient closed-loop control by directly learning\npolicies. As such, our notion of heterogeneity captures differences in closed-loop performance across\nclusters as opposed to similarity metrics imposed on open-loop system matrices in the papers above.\n\u2022Meta, Multi-Task, and Transfer Learning in Control.Under the umbrella framework of\nmeta and transfer learning, various recent papers (Wang et al., 2023a; Toso et al., 2024; Aravind et al.,\n2024; Stamouli et al., 2025) have used PO methods to study how information from multiple LTI\nsystems can be aggregated to learn policies that adapt across similar systems. Our formulation, which\nseeks to find apersonalizedpolicy for every system, departs fundamentally from this line of work\nwhich instead aims to learn a common policy for all systems. In this regard, we note that the closely\nrelated papers of Wang et al. (2023a) and Toso et al. (2024) need to assume thatallthe systems are\nsufficiently similar to admit a common stabilizing set. Even under this restrictive assumption, the\nresults in these papers indicate that the sub-optimality gap exhibits an additive heterogeneity-induced\nbias term that might negate the speedups from collaboration. In contrast, our work does not require\na common stabilizing policy to exist for systems across clusters. Furthermore, our personalization\napproach completely eliminates heterogeneity-induced biases. We also note that our approach incurs\na logarithmic (in agents and samples) communication cost as opposed to the linear cost in Wang\n3\nKANAKERIBAJAJVERMAGUPTAMITRA\net al. (2023a). Finally, complementary to our clustering-based approach, ideas from representation\nlearning (Zhang et al., 2023; Guo et al., 2023; Lee et al., 2025) and domain randomization (Fujinami\net al., 2025) have also been recently used to improve data-efficiency in dynamic control tasks.\n2. Problem Formulation\nWe consider a setting with Nagents partitioned into Hdisjoint clusters {M j}j\u2208[H].With each\ncluster j\u2208[H] , we associate a tuple Sj= (A j, Bj, Qj, Rj), comprising a system matrix Aj\u2208Rn\u00d7n,\na control input matrix Bj\u2208Rn\u00d7m, and two positive definite matrices Qj\u2208Rn\u00d7n,Rj\u2208Rm\u00d7mthat\ndefine the LQR cost function for cluster j. Each agent in cluster jinteracts with thesame instance\nof the LQR problem specified by Sj, and aims to find a linear policy of the form ut=\u2212Kx tthat\nminimizes the following infinite-horizon discounted cost:\nCj(K) =E\"\u221eX\nt=0\u03b3t\u0010\nx\u22a4\ntQjxt+u\u22a4\ntRjut\u0011#\nsubject tox t+1=A jxt+B jut+zt,(1)\nwhere x0= 0andxt,ut, and ztare the state, control input (action), and exogenous process noise,\nrespectively, at time t,\u03b3\u2208(0,1) is a discount factor, and Kis a control gain matrix. We make the\nstandard assumption that the pair (Aj, Bj)is controllable for every j\u2208[H] . Following Malik et al.\n(2020), we assume thatz tis sampled independently from a distributionD, such that:\nE[zt] = 0,E[z tz\u22a4\nt] =I,and\u2225z t\u22252\n2\u2264B,\u2200t,(2)\nwhere B >0 is some positive constant. For the LQR problem described in (1), it is well known (Bert-\nsekas, 2015) that the optimal control law is a linear feedback policy of the form ut=\u2212K\u2217\njxt, where\nK\u2217\njis the optimal control gain matrix for cluster j. When Sjis known, each agent in Mjcan obtain\nK\u2217\njby solving the discrete-time algebraic Riccati equation (DARE) (Anderson and Moore, 2007).\nHowever, our interest is in the learning scenario where the system matrices {(Aj, Bj)}j\u2208[H] are\nunknownto the agents. Even in this setting, it is known that policy optimization (PO) algorithms that\ntreat the control gain as the optimization variable converge to the optimal policy (Fazel et al., 2018;\nMalik et al., 2020). The implementation of such algorithms relies on noisytrajectory rolloutsto\ncompute estimates of policy gradients.1Specifically, given Tindependent rollouts from the tuple Sj,\neach agent within Mjcan generate a gain \u02c6Ksuch that with high probability, Cj(\u02c6K)\u2212C j(K\u2217\nj)\u2264\n\u02dcO(1/\u221a\nT)(Malik et al., 2020). Our goal is to investigate whether this sample-complexity bound can\nbe improved by leveraging the cluster structure in our problem.\nTo achieve potential gains in sample-complexity via collaboration, we allow the agents to\ncommunicate via a central server, and make the following assumption that is common in the literature\non collaborative/federated learning (Kone \u02c7cn`y et al., 2016; McMahan et al., 2017).\nAssumption 1 The noise processes across agents are statistically independent, i.e., for all i1, i2\u2208\n[N]such that i1\u0338=i2, the noise stochastic processes {z(i1)\nt}and{z(i2)\nt}are mutually independent.\nHere, with a slight overload of notation, we use {z(i)\nt}to denote the noise process for agent i\u2208[N] .\nAlthough the above assumption suggests that exchange of information between agents can accelerate\nthe learning of an optimal policy, collaboration is complicated by theheterogeneityamong clusters,\ndue to the difference in system dynamics (Aj, Bj)and in task objectives (Qj, Rj). To capture such\n1. The notion of a rollout will be made precise later in this section.\n4\nHARNESSINGDATA FROMCLUSTEREDLQR SYSTEMS\nheterogeneity across clusters, we take inspiration from the notion of \u201ccluster separation gaps\" in\nsupervised learning (Ghosh et al., 2022; Su et al., 2022; Sattler et al., 2020), and introduce\n\u2206:= min\nj1,j2\u2208[H]:j 1\u0338=j2|Cj1(K\u2217\nj1)\u2212C j2(K\u2217\nj2)|.(3)\nWe assume a non-zero separation across clusters, i.e., \u2206>0 . While one can certainly formu-\nlate alternative notions of heterogeneity, we will show (in Theorem 2) that our metric of cluster-\ndissimilarity, as captured by \u2206in(3), can be suitably exploited to separate clusters and learn\npersonalized policies for each cluster. In particular, our results reveal thatheterogeneity can be\nhelpful:a larger value of \u2206leads to faster cluster separation using fewer rollouts. With the above\nideas in place, we are now ready to formally state our problem of interest. For each agent i\u2208[N] ,\nlet us use\u03c3(i)to represent the index of the cluster to which it belongs.\nProblem 1 (Clustered LQR Problem) Let \u03b4\u2208(0,1) be a given failure probability. Suppose\nevery agent i\u2208[N] has access to Tindependent rollouts from its corresponding system S\u03c3(i).\nDevelop an algorithm that returns {\u02c6Ki}i\u2208[N] such that with probability at least 1\u2212\u03b4 , the\nfollowing is true\u2200i\u2208[N]:\nC\u03c3(i)(\u02c6Ki)\u2212C \u03c3(i)(K\u2217\n\u03c3(i))\u2264\u02dcO\uf8eb\n\uf8ed1q\n|M\u03c3(i)|T\uf8f6\n\uf8f8.\nIn simple words, our goal is to come up with an algorithm that generates a personalized control\npolicy for every agent that benefits from the collective information available within that agent\u2019s\ncluster. This is quite non-trivial due to the following technicalchallenges.\n\u2022In our setting, the system dynamics and the cluster identities are both unknown a priori. Thus,\nour problem requires learning the cluster identities and optimal policies simultaneously.\n\u2022The clustering process is complicated by two main issues. First, the information used for\nclustering is based on noisy function evaluations that are insufficient for estimating the system models,\nruling out system-identification-based approaches in Toso et al. (2023) and Rui and Dahleh (2025).\nThus, we need to develop amodel-freeclustering algorithm. Second, the minimum separation gap \u2206\nin(3)is assumed to be unknown, ruling out the possibility of simple one-shot clustering approaches.\n\u2022Unlike supervised learning problems in Ghosh et al. (2020) and Su et al. (2022) where\nmisclustering only introduces a bias due to heterogeneity, the price of misclustering can be more\nsevere in our control setting. In particular, since the system tuples across clusters are allowed to be\narbitrarily different, transfer of information across clusters can lead to destabilizing policies.\nIn the next section, we will develop the PCPO algorithm that addresses the above challenges\nand solves Problem 1, while incurring only a logarithmic (with respect to the number of agents and\nrollouts) communication cost. In preparation for the next section, we now define the notion of a\nrolloutand a zeroth-order gradient estimator. Given a policy K, a rollout for an agent i\u2208 M jyields\nanoisy sampleof the infinite-horizon trajectory cost, defined as:\nCj(K;Z(i)) =\u221eX\nt=0\u03b3t\u0010\nx\u22a4\ntQjxt+u\u22a4\ntRjut\u0011\n,wherex t+1=A jxt+B jut+zt, ut=\u2212Kx t,(4)\n5\nKANAKERIBAJAJVERMAGUPTAMITRA\nx0= 0andZ(i)={z(i)\nt}. We will interpret each rollout as a sample. Using such noisy function\nevaluations for a policy Krun by an agent i\u2208 M j, we define the M-minibatched zeroth-order\ngradient estimator with a smoothing radiusr, as follows (Fazel et al., 2018; Malik et al., 2020):\ngi(K):=1\nMMX\nk=1Cj(K+rU k;Z(i)\nk)\u0012D\nr\u0013\nUk,(5)\nwhere D=mn ,Ukis drawn independently from a uniform distribution over matrices with unit\nFrobenius norm, and Z(i)\nkare independent copies of Z(i)for all k\u2208[M] . In the sequel, for an\nagent i\u2208 M j, we use the shorthand ZOi(K, M, r) to refer to the M-minibatched zeroth-order\ngradient estimator at policy Kwith smoothing radius r, as defined in (5). We use the notation c(p,_)\nto denote problem-parameter-dependent constants and provide their expressions in Appendix A of\nKanakeri et al. (2025). We make the standard assumption (Fazel et al., 2018; Malik et al., 2020)\nthat each agent ihas access to an initial controller K(0)\nithat lies within its respective stabilizing set:\n{K\u2208Rm\u00d7n:\u03c1(A \u03c3(i)\u2212B \u03c3(i)K)<1} , where \u03c1(X) is the spectral radius of a matrix X\u2208Rn\u00d7n.\n3. Description of the Algorithm\nIn this section, we present our proposed algorithm, Personalized and Collaborative Policy Opti-\nmization (PCPO) (Algorithm 1), which effectively addresses Problem 1 by carefully accounting for\nits inherent challenges. Since the cluster separation gap \u2206is unknown, we propose asequential\neliminationstrategy to identify the correct clusters. While the idea of sequential elimination has\nbeen explored in the context of multi-armed bandits (Even-Dar et al., 2006), we show that a similar\napproach can be used to effectively cluster LTI systems that satisfy the heterogeneity metric defined\nin(3). The algorithm proceeds in epochs (indexed by l) of increasing duration, where in each epoch,\neach agent iupdates two sequences: a local sequence, {X(l)\ni}l\u22650, and a global sequence, {\u02c6K(l)\ni}l\u22650.\nThe local sequence is updated using the zeroth-order policy optimization algorithm in Fazel et al.\n(2018); Malik et al. (2020), and is used exclusively for clustering the agents. The global sequence is\nupdated by aggregating the gradient estimates from all the agents within an appropriately defined\nneighborhood set. After each epoch, the neighborhood sets are updated based on the concentration\nof the estimated cost around the optimal cost. As the number of rollouts increases across epochs, this\nconcentration becomes tighter, hence pruning out misclustered agents over successive epochs. The\nvarious components of the algorithm are succinctly captured in Figure 1. In Section 4, we show that\nthe neighborhood sets eventually converge to the correct clusters with high probability, after which\nthe global sequence enjoys the collaborative gains without any heterogeneity bias. In what follows,\nwe elaborate on the rationale behind the design of the various components ofPCPO.\nBuilding intuition.Correctly identifying the agents\u2019 clusters is crucial to reap any potential ben-\nefits from collaboration, as collaborating with agents from a different cluster can lead to destabilizing\npolicies. In this regard, the major difficulty arises from the fact that the cluster separation gap \u2206\nin(3)is unknown a priori. To appreciate the associated challenges, as a thought experiment, let us\nconsider a simpler case where \u2206is known. Under this scenario, each agent can locally run policy\noptimization to obtain a policy in a sufficiently close neighborhood of the optimal policy. Then, each\nagent can evaluate the cost at this policy and ensure that it is concentrated around the optimal cost.\nIf the neighborhood radius, which depends on \u2206, is carefully chosen, it is easy to see that such a\none-shot approach leads to correct clustering. However, when \u2206is unknown, one-shot clustering\nmay no longer work, motivating thesequential clusteringidea in our proposed PCPO Algorithm.\n6\nHARNESSINGDATA FROMCLUSTEREDLQR SYSTEMS\nFigure 1: Illustration of the epoch-based structure of PCPO , where each epoch involves three key\nsteps: local policy optimization (PO), cost estimation, and global PO.\nSequential elimination.In each epoch lofPCPO , for every agent i\u2208[N] , the server maintains\na neighborhood set N(l)\nias an estimate of the true cluster M\u03c3(i). All such neighborhood sets are\ninitialized from the set of all agents and sequentially pruned over epochs. For pruning, we start with\nan initial estimate of \u2206, denoted by \u22060, and update it by halving its value at the beginning of each\nepoch lto obtain \u2206l(Line 3 of Algo. 1). Our goal is to ensure that for all agents, the estimated cost\nin epoch lis in the \u2206l/4neighborhood of its optimal cost. We achieve this in a two-step process,\nwhere we first perform local policy optimization to obtain a policy that is \u2206l/8-suboptimal (Line 4).\nThelocalPO subroutine performs Ml-minibatched policy optimization for Rliterations starting\nwith a controller X(l\u22121)\ni . In every iteration t\u2208[R l],X(i,t), thet-th sub-iterate of localPO , is\nupdated as X(i,t+1) \u2190X (i,t)\u2212\u03b7g i(X(i,t)), where gi(X(i,t)) =ZO i(X(i,t), Ml, r(loc)\nl). Then, we\nestimate the cost at the policy X(l)\niobtained from localPO with an error tolerance of \u2206l/8using\nMlrollouts (Line 5). Having achieved the desired cost-estimation accuracy of \u2206l/4for all agents,\nwe prune the neighborhood sets according to (7)in Line 13. Eventually, as \u2206l\u2264\u2206 , which happens\ninO(log(\u2206 0/\u2206))epochs, correct clustering takes place, as elaborated in the next section.\nLocal and Global Sequences.To motivate the need for maintaining two sequences in PCPO , let\nus again consider the case where \u2206is known, where it would suffice for the agents to only maintain a\nsingle sequence to run local PO until clustering, as discussed earlier in the one-shot clustering scheme.\nAfter the clusters are identified, the same sequence can be used for collaboration. In our setting,\nhowever, although the neighborhood sets eventually converge to the correct clusters in logarithmic\nnumber of epochs with respect to 1/\u2206, the number of such epochs cannot be determined a priori with-\nout knowledge of \u2206, making it difficult to decide when to initiate collaboration. Furthermore, with a\nsingle sequence, collaborating with misclustered agents can lead to an undesirable scenariowhere the\nsequence used to cluster is itself contaminated due to misclustering. PCPO carefully navigates this\ndifficulty by maintaining two sequences of policies at each agent. The local sequence, {X(l)\ni}l\u22650, is\nused purely for clustering, and the global sequence, {\u02c6K(l)\ni}l\u22650, is updated by aggregating gradients\nfrom agents within the neighborhood setN(l\u22121)\ni from the previous epochl\u22121; see (6) in Line 9.\nLogarithmic communication.Since both local and global PO are performed for Rliterations\nwithMlrollouts per iteration, the overall sample complexity per epoch is Tl= 2R lMl+M l, where\nthe additional Mlrollouts are due to the cost estimation step. Each iteration of global PO proceeds as\nfollows. First, for every agent, the server combines the minibatched zeroth-order gradient estimates\nas per (6). Then, the agents update the iterates using the averaged gradient with an appropriately\nchosen but fixed step size \u03b7(Line 10). Since the above essentially incurs O(R l)communication steps\n7\nKANAKERIBAJAJVERMAGUPTAMITRA\nAlgorithm 1Personalized and Collaborative Policy Optimization (PCPO)\n1:Initialization:\u2206 0;\u2200i\u2208[N], \u02c6K(0)\ni\u2190K(0)\ni, X(0)\ni\u2190K(0)\ni,N(0)\ni\u2190[N].\n2:Forl= 1,2, . . . ,\n3:At Each Agenti:\u2206 l\u2190\u2206l\u22121\n2, \u03b4l\u2190\u03b4\n2l2, \u03b7\u2190c (p,1), Rl\u2190c (p,2)log\u0010c(p,3)N\n\u22062\nl\u0011\nMl\u2190c(p,4)\n\u22062\nllog\u0010\n8DNR l\n\u03b4l\u0011\n,\u02dcrl\u2190\u0012\nc(p,5)\u221aMlr\nlog\u0010\n8DNR l\n\u03b4l\u0011\u00131/2\n, r(loc)\nl\u2190min{c (p,6),\u02dcrl}.\n4:Local Policy Optimization:X(l)\ni\u2190localPO(X(l\u22121)\ni, Ml, Rl, r(loc)\nl).\n5:Cost estimation: \u02c6C\u03c3(i)(X(l)\ni)\u21901\nMlPMl\nj=1C\u03c3(i)(X(l)\ni,Z(i)\nj).\n6: Initialize Y(0)\ni\u2190\u02c6K(l\u22121)\niand set r(global)\n(i,l)\u2190min\u001a\nc(p,6),\u02dcrl\n|N(l\u22121)\ni|1/4\u001b\n\u25b7For collaborative PO\n7:Fork= 0,1, . . . , R l\u22121\n8:At Each Agenti:Transmitg i(Y(k)\ni)\u2190ZO i(Y(k)\ni, Ml, r(global)\n(i,l))to the Server.\n9:At Server:Compute and transmit the averaged gradient estimate as follows:\nGi\u21901\n|N(l\u22121)\ni|X\nj\u2208N(l\u22121)\nigj(Y(k)\nj).(6)\n10:At Each Agenti:Y(k+1)\ni\u2190Y(k)\ni\u2212\u03b7G i. \u25b7Global policy update via collaboration\n11:End For\n12:At Each Agenti:Update \u02c6K(l)\ni\u2190Y(Rl)\ni.TransmitX(l)\ni,\u02c6C\u03c3(i)(X(l)\ni)to the server.\n13:At Server:Update the neighborhood set as follows:\u25b7Sequential elimination\nN(l)\ni\u2190 {j\u2208 N(l\u22121)\ni :\f\f\f\u02c6C\u03c3(j)(X(l)\nj)\u2212\u02c6C\u03c3(i)(X(l)\ni)\f\f\f\u2264\u2206 l/2}(7)\n14:IfN(l)\ni\u0338=N(l\u22121)\ni for somei\u2208[N]:\u25b7Reinitialization to ensure stability\n15:For all agentsi\u2208[N], update and transmit \u02c6K(l)\nias follows:\n\u02c6K(l)\ni\u2190argmin\n{X(l)\nj:j\u2208N(l)\ni}{\u02c6C\u03c3(j)(X(l)\nj) :j\u2208 N(l)\ni}.(8)\n16:End For\nper epoch, the total communication complexity of PCPO isO(R l\u00d7Number of Epochs) . Based on\nour choice of parameters, both objects in the above product are logarithmic in the number of agents\nN, the gap1/\u2206, and the number of total rollouts per agent, namelyT.\nNote on reinitialization.Since the server averages gradients in every epoch based on the\nneighborhood set, agents inevitably collaborate across clusters until correct clustering is achieved.\nThis can lead to destabilizing policies in the global sequence. To mitigate this, at the end of each\nepoch, we reinitialize the global policy sequences for all agents whenever any neighborhood set is\nupdated, as specified in (8). In the next section, we establish in Theorem 1 that the neighborhood\nsets eventually converge to the correct clusters and cease to update. Consequently, reinitialization\n8\nHARNESSINGDATA FROMCLUSTEREDLQR SYSTEMS\nensures that the global sequences of agents within the same cluster evolve identically and achieve\ncollaborative gains once the correct clusters are identified.\n4. Main Results\nOur main results concern the two key components of the PCPO algorithm: (i) identifying the\ncorrect clusters via sequential elimination, and (ii) performing collaborative policy optimization with\nlogarithmic communication. The following theorem captures the clustering component.\nTheorem 1 (Clustering with sequential elimination) Define L= min{l\u22081,2, . . .: \u2206 l\u2264\u2206/2}.\nGiven a failure probability \u03b4\u2208(0,1) , with probability at least 1\u2212\u03b4/2 , the following statements\nconcerning the neighborhood sets from thePCPOalgorithm hold for every agenti\u2208[N]:\n1. In every epochl,we haveM \u03c3(i)\u2286 N(l)\ni.\n2. For any epochlsuch thatl\u2265L, we haveM \u03c3(i)=N(l)\ni.\nDiscussion.The key technical contribution of Theorem 1 lies in showing that the sequential\nelimination strategy successfully lets the neighborhood sets converge to the correct clusters with high\nprobability. In particular, we show that the true clusters are included in the neighborhood sets for all\nagents in each epoch, and there exists an epoch Lafter which the neighborhood sets have converged\nto the true clusters and remain fixed for all subsequent epochs (l\u2265L) . Later in this section, we\nprovide a proof sketch and discuss how these claims follow from the local policy optimization and\nthe cost estimation step, relying on our notion of the cluster separation gap as defined in (3). The\nfollowing theorem captures the key result concerning the collaborative optimization part inPCPO.\nTheorem 2 (Collaborative Policy Optimization) Let the failure probability be \u03b4\u2208(0,1) . Define\nL= min{l\u22081,2, . . .: \u2206 l\u2264\u2206/2} and let \u00afLdenote the last epoch. If the number of rollouts per\nagent satisfies T\u2265 \u02dcO(1/\u22062), and Assumption 1 holds, then \u00afL > L and\u02c6K(\u00afL)\nisatisfies the following\nwith probability at least1\u2212\u03b4for every agenti\u2208[N]:\nC\u03c3(i)(\u02c6K(\u00afL)\ni)\u2212C \u03c3(i)(K\u2217\n\u03c3(i))\u2264O\uf8eb\n\uf8edc(p,7)q\nlog\u00008DNT\n\u03b4\u0001\nq\nT|M \u03c3(i)|\uf8f6\n\uf8f8.(9)\nDiscussion.It was shown in Malik et al. (2020) that zeroth-order policy optimization provides a\n\u02dcO(1/\u221a\nT)suboptimal policy using Trollouts for a single system LQR problem. In contrast, Wang\net al. (2023a) showed collaborative gains for a federated LQR setting while incurring additive bias\nterms that depend on the heterogeneity gap. Moreover, the results in Wang et al. (2023a) apply only\nto systems with bounded heterogeneity. Theorem 2 bridges this gap by showing that collaborative\ngains (as evidenced by theq\n|M\u03c3(i)|-factor speedup for each agent iin(9)) can be achieved without\nany additive bias through careful cluster identification and collaboration exclusively within clusters.\nFurthermore, these gains hold for systems that can bearbitrarily different, as long as their optimal\ncosts are separated according to (3).\nCorollary 3 (Logarithmic communication complexity) The PCPO algorithm guarantees a loga-\nrithmic communication complexity with respect to the total number of rollouts T, the number of\nagentsN, and the inverse of the separation gap1/\u2206.\n9\nKANAKERIBAJAJVERMAGUPTAMITRA\nIn the following, we provide proof sketches for both Theorem 1 and Theorem 2, while deferring\nthe detailed proofs to Kanakeri et al. (2025). The statements made in the proof sketches are proba-\nbilistic in nature. However, to keep the exposition simpler, we omit specifying the success/failure\nprobability of the statements and refer the readers to Kanakeri et al. (2025) for such details.\nProof sketch for Theorem 1.We start by showing that the estimated cost for each agent is in the\n\u2206l/4neighborhood of its optimal cost, i.e., in each epochl, for each agenti, we prove that\n|\u02c6C\u03c3(i)(X(l)\ni)\u2212C \u03c3(i)(K\u2217\n\u03c3(i))| \u2264\u2206 l/4.(10)\nNote that for any agent j\u2208 M \u03c3(i), since C\u03c3(i)(K\u2217\n\u03c3(i)) =C \u03c3(j)(K\u2217\n\u03c3(j)), in light of (10), it is\napparent that such an agent will pass the requirement in (7), and hence, never be eliminated from the\nneighborhood sets of agent i. This explains the first claim of Theorem 1. As for the second claim, note\nthat for any j /\u2208 M \u03c3(i), in light of our dissimilarity metric \u2206in(3),|C\u03c3(i)(K\u2217\n\u03c3(i))\u2212C \u03c3(j)(K\u2217\n\u03c3(j))| \u2265\n\u2206.Now for an epoch lsuch that \u2206l\u2264\u2206/2 , the above inequality can be combined with that in (10)\nto see that |\u02c6C\u03c3(i)(X(l)\ni)\u2212\u02c6C\u03c3(j)(X(l)\nj)| \u22653\u2206/4 , violating the requirement for inclusion in (7). It\nremains to establish (10), which follows from two guarantees: (i) |\u02c6C\u03c3(i)(X(l)\ni)\u2212C \u03c3(i)(X(l)\ni)| \u2264\n\u2206l/8,and (ii) |C\u03c3(i)(X(l)\ni)\u2212C \u03c3(i)(K\u2217\n\u03c3(i))| \u2264\u2206 l/8. The second guarantee follows from an analysis\nof the local PO sub-routine in Line 4 of Algo. 1, drawing on Malik et al. (2020); the first follows\nfrom analyzing the cost estimation step in Line 5 based on a simple Hoeffding bound.\nProof sketch for Theorem 2.We prove Theorem 2 by conditioning on the event where the claims\nmade in Theorem 1 hold. As agents collaborate exclusively within their respective clusters and the\nreinitialization step synchronizes their global sequences after correct clustering, the gradient estimate\nobtained by averaging (see (6)) estimates of agents within a cluster enjoys avariance reduction effect\nunder Assumption 1. Using this, and the fact that the LQR cost satisfies a \u03d5-smoothness and \u00b5-PL\ncondition locally (Fazel et al., 2018; Malik et al., 2020), we establish that in each iteration kof the\nfinal epoch \u00afL, the following recursion holds with probability1\u2212\u03b4\u2032:\nSk+1\u2264\u0010\n1\u2212\u03b7\u00b5\n4\u0011\nSk+ 3\u03b7\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8edc2\n(p,8)D2\n(r(global)\n(i,\u00afL))2|M\u03c3(i)|M\u00afLlog\u00122D\n\u03b4\u2032\u0013\n| {z }\ns1+\u03d52(r(global)\n(i,\u00afL))2\n|{z}\ns2\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8,(11)\nwhere Sk:=C \u03c3(i)(Y(k)\ni)\u2212C \u03c3(i)(K\u2217\n\u03c3(i)). The term s1is due to the concentration of the minibatched\ngradient estimate around the gradient of a smoothed cost defined in Appendix A of Kanakeri et al.\n(2025); this is the term that benefits from collaboration. The term s2captures the bias that arises\nwhen estimating gradients from noisy function evaluations. To ensure that this bias term does not\nnegate the collaborative speedup in s1, we choose the smoothing radius r(global)\n(i,\u00afL)to minimize the\nsums1+s2. Unrolling the recursion for Rliterations (with the choice of RlinPCPO ) provides\nthe per epoch convergence with rate \u02dcO\u0010\n1/q\n|M\u03c3(i)|M\u00afL\u0011\n. Finally, using the fact that Mlincreases\nexponentially with epochs, we establish thatM \u00afL=\u02dc\u2126(T).\n5. Conclusion\nWe developed a novel clustering-based approach for learning personalized control policies using data\nfrom heterogeneous dynamical processes. As future work, we will explore (i) alternative measures of\ndissimilarity across systems, (ii) more general dynamical processes, and (iii) online settings.\n10\nHARNESSINGDATA FROMCLUSTEREDLQR SYSTEMS\nReferences\nAlekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy\ngradient methods: Optimality, approximation, and distribution shift.Journal of Machine Learning\nResearch, 22(98):1\u201376, 2021.\nBrian DO Anderson and John B Moore.Optimal control: linear quadratic methods. Courier\nCorporation, 2007.\nAshwin Aravind, Mohammad Taha Toghani, and C\u00e9sar A Uribe. A moreau envelope approach for\nLQR meta-policy estimation. In2024 IEEE 63rd Conference on Decision and Control (CDC),\npages 415\u2013420. IEEE, 2024.\nDimitri P Bertsekas. Dynamic programming and optimal control 4th edition, volume ii.Athena\nScientific, 2015.\nYiting Chen, Ana M Ospina, Fabio Pasqualetti, and Emiliano Dall\u2019Anese. Multi-task system\nidentification of similar linear time-invariant dynamical systems. In2023 62nd IEEE Conference\non Decision and Control (CDC), pages 7342\u20137349. IEEE, 2023.\nEyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and\nstopping conditions for the multi-armed bandit and reinforcement learning problems.Journal of\nmachine learning research, 7(6), 2006.\nMaryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gradient\nmethods for the linear quadratic regulator. InInt. Conf. on Machine Learning, pages 1467\u20131476.\nPMLR, 2018.\nTesshu Fujinami, Bruce D Lee, Nikolai Matni, and George J Pappas. Domain randomization is\nsample efficient for linear quadratic control.arXiv preprint arXiv:2502.12310, 2025.\nAvishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for\nclustered federated learning.Advances in neural information processing systems, 33:19586\u201319597,\n2020.\nAvishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for\nclustered federated learning.IEEE Transactions on Information Theory, 68(12):8076\u20138091, 2022.\nBenjamin Gravell, Peyman Mohajerin Esfahani, and Tyler Summers. Learning optimal controllers\nfor linear systems with multiplicative noise via policy gradient.IEEE Transactions on Automatic\nControl, 66(11):5283\u20135298, 2020.\nTaosha Guo, Abed AlRahman Al Makdah, Vishaal Krishnan, and Fabio Pasqualetti. Imitation and\ntransfer learning for lqg control.IEEE Control Systems Letters, 7:2149\u20132154, 2023.\nBin Hu, Kaiqing Zhang, Na Li, Mehran Mesbahi, Maryam Fazel, and Tamer Ba\u00b8 sar. Toward a\ntheoretical foundation of policy optimization for learning control policies.Annual Review of\nControl, Robotics, and Autonomous Systems, 6(1):123\u2013158, 2023.\n11\nKANAKERIBAJAJVERMAGUPTAMITRA\nChi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. A short note\non concentration inequalities for random vectors with subgaussian norm.arXiv preprint\narXiv:1902.03736, 2019.\nVinay Kanakeri, Shivam Bajaj, Ashwin Verma, Vijay Gupta, and Aritra Mitra. Harnessing data from\nclustered LQR systems: Personalized and collaborative policy optimization.arXiv preprint, 2025.\nJakub Kone \u02c7cn`y, H Brendan McMahan, Felix X Yu, Peter Richt\u00e1rik, Ananda Theertha Suresh, and\nDave Bacon. Federated learning: Strategies for improving communication efficiency.arXiv\npreprint arXiv:1610.05492, 2016.\nBruce D Lee, Leonardo F Toso, Thomas T Zhang, James Anderson, and Nikolai Matni. Regret\nanalysis of multi-task representation learning for linear-quadratic adaptive control. InProceedings\nof the AAAI Conference on Artificial Intelligence, volume 39, pages 18062\u201318070, 2025.\nDhruv Malik, Ashwin Pananjady, Kush Bhatia, Koulik Khamaru, Peter L Bartlett, and Martin J\nWainwright. Derivative-free methods for policy optimization: Guarantees for linear quadratic\nsystems.Journal of Machine Learning Research, 21(21):1\u201351, 2020.\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.\nCommunication-efficient learning of deep networks from decentralized data. InArtificial Intelli-\ngence and Statistics, pages 1273\u20131282. PMLR, 2017.\nAditya Modi, Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Joint\nlearning of linear time-invariant dynamical systems.Automatica, 164:111635, 2024.\nAmirreza Neshaei Moghaddam, Alex Olshevsky, and Bahman Gharesifard. Sample complexity\nof the linear quadratic regulator: A reinforcement learning lens.Journal of Machine Learning\nResearch, 26(151):1\u201350, 2025.\nHesameddin Mohammadi, Armin Zare, Mahdi Soltanolkotabi, and Mihailo R Jovanovi \u00b4c. Global\nexponential convergence of gradient methods over the nonconvex landscape of the linear quadratic\nregulator. In2019 IEEE 58th Conference on Decision and Control (CDC), pages 7474\u20137479.\nIEEE, 2019.\nHesameddin Mohammadi, Mahdi Soltanolkotabi, and Mihailo R Jovanovi \u00b4c. On the linear conver-\ngence of random search for discrete-time LQR.IEEE Control Systems Letters, 5(3):989\u2013994,\n2020.\nMaryann Rui and Munther A Dahleh. Learning clusters of partially observed linear dynamical\nsystems. In2025 American Control Conference (ACC), pages 3545\u20133550. IEEE, 2025.\nFelix Sattler, Klaus-Robert M\u00fcller, and Wojciech Samek. Clustered federated learning: Model-\nagnostic distributed multitask optimization under privacy constraints.IEEE transactions on neural\nnetworks and learning systems, 32(8):3710\u20133722, 2020.\nCharis Stamouli, Leonardo F Toso, Anastasios Tsiamis, George J Pappas, and James Anderson.\nPolicy gradient bounds in multitask LQR.arXiv preprint arXiv:2509.19266, 2025.\n12\nHARNESSINGDATA FROMCLUSTEREDLQR SYSTEMS\nLili Su, Jiaming Xu, and Pengkun Yang. Global convergence of federated learning for mixed\nregression.Advances in Neural Information Processing Systems, 35:29889\u201329902, 2022.\nLeonardo F Toso, Han Wang, and James Anderson. Learning personalized models with clustered\nsystem identification. In2023 62nd IEEE Conference on Decision and Control (CDC), pages\n7162\u20137169. IEEE, 2023.\nLeonardo Felipe Toso, Donglin Zhan, James Anderson, and Han Wang. Meta-learning linear\nquadratic regulators: a policy gradient maml approach for model-free LQR. In6th Annual\nLearning for Dynamics & Control Conference, pages 902\u2013915. PMLR, 2024.\nOmkar Tupe, Max Hartman, Lav R Varshney, and Saurav Prakash. Federated nonlinear system\nidentification.arXiv preprint arXiv:2508.15025, 2025.\nHan Wang, Leonardo F Toso, Aritra Mitra, and James Anderson. Model-free learning with het-\nerogeneous dynamical systems: A federated LQR approach.arXiv preprint arXiv:2308.11743,\n2023a.\nHan Wang, Leonardo Felipe Toso, and James Anderson. Fedsysid: A federated approach to\nsample-efficient system identification. InLearning for Dynamics and Control Conference, pages\n1308\u20131320. PMLR, 2023b.\nLei Xin, Lintao Ye, George Chiu, and Shreyas Sundaram. Learning dynamical systems by leveraging\ndata from similar systems.IEEE Transactions on Automatic Control, 2025.\nKaiqing Zhang, Bin Hu, and Tamer Basar. Policy optimization for h2linear control with h\u221e\nrobustness guarantee: Implicit regularization and global convergence.SIAM Journal on Control\nand Optimization, 59(6):4081\u20134109, 2021.\nThomas T Zhang, Katie Kang, Bruce D Lee, Claire Tomlin, Sergey Levine, Stephen Tu, and Nikolai\nMatni. Multi-task imitation learning for linear dynamical systems. InLearning for Dynamics and\nControl Conference, pages 586\u2013599. PMLR, 2023.\n13\nKANAKERIBAJAJVERMAGUPTAMITRA\nAppendix A. Properties of the LQR problem\nNotation.For matrices A\u2208Rm\u00d7nandB\u2208Rm\u00d7n, we use \u2225A\u2225 to denote the Frobenius norm of\nAwhich is defined as \u2225A\u2225=p\ntrace(A\u22a4A). We use \u27e8A, B\u27e9 to denote the Frobenius inner-product\ndefined as\u27e8A, B\u27e9=trace(A\u22a4B).\nIn this section, we discuss some of the properties of the LQR cost that were established in Fazel\net al. (2018); Malik et al. (2020). In particular, the LQR cost in (1)is locally Lipschitz, locally\nsmooth, and enjoys a gradient-domination property over the set of stabilizing controllers. These key\nproperties aid in the convergence analysis of the model-free policy gradient algorithm, and we use\nthem in the proofs of Theorem 1 in Appendix B and Theorem 2 in Appendix C. However, to show\nthe convergence, it is crucial to ensure that the policy gradient iterates always lie within a restricted\nsubset of the stabilizing set with high probability. In Malik et al. (2020), such a restricted set is\nchosen based on the initial suboptimality gap. In our setting, given access to a set of initial stabilizing\ncontrollers for all agents, {K(0)\ni}i\u2208[N] , and our initial guess for the cluster separation gap, \u22060, we\ndefine \u02dc\u22060:= max{max i\u2208[N](C\u03c3(i)(K(0)\ni)\u2212C \u03c3(i)(K\u2217\n\u03c3(i))),\u2206 0}, and the restricted sets as follows\nfor allj\u2208[H]:\nG0\nj:={K\u2208Rm\u00d7n:Cj(K)\u2212C j(K\u2217\nj)\u226410 \u02dc\u22060}.(12)\nProperties of the LQR cost.For each system j\u2208[H] , on the restricted domain G0\nj, Malik et al.\n(2020) showed that the local properties hold uniformly, i.e., \u2203\u03d5j>0, \u03bb j>0, \u03c1 j>0, such that the\nLQR cost in (1)is(\u03bbj, \u03c1j)-locally Lipschitz and (\u03d5j, \u03c1j)-locally smooth for all policies K\u2208 G0\nj.\nFurthermore, it is known that the LQR cost satisfies the PL (gradient-domination) condition for all\npolicies in the stabilizing set (see Lemma 3 of Malik et al. (2020)). Denoting the parameter for the\nPL condition for system jby\u00b5j>0, we define \u00b5:= min{\u00b5 1, \u00b52, . . . , \u00b5 H}. Similarly, defining\n\u03d5:= max{\u03d5 1, \u03d52, . . . , \u03d5 H},\u03bb:= max{\u03bb 1, \u03bb2, . . . , \u03bb H}, and \u03c1:= min{\u03c1 1, \u03c12, . . . , \u03c1 H}, we can\nensure that for every system j\u2208[H] , the cost in (1)is(\u03bb, \u03c1) -locally Lipschitz and (\u03d5, \u03c1) -locally\nsmooth for all policies in their respective restricted sets G0\nj, and \u00b5-PL in their respective stabilizing\nsets. The following lemmas from Malik et al. (2020) capture these properties.\nLemma 4 (LQR cost is locally Lipschitz). For any system j\u2208[H] , given a pair of policies\n(K, K\u2032)\u2208(G0\nj\u00d7 G0\nj), if\u2225K\u2212K\u2032\u2225 \u2264\u03c1, we have\n\f\fCj(K)\u2212C j(K\u2032)\f\f\u2264\u03bb\u2225K\u2212K\u2032\u2225.(13)\nLemma 5 (LQR cost has locally Lipschitz gradients.) For any system j\u2208[H] , given a pair of\npolicies(K, K\u2032)\u2208(G0\nj\u00d7 G0\nj), if\u2225K\u2212K\u2032\u2225 \u2264\u03c1, we have\n\r\r\u2207Cj(K)\u2212 \u2207C j(K\u2032)\r\r\u2264\u03d5\u2225K\u2212K\u2032\u2225.(14)\nLemma 6(LQR cost satisfies PL.) For any systemj\u2208[H], given a stable policyK, we have\n\u2225\u2207C j(K)\u22252\u2265\u00b5(C j(K)\u2212C j(K\u2217\nj)).(15)\nSmoothed cost and the properties of the gradient estimate.The smoothed cost with a radius\nrfor a system j\u2208[H] is defined as Cj,r(K):=E[C j(K+rv)] , where vis uniformly distributed\nover all matrices in Rm\u00d7nwith the Frobenius norm of at most 1. It is shown in Fazel et al. (2018);\nMalik et al. (2020) that the zeroth-order gradient estimate gi(\u00b7)as defined in (5)for an agent iis an\n14\nHARNESSINGDATA FROMCLUSTEREDLQR SYSTEMS\nunbiased estimator of the gradient of the smoothed cost. In particular, for all systems j\u2208[H] and all\nagentsi\u2208 M j, we have the following properties for allK\u2208 G0\njandr\u2208(0, \u03c1):\nE[gi(K)] =\u2207C \u03c3(i),r(K)(16)\n\u2225\u2207C j,r(K)\u2212 \u2207C j(K)\u2225 \u2264\u03d5r.(17)\nFurthermore, it is known that the noisy rollout cost Cj(K+rU;Z(i))is uniformly bounded\n\u2200K\u2208 G0\nj,\u2200r\u2208(0, \u03c1) and all U\u2208Rm\u00d7nwith\u2225U\u2225= 1 . In other words, there exists G(j)\n\u221e\u22650such\nthatCj(K+rU;Z(i))\u2264G(j)\n\u221e(see Lemma 11 of Malik et al. (2020) for the expression of G(j)\n\u221e.) Let\nus define G\u221e:= max{G(1)\n\u221e, G(2)\n\u221e, . . . , G(H)\n\u221e}. Hence, for any agent i\u2208[N] , we have the following\n\u2200K\u2208 G0\nj,\u2200r\u2208(0, \u03c1)and allU\u2208Rm\u00d7nwith\u2225U\u2225= 1:\nC\u03c3(i)(K+rU;Z(i))\u2264G \u221e.(18)\nWe then have the following concentration result that will prove useful in establishing \u201cvariance-\nreduction\" effects.\nLemma 7 (Concentration of the zeroth-order gradient estimates). For any system j\u2208[H] , given\na policy K\u2208 G0\nj, a smoothing radius r\u2208(0, \u03c1) and a failure probability \u03b4\u2032\u2208(0,1) , the following\nholds for the M-minibatched zeroth-order gradient estimate of an agent i\u2208 M jwith probability at\nleast1\u2212\u03b4\u2032:\n\u2225gi(K)\u2212 \u2207C j,r(K)\u2225 \u2264\u0010\nG\u221e+\u03bb\u03c1\nD+\u03d5\u03c12\nD\u0011\nD\nr\u221a\nMs\nlog\u00122D\n\u03b4\u2032\u0013\n.(19)\nProofWe have \u2225gi(K)\u2212 \u2207C j,r(K)\u2225=\r\r\r1\nMPM\nk=1(g(i,k)(K)\u2212 \u2207C j,r(K))\r\r\r, where we denoted\nthek-th component of the minibatch as g(i,k)(K). Recall from (5)that this k-th component takes the\nform\ng(i,k)(K) =C j(K+rU k;Z(i)\nk)\u0012D\nr\u0013\nUk.\nHence, we have \u2225g(i,k)(K)\u2225 \u2264D\nrG\u221edue to (18)as\u2225Uk\u2225= 1 . Let us define c(p,8):=\u0010\nG\u221e+\u03bb\u03c1\nD+\u03d5\u03c12\nD\u0011\n.\nWe then have\n\u2225g(i,k)(K)\u2212 \u2207C j,r(K)\u2225=\u2225g (i,k)(K)\u2212 \u2207C j,r(K) +\u2207C j(K)\u2212 \u2207C j(K)\u2225\n(a)\n\u2264 \u2225g (i,k)(K)\u2225+\u2225\u2207C j,r(K)\u2212 \u2207C j(K)\u2225+\u2225\u2207C j(K)\u2225\n(b)\n\u2264D\nrG\u221e+\u03d5r+\u03bb\n=D\nr\u0012\nG\u221e+r2\nD\u03d5+r\nD\u03bb\u0013\n(c)\n\u2264D\nr\u0012\nG\u221e+\u03c12\nD\u03d5+\u03c1\nD\u03bb\u0013\n=D\nrc(p,8).\n15\nKANAKERIBAJAJVERMAGUPTAMITRA\nTable 1: Relevant notation and definitions\nNotation Definition\nMl Minibatch size used to estimate zeroth-order gradients in thel-th epoch.\nRl Number of steps/iterations of the local and global policy optimization in thel-th epoch.\n\u03b7Step size for both local and global policy optimization.\nrl Smoothing radius used in the zeroth-order gradient estimates in thel-th epoch.\n\u2206l Estimate of\u2206used to cluster the agents in thelth epoch.\nN(l)\ni Neighborhood set corresponding to thei-th agent in thel-th epoch.\nX(l)\ni Local policy for thei-th agent in thel-th epoch.\n\u02c6K(l)\ni Global policy for thei-th agent in thel-th epoch.\nIn the above, (a)follows from the triangle inequality, and (b)follows from the uniform-boundedness\nof the noisy rollout together with (5), the bounded bias property as shown in (17), and the local-\nLipschitz property in Lemma 4. Finally, (c)follows from using r < \u03c1 . Hence, g(i,k)(K)\u2212\u2207C j,r(K)\nhas a bounded norm and therefore belongs to a class of norm sub-Gaussian random matrices (Jin\net al., 2019). Furthermore, it has zero mean due to (16). Therefore, the concentration result follows\nfrom a direct application of Corollary 7 from Jin et al. (2019) which provides a Hoeffding-type\ninequality for norm sub-Gaussian random matrices.\nCorollary 8 (Concentration of the collaborative zeroth-order gradient estimates.) Suppose As-\nsumption 1 holds. For any system j\u2208[H] , given a policy K\u2208 G0\nj, a smoothing radius r\u2208(0, \u03c1) ,\ndefine the collaborative zeroth-order gradient estimate as Gj(K) =1\n|Mj|P\ni\u2208M jgi(K), where\ngi(K) is the M-minibatched gradient estimate from agent i\u2208 M j. Let\u03b4\u2032\u2208(0,1) . The following\nholds with probability at least1\u2212\u03b4\u2032:\n\u2225Gj(K)\u2212 \u2207C j,r(K)\u2225 \u2264\u0010\nG\u221e+\u03bb\u03c1\nD+\u03d5\u03c12\nD\u0011\nD\nrp\n|Mj|Ms\nlog\u00122D\n\u03b4\u2032\u0013\n.(20)\nProofUnder Assumption 1, we note that the noise processes for all agents in Mjare independent.\nThe proof then follows from Lemma 7 as the collaborative zeroth-order gradient estimate can be\ninterpreted as a gradient estimate for systemjwith a|M j|-fold increased minibatch size.\nNote on the problem dependent constants.In Malik et al. (2020), the values of the constants\n\u03bbj, \u03d5j, \u03c1j, G(j)\n\u221eare first derived locally in terms of the local cost Cj(K), and then, the global\nparameters are obtained by noting that the local cost is uniformly bounded over the restricted domain\nas shown in Lemma 9 of Malik et al. (2020). Since we have a different definition of the restricted\ndomain, the values of our parameters vary from the ones provided in Malik et al. (2020). That said,\nthe global parameters in our setting can be derived exactly in the same way as in Malik et al. (2020)\nby bounding the local cost asC j(K)\u226410 \u02dc\u22060+C j(K\u2217\nj).\nFor convenience, we compile all the relevant notation in Table 1.\nIn the main text, we used the notation c(p,_)to denote the problem-parameter-dependent constants\nwhich are defined in the following.\n16\nHARNESSINGDATA FROMCLUSTEREDLQR SYSTEMS\nc(p,8)=\u0012\nG\u221e+\u03bb\u03c1\nD+\u03d5\u03c12\nD\u0013\nc(p,9)=12c(p,8)\n\u00b5\u0012\nmax\u001ap\n\u03d5,1\n\u03c1\u001b\u00132\nc(p,10) = maxn\n\u22062\n0,256c2\n(p,9)D2, c2\n(p,8)D2\u22062\n0,36G2\n\u221eo\nc(p,11) = \n\u22062\n0\nc(p,10) log\u00008DN\n\u03b4\u0001!\nc(p,12) =4\n\u03b7\u00b5 \nlog \nc(p,10)N\u02dc\u22062\n0\n\u22062\n0!\n+ log(4)!\nc(p,13) = 4 max{1, c (p,9)}q\nc(p,12) log(c (p,11)T)\nc(p,1)= min\uf8f1\n\uf8f2\n\uf8f38\n\u00b5,1\n4\u03d5,\u03c1\n\u03bb+ 2 maxn\u221a\u03d5,1\n\u03c1o\uf8fc\n\uf8fd\n\uf8fe\nc(p,2)=4\n\u03b7\u00b5\nc(p,3)=\u02dc\u22060max{16,10c (p,10)}\nc(p,4)=c(p,10)\nc(p,5)=c(p,8)D\n\u03d5\nc(p,6)=\u03c1\nc(p,7)=Dc (p,13)(21)\nBased on the above definitions of the problem-parameter-dependent constants, we provide the\nvalues used for the hyperparameters in thel-th epoch of thePCPOalgorithm in Table 2.\n17\nKANAKERIBAJAJVERMAGUPTAMITRA\nTable 2: Hyperparameters with their values in thelth epoch\nHyperparameters Values\n\u2206l\u22060\n2l\n\u03b4l\u03b4\n2l2\n\u03b7 c(p,1)\nRl c(p,2)log\u0010c(p,3)N\n\u22062\nl\u0011\nMlc(p,4)\n\u22062\nllog\u0010\n8DNR l\n\u03b4l\u0011\n\u02dcrl\u0012\nc(p,5)\u221aMlr\nlog\u0010\n8DNR l\n\u03b4l\u0011\u00131/2\nr(loc)\nlmin{c (p,6),\u02dcrl}\nr(global)\nlmin\u001a\nc(p,6),\u02dcrl\n|N(l\u22121)\ni|1/4\u001b\nAppendix B. Proof of Theorem 1\nIn this section, we provide the proof of Theorem 1 which concerns the clustering aspect of the PCPO\nalgorithm. In particular, we show that, with high probability, the true clusters are included in the\nneighborhood sets for all agents in each epoch, and if epoch l\u2265L= min{l\u22081,2, . . .: \u2206 l\u2264\u2206/2} ,\nthe neighborhood sets are identical to the clusters. More specifically, we show that for all agents\ni\u2208[N] , with probability at least 1\u2212\u03b4/2 ,M\u03c3(i)\u2286 N(l)\niin every epoch l, and if l\u2265L , then\nM\u03c3(i)=N(l)\ni.\nTo prove both claims, it suffices to show that with high probability, the estimated cost at a locally\noptimized policy is concentrated in the \u2206l/4-neighborhood of the optimal cost in every epoch lfor\nall agents i\u2208[N] . To see this, consider a \u201cgood\u201d event that occurs with probability 1\u2212\u03b4/2 where\nthe following holds for all agents i\u2208[N] in every epoch l(we will prove that such an event exists\nlater in this section):\n|\u02c6C\u03c3(i)(X(l)\ni)\u2212C \u03c3(i)(K\u2217\n\u03c3(i))| \u2264\u2206 l/4.(22)\nOn this \u201cgood\u201d event, in what follows, we show that the first claim of Theorem 1 holds. Accordingly,\nfix an agent iand consider an agent j\u2208 M \u03c3(i). We now show by induction that jbelongs to N(l)\ni\nin every epoch l. For the base case of induction, note that since we initialize the neighborhood sets\nwith all agents, j\u2208 N(0)\ni. Next, for an epoch l\u22121\u22651 , let us assume that j\u2208 N(l\u22121)\ni . Since\nC\u03c3(i)(K\u2217\n\u03c3(i)) =C \u03c3(j)(K\u2217\n\u03c3(j))as a consequence of j\u2208 M \u03c3(i), in the l-th epoch under the \u201cgood\u201d\n18\nHARNESSINGDATA FROMCLUSTEREDLQR SYSTEMS\nevent where (22) holds, we have\n|\u02c6C\u03c3(i)(X(l)\ni)\u2212\u02c6C\u03c3(j)(X(l)\nj)| \u2264 | \u02c6C\u03c3(i)(X(l)\ni)\u2212C \u03c3(i)(K\u2217\n\u03c3(i))|+| \u02c6C\u03c3(j)(X(l)\nj)\u2212C \u03c3(j)(K\u2217\n\u03c3(j))|\n\u2264\u2206 l/4 + \u2206 l/4 = \u2206 l/2,\nimplying that j\u2208 N(l)\nibased on the neighborhood set update rule in (7). Hence, by induction,\nj\u2208 N(l)\niin every epoch, therefore establishing the first claim of Theorem 1.\nNext, we show that on the \u201cgood\u201d event where (22) holds for all agents in every epoch, the\nsecond claim of Theorem 1 is also true. We prove this claim via contradiction. To proceed, suppose\nthat there exist an epoch l\u2265L , an agent i, and an agent j /\u2208 M \u03c3(i)such that j\u2208 N(l)\ni. Then, we\nhave the following in light of the heterogeneity metric defined in (3):\n\u2206\u2264\f\f\fC\u03c3(i)(K\u2217\n\u03c3(i))\u2212C \u03c3(j)(K\u2217\n\u03c3(j))\f\f\f\n\u2264\f\f\fC\u03c3(i)(K\u2217\n\u03c3(i))\u2212\u02c6C\u03c3(i)(X(l)\ni) +\u02c6C\u03c3(j)(X(l)\nj)\u2212C \u03c3(j)(K\u2217\n\u03c3(j)) +\u02c6C\u03c3(i)(X(l)\ni)\u2212\u02c6C\u03c3(j)(X(l)\nj)\f\f\f\n\u2264\f\f\fC\u03c3(i)(K\u2217\n\u03c3(i))\u2212\u02c6C\u03c3(i)(X(l)\ni)\f\f\f+\f\f\f\u02c6C\u03c3(j)(X(l)\nj)\u2212C \u03c3(j)(K\u2217\n\u03c3(j))\f\f\f+\f\f\f\u02c6C\u03c3(i)(X(l)\ni)\u2212\u02c6C\u03c3(j)(X(l)\nj)\f\f\f\n(a)\n\u2264\u2206 l/4 + \u2206 l/4 +\f\f\f\u02c6C\u03c3(i)(X(l)\ni)\u2212\u02c6C\u03c3(j)(X(l)\nj)\f\f\f\n(b)\n\u2264\u2206/4 +\f\f\f\u02c6C\u03c3(i)(X(l)\ni)\u2212\u02c6C\u03c3(j)(X(l)\nj)\f\f\f,\nwhere (a)holds due to (22), and (b)follows as \u2206l\u2264\u2206/2 since l\u2265L . The above set of inequalities\nimply that\f\f\f\u02c6C\u03c3(i)(X(l)\ni)\u2212\u02c6C\u03c3(j)(X(l)\nj)\f\f\f\u2265(3/4)\u2206\u2265(3/2)\u2206 l, contradicting our assumption that\nj\u2208 N(l)\niwhich requires that\f\f\f\u02c6C\u03c3(i)(X(l)\ni)\u2212\u02c6C\u03c3(j)(X(l)\nj)\f\f\f\u2264\u2206 l/2. Therefore, N(l)\ni=M \u03c3(i)for all\nl\u2265L, establishing the second claim of Theorem 1.\nNow, it remains to prove that the \u201cgood\u201d event where (22) holds for all agents in every epoch\noccurs with probability at least1\u2212\u03b4/2. To do so, for an agenti\u2208[N]in epochl, we have\n\f\f\f\u02c6C\u03c3(i)(X(l)\ni)\u2212C \u03c3(i)(K\u2217\n\u03c3(i))\f\f\f\u2264\f\f\fC\u03c3(i)(X(l)\ni)\u2212C \u03c3(i)(K\u2217\n\u03c3(i))\f\f\f+\f\f\f\u02c6C\u03c3(i)(X(l)\ni)\u2212C \u03c3(i)(X(l)\ni)\f\f\f.\nTherefore, to show (22), it suffices to show the following two guarantees for all agents in every\nepoch:\n(a)\f\f\fC\u03c3(i)(X(l)\ni)\u2212C \u03c3(i)(K\u2217\n\u03c3(i))\f\f\f\u2264\u2206 l/8\n(b)\f\f\f\u02c6C\u03c3(i)(X(l)\ni)\u2212C \u03c3(i)(X(l)\ni)\f\f\f\u2264\u2206 l/8.(23)\nNow, let us establish the claims in (23) via induction across epochs. Let us assume that for a fixed\nepoch l\u22121\u22651 , the claims in (23) hold for all agents i\u2208[N] in every epoch k\u2264 {1,2, . . . , l\u22121} ,\nwith probability at least (1\u2212Pl\u22121\nj=1\u03b4j\n2). Denoting this event as El\u22121, in the following, we show that\nthe claims in (23) hold in epoch l,\u2200i\u2208[N] with probability at least (1\u2212\u03b4l\n2)conditioned on the\neventE l\u22121.\nIn the following, fixing an agent i\u2208[N] , we show: (a)\f\f\fC\u03c3(i)(X(l)\ni)\u2212C \u03c3(i)(K\u2217\n\u03c3(i))\f\f\f\u2264\u2206 l/8\nwith probability at least 1\u2212\u03b4 l/(4N) conditioned on the event El\u22121, and (b)\f\f\f\u02c6C\u03c3(i)(X(l)\ni)\u2212C \u03c3(i)(X(l)\ni)\f\f\f\u2264\n19\nKANAKERIBAJAJVERMAGUPTAMITRA\n\u2206l/8with probability at least 1\u2212\u03b4 l/(4N) conditioned on the intersection of the events El\u22121and the\none where item (a) in (23) holds. The following lemma provides the convergence of the local policy\noptimization sub-routine in epochlwhich aids in establishing claim (a) from (23).\nLemma 9 (Local Policy Optimization.) For any agent i\u2208 M j, given a policy K0\u2208 G0\nj, letKRbe\nthe output of the localPO(K 0, M, R, r) subroutine with step size \u03b7. Then, for any \u03b4\u2032\u2208(0,1/R) ,\nwith probability at least1\u2212\u03b4\u2032R,K R\u2208 G0\njand we have the following:\nCj(KR)\u2212C j(K\u2217\nj)\u2264\u0010\n1\u2212\u03b7\u00b5\n4\u0011R\n(Cj(K0)\u2212C j(K\u2217\nj)) + \nc(p,9)D\u221a\nMs\nlog\u00122D\n\u03b4\u2032\u0013!\n,(24)\nwhen\u03b7=c (p,1), M\u2265c(p,4)\n\u22062\n0log(2D/\u03b4\u2032), r= min{\u03c1,\u02dcr}, where\u02dcr=\u0012\nc(p,5)\u221a\nMq\nlog\u00002D\n\u03b4\u2032\u0001\u00131/2\n.\nThe proof of Lemma 9 is provided in Appendix B.1. We use Lemma 9 to analyze the local policy\noptimization step in line 4 of the PCPO algorithm that helps in establishing\f\f\fC\u03c3(i)(X(l)\ni)\u2212C \u03c3(i)(K\u2217\n\u03c3(i))\f\f\f\u2264\n\u2206l/8with probability at least 1\u2212\u03b4 l/(4N) . More precisely, the settings for the hyperparameters\n(\u03b7, M l, r(loc)\nl, Rl)from Table 2 meet the requirement for the corresponding hyperparameters in\nLemma 9. Furthermore, conditioned on the event El\u22121, claim (a)in(23) implies that X(l\u22121)\ni\u2208 G0\n\u03c3(i).\nHence, the following holds due to Lemma 9 with probability at least 1\u2212\u03b4\u2032Rlfor some \u03b4\u2032\u2208(0,1/R l):\nC\u03c3(i)(X(l)\ni)\u2212C \u03c3(i)(K\u2217\n\u03c3(i))\u2264\u0010\n1\u2212\u03b7\u00b5\n4\u0011Rl(C\u03c3(i)(X(l\u22121)\ni)\u2212C \u03c3(i)(K\u2217\n\u03c3(i)))\n| {z }\ns1+ \nc(p,9)D\u221aMls\nlog\u00122D\n\u03b4\u2032\u0013!\n| {z }\ns2.\nNote that (C\u03c3(i)(X(l\u22121)\ni)\u2212C \u03c3(i)(K\u2217\n\u03c3(i)))\u2264\u2206 l\u22121/8\u2264\u2206 0\u2264\u02dc\u22060as a result of conditioning on the\neventEl\u22121where claim (a)of(23) holds. Hence, from Table 2, setting Rl=c(p,2)log\u0010c(p,3)N\n\u22062\nl\u0011\n\u2265\n4\n\u03b7\u00b5log\u0010\n16\u02dc\u22060\n\u2206l\u0011\nensures s1\u2264\u2206 l/16. Similarly, setting Ml=c(p,4)\n\u22062\nllog\u00002D\n\u03b4\u2032\u0001\n\u2265162c2\n(p,9)D2\n\u22062\nllog\u00002D\n\u03b4\u2032\u0001\nensures s2\u2264\u2206 l/16. Finally, setting \u03b4\u2032=\u03b4l/(4NR l)provides us with C\u03c3(i)(X(l)\ni)\u2212C \u03c3(i)(K\u2217\n\u03c3(i))\u2264\n\u2206l/8, and hence X(l)\ni\u2208 G0\n\u03c3(i)with probability at least (1\u2212\u03b4 l/(4N)) , based on Lemma 9. Let us\ndenote this event by \u02dcE(l,1).\nNow, we show that\f\f\f\u02c6C\u03c3(i)(X(l)\ni)\u2212C \u03c3(i)(X(l)\ni)\f\f\f\u2264\u2206 l/8with probability at least 1\u2212\u03b4 l/(4N)\nconditioned on the event \u02dcE(l,1). We have \u02c6C\u03c3(i)(X(l)\ni) =1\nMlPMl\nj=1C\u03c3(i)(X(l)\ni,Z(i)\nj)andE[C\u03c3(i)(X(l)\ni,Z(i)\nj)] =\nC\u03c3(i)(X(l)\ni). Furthermore, since on event \u02dcE(l,1),X(l)\ni\u2208 G0\nj, we have C\u03c3(i)(X(l)\ni,Z(i)\nj)\u2264G \u221edue to\n(18). Using Hoeffding\u2019s inequality, the following holds for alls\u22650:\nP\uf8eb\n\uf8ed\f\f\f\f\f\f1\nMlMlX\nj=1C\u03c3(i)(X(l)\ni,Z(i)\nj)\u2212C \u03c3(i)(X(l)\ni)\f\f\f\f\f\f\u2265s\uf8f6\n\uf8f8\u22642 exp\u0012\u22122s2Ml\nG2\u221e\u0013\n.\nSetting s= \u2206 l/8, and requiring the failure probability on the R.H.S to be lesser than \u03b4l/(4N) leads\nto the requirement: Ml\u226536G2\n\u221e\n\u22062\nllog\u0010\n8N\n\u03b4l\u0011\nwhich is satisfied by our choice of Mlfrom Table 2.\n20\nHARNESSINGDATA FROMCLUSTEREDLQR SYSTEMS\nHence,\f\f\f\u02c6C\u03c3(i)(X(l)\ni)\u2212C \u03c3(i)(X(l)\ni)\f\f\f\u2264\u2206 l/8with probability at least 1\u2212\u03b4 l/(4N) . Let us denote\nthis event as \u02dcE(l,2).\nNow, to show the two claims in (23), let us define an event \u02dcEl=\u02dcE(l,1)\u2229\u02dcE(l,2). Then,\nP(\u02dcEl|El\u22121) =P( \u02dcE(l,2)|\u02dcE(l,1),El\u22121)P(\u02dcE(l,1)|El\u22121)\u2265(1\u2212\u03b4 l/(4N))(1\u2212\u03b4 l/(4N))\u22651\u2212\u03b4 l/(2N) .\nTherefore, on event \u02dcEl, both the guarantees: (a)\f\f\fC\u03c3(i)(X(l)\ni)\u2212C \u03c3(i)(K\u2217\n\u03c3(i))\f\f\f\u2264\u2206 l/8and (b)\f\f\f\u02c6C\u03c3(i)(X(l)\ni)\u2212C \u03c3(i)(X(l)\ni)\f\f\f\u2264\u2206 l/8hold with probability at least 1\u2212\u03b4 l/(2N) . Union bounding\nover all the agents, with probability at least 1\u2212\u03b4 l/2, both claims in (23) hold for all agents i\u2208[N]\non the event \u02dcElafter conditioning on the eventE l\u22121.\nFinally, defining an eventE l=\u02dcEl\u2229 El\u22121, we have,\nP(El) =P( \u02dcEl|El\u22121)P(E l\u22121)\u2265(1\u2212\u03b4 l/2)\uf8eb\n\uf8ed1\u2212l\u22121X\nj=1\u03b4j\n2\uf8f6\n\uf8f8\u2265\uf8eb\n\uf8ed1\u2212lX\nj=1\u03b4j\n2\uf8f6\n\uf8f8.\nSince\u03b4 l=\u03b4/(2l2), we havePl\nj=1\u03b4j\n2=Pl\nj=1\u03b4\n4j2\u2264\u03b4/2. This completes the proof of Theorem 1.\nB.1. Proof of Lemma 9\nIn this section, we prove Lemma 9 which provides the convergence of the localPO subroutine. Fix\na system j\u2208[H] and let Ktdenote the controller in the t-th iteration of localPO\u2200t= 0,1, . . . , R .\nNote that the localPO sub-routine proceeds as follows: starting with a controller K0\u2208 G0\nj, in\nevery iteration t,Ktis updated as Kt+1=K t\u2212\u03b7g(K t), where g(K t) =ZO(K t, M, r) is the\nM-minibatched zeroth-order gradient estimate with a smoothing radius r. We prove the statement\nvia induction. Given the base case K0\u2208 G0\njand\u03b4\u2032\u2208(0,1/R) , let us assume that in the t-th iteration\nthe following holds for all\u03c4\u2208 {1,2, . . . , t}with probability at least(1\u2212\u03b4\u2032t) :\nK\u03c4\u2208 G0\nj\nCj(K\u03c4)\u2212C j(K\u2217\nj)\u2264\u0010\n1\u2212\u03b7\u00b5\n4\u0011\n(Cj(K\u03c4\u22121)\u2212C j(K\u2217\nj)) +\u03b7\u00b5\n4 \nc(p,9)D\u221a\nMs\nlog\u00122D\n\u03b4\u2032\u0013!\n.(25)\nLet us denote the event where both the claims in (25) hold by Et. Now, conditioned on the event Et,\nin the following, we will show that with probability at least1\u2212\u03b4\u2032,Kt+1\u2208 G0\njand\nCj(Kt+1)\u2212C j(K\u2217\nj)\u2264\u0010\n1\u2212\u03b7\u00b5\n4\u0011\n(Cj(Kt)\u2212C j(K\u2217\nj)) +\u03b7\u00b5\n4 \nc(p,9)D\u221a\nMs\nlog\u00122D\n\u03b4\u2032\u0013!\n.\nIn what follows, we omit the subscript notation jfor convenience. Conditioned on the event Et,\nwe begin by analyzing the one-step progress in the(t+ 1)-th iteration oflocalPO.\nFrom Lemma 7, as the event Etensures that Kt\u2208 G0, we have \u2225g(K t)\u2212 \u2207C r(Kt)\u2225 \u2264\nc(p,8)D\nr\u221a\nMq\nlog\u00002D\n\u03b4\u2032\u0001\nwith probability at least (1\u2212\u03b4\u2032). Let us denote this event by \u02dcEt. Define et:=\n21\nKANAKERIBAJAJVERMAGUPTAMITRA\ng(K t)\u2212 \u2207C(K t). Conditioned on the event \u02dcEt\u2229Et, we have\n\u2225et\u2225=\u2225g(K t)\u2212 \u2207C r(Kt) +\u2207C r(Kt)\u2212 \u2207C(K t)\u2225\n(a)\n\u2264 \u2225g(K t)\u2212 \u2207C r(Kt)\u2225+\u2225\u2207C r(Kt)\u2212 \u2207C(K t)\u2225\n(b)\n\u2264c(p,8)D\nr\u221a\nMs\nlog\u00122D\n\u03b4\u2032\u0013\n+\u03d5r,(26)\nwhere (a)follows from the triangle inequality and the (b)due to the event \u02dcEt\u2229Etand(17). Let us\ndefine cp=c(p,8)D\nr\u221a\nMq\nlog\u00002D\n\u03b4\u2032\u0001\n+\u03d5r . Set \u02dcr=\u0012\nc(p,8)D\n\u03d5\u221a\nMq\nlog\u00002D\n\u03b4\u2032\u0001\u00131/2\n,r= min{\u03c1,\u02dcr} , and define\nZ:=c(p,8)D\u221a\nMq\nlog\u00002D\n\u03b4\u2032\u0001\n. Based on the choice M\u2265c(p,4)\n\u22062\n0log(2D/\u03b4\u2032), we have Z\u22641 , yielding the\nfollowing sequence of bounds onc p:\ncp=Z\nr+\u03d5r\n\u2264max\u001aZ\n\u02dcr+\u03d5\u02dcr,Z\n\u03c1+\u03d5\u03c1\u001b\n(a)\n\u2264max(\n2p\nZ\u03d5,\u221a\nZ\n\u03c1+\u03d5\u02dcr)\n= max(\n2p\nZ\u03d5,\u221a\nZ\n\u03c1+p\nZ\u03d5)\n\u22642\u221a\nZmax\u001ap\n\u03d5,1\n\u03c1\u001b\n(27)\n(b)\n\u22642 max\u001ap\n\u03d5,1\n\u03c1\u001b\n,(28)\nwhere(a)and(b)follow fromZ\u22641. Based on the above, we have\n\u03b7\u2225g(K t)\u2225 \u2264\u03b7(\u2225\u2207C(K t)\u2225) +\u2225e t\u2225\n(a)\n\u2264\u03b7(\u03bb+c p)\n(b)\n\u2264\u03b7\u0012\n\u03bb+ 2 max\u001ap\n\u03d5,1\n\u03c1\u001b\u0013\n,\nwhere (a)follows from Lemma 4 and (b)follows from (28). Setting the RHS \u2264\u03c1 leads to\nthe requirement \u03b7\u2264\u03c1\n\u03bb+2 maxn\u221a\u03d5,1\n\u03c1owhich is satisfied by setting \u03b7=c (p,1). This ensures that\n22\nHARNESSINGDATA FROMCLUSTEREDLQR SYSTEMS\n\u2225Kt+1\u2212K t\u2225 \u2264\u03c1. Using the local smoothness property (Lemma 5), we then have\nC(K t+1)\u2212C(K t)\u2264 \u27e8\u2207C(K t), Kt+1\u2212K t\u27e9+\u03d5\n2\u2225Kt+1\u2212K t\u22252\n=\u2212\u03b7\u27e8\u2207C(K t), g(K t)\u27e9+\u03d5\u03b72\n2\u2225g(K t)\u22252\n=\u2212\u03b7\u27e8\u2207C(K t),\u2207C(K t) +e t\u27e9+\u03d5\u03b72\n2\u2225\u2207C(K t) +e t\u22252\n(a)\n\u2264 \u2212\u03b7\u2225\u2207C(K t)\u22252\u2212\u03b7\u27e8\u2207C(K t), et\u27e9+\u03d5\u03b72\u2225\u2207C(K t)\u22252+\u03d5\u03b72\u2225et\u22252\n(b)\n\u2264 \u2212\u03b7(1\u2212\u03d5\u03b7)\u2225\u2207C(K t)\u22252+\u03b7\n2\u2225\u2207C(K t)\u22252+\u03b7\n2\u2225et\u22252+\u03d5\u03b72\u2225et\u22252\n=\u2212\u03b7\n2(1\u22122\u03d5\u03b7)\u2225\u2207C(K t)\u22252+\u03b7\n2(1 + 2\u03d5\u03b7)\u2225e t\u22252\n(c)\n\u2264 \u2212\u03b7\n4\u2225\u2207C(K t)\u22252+3\u03b7\n4c2\np.\nIn the above, we used \u2225A+B\u22252\u22642\u2225A\u22252+ 2\u2225B\u22252in(a), and\u22122\u27e8A, B\u27e9 \u2264 \u2225A\u22252+\u2225B\u22252in(b)\nwhere AandBare any matrices in Rm\u00d7n. In(c), we used \u03b7\u22641/(4\u03d5) (satisfied by our choice\n\u03b7=c (p,1)) and\u2225et\u2225 \u2264c p. Denoting the suboptimality gap as St=C(K t)\u2212C(K\u2217), and using the\nPL condition (15) in the above, we obtain the following with probability at least1\u2212\u03b4\u2032:\nSt+1\u2264\u0010\n1\u2212\u03b7\u00b5\n4\u0011\nSt+3\u03b7\n4c2\np\n\u2264\u0010\n1\u2212\u03b7\u00b5\n4\u0011\nSt+\u03b7\u00b5\n4\u00123\n\u00b5c2\np\u0013\n.(29)\nOn eventE t, since we haveK t\u2208 G0\nj,St\u226410 \u02dc\u22060. Furthermore, due to (27), we have\n3\n\u00b5c2\np\u226412\n\u00b5\u0012\nmax\u001ap\n\u03d5,1\n\u03c1\u001b\u00132c(p,8)D\u221a\nMs\nlog\u00122D\n\u03b4\u2032\u0013\n.\nDefining c(p,9):=12c(p,8)\n\u00b5\u0010\nmaxn\u221a\u03d5,1\n\u03c1o\u00112\nand setting M\u2265c2\n(p,9)D2\n\u22062\n0log\u00002D\n\u03b4\u2032\u0001\n, which is satisfied\nbyM\u2265c(p,4)\n\u22062\n0log(2D/\u03b4\u2032)from the statement of Lemma 9, ensures that3\n\u00b5c2\np\u2264\u2206 0\u226410 \u02dc\u22060. Hence,\nbased on (29), we have St+1\u2264\u0000\n1\u2212\u03b7\u00b5\n4\u0001\n10\u02dc\u22060+\u03b7\u00b5\n410\u02dc\u22060\u226410 \u02dc\u22060.Therefore, conditioned on the\nevent \u02dcEt\u2229Et,Kt+1\u2208 G0\njand\nSt+1\u2264\u0010\n1\u2212\u03b7\u00b5\n4\u0011\nSt+\u03b7\u00b5\n4 \nc(p,9)D\u221a\nMs\nlog\u00122D\n\u03b4\u2032\u0013!\n.\nNow, let us define Et+1:=\u02dcEt\u2229Et. We have P(\u02dcEt\u2229Et) =P( \u02dcEt|Et)P(E t)\u2265(1\u2212\u03b4\u2032)(1\u2212\u03b4\u2032t)\u2265\n1\u2212\u03b4\u2032(t+ 1) . This completes the induction step. To prove the statement of Lemma 9, since\n\u03b7=c (p,1)\u22648/\u00b5 , for any R\u22651 , we can unroll the recursion on the event ERwhich occurs with\n23\nKANAKERIBAJAJVERMAGUPTAMITRA\nprobability1\u2212\u03b4\u2032R. Doing so, we obtain the following which completes the proof:\nSR\u2264\u0010\n1\u2212\u03b7\u00b5\n4\u0011R\nS0+R\u22121X\nk=0\u0010\n1\u2212\u03b7\u00b5\n4\u0011k\u03b7\u00b5\n4 \nc(p,9)D\u221a\nMs\nlog\u00122D\n\u03b4\u2032\u0013!\n\u2264\u0010\n1\u2212\u03b7\u00b5\n4\u0011R\nS0+ \nc(p,9)D\u221a\nMs\nlog\u00122D\n\u03b4\u2032\u0013!\n.(30)\n24\nHARNESSINGDATA FROMCLUSTEREDLQR SYSTEMS\nAppendix C. Proof of Theorem 2\nWe prove Theorem 2 by conditioning on the event where the claims in (23) hold for all agents in\nevery epoch. In Appendix B, we showed that such an event occurs with probability at least 1\u2212\u03b4/2\nand let us denote it by EThm1. Furthermore, under this event, the claims of Theorem 1 hold as shown\nin Appendix B. In particular, the true clusters are always contained in the neighborhood sets and\ncorrect clustering takes place at the latest during the L-th epoch, ensuring that the agents collaborate\nsolely within their own clusters from the (L+ 1) -th epoch onward. With that in mind, we consider\nthe following approach to prove Theorem 2. First, we take for granted that the last epoch occurs\nafter the correct clustering takes place, i.e, \u00afL > L, and later show that this is indeed true if the total\nnumber of rollouts T\u2265 \u02dcO(1/\u22062). Next, we show that for any l > L (note that at least one such\nepoch exists in light of \u00afL > L ,) the policy at the start of the collaborative policy optimization remains\nin the corresponding restricted domain with high probability, i.e, \u02c6K(l\u22121)\ni\u2208 G0\n\u03c3(i). Then, we focus\non the last epoch \u00afLand provide the convergence guarantee, and finally conclude by analyzing the\nnumber of rollouts needed to ensure \u00afL > L.\nConditioned on the event EThm1, we follow an induction based argument to show that \u02c6K(l\u22121)\ni\u2208\nG0\n\u03c3(i)for all agents i\u2208[N] in every epoch l > L . Let us define \u02dcLas the first epoch where correct\nclustering takes place. Due to Theorem 1, since the correct clustering takes place at the latest during\ntheLth epoch, \u02dcL\u2264L , and moreover, since the neighborhood sets are sequentially pruned with\nno new agents getting added to the neighborhood sets, we have M\u03c3(i)=N(l)\nifor all agents in\nevery epoch l\u2265\u02dcL. Furthermore, \u02dcLbeing the first epoch where correct clustering takes place,\nN\u02dcL\u22121\ni\u0338=N\u02dcL\ni=M \u03c3(i)for some agenti\u2208[N], hence causing reinitializtion as shown in (8). After\nthis reinitialization, the global sequences for all the agents are updated by collaborating within their\nrespective clusters, and hence the global sequences for two agents within a cluster evolve identically\nin light of (6). In other words, for all agents i, j, ifM\u03c3(i)=M \u03c3(j), then for all l\u2265\u02dcL, we have the\nfollowing on the eventE Thm1:\n\u02c6K(l)\ni=\u02c6K(l)\nj\nM\u03c3(i)=N(l)\ni=N(l)\nj=M \u03c3(j)(31)\nTaking this into account, we show that \u02c6K(l\u22121)\ni\u2208 G0\n\u03c3(i)for all agents i\u2208[N] in every epoch l >\u02dcL\nvia induction across epochs. For the base case l=\u02dcL+ 1 , as a consequence of reinitialization during\nthe\u02dcLth epoch, and since X(\u02dcL)\ni\u2208 G0\n\u03c3(i)for all agents i\u2208[N] as a result of conditioning on the event\nEThm1, we have \u02c6K(\u02dcL)\ni\u2208 G0\n\u03c3(i)for all agents i\u2208[N] . Let us assume that for an epoch l\u2265\u02dcL+ 1 ,\nwith probability at least (1\u2212Pl\u22121\nj=1\u03b4j\n4), we have \u02c6K(t\u22121)\ni\u2208 G0\n\u03c3(i)for all agents i\u2208[N] and for\nallt\u2208 { \u02dcL+ 1, \u02dcL+ 2, . . . , l} . With a slight abuse of notation, let us denote this event by El\u22121.\nNext, conditioned on the event EThm1\u2229El\u22121, we show that \u02c6K(l)\ni\u2208 G0\n\u03c3(i)for all agents i\u2208[N] with\nprobability at least1\u2212\u03b4 l/4.\nIn what follows we fix an agent i\u2208[N] and omit the notation iand\u03c3(i) for convenience. Given\nthatK(l\u22121)\u2208 G0on the event EThm1\u2229El\u22121, we focus on analyzing the iterates {Y(k)}0\u2264k<R lin\nthelth epoch. The iterates are updated as follows: Y(k+1)=Y(k)\u2212\u03b7G(Y(k))withY(0)=\u02c6K(l\u22121),\nwhere we used G(Y(k))to denote the averaged zeroth-order gradient estimate as shown in (6)in the\nkth iteration. Note that in the light of Assumption 1, the averaged gradient estimate is an unbiased\n25\nKANAKERIBAJAJVERMAGUPTAMITRA\nestimate of \u2207Cr(Y(k))with an effective minibatch size of Ml|M|. Therefore, we follow an approach\nsimilar to the one from the proof of Lemma 9 in Appendix B.1 to analyze the one-step progress and\nto show that Y(Rl)=\u02c6Kl\u2208 G0. More specifically, we follow an induction based approach across\niterations and establish one-step recursion similar to (25) and finally unroll the recursion to obtain\nsomething similar to (30). However, a key difference arises from the fact that the second term in the\nRHS of both (25) and(30) will now enjoy avariance reductioneffect due to collaboration in light of\nAssumption 1 as shown in Corollary 8.\nIn particular, following the induction approach from the proof of Lemma 9 in Appendix B.1,\nin the kth iteration, we have the following concentration with probability at least (1\u2212\u03b4\u2032)after\nconditioning on the event where the previous iterations satisfy similar guarantees as in (25):\n\u2225G(Y(k))\u2212 \u2207C r(Y(k))\u2225 \u2264c(p,8)D\nrp\nMl|M|s\nlog\u00122D\n\u03b4\u2032\u0013\n.\nConditioned on the event where the gradient estimate is concentrated as above, and defining ek:=\nG(Y(k))\u2212 \u2207C(Y(k)), we have \u2225ek\u2225 \u2264c(p,8)D\nr\u221a\nMl|M|q\nlog\u00002D\n\u03b4\u2032\u0001\n+\u03d5r following the arguments up to\n(26). Now, let us define cp=c(p,8)D\nr\u221a\nMl|M|q\nlog\u00002D\n\u03b4\u2032\u0001\n+\u03d5r . Setting \u02dcr=\u0012\nc(p,8)D\n\u03d5\u221aMlq\nlog\u00002D\n\u03b4\u2032\u0001\u00131/2\nand\nr= min{\u03c1,\u02dcr\n|M|1/4}, we obtain the following bound on cpprovided Z:=c(p,8)D\u221aMlq\nlog\u00002D\n\u03b4\u2032\u0001\n\u22641\nwhich is ensured by our setting forM lin Table 2.\ncp=Z\nrp\n|M|+\u03d5r\n\u2264max\u001aZ\n\u02dcr|M|1/4+\u03d5\u02dcr\n|M|1/4,Z\n\u03c1|M|1/2+\u03d5\u03c1\u001b\n\u2264max(\n2\u221aZ\u03d5\n|M|1/4,\u221a\nZ\n\u03c1|M|1/4+\u03d5\u02dcr\n|M|1/4)\n\u22642\u221a\nZ\n|M|1/4max\u001ap\n\u03d5,1\n\u03c1\u001b\n(32)\n\u22642 max\u001ap\n\u03d5,1\n\u03c1\u001b\n.(33)\nBased on the above, we choose \u03b7=c (p,1)\u2264\u03c1\n\u03bb+2 maxn\u221a\u03d5,1\n\u03c1oto ensure that \u2225Y(k+1)\u2212Y(k)\u2225 \u2264\u03c1 .\nDefining Sk=C(Y(k))\u2212C(K\u2217)and following the analysis from Appendix B.1 up to (29) and\nusing the bound onc pfrom (32), we obtain\nSk+1\u2264\u0010\n1\u2212\u03b7\u00b5\n4\u0011\nSk\n|{z}\ns1+\u03b7\u00b5\n4 \nc(p,9)Dp\nMl|M|s\nlog\u00122D\n\u03b4\u2032\u0013!\n| {z }\ns2,\nwith probability at least 1\u2212\u03b4\u2032. Note that since |M| \u22651 , the term s2is not greater than the\ncorresponding term from (25), and hence s2\u2264\u03b7\u00b5\n4\u22060\u2264\u03b7\u00b5\n410\u02dc\u22060. Meanwhile, the term s1\u2264\n26\nHARNESSINGDATA FROMCLUSTEREDLQR SYSTEMS\n\u0000\n1\u2212\u03b7\u00b5\n4\u0001\n10\u02dc\u22060as we have conditioned on the event where Y(k)\u2208 G0similar to the proof of\nLemma 9. This ensures that Y(k+1)\u2208 G0. Now, unrolling the recursion, we have with probability at\nleast1\u2212\u03b4\u2032Rl,Y(Rl)=\u02c6K(l)\u2208 G0and the following:\nC(\u02c6K(l))\u2212C(K\u2217)\u2264\u0010\n1\u2212\u03b7\u00b5\n4\u0011Rl(C(\u02c6K(l\u22121))\u2212C(K\u2217)) + \nc(p,9)Dp\nMl|M|s\nlog\u00122D\n\u03b4\u2032\u0013!\n.(34)\nSetting \u03b4\u2032=\u03b4l/(4R lN)and applying an union bound over all agents, we have the above guarantee\nfor all agents with probability at least1\u2212\u03b4 l/4.\nLet us denote this event by \u02dcEl. DefiningE l=\u02dcEl\u2229El\u22121, we have the following:\nP(E l|EThm1) =P( \u02dcEl|El\u22121,EThm1)P(E l\u22121|EThm1)\u2265(1\u2212\u03b4 l/4)\uf8eb\n\uf8ed1\u2212l\u22121X\nj=1\u03b4j\n4\uf8f6\n\uf8f8\u2265\uf8eb\n\uf8ed1\u2212lX\nj=1\u03b4j\n4\uf8f6\n\uf8f8.\nThis completes the induction argument. Hence, we have established that \u02c6K(l\u22121)\ni\u2208 G0\n\u03c3(i)and that\n(34) holds for all agentsi\u2208[N]in every epochl\u2265 \u02dcLwith probability at least\u0010\n1\u2212Pl\nj=1\u03b4j\n4\u0011\n.\nNext, we analyze the final convergence guarantee in the last epoch \u00afL. Conditioned on the event\nE\u00afL\u22121\u2229 E Thm1, we obtain (34) as shown in the following with probability at least 1\u2212\u03b4 \u00afL/4for all\nagentsi\u2208[N]:\nC\u03c3(i)(\u02c6K(\u00afL)\ni)\u2212C \u03c3(i)(K\u2217\n\u03c3(i))\u2264\u0010\n1\u2212\u03b7\u00b5\n4\u0011R\u00afL(C\u03c3(i)(\u02c6K(\u00afL\u22121)\ni )\u2212C \u03c3(i)(K\u2217\n\u03c3(i)))\n| {z }\ns1\n+ \nc(p,9)Dp\nM\u00afL|M|s\nlog\u00128DNR \u00afL\n\u03b4\u00afL\u0013!\n| {z }\ns2.\nIn the above, the settings of R\u00afLandM\u00afLfrom Table 2 ensures the following: from R\u00afL\u2265\n4\n\u03b7\u00b5log\u0012\nc(p,4)N10\u02dc\u22060\n\u22062\n\u00afL\u0013\n, note that the term\ns1\u2264exp\u0012\n\u2212\u03b7\u00b5R \u00afL\n4\u0013\nS0\u2264exp\u0012\n\u2212\u03b7\u00b5R \u00afL\n4\u0013\n10\u02dc\u22060\u2264\u22062\u00afL\nc(p,4)N.\nUsing M\u00afL=c(p,4)\n\u22062\n\u00afLlog\u0010\n8DNR \u00afL\n\u03b4\u00afL\u0011\nin the above, we have s1\u2264log\u0012\n8DNR \u00afL\n\u03b4\u00afL\u0013\nM\u00afLN. Furthermore, since\nM\u00afL=c(p,4)\n\u22062\n\u00afLlog\u0010\n8DNR \u00afL\n\u03b4\u00afL\u0011\n\u2265log\u0010\n8DNR \u00afL\n\u03b4\u00afL\u0011\n, we have\ns1\u2264r\nlog\u0010\n8DNR \u00afL\n\u03b4\u00afL\u0011\np\nM\u00afLN\u2264r\nlog\u0010\n8DNR \u00afL\n\u03b4\u00afL\u0011\np\nM\u00afLN\u2264r\nlog\u0010\n8DNR \u00afL\n\u03b4\u00afL\u0011\np\nM\u00afL|M|.\n27\nKANAKERIBAJAJVERMAGUPTAMITRA\nTogether with the term s2we obtain the following with probability at least 1\u2212\u03b4 \u00afL/4for all agents\ni\u2208[N]:\nC\u03c3(i)(\u02c6K(\u00afL)\ni)\u2212C \u03c3(i)(K\u2217\n\u03c3(i))\u22642 max{1, c (p,9)D}q\nM\u00afL|M\u03c3(i)|s\nlog\u00128DNR \u00afL\n\u03b4\u0013\n.(35)\nThe above holds on the event E\u00afLconditioned on the event EThm1. We have P(E \u00afL|EThm1)\u2265\u0010\n1\u2212P\u00afL\nj=1\u03b4j\n4\u0011\n=\u0010\n1\u2212P\u00afL\nj=1\u03b4\n8j2\u0011\n\u22651\u2212\u03b4/4. Therefore,\nP(EE\u00afL\u2229 E Thm1) =P(E E\u00afL|EThm1)P(E Thm1)\u2265(1\u2212\u03b4/4)(1\u2212\u03b4/2)\u22651\u2212(\u03b4/4 +\u03b4/2)\u22651\u2212\u03b4.\nNote that the guarantee in (35) provides a rate \u02dcO\u0010\n1/q\nM\u00afL|M\u03c3(i)|\u0011\n. It remains to show that\nM\u00afL=\u02dc\u2126(T) . Since \u2206l= \u2206 0/4landRl\u22651 for all l\u2208 {1,2, . . . , \u00afL}, consider the following as\nMl\u2264T\nMl\u2264T=\u21d24l\u2264T\u22062\n0\nc(p,10) log\u00008DN\n\u03b4\u0001\n=\u21d2l\u2264log \nT\u22062\n0\nc(p,10) log\u00008DN\n\u03b4\u0001!\n= log(c (p,11)T),(36)\nwhere we defined c(p,11) :=\u0012\n\u22062\n0\nc(p,10) log(8DN\n\u03b4)\u0013\n. Now, we use the upper bound on lto bound Rlas\nfollows:\nRl=4\n\u03b7\u00b5 \nlog \nc(p,10)N\u02dc\u22062\n0\n\u22062\n0!\n+llog(4)!\n(a)\n\u2264l \n4\n\u03b7\u00b5 \nlog \nc(p,10)N\u02dc\u22062\n0\n\u22062\n0!\n+ log(4)!!\n(b)\n\u2264c(p,12) log(c (p,11)T),\n28\nHARNESSINGDATA FROMCLUSTEREDLQR SYSTEMS\nwhere (a)follows as l\u22651 , and(b)follows from (36)withc(p,12) :=4\n\u03b7\u00b5\u0012\nlog\u0012\nc(p,10) N\u02dc\u22062\n0\n\u22062\n0\u0013\n+ log(4)\u0013\n.\nTherefore, the overall sample complexity has the following bound:\nT=\u00afLX\nl=1(2M lRl+M l)\n(a)\n\u2264\u00afLX\nl=1(3M lRl)\n(b)\n\u2264\u00afLX\nl=13c(p,12) log(c (p,11)T) \nc(p,10) log\u00008DNT\n\u03b4\u0001\n\u22062\n0!\n4l\n= 4c (p,12) log(c (p,11)T) \nc(p,10) log\u00008DNT\n\u03b4\u0001\n\u22062\n0!\n4\u00afL\n=\u21d2T\n4c(p,12) log(c (p,11)T)\u0012\nc(p,10) log(8DNT\n\u03b4)\n\u22062\n0\u0013\u22644\u00afL.\uf8fc\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8fd\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8fe(37)\nIn the above, (a)follows as Rl\u22651and(b)as we used Rl\u2264T inMl. Using the lower bound\non4\u00afLas obtained above in M\u00afL, we obtain M\u00afL\u2265Tlog\u00108DNR \u00afL\n\u03b4\u0011\n4c(p,12) log(c (p,11) T) log(8DNT\n\u03b4). Using this bound in\n(35), we have the following with probability1\u2212\u03b4:\nC\u03c3(i)(\u02c6K(\u00afL)\ni)\u2212C \u03c3(i)(K\u2217\n\u03c3(i))\u22644Dmax{1, c (p,9)}q\nc(p,12) log(c (p,11)T) log\u00008DNT\n\u03b4\u0001\nq\nT|M \u03c3(i)|\n=Dc(p,13)q\nlog\u00008DNT\n\u03b4\u0001\nq\nT|M \u03c3(i)|,(38)\nwhere we definedc (p,13) := 4 max{1, c (p,9)}q\nc(p,12) log(c (p,11)T).\nFinally, it remains to show that \u00afL > L when T\u2265 \u02dcO(1/\u22062). To ensure, \u00afL > L , consider the\nnumber of rollouts required up to (L+ 1) th epoch. From (37) with the summation from 1 to L+ 1\nwe have:\nL+1X\nl=1(2M lRl+M l)\u22644c (p,12) log(c (p,11)T) \nc(p,10) log\u00008DNT\n\u03b4\u0001\n\u22062\n0!\n4L+1.\nIn the above, settingT\u2265RHS to ensure \u00afL > L, we have\nT\u22654c (p,12) log(c (p,11)T) \nc(p,10) log\u00008DNT\n\u03b4\u0001\n\u22062\n0!\n4L+1\n(a)\n\u22654c (p,12) log(c (p,11)T) \nc(p,10) log\u00008DNT\n\u03b4\u0001\n\u22062\n0!\u0012\u22060\n\u2206\u00132\n.\n29\nKANAKERIBAJAJVERMAGUPTAMITRA\nIn the above, since L= min{l\u22081,2, . . .: \u2206 l\u2264\u2206/2} ,(a)follows from the fact that \u2206L+1=\n\u22060/(2L+1)\u2264\u2206/2 . Hence, when T\u2265 \u02dcO(1/\u22062), we have \u00afL > L . This completes the proof of\nTheorem 2.\nAppendix D. Proof of Corollary 3\nIn this section, we analyze the total communication complexity of the PCPO algorithm. In every\nepoch, each agent communicates with the server once in every iteration of the collaborative policy\noptimization subroutine, and once to send the local policy to update the neighborhood sets. Hence,\nthe overall communication complexity isP\u00afL\nl=1(Rl+ 1). Note thatP\u00afL\nl=1(Rl+ 1)\u2264(R \u00afL+ 1)\u00afL\nsinceR \u00afL=c(p,2)log\u0012\n2\u00afLc(p,3)N\n\u22062\n0\u0013\n\u2265c(p,2)log\u00102lc(p,3)N\n\u22062\n0\u0011\n=R l.\nFrom (37),\u00afLis logarithmic in T. Furthermore, R\u00afL=c(p,2)log\u0012\n2\u00afLc(p,3)N\n\u22062\n0\u0013\nis logarithmic in\nthe number of agents NandT. Finally, since T\u2265 \u02dcO(1/\u22062), the overall communication complexity\nis logarithmic inT,Nand1/\u2206.\n30\n",
    "title": "Harnessing Data from Clustered LQR Systems: Personalized and Collaborative Policy Optimization",
    "arxiv_id": "2511.17489v1",
    "category": "recent",
    "authors": [
      "Vinay Kanakeri",
      "Shivam Bajaj",
      "Ashwin Verma",
      "Vijay Gupta",
      "Aritra Mitra"
    ],
    "published": "2025-11-21",
    "num_pages": 30,
    "num_chars": 73318
  },
  {
    "text": "Enhancing Quranic Learning: A Multimodal Deep Learning\nApproach for Arabic Phoneme Recognition\nAyhan K\u00fc\u00e7\u00fckmanisaa, Derya Gelmezb, \u015e\u00fckr\u00fc Selim \u00c7al\u0131kb,cand Zeynep Hilal Kilimcib\naDepartment of Electronics and Communication Engineering, Kocaeli University, 41001, Kocaeli, Turkey\nbDepartment of Information Systems Engineering, Kocaeli University, 41001, Kocaeli, Turkey\ncMaviay Consultancy Company, Kocaeli University Technopark, 41275, Kocaeli, Turkey\nARTICLE INFO\nKeywords:\ncomputer aided language learning\nArabic pronunciation detection\naudio transformers\nWhisper\nUniSpeech\nBERT\nMultimodalABSTRACT\nRecentadvancesinmultimodaldeeplearninghavegreatlyenhancedthecapabilityofsystemsfor\nspeech analysis and pronunciation assessment. Accurate pronunciation detection remains a key\nchallengeinArabic,particularlyinthecontextofQur\u2019anicrecitation,wheresubtlephoneticdif-\nferencescanaltermeaning.Addressingthischallenge,thepresentstudyproposesatransformer-\nbased multimodal framework for Arabic phoneme mispronunciation detection that combines\nacoustic and textual representations to achieve higher precision and robustness. The framework\nintegrates UniSpeech-derived acoustic embeddings with BERT-based textual embeddings ex-\ntractedfromWhispertranscriptions,creatingaunifiedrepresentationthatcapturesbothphonetic\ndetailandlinguisticcontext.Todeterminethemosteffectiveintegrationstrategy,early,interme-\ndiate, and late fusion methods were implemented and evaluated on two datasets containing 29\nArabic phonemes, including eight hafiz sounds, articulated by 11 native speakers. Additional\nspeech samples collected from publicly available YouTube recordings were incorporated to\nenhance data diversity and generalization. Model performance was assessed using standard\nevaluation metrics\u2014accuracy, precision, recall, and F1-score\u2014allowing a detailed comparison\nof the fusion strategies. Experimental findings show that the UniSpeech\u2013BERT multimodal\nconfigurationprovidesstrongresultsandthatfusion-basedtransformerarchitecturesareeffective\nfor phoneme-level mispronunciation detection. The study contributes to the development of\nintelligent,speaker-independent,andmultimodalComputer-AidedLanguageLearning(CALL)\nsystems, offering a practical step toward technology-supported Qur\u2019anic pronunciation training\nand broader speech-based educational applications.\n1. Introduction\nTheHolyQur\u2019an,reveredasthedivinescriptureoftheIslamicworld,servesnotonlyasaspiritualtextbutalsoas\nacentralcultural,social,andeducationalreference.Formillionsofbelievers,theQur\u2019anisregardedastheliteralWord\nof God, guiding every aspect of life. In this context, accurate recitation is essential, as it forms the foundation of both\nindividualworshipandcollectiveritualpractice.PreservingthephoneticandsemanticintegrityoftheoriginalArabic\ntextisvitaltoconveyingitsmeaningcorrectly.Arabicpronunciation,particularlyinQur\u2019anicrecitation,requireshigh\nprecision: even a single mispronunciation can alter meaning or lessen the Qur\u2019an\u2019s aesthetic and spiritual impact.\nTherefore, developing automated tools that assist learners in achieving correct recitation is crucial for improving the\nquality of religious education and strengthening individuals\u2019 engagement with the Qur\u2019an, especially in communities\nwhere Arabic is not a native language (El Kheir et al., 2025).\nNon-native learners of Arabic often face difficulties in mastering Qur\u2019anic pronunciation. The phonetic and\nphonologicalstructureofArabic\u2014includingthemakhraj(pointsofarticulation)andsifat(articulatorycharacteristics)\nof letters\u2014requires careful attention. Errors in pronouncing even a single sound may cause changes in meaning or\nobscure the intended message. Traditionally, identifying and correcting such mispronunciations has depended on\nqualifiedinstructors,whichlimitsaccesstoeffectivetraining,particularlyinregionswithscarceeducationalresources.\nAutomatic pronunciation assessment systems provide a promising alternative. By analyzing speech input, detecting\npronunciation errors, and offering immediate feedback, they allow learners to practice independently while ensuring\naccessibility and consistency in evaluation (Algabri et al., 2022a; Bahi, 2024).\n\u2217Corresponding author: Ayhan K\u00fc\u00e7\u00fckmanisa\nayhan.kucukmanisa@kocaeli.edu.tr(A. K\u00fc\u00e7\u00fckmanisa);derya.glmz02@gmail.com(D. Gelmez);sselimcalik1@gmail.com\n(. \u00b8S. \u00c7al\u0131k);zeynep.kilimci@kocaeli.edu.tr(Z.H. Kilimci)\nORCID(s):\nSelim et al.:Preprint submitted to ElsevierPage 1 of 11arXiv:2511.17477v1  [cs.SD]  21 Nov 2025\nEnhancing Quranic Learning: A Multimodal Deep Learning Approach for Arabic Phoneme Recognition\nIn recent years, deep learning techniques have achieved significant progress in both natural language processing\n(NLP) and speech analysis. Among these methods, transformer-based architectures have gained prominence due to\ntheir ability to model long-term dependencies and capture linguistic subtleties that traditional approaches often miss.\nIn speech and pronunciation tasks, such models can recognize fine acoustic differences with high accuracy (Vaswani\net al., 2017). Furthermore, multimodal learning, which integrates multiple data types such as audio, text, and visual\ninputs,enablesamorecomprehensiverepresentationofspeechphenomena.Forexample,inArabicletterrecognition,\nmultimodalmethodscancombinetheaudiosignalwithcorrespondingtextualorphoneticinformation\u2014and,insome\ncases, visual cues such as lip movements\u2014to enhance robustness and reduce ambiguity (Zhu et al., 2023).\nBased on these developments, the present study introduces a multimodal transformer-based system for Arabic\nletter recognition and mispronunciation detection in Qur\u2019anic recitation. The proposed approach examines early,\nmiddle, and late fusion strategies for integrating acoustic and textual features. Transformer models are used to\nextract relevant information from audio signals, which is then combined with complementary modalities to produce\na unified representation. These fusion strategies are compared to determine which method most effectively supports\npronunciation accuracy. In addition to error detection, the system aims to serve as a practical learning aid, providing\nfeedback that helps learners improve their recitation quality.\nThe main contributions of this study are summarized below:\n\u2022A multimodal framework is developed for Arabic phoneme recognition in the Qur\u2019anic context, systematically\ncomparing early, mid, and late fusion strategies to clarify trade-offs and inform future research.\n\u2022The proposed system offers a practical and accessible solution for non-native Arabic speakers, supporting\nindividualized learning and improving access to quality religious education.\n\u2022The study demonstrates the applicability of deep learning and transformer models in pronunciation assessment\nby adapting these methods to the domain of religious education.\n\u2022The research illustrates how technological advances can be effectively aligned with cultural and religious\npractices, providing a foundation for future interdisciplinary work.\nTheremainderofthispaperisorganizedasfollows.Section2reviewsrelatedliteratureonQur\u2019anicpronunciation\nchallenges and previous research on automatic pronunciation evaluation for Arabic. Section 3 presents the proposed\nmultimodal fusion framework, describing the early, mid, and late fusion strategies in detail. Section 4 outlines\nthe dataset and experimental methodology, explaining how transformer-based architectures were applied for feature\nextraction and fusion. Section 5 reports and discusses the experimental results, and Section??concludes the paper\nwith the main findings and directions for future work.\n2. Literature Review\nThissectionreviewspreviousresearchonmispronunciationdetectionacrossdifferentlanguages,withaparticular\nfocus on studies related to Arabic phonemes and Qur\u2019anic recitation.\nEarlyeffortsinArabicmispronunciationdetectionprimarilyreliedondeepconvolutionalneuralnetworks(CNNs).\nIn (Nazir et al., 2019), two CNN-based approaches were introduced: a CNN-feature extraction method and a transfer\nlearning-based method. In the first approach, features extracted from intermediate CNN layers (layers 4\u20137) were\nclassified using KNN, SVM, and neural network classifiers, while the second approach adapted a pre-trained CNN\nthroughtransferlearning.Thetransferlearning-basedmethodachievedthebestperformance,reachinganaccuracyof\n92.2%,comparedwith82%fortraditionalhand-craftedfeaturesand91.7%forCNN-basedfeatures.Buildingonthese\nresults,(Akhtaretal.,2020)employedAlexNettoextractdeepfeaturesfromlayers6\u20138andtrainedclassifierssuchas\nKNN, SVM, and Random Forest. With the inclusion of transfer learning and feature selection, the approach achieved\nan average accuracy of 93.2%, highlighting the benefit of deeper and more discriminative CNN representations.\nEnd-to-end(E2E)automaticspeechrecognition(ASR)architectureshavealsobeenexploredformispronunciation\ndetection. The study by (Lo et al., 2020) introduced a CTC\u2013Attention hybrid model that unifies alignment and\nrecognitionwithinasingleframework,eliminatingtheneedforphoneme-levelforcedalignment.AppliedtoMandarin\nspeech data, this approach demonstrated improved accuracy and simplified model design compared with traditional\nDNN\u2013HMM systems.\nSelim et al.:Preprint submitted to ElsevierPage 2 of 11\nEnhancing Quranic Learning: A Multimodal Deep Learning Approach for Arabic Phoneme Recognition\nSeveral studies have specifically targeted Classical Arabic pronunciation. In (Asif et al., 2021), a deep neural\nnetworkwastrainedonanewlycollecteddatasettorecognizeshortvowels,achievinganaccuracyof95.77%.Similarly,\n(Farooq and Imran, 2021) focused on detecting errors in the articulation points of Arabic letters using RASTA-PLP\nfeature extraction combined with an HMM classifier. Reported accuracies reached 85%, 90%, and 98% for different\nexperimental setups, indicating the feasibility of technology-assisted pronunciation evaluation for Arabic. Another\nrelated work, (Alqadheeb et al., 2021), collected 2,892 audio samples covering 84 short-vowel classes and employed\nasequentialCNNon312phonemesfromtheArabicalphabet.Thesystemachieved100%testingaccuracyandaloss\nof 0.27, demonstrating the capability of CNN-based methods for precise Arabic phoneme classification.\nFor non-native Arabic learners, (Algabri et al., 2022b) developed a comprehensive Computer-Assisted Pronunci-\nation Training (CAPT) system capable of identifying mispronounced phonemes and their corresponding articulation\nfeatures (AFs). The problem was formulated as a multi-label recognition task, and an additional speech corpus was\nsynthesized using TTS technology. The best-performing model achieved a phoneme error rate (PER) of 3.83%, an\nF1-score of 70.53% for mispronunciation detection, and a detection error rate (DER) of 2.6% for AF identification,\noutperforming previous end-to-end systems.\nBeyond Arabic, recent studies have explored advanced neural architectures for multilingual pronunciation model-\ning.PeppaNet(Yanetal.,2023),forexample,proposedaunifiedend-to-endneuralmodelthatjointlyperformsalign-\nmentanddictation,integratingphoneticandphonologicalcuesthroughaselectivegatingmechanism.Experimentson\nthe L2-ARCTIC benchmark dataset showed notable improvements over earlier state-of-the-art systems.\nInthespecificcontextofQur\u2019anicrecitation,(HarereandJallad,2023)appliedLSTMmodelswithMFCCfeatures\ntodetectsequentialpronunciationerrorscorrespondingtoTajweedrules.UsingtheQDATdataset,themodelachieved\naccuracies of 96%, 95%, and 96% for Separate Stretching, Tight Noon, and Hide rules, respectively, surpassing\nconventional machine learning baselines.\nA recent study (Cal\u0131k et al., 2023) proposed an ensemble-based approach for detecting mispronunciations of\nArabicphonemeswithinacomputer-assistedlanguagelearning(CALL)framework.Thestudyintroducedanensemble\nmodel that integrates multiple conventional machine learning algorithms to identify mispronunciations and provide\ncorrectivefeedbackforArabicpronunciationlearning.Tothebestoftheauthors\u2019knowledge,thisworkrepresentsthe\nfirst comprehensive attempt to apply ensemble learning techniques to Arabic phoneme mispronunciation detection.\nIn the experiments, mel-frequency cepstral coefficients (MFCC) and Mel-spectrogram features were extracted and\ncombined with various classifiers to evaluate their impact on performance. The dataset consisted of recordings of 29\nArabic letters, including eight hafiz sounds, produced by 11 speakers, and was augmented through noise addition,\ntimeshifting,stretching,andpitchmodification.Experimentalresultsshowedthattheensemblevotingclassifierusing\nMel-spectrogram features achieved the best performance, with an accuracy of 95.9%, demonstrating the potential of\nensemble learning for improving Arabic pronunciation assessment systems.\nMorerecentworkhasshiftedtowardtransformer-basedandaudio-orientedarchitectures.(Caliketal.,2024)com-\nparedSEW,HuBERT,Wav2Vec,andUniSpeechmodelsforArabicphonememispronunciationdetection.Experiments\nconductedontwodatasetscovering29ArabicphonemesindicatedthatUniSpeechachievedthehighestaccuracy,while\nthestudyalsodiscussedpotentialareasforimprovement.Similarly,(Alrashoudietal.,2025)developedatransformer-\nbased approach for detecting and classifying phoneme-level errors by type (insertion, deletion, substitution). Using a\ndataset containing native and non-native Arabic speakers, the system achieved detection and diagnosis accuracies of\n91.3% and 80.8%, respectively, outperforming human evaluators in certain conditions.\nAddressing the challenge of data imbalance, (Lounis et al., 2025) proposed a one-class CNN-based framework\ntrainedexclusivelyoncorrectlypronouncedsamples,treatingmispronunciationsasoutliers.ExperimentsontheArabic\nSpeechMispronunciationDetectionDataset(ASMDD)yieldedanaccuracyofapproximately84%,demonstratingthe\npotentialofunsupervisedorsemi-supervisedstrategiesforthisdomain.Complementingthesedevelopments,(Haouhat\net al., 2025) conducted a comprehensive review of Arabic multimodal machine learning (MML) studies, identifying\nmajor datasets, applications, and challenges, and outlining directions for future research.\nIn contrast to prior works, which often focus solely on acoustic cues or limited subsets of Arabic phonemes,\nthe present study introduces a multimodal transformer-based framework that integrates both acoustic and textual\ninformation for Arabic phoneme mispronunciation detection. Furthermore, a dedicated dataset of Arabic letters has\nbeenconstructedtosupportmodeltrainingandbenchmarking.Thisapproachaimstoenhancedetectionaccuracyand\nrobustnesswhileprovidingafoundationforfuturestudiesonpronunciationassessmentandcomputer-assistedQur\u2019anic\neducation.\nSelim et al.:Preprint submitted to ElsevierPage 3 of 11\nEnhancing Quranic Learning: A Multimodal Deep Learning Approach for Arabic Phoneme Recognition\n3. Multimodal Fusion\nMultimodal fusion is an approach that combines information from multiple data sources to take advantage of the\ncomplementary strengths of each modality. This integration helps overcome the limitations of relying on a single\nmodality, promotes data diversity, and enables the creation of richer and more representative feature spaces. Fusion\nstrategies are generally grouped into three main categories: early, intermediate, and late fusion.\nIn the early fusion approach, features extracted from different modalities are directly combined and processed\nby a single model. This allows the model to learn interactions between modalities at the feature level but may\nalso introduce challenges such as increased dimensionality and potential incompatibility between modality scales.\nIn contrast, intermediate fusion processes each modality independently within separate networks and merges their\nfeaturerepresentationsatselectedintermediatelayers.Thisstrategyhelpsretainmodality-specificcharacteristicswhile\nallowing a more balanced information exchange. The late fusion approach involves training independent classifiers\nfor each modality and combining their outputs to reach a final decision. Late fusion preserves the independence\nof modalities and reduces the negative influence of potential errors from any single modality on the overall system\nperformance Li and Tang (2024).\nInthecurrentstudy,multimodalfusiontechniquesareappliedtoimprovemispronunciationdetectionbyintegrating\ncomplementary information from acoustic and textual modalities. UniSpeech embeddings Wang et al. (2021) are\nemployed to capture fine-grained phonetic and prosodic cues, while BERT embeddings Devlin et al. (2019) derived\nfrom phoneme-level transcriptions generated by Whisper Radford et al. (2022) encode semantic, syntactic, and\ncontextualinformation.Theintegrationofthesecomplementaryrepresentationsformsaunifiedembeddingspacethat\nmodels both phonetic and linguistic dimensions of speech. This shared representation aligns pronunciation patterns\nwith their linguistic context, which is critical for accurate mispronunciation detection. Detailed descriptions of the\nmodels used in this study are provided in the following subsections.\n3.1. Robust Speech Recognition (Whisper)\nThe Whisper model Radford et al. (2022) represents a recent advancement in automatic speech recognition\n(ASR), designed to transcribe speech across different languages and challenging acoustic environments. Whisper\nemploysatransformer-basedencoder\u2013decoderarchitecturecapableofmodelinglong-rangedependenciesandtemporal\nrelationships within audio sequences. By processing raw audio waveforms sampled at 16 kHz, the model effectively\nencodes phonetic and prosodic information, yielding accurate and contextually consistent transcriptions.\nA key feature of Whisper is its robustness to diverse acoustic conditions and speaker variations. Trained on a\nlarge-scalemultilingualandmultitaskdataset,themodelgeneralizeswellacrosslanguages,accents,andspeechstyles.\nItstrainingstrategycombinessupervisedlearningonlabeleddatasetswithmultitaskobjectives,enhancingadaptability\nand enabling reliable transcription performance even in noisy or non-standard speech.\nWhisper\u2019s self-attention mechanisms capture contextual relationships across complete utterances, allowing the\nmodel to maintain coherence between phonemes and words. This makes it particularly suitable for downstream\napplicationssuchasphoneme-levelanalysis, speechtranslation,andpronunciationassessment.Insummary,Whisper\nprovidesarobustandversatilefoundationforgeneratingphoneme-leveltranscriptionsthatareusedastextualinputin\nthe proposed multimodal framework.\n3.2. Unified Speech Representation Learning (UniSpeech)\nThe UniSpeech model Wang et al. (2021) is a transformer-based architecture developed to learn effective speech\nrepresentationsforawiderangeoftasks,includingautomaticspeechrecognition,languageidentification,andspeaker\nverification.Byleveragingself-attentionmechanisms,UniSpeechcapturesbothlocalandglobaldependencieswithin\naudio signals, enabling the extraction of detailed acoustic features that reflect the hierarchical structure of speech.\nAmajorstrengthofUniSpeechliesinitsmultilingualandmultitasktrainingdesign.Trainedonlargemultilingual\ndatasets,themodelgeneralizesefficientlyacrosslanguagesandtasks,makingitsuitableforapplicationsinvolvinglow-\nresourceorcross-lingualspeechprocessing.Furthermore,UniSpeechincorporatesunsupervisedandsemi-supervised\nlearningstrategiesthatallowittoutilizevastamountsofunlabeledaudiodata,improvingrobustnessagainstvariability\nin accents, recording conditions, and speaking styles. The use of transfer learning and knowledge distillation further\nenhances its adaptability, enabling the transfer of learned knowledge from high-resource to low-resource domains.\nExtensive benchmark evaluations have shown that UniSpeech achieves competitive results across various speech\nprocessing tasks. In this study, UniSpeech embeddings are used to represent the acoustic characteristics of Arabic\nphonemes, capturing subtle phonetic and prosodic features that are essential for accurate mispronunciation detection.\nSelim et al.:Preprint submitted to ElsevierPage 4 of 11\nEnhancing Quranic Learning: A Multimodal Deep Learning Approach for Arabic Phoneme Recognition\nFigure 1:The block diagram of proposed method\n3.3. Contextual Language Understanding (BERT)\nThe BERT model Devlin et al. (2019) is a transformer-based language representation model that produces deep\ncontextual embeddings from textual data. Through its bidirectional self-attention mechanism, BERT captures both\nsemanticmeaningandsyntacticstructure,allowingittointerpretwordsandphonemesinrelationtotheirsurrounding\ncontext.\nBERT is pretrained using masked language modeling and next sentence prediction objectives, enabling it to learn\ngeneral-purposelanguagerepresentationsfromlarge-scaleunlabeledtextcorpora.Whenfine-tunedonphoneme-level\ntranscriptionsproducedbyWhisper,BERTgeneratescontext-awaretextualembeddingsthatcomplementtheacoustic\nrepresentations obtained from UniSpeech. These embeddings encode linguistic correctness, semantic coherence, and\nphonetic consistency, supporting the identification of pronunciation errors within linguistic context.\nIntegratingBERTembeddingswithUniSpeechacousticfeaturesallowstheproposedframeworktojointlyconsider\nlinguistic and phonetic dimensions of speech. This multimodal alignment enhances the system\u2019s capacity to detect\nnuanced and context-dependent mispronunciations. Consequently, BERT serves as a reliable textual encoder within\nthe proposed multimodal learning framework for Arabic phoneme mispronunciation detection.\n4. Proposed framework\nThis section presents the proposed multimodal framework designed to detect mispronunciations of Arabic\nphonemesbyintegratingacousticandtextualinformation.Theframeworkcombinesstate-of-the-arttransformermod-\nelstocapturebothphoneticandlinguisticcues,formingaunifiedrepresentationthatsupportsaccuratepronunciation\nassessment. It consists of three main stages: data preprocessing and feature extraction, multimodal fusion of acoustic\nand textual embeddings, and classification of phoneme-level pronunciations. The following subsections describe\nthe multimodal fusion strategy and the models employed in detail, including Whisper for transcription generation,\nUniSpeechforacousticrepresentationlearning,andBERTforcontextuallanguageunderstanding.Theblockdiagram\nof proposed method is illustrated in Fig. 1.\n4.1. Dataset & Preprocessing\nThedatasetusedinthisstudywasadoptedfromtheworkof\u00c7al\u0131ketal.(Caliketal.,2024).Thisdatasetconsistsof\naudio recordings representing the pronunciation characteristics of the Arabic alphabet and was originally constructed\nusing two main sources: direct recordings from Qur\u2019anic reciters (Hafiz) and publicly available audio clips obtained\nfromYouTube.Itcontainsatotalof1015audiosamples,including232directlyrecordedArabicalphabetsoundsand\n783 distinct recordings collected from online sources, covering the full range of Arabic phonemes.\nThe raw audio data underwent a comprehensive preprocessing pipeline to ensure high performance during model\ntraining. The initial step of this process was to remove background noise from the recordings, thereby enhancing the\naudioquality.Subsequently,allaudiofileswerestandardizedandunifiedintoacommonformat.Thisstandardization\nincluded resizing each audio clip to a duration of 4 seconds and resampling to a frequency of 16 kHz.\nFollowing the completion of the data preprocessing, two distinct datasets (Dataset A and Dataset B) are used to\nevaluate the model\u2019s performance under different scenarios. Details of these dataset setups are given in Table 1.\nTwo dataset configurations were designed to evaluate the model under different training\u2013testing conditions. In\nDataset A, the 783 audio samples collected from YouTube were used for training, while the 232 recordings obtained\nfrom Qur\u2019anic reciters were reserved for testing, enabling the assessment of generalization on expert pronunciations.\nInDataset B, 80% of both YouTube and reciter recordings were used for training and the remaining 20% for\nSelim et al.:Preprint submitted to ElsevierPage 5 of 11\nEnhancing Quranic Learning: A Multimodal Deep Learning Approach for Arabic Phoneme Recognition\nTable 1\nDataset setups with sample and person distributions for YouTube and Hafiz sources.\nTrain Test\nYouTube Hafiz YouTube Hafiz\nDataset Sample Person Sample Person Sample Person Sample Person\nA 783 35 0 0 0 0 232 11\nB 626 28 186 9 157 7 46 2\ntesting,allowingevaluationonamorehomogeneousdatadistribution.Theseconfigurationsprovidedcomplementary\nperspectives on model robustness and generalization capability.\n4.2. Proposed Method\nInthisstudy,amultimodalframeworkintegratingbothacousticandtextualmodalitiesisdevelopedforthedetection\nof mispronunciations in Arabic phonemes. The proposed methodology consists of several stages, including feature\nextraction, model design, cross-validation training, fusion strategy implementation, and performance evaluation.\nFor the acoustic stream, the feature encoders of the proposed Transformer models operate over a receptive field\ncovering approximately 400 samples (about 25 ms) of audio. Since the encoding process is conducted using signals\nsampledat16kHz,allinputaudiorecordingsareresampledto16kHzduringpreprocessing.Theaudioinputsarethen\npadded or trimmed to a fixed duration of 4 seconds before being fed into the acoustic encoder.\nApre-trainedUniSpeechTransformermodelisemployedtoextractacousticembeddings.Forthetextualmodality,\nall audio recordings are processed using the Whisper model, which provides phoneme-level transcriptions for each\nutterance. Each audio sample is paired with its corresponding phoneme sequence and subsequently transformed into\nhigh-dimensional embeddings using a pre-trained multilingual BERT model.\nThe pre-trained weights of Transformer models trained on Arabic speech data are adopted as the initial weights\nin the proposed framework to improve convergence. Subsequently, each proposed model is fine-tuned using Qur\u2019anic\nrecitationdata,allowingittocapturepronunciationvariationsandphoneticcharacteristicsuniquetoQur\u2019anicArabic.\nTo prevent overfitting and to reliably assess the generalizability of the models, a five-fold cross-validation procedure\nis applied to the training data of both datasets (A and B). Each training set is divided into five equal subsets, where\nfour subsets are used for training and one subset is used for validation in each iteration. This process ensures that\neach subset serves once as a validation set, resulting in a total of five independent training\u2013validation cycles. After\ncompleting all cross-validation cycles, the model weights that achieve the best validation performance are selected,\nand the resulting models are evaluated on the test set, which is kept completely unseen during training. This strategy\nprovides an objective estimation of the models\u2019 true generalization capability.\nThree fusion strategies were implemented to integrate the complementary information provided by the acoustic\nand textual modalities. The architecture of the proposed deep learning-based fusion models is depicted in Figure??.\nEarlyFusion:Inthisapproach,acousticembeddingsobtainedfromtheUniSpeechencoderandtextualembeddings\nextractedfromtheBERTmodelwereconcatenatedimmediatelyafterthefeatureextractionstage.Toensurenumerical\ncompatibility between the two modalities, embeddings were normalized prior to concatenation. The fused vector was\nthen processed by a classification network composed of multiple fully connected layers with ReLU activations and\ndropout regularization. This design enabled the model to jointly capture acoustic and textual information from the\ninitial stage of learning, directly modeling cross-modal dependencies within the classifier.\nIntermediate Fusion: In this approach, each modality was first processed independently before integration.\nUniSpeech and BERT embeddings were passed through separate networks to reduce dimensionality and highlight\nsalient, modality-specific features. The resulting transformed embeddings were subsequently concatenated and fed\ninto a classifier network. This procedure allowed the model to first optimize each modality independently and then\nperform multimodal integration.\nLate Fusion: In this approach, the unimodal AudioOnly and TextOnly models were trained independently to\nconvergence. Hidden-layer embeddings from each model were extracted as modality-specific representations and\npassed through layers to reduce dimensionality and emphasize the most salient features. The reduced representations\nwereconcatenatedandprocessedbyaclassificationnetworkconsistingoffullyconnectedlayerswithReLUactivations\nSelim et al.:Preprint submitted to ElsevierPage 6 of 11\nEnhancing Quranic Learning: A Multimodal Deep Learning Approach for Arabic Phoneme Recognition\n(a) Early Fusion\n (b) Intermediate Fusion\n (c) Late Fusion\nFigure 2:The proposed deep learning-based fusion models (a) Early fusion, (b) Intermediate fusion, and (c) Late fusion.\nand dropout regularization. During this stage, the pretrained encoder weights of UniSpeech and BERT were frozen,\nensuring that only the fusion layers were updated and the individual encoders were not retrained.\nAll models were evaluated using accuracy, precision, recall, F1-score. All evaluations were conducted on the two\ndistinct datasets (A and B).\nThis methodology enables the integration of self-supervised acoustic embeddings with phoneme-level textual\nembeddings through early, intermediate, and late fusion strategies, allowing robust and accurate identification of\nmispronunciations in Arabic phonemes under varying recording conditions.\n5. Experimental results\nTheevaluationoftheproposedmethodsisquantifiedthroughtheutilizationoftheformulasprovidedinequations\n(1),(2),(3),and(4).Withintheseequations,eachclassisdesignatedas\ud835\udc36\ud835\udc65,whereTP(TruePositive)signifiesaccurate\nclassification of audio belonging to the\ud835\udc36\ud835\udc65class as\ud835\udc36\ud835\udc65itself. FP (False Positive) pertains to all non-\ud835\udc36\ud835\udc65samples\nerroneously classified as\ud835\udc36\ud835\udc65. TN (True Negative) encompasses all non-\ud835\udc36\ud835\udc65samples correctly not classified as\ud835\udc36\ud835\udc65.\nFN (False Negative) encapsulates instances where samples from\ud835\udc36\ud835\udc65are inaccurately not classified as\ud835\udc36\ud835\udc65.\n\ud835\udc34\ud835\udc50\ud835\udc50\ud835\udc62\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc66=\ud835\udc47 \ud835\udc43+\ud835\udc47 \ud835\udc41\n\ud835\udc47 \ud835\udc43+\ud835\udc47 \ud835\udc41+\ud835\udc39\ud835\udc43+\ud835\udc39\ud835\udc41(1)\n\ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b=\ud835\udc47 \ud835\udc43\n\ud835\udc47 \ud835\udc43+\ud835\udc39\ud835\udc43(2)\n\ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59=\ud835\udc47 \ud835\udc43\n\ud835\udc47 \ud835\udc43+\ud835\udc39\ud835\udc41(3)\n\ud835\udc39\u2212\ud835\udc5a\ud835\udc52\ud835\udc4e\ud835\udc60\ud835\udc62\ud835\udc5f\ud835\udc52= 2 \u00d7\ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b\u00d7\ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59\n\ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b+\ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59(4)\nThe training parameters for the proposed multimodal models are kept consistent across all three fusion strategies.\nEach model is trained with a batch size of 8 and a learning rate of3 \u00d7 10\u22125, optimized using the AdamW optimizer.\nThe maximum number of epochs is set to 30 for all fusion configurations.\nSelim et al.:Preprint submitted to ElsevierPage 7 of 11\nEnhancing Quranic Learning: A Multimodal Deep Learning Approach for Arabic Phoneme Recognition\nTo ensure fair hyperparameter selection, a grid search\u2013based tuning procedure is conducted on the validation set,\nwhere candidate values for the learning rate, batch size, and dropout rate are systematically evaluated. The selected\nconfiguration represents the best-performing combination identified through this search process.\nTo prevent overfitting, an early stopping mechanism is applied, which automatically terminates training when the\nvalidation loss fails to improve for a predefined number of epochs. This approach ensures that the models converge at\nthe point yielding the best generalization performance without unnecessary overtraining.\nThe results presented in Table 2 indicate that multimodal fusion strategies applied to the UniSpeech + BERT\nframework provide strong classification performance on both datasets. For Dataset A, the Early and Intermediate\nFusionstrategiesachievedidenticaltopperformanceacrossallevaluationmetrics(Accuracy:0.966,F1-Score:0.965),\ndemonstratingthatcross-modalinteractionsareequallyeffectivewhetherperformedattheinitialfeaturelevelorwithin\nintermediate network layers. This suggests that both fusion points enable efficient exploitation of complementary\nacoustic and linguistic cues.\nOn Dataset B, the Intermediate Fusion approach delivered the best results (Accuracy: 0.985, F1-Score: 0.985),\noutperforming both Early and Late Fusion configurations. This indicates that aligning modality-specific features at a\ndeeper layer of the network can yield superior representations when handling more diverse and challenging samples.\nMeanwhile, Early Fusion still maintains strong competitive performance, confirming its reliability as a robust and\nefficient baseline fusion strategy.\nConversely, Late Fusion consistently showed the lowest performance across both datasets, highlighting that\ncombiningindependentdecisionsattheoutputstagemaylimitsynergisticfeaturelearning.Sincehigh-levelmodality\ninteractionsoccuronlyafterclassification,thisapproachfailstofullyleveragecross-modalrelationshipsembeddedin\nthe data.\nOverall,thesefindingsdemonstratethatfeature-levelandintermediate-levelfusionaremoreeffectivethandecision-\nlevel integration, and that the optimal fusion position can vary depending on dataset complexity. The results support\nthe conclusion that multimodal representation learning\u2014rather than unimodal transformer processing\u2014drives the\nsubstantial improvements observed in downstream classification performance.\nTable 3 compares the performance of the proposed multimodal fusion model with the existing transformer-based\nstate-of-the-art (SOTA) method.\nFor Dataset A, the proposed multimodal fusion model achieves an accuracy of 0.966, outperforming the\ntransformer-based SOTA baseline by +2.2 percentage points, alongside consistent improvements across Precision,\nRecall, and F1-Score. This demonstrates that jointly modeling acoustic and linguistic information at the feature level\nenriches the representational space and enhances discriminative capability beyond what a unimodal transformer can\nachieve.\nTheimprovementbecomesmoreevidentforDatasetB,wheretheproposedapproachreachesanaccuracyof0.985\nversus0.970withtheSOTAmethod,reflectinga+1.5percentage-pointincrease.Again,allmetricsshowparallelgains,\nconfirmingthatcombiningBERT\u2019scontextuallanguageunderstandingwithUniSpeech\u2019sspeech-specificembeddings\nyields a more robust and generalizable classifier across varying data conditions.\nA key observation is that this advancement is achieved without increasing the complexity of a single transformer\nmodel. Instead of scaling a unimodal transformer to deeper or more parameter-heavy configurations, the proposed\napproach distributes learning across two modality-adapted transformer encoders and integrates them through an\neffective fusion mechanism. This highlights that performance improvements stem from cross-modal synergy rather\nthan model size or architectural depth.\nOverall, the findings confirm that the proposed multimodal fusion strategy offers a more accurate and efficient\nalternative to purely transformer-based unimodal models.\n6. Discussion\nTheexperimentalresultsshowthattheeffectivenessoftheproposedmultimodalfusionapproach,whichintegrates\nUniSpeech-based acoustic representationsand BERT-based textual embeddings to enhancespeech\u2013text classification\nperformance. Across both datasets, the fusion model outperformed the transformer-based state-of-the-art (SOTA)\nmethod.Thisindicatesthatcombiningmodality-specificrepresentationsprovidescomplementaryandmoredistinctive\ninformation than relying on a single transformer architecture.\nA key finding of this study is the clear benefit of cross-modal feature interaction. While transformer models\nsuch as UniSpeech or BERT individually capture high-quality representations within their respective domains, their\nSelim et al.:Preprint submitted to ElsevierPage 8 of 11\nEnhancing Quranic Learning: A Multimodal Deep Learning Approach for Arabic Phoneme Recognition\nTable 2\nEvaluation results of proposed fusion models on both datasets\nDataset Fusion Method Accuracy Precision Recall F1-Score\nA Early0.966 0.969 0.966 0.965\nA Intermediate0.966 0.969 0.966 0.965\nA Late 0.957 0.959 0.957 0.957\nB Early 0.970 0.974 0.970 0.970\nB Intermediate0.985 0.988 0.985 0.985\nB Late 0.956 0.964 0.956 0.955\nTable 3\nPerformance comparison with the state-of-the-art (SOTA)\nDataset Method Accuracy Precision Recall F1-Score\nA (Calik et al., 2024) 0.944 0.950 0.944 0.943\nA Proposed Method0.966 0.969 0.966 0.965\nB (Calik et al., 2024) 0.970 0.966 0.970 0.962\nB Proposed Method0.985 0.988 0.985 0.985\nunimodal nature limits the contextual richness required for robust decision-making in tasks where both speech and\nlinguisticcontextplayarole.Thefusion-basedmodelmitigatesthislimitationbybringingtogetherprosodic,spectral,\nand semantic information, resulting in more comprehensive and expressive latent feature spaces. This synergy was\nparticularly evident in the substantial gains seen in accuracy, precision, recall, and F1-score, especially on Dataset B,\nwhere the model achieved up to a 1.5 percentage-point improvement over the SOTA baseline.\nAnother notable aspect is that these improvements were achieved without increasing the architectural depth or\ncomputational burden of a single, monolithic transformer. Instead of scaling the size of the model, an approach\ncommonly adopted to boost performance in transformer-based systems, the proposed solution distributes learning\nacross two domain-optimized encoders and leverages a fusion network to integrate knowledge. This shows that, for\nmultimodalclassification,strategicfusiondesigncanbemoreimpactfulandefficientthanincreasingmodelcomplexity,\noffering both performance and computational advantages. Such findings align with emerging trends in multimodal\nlearning,whichemphasizetheimportanceofleveragingheterogeneouscuesratherthanrelyingsolelyonmodelscaling.\nIn summary, the findings validate that multimodal integration of speech and text features provides a stronger\nfoundation for classification tasks compared to unimodal transformer models, offering enhanced accuracy, improved\nrobustness,andbettergeneralizationwithoutaddedarchitecturalcomplexity.Theinsightsderivedfromthisstudycan\nguidethedevelopmentofnext-generationmultimodalsystemsandencourageashiftfrommodel-centricscalingtoward\nmodality-centric fusion strategies.\n6.1. Research Limitations\nDespite its contributions, several limitations should be acknowledged to contextualize the scope of the study.\nFirst, the datasets used for training and evaluation\u2014although enriched with diverse samples including YouTube\nrecordings\u2014remain relatively limited in scale and dialectal coverage. Arabic is characterized by significant regional\nand sociolinguistic variability; thus, models trained on a restricted subset of dialects may not fully generalize to all\npronunciationsoraccents.Futurelarge-scaledatacollectioneffortsarenecessarytoensurebroaderrepresentativeness.\nSecond, the speech data primarily consist of curated and relatively clean audio recordings, which may not\naccuratelyreflectreal-worldacousticconditions.Environmentalnoise,microphonevariability,andspontaneousspeech\nphenomena such as hesitations and co-articulations were not extensively represented in the dataset, limiting the\nframework\u2019s robustness in uncontrolled environments.\nThird, the study focused exclusively on transformer-based fusion architectures. While these models have shown\nstrong performance, other multi modal learning paradigms\u2014such as convolutional-recurrent hybrids, cross-attention\nnetworks, or graph-based fusion methods\u2014were not explored. Additionally, model interpretability was beyond the\npresentstudy\u2019sscope;adeeperinvestigationintowhichmodalityorfeaturesubsetcontributesmosttomispronunciation\ndetection could yield valuable insights for linguistic and pedagogical applications.\nSelim et al.:Preprint submitted to ElsevierPage 9 of 11\nEnhancing Quranic Learning: A Multimodal Deep Learning Approach for Arabic Phoneme Recognition\nFinally,althoughtheevaluationmetrics(accuracy,precision,recall,andF1-score)providedquantitativeinsightinto\nmodel performance, subjective human evaluation or expert linguistic assessment was not included. Such qualitative\nvalidation could further substantiate the model\u2019s pedagogical effectiveness in CALL systems.\n6.2. Potential Future Research\nBuilding on these findings, several promising avenues exist for future exploration. First, expanding the dataset to\ninclude additional speakers, genders, dialects, and recitation styles would enhance model generalization and enable\ncross-dialectal robustness. Collecting spontaneous, noisy, and emotionally varied speech samples would also allow\nmore realistic testing of model resilience. Second, integrating real-time pronunciation feedback mechanisms into\nthe multimodal framework could transform it into an interactive CALL tool. Embedding the model within mobile\nor web-based platforms would allow learners to receive immediate, context-aware feedback on pronunciation errors,\npotentiallyacceleratinglearningoutcomesandimprovingretention.Third,futurestudiesmayinvestigateadaptiveand\npersonalizedlearningmechanismsusingreinforcementlearningorspeakerembeddingtechniques,enablingthesystem\nto tailor feedback based on the learner\u2019s proficiency and progress. Such approaches could bridge the gap between\nautomatic assessment and individualized pedagogy. Fourth, exploring cross-lingual and multilingual multimodal\nfusion could extend this methodology to other morphologically rich or phonemically intricate languages, such as\nPersian, Urdu, or Turkish. Comparative studies across languages may also reveal universal patterns in multimodal\nphoneme learning.\nMoreover, incorporating prosodic, articulatory, and visual cues\u2014for instance, through lip-reading data or\nspectrogram-based articulatory feature modeling\u2014could provide a more holistic understanding of pronunciation and\narticulation. Finally, a potential future direction involves combining multimodal transformers with self-supervised\npretrainingonlargemultilingualcorpora,enhancingbothtransferabilityandefficiencyinlow-resourcecontexts.Col-\nlectively,thesedirectionsunderscorethatthepresentresearchnotonlyadvancesthestateofArabicmispronunciation\ndetection but also establishes a generalizable paradigm for multimodal speech assessment, bridging the gap between\nlinguistic theory, machine learning, and educational technology.\nDeclaration of Generative AI and AI-Assisted Technologies in the Writing Process\nDuringthepreparationofthisworktheauthorsusedChatGPTtoolinordertoimprovelanguageandreadability.After\nusing this tool, the authors reviewed and edited the content as needed and takes full responsibility for the content of\nthe publication.\nReferences\nAkhtar,S.,Hussain,F.,Raja,F.R.,Ehatisham-ulhaq,M.,Baloch,N.K.,Ishmanov,F.,Zikria,Y.B.,2020. Improvingmispronunciationdetectionof\narabic words for non-native learners using deep convolutional neural network features. Electronics 9, 963.\nAlgabri, M., Mathkour, H., Alsulaiman, M., Bencherif, M.A., 2022a. Mispronunciation detection and diagnosis with articulatory-level feedback\ngeneration for non-native arabic speech. Mathematics 10. doi:10.3390/math10152727.\nAlgabri, M., Mathkour, H., Alsulaiman, M., Bencherif, M.A., 2022b. Mispronunciation detection and diagnosis with articulatory-level feedback\ngeneration for non-native arabic speech. Mathematics 10, 2727.\nAlqadheeb,F.,Asif,A.,Ahmad,H.F.,2021.Correctpronunciationdetectionforclassicalarabicphonemesusingdeeplearning,in:2021International\nConference of Women in Data Science at Taif University (WiDSTaif), IEEE. pp. 1\u20136.\nAlrashoudi, N., Al-Khalifa, H., Alotaibi, Y., 2025. Improving mispronunciation detection and diagnosis for non-native learners of the arabic\nlanguage. Discover Computing 28, 1.\nAsif,A.,Mukhtar,H.,Alqadheeb,F.,Ahmad,H.F.,Alhumam,A.,2021. Anapproachforpronunciationclassificationofclassicalarabicphonemes\nusing deep learning. Applied Sciences 12, 238.\nBahi, H., 2024. Automatic pronunciation assessment and feedback for learners. World Scientific .\nCal\u0131k, S.S., Kucukmanisa, A., Kilimci, Z.H., 2023. An ensemble-based framework for mispronunciation detection of arabic phonemes. Applied\nAcoustics 212, 109593.\nCalik, S.S., Kucukmanisa, A., Kilimci, Z.H., 2024. A novel framework for mispronunciation detection of arabic phonemes using audio-oriented\ntransformer models. Applied Acoustics 215.\nDevlin, J., Chang, M.W., Lee, K., Toutanova, K., 2019. Bert: Pre-training of deep bidirectional transformers for language understanding.\narXiv:1810.04805.\nElKheir,Y.,Ibrahim,O.,Meghanani,A.,Almarwani,N.,etal.,2025. Towardsaunifiedbenchmarkforarabicpronunciationassessment:Qur\u2019anic\nrecitation as case study. Interspeech 2025 .\nFarooq, J., Imran, M., 2021. Mispronunciation detection in articulation points of arabic letters using machine learning, in: 2021 International\nConference on Computing, Electronic and Electrical Engineering (ICE Cube), IEEE. pp. 1\u20136.\nHaouhat, A., Bellaouar, S., Nehar, A., Cherroun, H., Abdelali, A., 2025. Arabic multimodal machine learning: Datasets, applications, approaches,\nand challenges. arXiv preprint arXiv:2508.12227 .\nSelim et al.:Preprint submitted to ElsevierPage 10 of 11\nEnhancing Quranic Learning: A Multimodal Deep Learning Approach for Arabic Phoneme Recognition\nHarere,A.A.,Jallad,K.A.,2023. Mispronunciationdetectionofbasicquranicrecitationrulesusingdeeplearning. arXivpreprintarXiv:2305.06429\n.\nLi, S., Tang, H., 2024. Multimodal alignment and fusion: A survey.arXiv:2411.17040.\nLo, T.H., Weng, S.Y., Chang, H.J., Chen, B., 2020. An effective end-to-end modeling approach for mispronunciation detection. arXiv preprint\narXiv:2005.08440 .\nLounis,M.,Dendani,B.,Bahi,H.,2025. One-classconvolutionalneuralnetworkforarabicmispronunciationdetection,in:IntelligentSystemsand\nPattern Recognition, Springer Nature Switzerland. pp. 311\u2013320.\nNazir, F., Majeed, M.N., Ghazanfar, M.A., Maqsood, M., 2019. Mispronunciation detection using deep convolutional neural network features and\ntransfer learning-based model for arabic phonemes. IEEE Access 7, 52589\u201352608.\nRadford, A., Kim, J.W., Xu, T., Brockman, G., Mcleavey, C., Sutskever, I., 2022. Robust speech recognition via large-scale weak supervision.\narXiv:2212.04356.\nVaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,L.,Polosukhin,I.,2017. Attentionisallyouneed,in:Advances\nin Neural Information Processing Systems (NeurIPS).\nWang, C., Wu, Y., Qian, Y., Kumatani, K., Liu, S., Wei, F., Zeng, M., Huang, X., 2021. Unispeech: Unified speech representation learning with\nlabeled and unlabeled data.arXiv:2101.07597.\nYan, B.C., Wang, H.W., Chen, B., 2023. Peppanet: Effective mispronunciation detection and diagnosis leveraging phonetic, phonological, and\nacoustic cues, in: 2022 IEEE Spoken Language Technology Workshop (SLT), IEEE. pp. 1045\u20131051.\nZhu, D., Liu, J., Yang, Y., Lin, Z., 2023. Multimodal learning with transformers: A survey. IEEE Transactions on Pattern Analysis and Machine\nIntelligence .\nSelim et al.:Preprint submitted to ElsevierPage 11 of 11\n",
    "title": "Enhancing Quranic Learning: A Multimodal Deep Learning Approach for Arabic Phoneme Recognition",
    "arxiv_id": "2511.17477v1",
    "category": "recent",
    "authors": [
      "Ayhan Kucukmanisa",
      "Derya Gelmez",
      "Sukru Selim Calik",
      "Zeynep Hilal Kilimci"
    ],
    "published": "2025-11-21",
    "num_pages": 11,
    "num_chars": 46846
  },
  {
    "text": "Under consideration for publication in J. Fluid Mech. 1\nBanner appropriate to article type will appear here in typeset article\nAddressingA PosterioriPerformance Degradation\nin Neural Network Subgrid Stress Models\nAndy Wu1\u2020and Sanjiva K. Lele1, 2\n1Department of Aeronautics and Astronautics, Stanford University, Stanford, California, USA\n2Department of Mechanical Engineering, Stanford University, Stanford, California, USA\n(Received xx; revised xx; accepted xx)\nNeural network subgrid stress models often havea prioriperformance that is far better than\nthea posterioriperformance, leading to neural network models that look very promisinga\nprioricompletely failing ina posterioriLarge Eddy Simulations (LES). This performance\ngap can be decreased by combining two different methods, training data augmentation and\nreducing input complexity to the neural network. Augmenting the training data with two\ndifferent filters before training the neural networks has no performance degradationa priori\nas compared to a neural network trained with one filter.A posteriori, neural networks trained\nwith two different filters are far more robust across two different LES codes with different\nnumerical schemes. In addition, by ablating away the higher order terms input into the neural\nnetwork, thea prioriversusa posterioriperformance changes become less apparent. When\ncombined, neural networks that use both training data augmentation and a less complex set\nof inputs havea posterioriperformance far more reflective of theira priorievaluation.\nKey words:Authors should not enter keywords on the manuscript\n1. Introduction\nLarge Eddy Simulations (LES) provides an economical paradigm for high fidelity flow\nprediction by resolving the larger, energy containing scales and modelling the spatial scales\nof turbulence that are smaller than the grid scale through a subgrid scale (SGS) closure (Sagaut\n2006). Spatially filtering the Navier-Stokes equations results in an unclosed term called the\nsubgrid stress tensor,\ud835\udf0f \ud835\udc56\ud835\udc57, which represents the effect of the subgrid spatial scales of turbulence\non the resolved flow (Pope 2000).\nRecently, with the availability of Direct Numerical Simulation (DNS) data online and\nadvances in deep learning theory, neural networks have been explored as subgrid stress\nmodels (Sarghiniet al.2003; Becket al.2019; Park & Choi 2021; Choet al.2024; Xieet al.\n2020a,b; Stofferet al.2021; Kanget al.2023; Maejimam & Kawai 2024; Chenget al.2022;\nWu & Lele 2025b). A persistent challenge is the discrepancy in neural networka prioriand\na posterioriperformance (Becket al.2019; Beck & Kurz 2021; Stofferet al.2021; Park &\n\u2020Email address for correspondence: awu1018@stanford.eduarXiv:2511.17475v1  [physics.flu-dyn]  21 Nov 2025\n2\nChoi 2021), which can be interpreted as a distribution shift between the training data and the\nLES simulation (Huet al.2025). Due to this distribution shift, ad-hoc methods are sometimes\nused, such as adding an additional Smagorinsky term to help the neural network dissipate\nenergy so that simulation remains stable (Becket al.2019). Two mechanisms may contribute\nto the distribution shift. First, the input fields provided to the neural network during training,\nwhich are generated assuming an explicit filter (oftentimes a box filter), can differ from the\nLES resolved fields, since the LES filter is implicit and is a product of both the grid and the\nnumerical method (Sagaut 2006). Second, the output SGS stress statistics also differ across\nfiltering operations. For example, the omission or use of two-thirds dealiasing in spectral\nmethods modifies the effective filter transfer function, producing different SGS statistics.\nThis motivates the hypothesis that neural networks trained on one filter may struggle to\ngeneralize to LES solversa posteriori. To address these two mechanisms of distribution\nshift, this work introduces a multi-filter data augmentation strategy that exposes the neural\nnetwork to various plausible filtered inputs and SGS stress distributions.\nReinforcement learning approaches (Kimet al.2022; Bae & Koumoutsakos 2022; Kurz\net al.2023) aim to bypass these issues by learning the SGS model (or wall model) directly\nin a LES solver. However, models learned in this manner are limited in their transferability\nto other solvers, as they learn the combined effect of solver-specific numerical effects and\nturbulence physics. In contrast, the present supervised learning approach aims to not learn\nsolver peculiarities, and incorporates known filtering operations into the training data. The\nfilters used do not mimic the specific transfer function of any specific LES solver and instead\nare inspired by common LES numerical behavior.\nFurthermore, another potential source of distribution shift can come from the use of\nincreasingly complex neural network inputs, such as higher order powers of the velocity\ngradient tensor. For example, the second invariant\ud835\udc44=0.5(\ud835\udc432\u2212trace(\ud835\udc492\n\ud835\udc56\ud835\udc57)))where\ud835\udc43=\ntrace(\ud835\udc49\ud835\udc56\ud835\udc57)and\ud835\udc49\ud835\udc56\ud835\udc57is the velocity gradient tensor, and the third invariant\ud835\udc45=\ud835\udc51\ud835\udc52\ud835\udc61(\ud835\udc49 \ud835\udc56\ud835\udc57)\nare more sensitive to aliasing and differences in numerical methods (where\ud835\udc51\ud835\udc52\ud835\udc61denotes\nthe determinant). Therefore, a systematic ablation of neural network input complexity is\nperformed to analyze the effect of inputs to neural network performancea posteriori.\nThe contributions of this work are twofold. First, multi-filter data augmentation is shown\nto significantly improve the robustness of learned SGS models across small architecture\nperturbations and across LES solvers with different numerical methods. Second, an ablation\nof input feature complexity reveals that more complex neural network inputs suffer more\nfroma posterioriperformance degradation. Together, these results enable the training of\nneural networks that are far more robusta posteriori, and are more indicative of theira priori\nperformance.\n2. Numerical Details\n2.1.LES\nLES simulations solve the filtered Navier-Stokes equations. The filtering operation is defined\nbelow:\n\u00af\ud835\udf12(x,\ud835\udc61)=\u222b\n\ud835\udc3a(r)\ud835\udf12(x\u2212r,\ud835\udc61)\ud835\udc51\ud835\udc5f,\u222b\n\ud835\udc3a(r)\ud835\udc51\ud835\udc5f=1 (2.1)\nwhere\ud835\udf12is the flow variable to be filtered,\ud835\udc3a(r)is the filtering kernel, and \u00af\ud835\udf12is the filtered\nflow variable. In all sections, the overbar operator is used to denote filtered quantities. For\nrobust subgrid stress modeling, the filtering operation must not be oscillatory in physical\nspace, as this would create additional small scales purely through the act of filtering (Sagaut\n3\n2006). This means that the spectral cutoff filter, which is in the shape of a sinc function in\nphysical space (while being sharp in spectral space) is not appropriate as a filter for subgrid\nstress modeling. With the definition of the filter above, the LES governing equations are\nwritten below: (in Einstein\u2019s summation notation)\n\ud835\udf15\u00af\ud835\udc62\ud835\udc56\n\ud835\udf15\ud835\udc65\ud835\udc56=0\n\ud835\udf15\u00af\ud835\udc62\ud835\udc56\n\ud835\udf15\ud835\udc61+\u00af\ud835\udc62\ud835\udc57\ud835\udf15\u00af\ud835\udc62\ud835\udc56\n\ud835\udf15\ud835\udc65\ud835\udc57=\u22121\n\ud835\udf0c\ud835\udf15\u00af\ud835\udc5d\n\ud835\udf15\ud835\udc65\ud835\udc56+\ud835\udf08\ud835\udf152\u00af\ud835\udc62\ud835\udc56\n\ud835\udf15\ud835\udc65\ud835\udc57\ud835\udf15\ud835\udc65\ud835\udc57\u2212\ud835\udf15\ud835\udf0f\ud835\udc56\ud835\udc57\n\ud835\udf15\ud835\udc65\ud835\udc57\n\ud835\udf0f\ud835\udc56\ud835\udc57=\ud835\udc62\ud835\udc56\ud835\udc62\ud835\udc57\u2212\u00af\ud835\udc62\ud835\udc56\u00af\ud835\udc62\ud835\udc57(2.2)\nwhere\ud835\udc5dis the filtered pressure,\ud835\udf0cis the density, \ud835\udc62is the filtered velocity, and\ud835\udf08is the\nkinematic viscosity. As seen, an unclosed term,\ud835\udf0f \ud835\udc56\ud835\udc57, is obtained by filtering the momentum\nequation. In LES, only the filtered values of the dependent variables are available, thus,\ud835\udf0f \ud835\udc56\ud835\udc57\ncan\u2019t be computed and has to be modelled. In the present work, the neural network will learn\nto approximate\ud835\udf0f \ud835\udc56\ud835\udc57directly.\n2.2.A Posteriori Flow Solvers\nSince this work involves drawing conclusions abouta posterioriperformance, two in-house\nLES flow solvers, PadeOps and PadeLibs (with different numerical methods) are used (Ghate\n& Lele 2017; Songet al.2024). When running forced homogenous isotropic turbulence\n(HIT), PadeOps is a Fourier-Spectral code with RK4-5 time stepping (Bogacki & Shampine\n1996). Therefore, two-thirds dealiasing is applied due to the non-linear component in the\nconvective acceleration term. Meanwhile, PadeLibs is a 6th order compact code with RK3\ntime stepping (with no 2/3 dealiasing). These two codes have vastly different numerical\nmethods, so thea posterioriconclusions obtained hold for different numerical schemes.\n3. Filtering and Neural Network Details\nNeural network SGS models depend on explicitly filtered DNS data, making the choice\nof filters critical. Mismatches between the explicit filters chosen and the implicit filters\nencountered ina posterioriLES can produce both input-side and output-side distribution\nshifts. To reduce such mismatches, the present work adopts multi-filter training, selecting\nexplicit filters to diversify the training distribution so that the filters span representative LES\nfiltering behaviors without approximating a specific LES solver\u2019s transfer function. The first\nexplicit filter chosen is the box filter, as it is a common filter used in previous studies. The\nbox filter is defined as follows:\n\u00af\ud835\udf12(x,\ud835\udc61)=\u222b\n\ud835\udc49\ud835\udf12(x\u2212r,\ud835\udc61)\n|\ud835\udc49|\ud835\udc51r(3.1)\nwhere\ud835\udc49is the volume (or region) of the box filter,|\ud835\udc49|is its measure. The box filter is not\noscillatory in physical space, but is oscillatory in spectral space.\n3.0.1.DSCF: A Custom Localized Low-Pass Filter\nTo approximate a \u201ccleaner\u201d grid-cutoff at the Nyquist wavenumber, a discrete spectral cutoff\nfilter (DSCF) is introduced. The DSCF approximates the spectral cutoff filter and avoids the\noscillatory kernel of an ideal spectral cutoff filter in physical space. To construct this localized\nlow-pass filter kernel, a discrete rectangular transfer function on a uniform frequency grid\nwas constructed, and then the inverse discrete Fourier transform was taken. The discrete\n4\nFigure 1: Characterizations of the DSCF in both physical and spectral space.\ud835\udf14 \ud835\udc50=2\ud835\udf0b\ud835\udc53\ud835\udc50\ncorresponds to the filter cutoff point, here designated at a 16\u0394 \ud835\udc37\ud835\udc41\ud835\udc46 filter width, while\n\ud835\udf14\ud835\udc50=2\ud835\udf0b\ud835\udc53\ud835\udc5b, \ud835\udc53\ud835\udc5b\u2208[\u22120.5,0.5]denotes the normalized angular frequency. Two different\nDSCF versions are shown, one with a 17 and the other with a 33 point support.\nfrequency response (from the rectangular transfer function) was specified as\n\ud835\udc3a\ud835\udc58=(\n1,|\ud835\udc62\ud835\udc58|\u2a7d\ud835\udc53\ud835\udc50,\n0,otherwise,(3.2)\nwhere\ud835\udc62\ud835\udc58\u2208[\u22120.5,0.5)denotes the normalized discrete frequency and\ud835\udc53 \ud835\udc50is the prescribed\ncutoff. This would be the grid scale cutoff of LES, for example, if one wanted to filter the\nDNS data at a\u0394 \ud835\udc3f\ud835\udc38\ud835\udc46=16\u0394\ud835\udc37\ud835\udc41\ud835\udc46 filter width, then\ud835\udc53 \ud835\udc50=1\n16=0.03125. The corresponding\nspatial filter kernel is obtained using the inverse discrete Fourier transform, followed by a\nnormalization to ensure\u00cd\n\ud835\udc5b\ud835\udc54\ud835\udc5b=1.\nThe filtering operation applied to a discrete field\ud835\udf12 \ud835\udc5bis then given by the discrete convolution\n\u00af\ud835\udf12\ud835\udc5b=\ud835\udc41/2\u2211\ufe01\n\ud835\udc5a=\u2212\ud835\udc41/2\ud835\udc54\ud835\udc5a\ud835\udf12\ud835\udc5b\u2212\ud835\udc5a.(3.3)\nAlthough the construction in (3.2) is formally based on an ideal spectral cutoff filter, the\nuse of a discrete frequency grid and discrete inverse transform produce a spatial kernel that\nis compactly supported and strictly positive in physical space. Both the physical space and\nspectral space characterizations of the DSCF are shown in figure 1. As seen in figure 1, the\nfrequency response is not perfectly rectangular, instead exhibiting a smoother roll-off with\nmild high wavenumber oscillations when compared to a box filter. Meanwhile, in physical\nspace, the filter is strictly positive, non-oscillatory, and has finite support. A drawback to\nDSCF is that achieving good frequency behavior requires roughly twice as many points in the\nphysical space kernel. For example, if filtering at a 16\u0394 \ud835\udc37\ud835\udc41\ud835\udc46 filter width, a box filter uses 17\npoints, while the DSCF requires the use of 33 points. Using only 17 points causes the DSCF\nto \u201croll off\u201d more slowly, removing less high frequency content. Meanwhile, the 33 point\nDSCF matches the initial decay of the box filter but significantly suppresses the oscillations\n(peak oscillation magnitude of 0.028 vs 0.22), and attenuates frequencies beyond the grid\ncutoff more than the Gaussian filter. As such, if trying to filter at a LES grid cutoff frequency\nof\ud835\udc65\u0394\ud835\udc37\ud835\udc41\ud835\udc46, the corresponding DSCF support is always set to 2\ud835\udc65+1 (to maintain symmetry\nof the kernel). Training datasets are formed using (i) only the box filter and (ii) both the box\nfilter and the DSCF. Neural networks trained with these datasets are evaluated in PadeLibs,\nwhose implicit filtering properties differ from both the box filter and the DSCF.\n3.0.2.Two-Thirds Dealiasing as an Additional Filter\nPadeOps uses a two-thirds dealiasing procedure, a filtering operation that modifies the\neffective transfer function. To study how multi-filter training interacts with this procedure,\na filter is derived that is consistent with two-thirds dealiasing. The total subgrid stress can\n5\nFigure 2: Overall graph neural network architecture. Note that \u201cres. conn.\u201d denotes\nresidual connections.\nbe calculated by applying the two-thirds dealiasing operation on top of the already filtered\nNavier-Stokes equations, defined as the tilde operation:\n\ud835\udf0f\ud835\udc53\n\ud835\udc56\ud835\udc57=g\ud835\udc62\ud835\udc56\ud835\udc62\ud835\udc57\u2212\u02dc\u00af\ud835\udc62\ud835\udc56\u02dc\u00af\ud835\udc62\ud835\udc57 (3.4)\nAs seen in equation 3.4, the subgrid stress tensor, after accounting for this two-thirds de-\naliasing operation, is denoted as\ud835\udf0f\ud835\udc53\n\ud835\udc56\ud835\udc57. The two-thirds de-aliasing operation is computed as\nan additional spectral cutoff filter on top of the box filtered data. This filtering operation\nwill be called box and two-thirds (BTF), and is used alongside the box filter to form the\naugmented dataset for PadeOps. Note that the BTF is not intended to exactly replicate the\ntransfer function of PadeOps, and it is unlikely that the transfer function of a fourier-spectral\ncode with 2/3 dealiasing is identical to BTF. Rather, BTF provides the neural network with\nexposure to the type of behavior that two-thirds dealiasing qualitatively introduces, to reduce\nthe training data distribution shift compared toa posterioriPadeOps.\n3.1.Neural Network details\nThe neural network used is a hybrid tensor basis neural network (TBNN) from Linget al.\n(2016) and graph neural network (GNN). In a classic TBNN, an Artificial Neural Network\n(ANN) maps inputs to scalar coefficients of the tensor basis expansion, Here, the ANN front-\nend architecture is substituted with a GNN, which allows for learned input stencils instead of\nprescribed input stencils (Abekawaet al.2023). To recap, the tensor basis expansion for LES,\nwhere the neural network predicts 8 scalar coefficients\ud835\udc50 1\u2212\ud835\udc50 8, can be written as (Stallcup\net al.2022; Wu & Lele 2025b):\n\ud835\udf0f\ud835\udc56\ud835\udc57=\ud835\udc50 1I+\ud835\udc50 2\u00afS+\ud835\udc50 3\u00afS2+\ud835\udc504\u00af\ud835\udec02+\ud835\udc505(\u00afS\u00af\ud835\udec0\u2212 \u00af\ud835\udec0\u00afS)+\ud835\udc50 6\u00af\ud835\udec0\u00afS\u00afR+\ud835\udc50 7(\u00afS2\u00af\ud835\udec0\u2212 \u00af\ud835\udec0\u00afS2)+\ud835\udc50 8(\u00af\ud835\udec0\u00afS\u00af\ud835\udec02\u2212\u00af\ud835\udec02\u00afS\u00af\ud835\udec0)\n(3.5)\nwhere \u00afSand \u00af\ud835\udec0denote the filtered strain rate or rotation rate tensor. The invariants, which\nare costly to compute, can be substituted with the velocity gradient tensor with no change in\nneural network accuracy (Wu & Lele 2025a). This is taken one step further, where one can\nuse the invariants of the velocity gradient tensor (\ud835\udc43,\ud835\udc44,\ud835\udc45), as well as the magnitudes of the\nstrain rate and rotation rate tensor as inputs instead (|\ud835\udc46|,|\u03a9|). The neural network architecture\nis shown in figure 2, which has many similarities to Wu & Lele (2025a). As seen, there are\ntwo different inputs to the neural network. The first group of inputs (such as\ud835\udc43,\ud835\udc44,\ud835\udc45) pass\nthrough layers with learnable weights and nonlinear activation functions. In contrast, the\ntensor basis inputs are passed directly to the model without undergoing any nonlinearities\nor learnable transformations that alter their overall structure. Instead, the tensor basis serve\nas an inductive bias for the neural network so that any output of the neural network obeys\nthe structure of\ud835\udf0f \ud835\udc56\ud835\udc57. This distinction is important since the neural network is more sensitive\nto inputs that go through nonlinear transformations (Novaket al.2018). Simplifying these\n6\nTable 1: Neural Network Configurations\nNumber of Filters Inputs Neural Network Structure Flow Solver Integrated Normalization Neural Network Name\nOne filter (box)\ud835\udc43,\ud835\udc44,\ud835\udc45,|\ud835\udc46|,|\u03a9|Original PadeOps, Padelibs Global NN-Box-Complex-Original-G\nTwo filter (box and BTF)\ud835\udc43,\ud835\udc44,\ud835\udc45,|\ud835\udc46|,|\u03a9|Original PadeOps Global NN-BoxBTF-Complex-Original-G\nTwo filter (box and DSCF)\ud835\udc43,\ud835\udc44,\ud835\udc45,|\ud835\udc46|,|\u03a9|Original Padelibs Global NN-BoxDSCF-Complex-Original-G\nOne filter (box)\ud835\udc43,\ud835\udc44,\ud835\udc45,|\ud835\udc46|,|\u03a9|Additional LN PadeOps, Padelibs Global NN-Box-Complex-ALN-G\nTwo filter (box and BTF)\ud835\udc43,\ud835\udc44,\ud835\udc45,|\ud835\udc46|,|\u03a9|Additional LN PadeOps Global NN-BoxBTF-Complex-ALN-G\nTwo filter (box and DSCF)\ud835\udc43,\ud835\udc44,\ud835\udc45,|\ud835\udc46|,|\u03a9|Additional LN Padelibs Global NN-BoxDSCF-Complex-ALN-G\nOne filter (box)\ud835\udc43,|\ud835\udc46|,|\u03a9|Original PadeOps, Padelibs Global NN-Box-Simple-Original-G\nTwo filter (box and BTF)\ud835\udc43,|\ud835\udc46|,|\u03a9|Original PadeOps Global NN-BoxBTF-Simple-Original-G\nTwo filter (box and DSCF)\ud835\udc43,|\ud835\udc46|,|\u03a9|Original Padelibs Global NN-BoxDSCF-Simple-Original-G\nOne filter (box)\ud835\udc43,|\ud835\udc46|,|\u03a9|Additional LN PadeOps, Padelibs Global NN-Box-Simple-ALN-G\nTwo filter (box and BTF)\ud835\udc43,|\ud835\udc46|,|\u03a9|Additional LN PadeOps Global NN-BoxBTF-Simple-ALN-G\nTwo filter (box and DSCF)\ud835\udc43,|\ud835\udc46|,|\u03a9|Additional LN Padelibs Global NN-BoxDSCF-Simple-ALN-G\nOne filter (box)\ud835\udc43,|\ud835\udc46|,|\u03a9|Original PadeOps, Padelibs Local NN-Box-Simple-Original-L\nTwo filter (box and BTF)\ud835\udc43,|\ud835\udc46|,|\u03a9|Additional LN PadeOps Local NN-BoxBTF-Simple-ALN-L\nTwo filter (box and DSCF)\ud835\udc43,|\ud835\udc46|,|\u03a9|Additional LN Padelibs Local NN-BoxDSCF-Simple-ALN-L\ninputs (less higher powers of the velocity gradient tensor) would reduce their susceptibility\nto distribution shifts, leading to bettera posterioriperformance. To investigate this, neural\nnetworks are trained with two different sets of inputs,\ud835\udc43,\ud835\udc44,\ud835\udc45,|\ud835\udc46|,|\u03a9|(\u201ccomplex inputs\u201d) and\n\ud835\udc43,|\ud835\udc46|,|\u03a9|(\u201csimple inputs\u201d). The simpler set is hypothesized to perform bettera posteriori.\nThe neural network model inputs and outputs should be dimensionless. A global normal-\nization (max-min normalization) is used for the input fields:\n\ud835\udc65\ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a=\ud835\udc65\u2212\ud835\udc65\ud835\udc5a\ud835\udc56\ud835\udc5b\n\ud835\udc65\ud835\udc5a\ud835\udc4e\ud835\udc65\u2212\ud835\udc65\ud835\udc5a\ud835\udc56\ud835\udc5b\u22172\u22121 (3.6)\nwhile a local normalization is adopted for the tensor basis and the subgrid stress tensor:\n\ud835\udc4a\ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a\n\ud835\udc56\ud835\udc57=\ud835\udc4a\ud835\udc56\ud835\udc57\n|\ud835\udc49\ud835\udc56\ud835\udc57|\ud835\udc65,\ud835\udf0f\u2217\n\ud835\udc56\ud835\udc57=\ud835\udf0f\ud835\udc56\ud835\udc57\n\u03942\n\ud835\udc3f\ud835\udc38\ud835\udc46|\ud835\udc49\ud835\udc56\ud835\udc57|2(3.7)\nwhere\ud835\udc4a\ud835\udc56\ud835\udc57is the tensor to be normalized,\ud835\udc49 \ud835\udc56\ud835\udc57denotes the velocity gradient tensor,|\ud835\udc49 \ud835\udc56\ud835\udc57|its\nmagnitude,\ud835\udc65the exponent to ensure dimensionless-ness,\u0394 \ud835\udc3f\ud835\udc38\ud835\udc46corresponds to the LES grid\nspacing, and repeated indices do not imply summation.\nTo ensure that the results are consistent across slight perturbations to the neural network\narchitecture, a variant architecture is introduced with additional layer normalization opera-\ntions between select GNN layers and the first 1x1 convolution layer. Input normalization is\nalso varied, using local normalization. Training follows (Wu & Lele 2025b) with a composite\nloss function that minimizes the root mean squared error between the predicted and actual\nsubgrid stress tensor (denoted as non-bolded RMSE), and the RMSE between the predicted\nand actual energy dissipation,\ud835\udf16=\u2212\ud835\udf0f \ud835\udc56\ud835\udc57\u00af\ud835\udc46\ud835\udc56\ud835\udc57(denoted as non-bolded DRMSE):\n\ud835\udc3f=RMSE(\ud835\udf0f \ud835\udc56\ud835\udc57,\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51\u2212\ud835\udf0f\ud835\udc56\ud835\udc57)/RMS(\ud835\udf0f\ud835\udc56\ud835\udc57)+RMSE(\ud835\udf16 \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51\u2212\ud835\udf16)/RMS(\ud835\udf16)(3.8)\nwhereRMSEis the root-mean squared error operation andRMSis the root-mean square\noperation.\n3.2.All Neural Network Configurations\nA summary of all the neural networks trained are seen in table 1. Neural networks are\ntrained with different training data (one filter versus augmented training data), different\ninputs (complex versus simple), and slightly different architectures (more or less layer\nnormalization). All neural networks are trained on forced HIT at a filter width of 16\u0394 \ud835\udc37\ud835\udc41\ud835\udc46.\n7\nTable 2:A prioritesting, each value is given as mean (standard deviation).\nNeural Network RMSE DRMSE Correlation\nNN-Box-Complex-Original-G 0.751 (0.017) 0.664 (0.026) 0.660 (0.020)\nNN-BoxBTF-Complex-Original-G 0.772 (0.017) 0.693 (0.028) 0.636 (0.020)\nNN-BoxDSCF-Complex-Original-G 0.755 (0.017) 0.675 (0.028) 0.660 (0.020)\nNN-Box-Complex-ALN-G 0.750 (0.017) 0.661 (0.026) 0.661 (0.020)\nNN-BoxBTF-Complex-ALN-G 0.753 (0.017) 0.664 (0.026) 0.659 (0.021)\nNN-BoxDSCF-Complex-ALN-G 0.758 (0.017) 0.675 (0.028) 0.660 (0.021)\nNN-Box-Simple-Original-G 0.751 (0.017) 0.662 (0.024) 0.660 (0.020)\nNN-BoxBTF-Simple-Original-G 0.758 (0.017) 0.668 (0.028) 0.657 (0.021)\nNN-BoxDSCF-Simple-Original-G 0.757 (0.017) 0.672 (0.027) 0.661 (0.020)\n4.A Priori Analysis\n4.1.One Filter versus Two Filter Results\nAs seen in table 2, where the neural networks are evaluated on the test set box filtered data,\nthere is noa prioriperformance degradation when training on two filters versus training on\none filter since all RMSE, DRMSE, and correlation coefficient values are within 5 percent\nof each other. For example, neural networks trained with a max-min normalization and\nusing the set of inputs\ud835\udc43,\ud835\udc44,\ud835\udc45,|\ud835\udc46|,|\u03a9|have all have RMSE values in the 0.75-0.78 range,\nDRMSE values in the 0.66-0.70 range, and correlation coefficients in the 0.63-0.66 range.\nAlso, using a complex or simple set of inputs has negligible difference on the neural network\nperformance, suggesting that just using|\ud835\udc46|,|\u03a9|is sufficient (\ud835\udc43=0 for incompressible flow,\nand is manually masked to be zero to prevent the neural network from overfitting to noise. It\nis kept so that the model can generalize to compressibility in future work).\n5.A Posteriori Analysis\nThe neural networks are evaluated on two different flow solvers running forced HIT\non a 64\u00d764\u00d764 grid (corresponding to 16\u0394 \ud835\udc37\ud835\udc41\ud835\udc46) and a Taylor Reynolds number of\n820. A quantitative metric is also given, sum spectral error (\ud835\udc46\ud835\udc46\ud835\udc38=\u00cd\n\ud835\udc58|(\ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udc38\ud835\udc37\ud835\udc41\ud835\udc46 \ud835\udc58)\u2212\n\ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udc38\ud835\udc3f\ud835\udc38\ud835\udc46 \ud835\udc58))|,\ud835\udc58\ud835\udc43\ud835\udc4e\ud835\udc51\ud835\udc52\ud835\udc3f\ud835\udc56\ud835\udc4f\ud835\udc60\u2208[0,35],\ud835\udc58 \ud835\udc43\ud835\udc4e\ud835\udc51\ud835\udc52\ud835\udc42\ud835\udc5d\ud835\udc60\u2208[0,24]), where\ud835\udc38(\ud835\udc58)denotes the energy\nspectra, with differences in \u201ck\u201d since PadeOps has 2/3 dealiasing. While SSE is a good\nfirst-order metric, it must be interpreted in a physical context. SSE is computed using the\nlogarithm of the energy spectra values, rendering it artificially sensitive to deviations in\nthe higher-wavenumber, dissipative range. This can manifest in SSE oftentimes assigning a\nlarger SSE value to dissipative spectra even if it is physically desirable, while not putting\nenough emphasis on high-wavenumber energy buildup. Therefore, visual inspection of the\nspectral decay or pile-up is still essential.\n5.1.PadeOps\nThe neural networks are integrated into PadeOps, and after running to statistical stationarity,\nthe spectra are shown below in figures 3a-3c. One can see that training with two filters\nsignificantly increases the robustness of the neural networka posteriori, as the spectra for\nneural networks trained with only one filter vary significantly in figures 3a-3b for small\nperturbations in the neural network architecture. Note that as seen in thea priorianalysis\n8\n(a)\n (b)\n (c)\nFigure 3:A posterioriPadeOps HIT Spectra, SSE is given in the legend (lower the better)\n(a)\n (b)\n (c)\nFigure 4:A posterioriPadelibs HIT Spectra, SSE is given in the legend (lower the better)\nsection, all neural networks have similara prioriperformance, but thea posterioriSSE for\nneural networks trained with one filter can vary by a factor of 4. When considering fig. 3c,\nusing a simpler set of inputs also significantly helps reduce thea posteriorivariability across\nvarious neural network architectures and normalizations, which is also seen in the SSE. Note\nthat in situations where the one filter approach performs well, the two filter approach also\nperforms well, while if the one filter approach performs poorly, oftentimes the two filter\napproach performs better.\n5.2.PadeLibs\nThe neural networka posterioriresults for PadeLibs, after running to statistical stationarity,\nare shown in figures 4a-4c to investigate if the results hold for a different flow solver with\ndifferent numerics. From figure 4a and figure 4b, one can see the same trend: training\nwith two filters significantly increases the robustness of the neural networka posteriori, as\ntraining with one filter has very large variance for slight perturbations of neural network\narchitectures, and has large SSE values even thougha prioriperformance of the neural\nnetworks are similar. The same trend holds for figure 4c. Using the simpler set of inputs leads\nto more consistenta posterioriperformance across different neural network normalizations\nand architectures. Models trained with one filter that perform well also show good results\nwhen trained on two filters, witha posterioriSSE values more similar to one another,\nconsistent witha priori results. This suggests that using the simpler input set results in\nless distribution shift. The robustness seen likely stems from the simpler input set having\nless numerical error amplification. Numerical error and aliasing often manifest as high-\nwavenumber perturbations, which are often amplified when taking higher powers of the\nvelocity gradient tensor. Thus, the \u201ccomplex\u201d input set suffers from a larger distribution\nshift. Even when the normalization is changed or the neural network architecture changes\nslightly, the simpler input set retains a reasonable spectra shape, with consistent results\nbetween one filter versus two filter cases. In general, when models trained with one filter\n9\nperform poorly, their two filter counterparts perform better, while if models trained on one\nfilter perform well, their two filter counterparts also perform well.\n5.3.Combined A Posteriori Analysis\nOverall, combining data augmentation and simpler inputs yield robusta posterioriperfor-\nmance across solvers with different numerical methods. Training with two filters allows\nthe neural network to see more diverse input and output distributions, leading to reduced\ndistribution shifta posteriori. This allows the neural network, even though it is trained on\ngeneric filters not exactly mimicking the LES implicit transfer function, to perform more\naccurately to unseen implicit filters as it is not overfitting to the kernel artifacts associated with\none specific filter (e.g. Gibbs Oscillations). Meanwhile, the \u201csimple\u201d training set outperforms\nthe more complex training set because it is able to reduce the input distribution shift, as higher\norder powers of the velocity gradient tensor suffer more from numerical artifacts such as\naliasing, especially at the grid cutoff resolution. By relying on the lower-order terms, the\n\u201csimple\u201d inputs allow the neural network to be less exposed to and not overfit to high-\nwavenumber numerical artifacts, increasing thea posteriorirobustness of the neural network\nmodel.\n6. Conclusion\nTwo methods to increase thea posteriorirobustness of neural networks are proposed, data\naugmentation and decreasing the complexity of neural network inputs. Data augmentation\ninvolves training with multiple filters in the training data, and two filters have been proposed.\nBTF accounts for two-thirds dealiasing as an additional filter, whereas DSCF is non-\noscillatory in physical space while approximating a sharp spectral cutoff filter in spectral\nspace.A priori, neural networks suffer no performance degradation when trained on one filter\nas compared to two filters, suggesting that neural network models are able to distinguish\nbetween various filters.A posteriori, neural network models trained with two filters are\nsignificantly more robust than neural networks trained with one filter, and this trend is seen\nacross two different LES flow solvers. Furthermore, neural network models with less complex\ninputs perform better, significantly reducing the distribution shifta posteriori. Training with\ntwo different filters does not reduce the performance of a neural network subgrid stress model\neithera prioriora posteriori. These trends hold across small neural network architecture\nperturbations and input normalizations. Training with data filtered with two different filters\nand also using less complex inputs to neural networks significantly increases the robustness\nof neural networksa posteriori.\nAcknowledgements.This work used CPU and GPU resources via Bridges2 at the Pittsburgh Supercomputing\nCenter through allocation PHY230025 from the Advanced Cyberinfrastructure Coordination Ecosystem:\nServices & Support (ACCESS) program, which is supported by National Science Foundation grants\n#2138259, #2138286, #2138307, #2137603, and #2138296. DNS data is provided by the Johns Hopkins\nTurbulence Database.\nFunding.Andy Wu is partially supported by NASA Cooperative Agreement number 80NSSC22M0108 and\nNorthrop Grumman, as well as the NDSEG Fellowship.\nDeclaration of interests. Declaration of Interests. The authors report no conflict of interest.\nREFERENCES\nAbekawa, A, Minamoto, Y, Osawa, K, Shimamoto, H & Tanahashi, M2023 Exploration of robust\nmachine learning strategy for subgrid scale stress modeling.Physics of Fluids35(1).\n10\nBae, HJ & Koumoutsakos, P2022 Scientific multi-agent reinforcement learning for wall-models of\nturbulent flows.Nature Communications13(1), 1443.\nBeck, A, Flad, D & Munz, C-D2019 Deep neural networks for data-driven LES closure models.Journal\nof Computational Physics398(108910).\nBeck, A & Kurz, M2021 A Perspective on Machine Learning Methods in Turbulence Modelling.Surveys\nfor Applied Mathematics and Mechanics44(1).\nBogacki, P & Shampine, LF1996 An efficient Runge-Kutta (4,5) pair.Computers & Mathematics with\nApplications32(6), 15\u201328.\nCheng, Y, Giometto, MG, Kauffmann, P, Lin, L, Cao, C, Zupnick, C, Li, H, Li, Q, Huang, Y,\nAbernathey, R & others2022 Deep learning for subgrid-scale turbulence modeling in large-\neddy simulations of the convective atmospheric boundary layer.Journal of Advances in Modeling\nEarth Systems14(5), e2021MS002847.\nCho, C, Park, J & Choi, H2024 A recursive neural-network-based subgrid-scale model for large eddy\nsimulation: application to homogeneous isotropic turbulence.Journal of Fluid Mechanics1000.\nGhate, AS & Lele, SK2017 Subfilter-scale Enrichment of Planetary Boundary Layer Large Eddy\nSimulation Using Discrete Fourier\u2013Gabor modes.Journal of Fluid Mechanics819.\nHu, Z, Subramaniam, A, Kuang, Z, Lin, J, Yu, S, Hannah, WM, Brenowitz, ND, Romero, J\n& Pritchard, MS2025 Stable machine-learning parameterization of subgrid processes in a\ncomprehensive atmospheric model learned from embedded convection-permitting simulations,\narXiv: 2407.00124.\nKang, M, Jeon, Y & You, D2023 Neural-network-based mixed subgrid-scale model for turbulent flow.\nJournal of Fluid Mechanics962.\nKim, J, Kim, H, Kim, J & Lee, C2022 Deep reinforcement learning for large-eddy simulation modeling in\nwall-bounded turbulence.Physics of Fluids34(10).\nKurz, M, Offenh \u00a8auser, P & Beck, A2023 Deep reinforcement learning for turbulence modeling in large\neddy simulations.International journal of heat and fluid flow99, 109094.\nLing, J, Kurzawski, A & Templeton, J2016 Reynolds averaged turbulence modelling using deep neural\nnetworks with embedded invariance.Journal of Fluid Mechanics807, 155\u2013166.\nMaejimam, S & Kawai, S2024 Coarse-grid large-eddy simulation by unsupervised-learning-based sub-grid\nscale modeling. InAIAA SciTech 2024 Forum, pp. AIAA 2024\u20131361.\nNovak, R, Bahri, Y, Abolafia, DA, Pennington, J & Sohl-Dickstein, J2018 Sensitivity and\ngeneralization in neural networks: an empirical study, arXiv: 1802.08760.\nPark, J & Choi, H2021 Toward neural-network-based large eddy simulation: application to turbulent\nchannel flow.Journal of Fluid Mechanics914.\nPope, SB2000Turbulent Flows, 1st edn. Cambridge University.\nSagaut, P2006Large Eddy Simulation for Incompressible Flows: An Introduction, 3rd edn. Springer\nScience & Business Media.\nSarghini, F, De Felice, G & Santini, S2003 Neural networks based subgrid scale modeling in large eddy\nsimulations.Computers & Fluids32(1), 97\u2013108.\nSong, H, Ghate, AS, Matsuno, KV, West, JR, Subramaniam, A & Lele, SK2024 A robust compact\nfinite difference framework for simulations of compressible turbulent flows.Journal of Computational\nPhysics519, 113419.\nStallcup, EW, Kshitij, A & Dahm, WJ2022 Adaptive Scale-Similar Closure for Large Eddy Simulations.\nPart 1: Subgrid Stress Closure. InAIAA SciTech, pp. AIAA 2022\u20130595. San Diego, CA: AIAA.\nStoffer, R, Leeuwen, CM, Podareanu, D, Codreanu, V, Veerman, MA, Janssens, M, Hartogensis,\nOK & Heerwaarden, CC2021 Development of a large-eddy simulation subgrid model based on\nartificial neural networks: a case study of turbulent channel flow.Geoscientific Model Development.\nWu, A & Lele, SK2025aSubgrid stress modelling with multi-dimensional state space sequence models,\narXiv: 2511.10910.\nWu, A & Lele, SK2025bTwo neural network unet architecture for subfilter stress modeling.Physical\nReview Fluids10(1), 014601.\nXie, C, Wang, J & Weinan, E2020aModeling subgrid-scale forces by spatial artificial neural networks in\nlarge eddy simulation of turbulence.Physical Review of Fluids5(5).\nXie, C, Yuan, Z & Wang, J2020bArtificial neural network-based nonlinear algebraic models for large\neddy simulation of turbulence.Physical of Fluids32(11).\n",
    "title": "Addressing A Posteriori Performance Degradation in Neural Network Subgrid Stress Models",
    "arxiv_id": "2511.17475v1",
    "category": "recent",
    "authors": [
      "Andy Wu",
      "Sanjiva K. Lele"
    ],
    "published": "2025-11-21",
    "num_pages": 10,
    "num_chars": 32112
  },
  {
    "text": "MASKED-AND-REORDEREDSELF-SUPERVISION FOR\nREINFORCEMENTLEARNING FROMVERIFIABLEREWARDS\nZhen Wang\u2217\nDP TechnologyZhifeng Gao\nDP TechnologyGuolin Ke\nDP Technology\nNovember 24, 2025\nABSTRACT\nTest-time scaling has been shown to substantially improve large language models\u2019 (LLMs) mathemat-\nical reasoning. However, for a large portion of mathematical corpora, especially theorem proving,\nRLVR\u2019s scalability is limited: intermediate reasoning is crucial, while final answers are difficult to\ndirectly and reliably verify. Meanwhile, token-level SFT often degenerates into rote memorization\nrather than inducing longer chains of thought. Inspired by BERT\u2019s self-supervised tasks, we propose\nMR-RLVR (Masked-and-Reordered RLVR), which constructs process-level self-supervised rewards\nvia \u201cmasked-then-fill\u201d and \u201cstep reordering\u201d to extract learnable signals from intermediate reasoning.\nOur training pipeline comprises two stages: we first perform self-supervised training on sampled\nmathematical calculation and proof data; we then conduct RLVR fine-tuning on mathematical cal-\nculation datasets where only outcomes are verifiable. We implement MR-RLVR on Qwen2.5-3B\nand DeepSeek-R1-Distill-Qwen-1.5B, and evaluate on AIME24, AIME25, AMC23, and MATH500.\nUnder a fixed sampling and decoding budget, MR-RLVR achieves average relative gains over the\noriginal RLVR of +9.86% Pass@1, +5.27% Pass@5, and +4.00% Pass@8. These results indicate\nthat incorporating process-aware self-supervised signals can effectively enhance RLVR\u2019s scalability\nand performance in only outcome-verifiable settings.\n1 Introduction\nLarge language models (LLMs) have recently made rapid progress on mathematical and scientific reasoning tasks,\ndriven by techniques such as chain-of-thought prompting, diverse sampling, and test-time scaling. Reinforcement\nlearning (RL) has further improved performance on reasoning-intensive tasks including mathematical problem solving,\ncode generation, and program synthesis [Yang et al., 2025, Guo et al., 2025]. A central question in these settings\nis how to design reward signals that guide models toward reliable and generalizable reasoning strategies. Verifiable\nrewards obtained by programmatically checking final answers or executing unit tests offer a practical solution: whether\na model output satisfies predefined symbolic or numeric constraints can often be determined automatically, providing\nscalable and low-cost supervision for RL [Guo et al., 2024, Yang et al., 2024]. Reinforcement Learning from Verifiable\nRewards (RLVR) [Shao et al., 2024] instantiates this idea by directly optimizing policies to pass symbolic or numerical\nchecks at the level of final answers, and has shown strong performance on code generation and mathematical reasoning\ntasks [Shao et al., 2024, Guo et al., 2025].\nHowever, terminally verifiable rewards primarily constrain the final answer. To further improve complex multi-step\nreasoning, it is crucial yet challenging to construct equally informative training signals for theintermediatereasoning\nprocess. One line of work explicitly leverages intermediate steps: process supervision and Process Reward Model\n(PRM) frameworks [Lightman et al., 2023, Guan et al., 2025] provide step-level supervision by scoring or classifying\nintermediate steps, improving stability and interpretability in mathematical reasoning. These approaches, however,\ntypically require large-scale, high-quality human annotations, and in complex theorem-proving scenarios it is inherently\ndifficult to decide whether a local step is reasonable. This leads to high annotation costs and limits scalability to diverse\nlarge-scale corpora. In addition, token-level supervised fine-tuning (SFT) on chain-of-thought data often degenerates\n\u2217Corresponding author.wangz@dp.techarXiv:2511.17473v1  [cs.CL]  21 Nov 2025\nAPREPRINT- NOVEMBER24, 2025\ninto imitating specific solution templates rather than learning transferable reasoning strategies [Lightman et al., 2023].\nHuman-centric supervision thus struggles to simultaneously achieve low annotation cost, scalability, and transferability\nof reasoning ability.\nRLVR removes the need for step-level labels but provides only weak constraints on the intermediate reasoning trajectory.\nAs a result, it is susceptible toprocess hallucinations: the model may generate plausible-looking yet incorrect or\nredundant reasoning steps, and such errors are often difficult to detect and correct using only terminal verification. In\nmany mathematical and theorem-proving datasets, correctness at the step level is important in its own right, yet hard\nto verify via a unified, low-cost programmatic procedure. Complementary to process supervision, recent work has\nexplored self-supervised signals or existing trajectories as denser rewards for reasoning, for example by using model\nconfidence in reference answers as intrinsic rewards or by designing next-segment reasoning objectives from expert\ntraces and pretraining corpora and converting them into RL training signals [Yu et al., 2025, Li et al., 2025, Deng et al.,\n2025]. These approaches mitigate reward sparsity without additional human annotations or domain-specific verifiers,\nsuggesting that combining self-supervision with RL is a promising direction for strengthening multi-step reasoning.\nWe ask whether it is possible to extractprocess-levelsignals directly from existing mathematical reasoning trajectories\nand convert them into verifiable rewards that are compatible with RLVR, without relying on additional human process\nannotations or explicit expert action sequences. Our starting point is the observation that self-supervised destruction\u2013\nreconstruction objectives, such as those used in BERT[Devlin et al., 2019, Raffel et al., 2023], help models capture\nsemantic and structural dependencies within context by masking and reconstructing missing spans. This property\nnaturally aligns with modeling the constraints and dependencies between steps in multi-step reasoning.\nIn this work, we propose MR-RLVR (Masked-and-Reordered RLVR), which augments RLVR with dense, structured\nprocess-level self-supervision derived from mathematical reasoning trajectories. Under a setting where only terminal\nrewards are externally verifiable, we construct internal process rewards that can be computed automatically and\nintegrated into RL training. Concretely, we design two process-aware tasks on proof-style and computation-style\ntrajectories: (1) Masked-Then-Fill, which masks key formulas, reasoning steps, or theorem invocations and requires the\nmodel to reconstruct the missing content given surrounding context; and (2) Step Reordering, which shuffles reasoning\nsteps and asks the model to recover a coherent logical order. For both tasks, we define process-level rewards based on\nthe match between generated spans and reference reasoning at the levels of mathematical entities and text, and on the\nagreement between predicted and reference step orders. These rewards can be computed automatically from existing\ntrajectories and used directly as RL signals, without any additional human annotation.\nMR-RLVR adopts a simple two-stage training framework. In Stage I, we use only the process-level rewards described\nabove to perform RLVR updates on mathematical corpora that jointly cover proof-style and computation-style reasoning,\nencouraging the policy to produce reasoning processes with more coherent local logic and clearer step dependencies.\nIn Stage II, starting from the Stage I checkpoint, we fine-tune the model on computational math problems with\nprogrammatically verifiable final answers, using only terminally verifiable rewards. The model first learns better\nreasoning structure under dense process-level signals, and then adapts to verifiable tasks under sparse but precise\nterminal supervision. This combination aims to improve the stability and scalability of RLVR on complex mathematical\nreasoning tasks without increasing human annotation cost, and to reduce process hallucinations.\nOur contributions are summarized as follows:\n1.Process-level self-supervision as verifiable rewards.We propose a framework that designs Mask-Then-Fill\nand Step Reordering tasks on mathematical reasoning trajectories and converts their outcomes into process-\nlevel rewards based on mathematical-entity matching and ordering consistency. This allows RLVR to receive\nfine-grained process supervision without any human process annotations.\n2.Two-stage MR-RLVR training under terminal-verifiable supervision.We introduce a two-stage training\nprocedure that first performs RLVR pretraining with process-level rewards on diverse proof and computational\nreasoning corpora, and then applies outcome-level RLVR fine-tuning on computational problems with verifiable\nfinal answers, alleviating exploration difficulties under sparse rewards.\n3.Empirical gains and data efficiency on mathematical reasoning benchmarks.On Qwen2.5-3B and\nDeepSeek-R1-Distill-Qwen-1.5B, MR-RLVR consistently outperforms a GRPO baseline on AIME24,\nAIME25, AMC23, and MATH500, achieving an average relative improvement of about 9.86% inPass@1 ,\n5.27% inPass@5 and4.00% inPass@8 under a fixed sampling budget. In low-data regimes, MR-RLVR also\nexhibits better sample efficiency than standard RLVR.\n2\nAPREPRINT- NOVEMBER24, 2025\n2 Related work\nPRM and RLVR for math reasoning.From the perspective of training signals, large-model training for mathematical\nreasoning typically follows two lines:process supervisionandoutcome-verifiable rewards. The former is exemplified\nby Process Reward Models (PRMs) combined with MCTS-style search, which score intermediate steps and use step-\nlevel value estimates to guide tree expansion and pruning, thereby achieving stable, interpretable, and search-capable\nreinforcement reasoning on math and code tasks [Lightman et al., 2023, Wang et al., 2024, Zhang et al., 2024, Guan et al.,\n2025]. However, such methods require expensive step-level annotations (or large-model scoring) as well as additional\nsearch infrastructure. In contrast, the RLVR line avoids explicit step labels and relies solely on programmable outcome\nverifiers that score final answers or executable code [Shao et al., 2024]. On automatically gradable benchmarks such\nas AIME, AMC, and MATH, systems including DeepSeek-Math, and Qwen2.5-Math have demonstrated substantial\ngains in mathematical and code reasoning under this paradigm [Guo et al., 2024, Shao et al., 2024, Guo et al., 2025,\nYang et al., 2024, 2025]. Nevertheless, standard RLVR imposes almost no constraints on the intermediate reasoning\nprocess and is thus prone to process hallucinations and redundant steps. In comparison, MR-RLVR preserves the RLVR\nassumption of relying only on outcome-verifiable rewards, without introducing external PRMs or human step labels;\ninstead, it automatically constructs self-supervised tasks such as masked reconstruction and step permutation on existing\nmathematical reasoning trajectories, converts their completion quality into process-level rewards usable by RL, and\ncombines them with a second-stage RLVR training based on outcome rewards.\nSelf-supervised process signals for reasoning tasks.A complementary line of work explores self-supervisedprocess\nsignalsfor reasoning tasks. ClozeMath adapts text-infilling and PrefixLM objectives to the mathematical setting\nby masking intermediate equations during supervised fine-tuning and requiring the model to recover them, thereby\nstrengthening the modeling of key mathematical entities and local structure; however, its self-supervised signal is\nonly used as an SFT loss and is not explicitly converted into RL rewards [Pham et al., 2025]. RLPR, RLPT, and\nSRL instead embed self-supervised signals into reinforcement learning or preference-optimization frameworks: RLPR\nuses the model\u2019s (relative) generation probability of a reference answer as an intrinsic reward, extending RLVR to\ngeneral domains without human scoring [Yu et al., 2025]; RLPT defines a next-chunk prediction objective on large-\nscale unlabeled text and employs an auxiliary model to score semantic consistency between the prediction and the\nground-truth continuation as a reward [Li et al., 2025]; SRL decomposes expert solutions into sequences of actions\nand uses action-level similarity as rewards to guide stepwise imitation of expert trajectories [Deng et al., 2025]. These\nworks demonstrate that probabilities, semantic consistency, and action similarity can all serve as effective intrinsic\nrewards, but their reward designs are mostly centered on the consistency or similarity of whole answers or relatively\nlong segments, with limited specialization for key entities, local logical dependencies, and step ordering in mathematical\nreasoning. MR-RLVR instead directly designs fine-grained, structure-aware self-supervised tasks (masked refilling and\nstep permutation) on mathematical reasoning trajectories and converts task outcomes into automatically computable\nprocess-level rewards that integrate seamlessly into an RLVR pipeline.\n3 Preliminaries\nA large language model (LLM) can be regarded as a conditional probabilistic model \u03c0\u03b8(y, z|x) , where x\u2208 X\ndenotes the input problem or context, z= (z 1, . . . , z T)represents the reasoning trajectory, and y\u2208 Y denotes the final\noutput. When a ground -truth answer is available, it is denoted by y\u22c6. The learning signal is provided by averifiable\nrewardfunction r(x, z, y)\u2208[0,1] , which quantifies the degree to which a model -generated solution satisfies the task\nspecification. This reward is computed automatically through a programmatic evaluation that checks the logical or\nfactual consistency of the model output. Depending on the task, rmay be instantiated through numerical tolerance\nscoring, symbolic or textual equivalence, structured output validation, or code-level unit and integration tests.\nRLVR objective.Reinforcement Learning from Verifiable Rewards (RLVR) maximizes the expected verifiable reward\nwhile penalizing divergence from a reference policy through a Kullback\u2013Leibler (KL) regularization term. Formally,\nthe optimization objective is\nmax\n\u03b8Ex\u223cD,(z,y)\u223c\u03c0 \u03b8(z,y|x) [r(x, z, y)]\u2212\u03b2E x\u223cD[KL(\u03c0 \u03b8(z, y|x)\u2225\u03c0 ref(z, y|x))],(1)\nwhere \u03c0refis a fixed or exponentially moving -averaged (EMA) reference policy, and \u03b2 >0 controls the trade -off\nbetween maximizing the verifiable reward and maintaining proximity to the reference distribution. The first term\nencourages the model to generate reasoning paths and answers that satisfy programmatically verifiable conditions,\nwhereas the KL penalty mitigates excessive policy drift and stabilizes training under the restricted data regime typical\nof verifiable tasks. This formulation parallels the general form of Reward -Regularized Policy Optimization (RRPO)\nand serves as the foundation for the GRPO update rule introduced below.\n3\nAPREPRINT- NOVEMBER24, 2025\nGRPO formulation.Generalized Reinforcement Policy Optimization (GRPO)[Shao et al., 2024]optimizes the policy\nby maximizing a clipped surrogate objective over groups of sampled outputs, providing stable updates within the RLVR\nframework. For each input q, a set of responses {oi}G\ni=1is drawn from the old policy \u03c0\u03b8old, and the policy parameters\nare optimized under the objective\nJGRPO(\u03b8) =E\uf8ee\n\uf8f01\nGGX\ni=11\n|oi||oi|X\nt=1min\u0010\n\u03c1\u03b8,i,t\u02c6Ai,t,clip(\u03c1 \u03b8,i,t,1\u2212\u03f5,1 +\u03f5) \u02c6Ai,t\u0011\uf8f9\n\uf8fb\u2212\u03b2 D KL(\u03c0\u03b8\u2225\u03c0ref),(2)\nwhere \u03c1\u03b8,i,t=\u03c0\u03b8(oi,t|q,oi,<t)\n\u03c0\u03b8old(oi,t|q,oi,<t)is the per -token likelihood ratio and \u02c6Ai,tis the corresponding advantage estimate.\nOutcome -level supervision assigns each sequence oia normalized scalar reward \u02c6Ai,t= (r i\u2212mean(r))/std(r) , shared\nacross all tokens in that sequence. The KL regularization term constrains deviation from reference model, and is\nestimated using the per-token unbiased form\nDKL(\u03c0\u03b8\u2225\u03c0ref) =\u03c0ref(oi,t|q, o i,<t)\n\u03c0\u03b8(oi,t|q, o i,<t)\u2212log\u03c0ref(oi,t|q, o i,<t)\n\u03c0\u03b8(oi,t|q, o i,<t)\u22121,(3)\nfollowing the unbiased estimator proposed by Schulman et al. [2015]. This formulation retains the stability properties\nof PPO while optimizing directly toward verifiable reward signals.\n4 Methodology\nMR-RLVR enables models to exploit intermediate reasoning even when only final answers are verifiable. Training pro-\nceeds in two stages. In Stage I, we derive process -level rewards from reasoning traces via process -aware self -supervised\ntasks (Masked -Then -Fill and Step Reordering) and run an initial RLVR phase using only these process -level rewards.\nIn Stage II, we fine -tune with RLVR supervised exclusively by programmatically verifiable final -answer rewards. This\ntwo-stage design first shapes the policy distribution with process -level signals and then trains under sparse outcome\nsupervision, yielding more informative gradients and more stable exploration. Figure 1 illustrates the framework: Stage\nI uses only process-level rewards; Stage II uses only final-answer rewards.\n4.1 Process-aware self-supervised data curation\nIn the first stage, we construct training data for process-aware self-supervised tasks, designed to guide the model\nin learning local logical consistency and step dependencies within reasoning trajectories. Specifically, we sample\nmathematical proof and computational reasoning problems to form the process-level self-supervised training corpus,\nwhile only the computational reasoning subset is used in Stage II fine-tuning.\nFor each problem xand its corresponding reasoning trajectory and answer (z, y) , we not only filter semantically\nwell-structured and symbolically valid reasoning texts, but also restructure overly verbose or redundant reasoning chains\nto obtain concise and logically coherent reasoning processes. Two structural transformations are then applied:\n(1)Masked-Then-Fill task.Key formulas, inference steps, or theorem invocations in the reasoning text are masked\nto create a list of masked positions M={m k}, along with the corresponding ground-truth completions. This task\nrequires the model to reconstruct missing reasoning content given contextual information.\n(2)Step Reordering task.The reasoning trajectory is decomposed into ordered steps {sj}, which are randomly\npermuted to form a perturbed sequence. The model is required to recover the correct order based on logical coherence.\nAfter these operations, we denote the processed reasoning sequence as \u02c6z. The model input consists of the problem\nstatement xand the modified reasoning \u02c6z, while the supervision signal involves either token restoration or step order\nprediction.\n4.2 Process-level reward design for self-supervision\nDuring the self-supervised phase, the model constructs a process-level reward signal based on either the Masked-Then-\nFill or Step Reordering task (only one of them is used per training run).\n(1) Masked-Then-Fill reward.For masked samples, hmasked locations are randomly selected for evaluation. The\nreward is defined as the mean semantic match score between model completions and ground truths, measured by\n4\nAPREPRINT- NOVEMBER24, 2025\nFigure 1: Overview of the MR-RLVR two-stage training framework. Stage I uses process-level rewards from self-\nsupervised tasks; Stage II uses final-outcome rewards.\nMathRuler for mathematical entity alignment and supplemented by textual similarity as fallback:\nrmask(x,\u02c6z, y) =1\nhhX\nk=1Match entity( \u02c6mk, m\u22c6\nk).(4)\n(2) Step Reordering reward.For the step reordering task, we measure how well the model recovers the correct\nposition of each reasoning step in the sequence. Let otruedenote the reference order over nreasoning steps, and opred\nthe permutation predicted by the model. We denote by postrue(k)the index of step kin the reference order otrue, and by\npospred(k)its index in the predicted ordero pred. We define a normalized position-based distance\ndpos(opred, otrue) =1\nnnX\nk=1I\u0002\npospred(k)\u0338= postrue(k)\u0003\n,\nwhere I[\u00b7]is the indicator function. This distance dpos\u2208[0,1] measures the fraction of steps that are placed at incorrect\npositions.\nThe corresponding step-order reward is then defined as\nrorder(x,\u02dcz, y) = 1\u2212d pos(opred, otrue),\nso that perfectly ordered sequences receive reward 1, while sequences in which all steps are misplaced receive reward 0.\nThis reward naturally lies in [0,1] and can be directly combined with the masked-then-fill reward within the MR-RLVR\nframework.\nAccordingly, the process-level reward is defined as\nrproc(x,\u02dcz, y) =I mask\u00b7rmask(x,\u02dcz, y) +I order\u00b7rorder(x,\u02dcz, y),(5)\nwhere Imask,Iorder\u2208 {0,1} are indicator functions for the Masked-Then-Fill and Step Reordering tasks with Imask+\nIorder= 1.\n5\nAPREPRINT- NOVEMBER24, 2025\n4.3 Stage II: fine-tuning with outcome-only rewards\nIn the second stage, we initialize from the checkpoint obtained through process-level reinforcement learning and\nfine-tune on programmatically verifiable problem instances. The model now generatescompletereasoning trajectories\nfrom the problem statement xand the model generates both reasoning zand answer y, receiving supervision from the\nfinal-outcome rewardr final(y\u22c6, y).\nOnly computational reasoning tasks are included here, as they feature open-ended reasoning trajectories but deterministic\nfinal answers. During fine-tuning, the model receives a binary verifiable reward:\nrfinal(y\u22c6, y) =I\u0002\nVerify(y\u22c6, y) =True\u0003\n,(6)\nwhere y\u22c6is the ground-truth answer and Verify(y\u22c6, y)denotes symbolic and numerical verification comparing the\ngenerated answeryagainsty\u22c6.\nThis signal is used to perform RLVR optimization under the GRPO objective. The two-stage scheme first shapes the\nreasoning distribution using dense process-level supervision, then refines it with sparse but precise outcome rewards,\nyielding stable optimization and verifiably correct multi-step reasoning behavior.\n5 Experiment\n5.1 Experimental Setup\nTraining Data CurationTo accommodate diverse reasoning styles, we construct our training corpus from two\ndata sources: DeepTheorem[Zhang et al., 2025] primarily contains theorem-proving problems, while DeepMath\n[He et al., 2025] focuses on computational reasoning tasks. We sample 10k problems equally from both datasets.\nSince the reasoning traces in DeepMath are generated by DeepSeek models and contain multiple internal verification\nsteps that may lead to information leakage within trajectories, we refine all DeepMath samples using GPT-o3-mini\nto ensure clean, step-by-step reasoning without redundant self-verification. As identifying key theorems, formulas,\nand reasoning steps for masking or reordering is non-trivial, we employ DeepSeek-R1-0528 to process the refined\nreasoning trajectories and generate task-specific annotations: masked positions for the Masked-Then-Fill task and step\nboundaries for the Step Reordering task (prompt templates in Appendix A). To avoid trivial masking tasks, we retain\nonly samples with at least 7 masked positions. From the filtered pool, we select 10k samples for each of the two tasks\n(Masked-Then-Fill and Step Reordering), randomly sampling 20k instances for Stage I training and 6k for validation.\nFor Stage II fine-tuning, we select 5k computational reasoning samples from the Stage I training data as the training set,\nwith 1.5k held out for validation, ensuring that Stage II focuses on verifiable computational problems with deterministic\nanswers.\nModel ConfigurationWe conduct experiments on Qwen2.5-3B-Base[Qwen et al., 2025] and DeepSeek-R1-Distill-\nQwen-1.5B[Guo et al., 2025]. We employ the GRPO objective for reinforcement learning. All experiments are\nconducted within the verl framework [Sheng et al., 2025] on a single node with 8 NVIDIA A100 (80 GB) or A800 (80\nGB) GPUs. To optimize GPU memory usage, several parameters differ slightly between Qwen-3B and DeepSeek-R1-\nDistill-Qwen-1.5. Detailed experimental hyperparameters can be found in the Appendix B.\nEvaluation SetupWe evaluate reasoning performance on four challenging mathematical benchmarks: AIME 2024,\nAIME 2025, AMC 2023 Li et al., 2024, and MATH500 Hendrycks et al., 2021. We report the unbiased estimator of\nPass@kChen et al. [2021], defined as\nPass@k=E x\u223cD\"\n1\u2212\u0000n\u2212c\nk\u0001\n\u0000n\nk\u0001#\n,(7)\nwhere nis the number of generated solutions per problem, cis the number of correct solutions, and k\u2208 {1,5,8} denotes\nthe number of attempts allowed. We set n= 64 for all evaluations. During inference, we use nucleus sampling with\ntemperature 0.6, top- p0.95, and a maximum generation length of 4096 tokens. Answers are verified programmatically\nthrough symbolic computation using MathRuler hiyouga [2025] and text matching.\n5.2 Main Results\nTable 1 presents the main experimental results comparing our MR-RLVR method against the GRPO baseline across\nfour mathematical reasoning benchmarks. Overall, MR-RLVR demonstrates consistent performance improvements over\n6\nAPREPRINT- NOVEMBER24, 2025\nTable 1: Performance comparison across mathematical reasoning benchmarks.\nQwen2.5-3B\nBenchmarkPass@1 (%) Pass@5 (%) Pass@8 (%)\nBase +GRPO +MR-RLVR Base +GRPO +MR-RLVR Base +GRPO +MR-RLVR\nAIME24 1.93 5.636.30(\u219112.04%)7.48 14.29 13.20(\u21937.61%)10.23 17.29 15.43(\u219310.74%)\nAIME25 0.73 2.032.76(\u219135.98%)3.58 8.5310.44(\u219122.44%)5.13 12.1014.05(\u219116.11%)\nAMC23 14.06 36.1340.82(\u219112.98%)41.70 60.3964.48(\u21916.82%)50.89 66.2969.80(\u21915.29%)\nMATH500 27.75 63.3065.87(\u21914.06%)62.97 79.8380.94(\u21911.39%)70.97 83.2983.85(\u21910.67%)\nDeepSeek-R1-Distill-Qwen-1.5B\nBenchmarkPass@1 (%) Pass@5 (%) Pass@8 (%)\nBase +GRPO +MR-RLVR Base +GRPO +MR-RLVR Base +GRPO +MR-RLVR\nAIME24 9.17 18.7019.43(\u21913.90%)21.93 36.4036.96(\u21911.54%)26.00 41.9842.08(\u21910.24%)\nAIME25 10.62 15.9417.24(\u21918.17%)23.10 27.4031.72(\u219115.77%)26.43 29.5035.43(\u219120.12%)\nAMC23 36.84 62.3063.01(\u21911.14%)66.57 84.2385.62(\u21911.65%)72.98 89.3089.48(\u21910.20%)\nMATH500 60.80 78.0578.51(\u21910.59%)85.04 90.0890.25(\u21910.19%)88.38 91.8591.97(\u21910.13%)\nWe report Pass@ k(k\u2208 {1,5,8} ) with n= 64 samples per problem. Arrows indicate relative improvement (%) over the GRPO\nbaseline.Boldindicates the best performance within each model family.\nGRPO, particularly in challenging scenarios where baseline performance is relatively low. On the Qwen2.5-3B model,\nwe observe substantial gains on AIME 2025, with Pass@1, Pass@5, and Pass@8 achieving relative improvements of\n35.98%, 22.44%, and 16.11%, respectively. Similar trends are evident in AIME24 Pass@1 (+12.04%) and across all\nmetrics on AMC23 (+5.29% to +12.98%).\nInterestingly, the performance gains exhibit interesting patterns across different model architectures and task difficulties.\nIn terms of average improvement magnitude, MR-RLVR yields significantly larger gains on Qwen2.5-3B (average\n8.29%) compared to DeepSeek-R1-Distill-Qwen-1.5B (average 4.47%). This discrepancy may be attributed to the\nfact that the former has only undergone basic pretraining without complex post-training procedures, thus offering\ngreater room for optimization through verification enhancement. Another notable observation is that both models show\nrelatively limited improvements on MATH500 (0.13%-4.06%). This is not only because the baseline performance is\nalready high (Pass@1>78%), but more importantly, the problems in MATH500 are relatively simple and the models\nhave largely mastered their solution patterns, resulting in minimal marginal gains from multi-round verification. In\ncontrast, our method demonstrates much stronger value on competition-level problems such as AIME and AMC. We\nalso observe minor performance degradation on AIME24 Pass@5/Pass@8 for Qwen2.5-3B, which we attribute to the\ninherent variance in the sampling process with n=64 samples.\nThese results validate the effectiveness of our MR-RLVR framework in improving mathematical reasoning capabilities.\nThe consistent gains on challenging benchmarks (AIME24, AIME25, AMC23) demonstrate that MR-RLVR success-\nfully enhances the model\u2019s ability to generate and verify correct solutions, particularly in scenarios where baseline\nperformance leaves substantial room for improvement. Meanwhile, the more modest improvements on relatively simple\ntasks like MATH500 indicate that our method provides maximum value when applied to high-difficulty problems at the\nboundary of model competence, which aligns well with the design principle of MR-RLVR, to tackle complex reasoning\ntasks not yet fully mastered by the model through iterative refinement.\n5.3 More Analysis about MR-RLVR\n5.3.1 Data Efficiency Analysis\nTo investigate the data efficiency of MR-RLVR, we conduct experiments with different training data scales on DeepSeek-\nR1-Distill-Qwen-1.5B. Table 2 compares MR-RLVR against the GRPO baseline using 1k and 3k training samples.\nResults show that MR-RLVR consistently outperforms GRPO across different data regimes. With 1k samples, MR-\nRLVR demonstrates significant improvements over GRPO, especially on Pass@5 and Pass@8 metrics, while Pass@1\nshows minimal gains. On AIME24, MR-RLVR achieves 26.90% and 31.84% for Pass@5/Pass@8 compared to GRPO\u2019s\n24.62% and 28.69%, representing relative gains of 9.26% and 10.99%, whereas Pass@1 remains unchanged at 11.09%.\nThis pattern suggests that with limited training data, the MR-RLVR framework primarily improves the model\u2019s ability\nto generate diverse high-quality candidates rather than directly enhancing single-sample accuracy. When scaled to\n3k samples, MR-RLVR maintains its advantage with a 10.08% relative improvement on AIME24 Pass@8 (40.97%\n7\nAPREPRINT- NOVEMBER24, 2025\nTable 2: Performance comparison of DeepSeek-R1-Distill-Qwen-1.5B under different training data scales.\nMethodAIME24 AIME25 AMC23 MATH500\nP@1 P@5 P@8 P@1 P@5 P@8 P@1 P@5 P@8 P@1 P@5 P@8\nBase Model 9.17 21.93 26.00 10.62 23.10 26.43 36.84 66.57 72.98 60.80 85.04 88.38\nTraining with 1k samples\n+GRPO 11.09 24.62 28.6911.9824.56 28.02 41.99 70.81 77.11 65.24 86.28 89.01\n+MR-RLVR11.09 26.90 31.8411.9325.45 28.91 42.73 72.82 79.25 65.67 86.49 89.24\nTraining with 3k samples\n+GRPO16.6732.41 37.22 15.31 28.67 31.97 58.01 81.8686.9176.4089.77 91.79\n+MR-RLVR 16.5635.13 40.97 15.57 29.37 32.35 59.02 82.7286.3176.2489.45 91.47\nTable 3: All experiments use DeepSeek-R1-Distill-Qwen-1.5B as the base model.Boldindicates better performance\nbetween GRPO and MR-RLVR at the same data scale.\nvs. 37.22%). This consistent advantage suggests that process-level self-supervision in MR-RLVR provides more\nsample-efficient learning signals than standard GRPO, enabling better generalization with limited training data. On\nthe simpler MATH500 benchmark, the gap narrows, confirming that MR-RLVR\u2019s benefits are most pronounced on\nchallenging problems.\n5.4 MR-RLVR Tasks for Data Augmentation\nGiven the sample efficiency gains demonstrated by MR-RLVR, we further explore self-supervised pretraining tasks for\nexpanding training signals without additional human annotations. We design two tasks, step reordering and masked-\nthen-fill, that leverage existing mathematical reasoning corpora to automatically generate diverse reasoning trajectories.\nTable 6 and Table 7 presents two representative cases together with model outputs.\nValue of the step reordering task.As shown in Table 6, in the case involving the Lebesgue differentiation theorem,\nthe model needs to restore 6 shuffled proof steps to their correct logical order. During this process, the model performs\ndetailed logical analysis of each step and identifies inter-step dependencies. For instance, the model recognizes that Step\n2 (defining F(x) ) is the starting point of the proof, Step 4 (providing the dominating function) is a necessary condition\nfor applying the Dominated Convergence Theorem, and Step 1 is the key theorem application step. This analytical\nprocess constitutes a structured reconstruction of the original proof: the model not only produces the correct ordering\nbut also generates an explanation of why this ordering is valid. Compared to the original shuffled steps, the model\nautomatically generates logical interpretations of each step and explicit annotations of inter-step dependencies during\nthe reordering process. These generated reasoning trajectories effectively complement the original concise reasoning\nprocess. While original proofs typically only provide key steps, the model\u2019s analysis reveals how to identify the overall\nproof structure and how to determine logical dependencies between steps, thereby making implicit reasoning structures\nexplicit. This automatically generated structured interpretation provides richer training signals for models to learn\ncomplete proof construction capabilities.\nValue of the masked-then-fill task.Table 7 presents a masked-then-fill case involving bitwise operations. The task\nrequires the model to complete three masked key formulas. The model successfully derives the first two: simplifying\nA\u22950xFFFFFFFF to\u223cA, and determining A=0x81010100 through bit analysis. However, at the third mask position\n(verification step), the model provides 0x7EFEFEFF\u22950x81010100=0xFFFFFFFF , while the original solution requires\ncomputing the addition 0x7EFEFEFF+0x81010100 . Although this equation is mathematically correct, since the\nXOR operation does yield 0xFFFFFFFF , it uses the wrong operator. More subtly, because these two numbers have\nno overlapping bits (no bit position is 1 in both numbers), addition and XOR happen to produce identical results in\nthis case. This coincidentally correct situation reveals speculative behavior: the model may directly apply the same\npattern after seeing XOR operators multiple times in the preceding text, or infer from context that the result should\nbe0xFFFFFFFFand then reverse-engineer a seemingly reasonable formula, rather than strictly following the required\nreasoning procedure. Such errors are more difficult to detect than obvious computational mistakes.\nOverall, the two tasks augment the training corpus in two complementary ways. First, correct trajectories generated\nduring step reordering and masked-then-fill provide detailed and structured reasoning traces that can be directly reused\nas additional training data. Second, speculative errors surfaced by the **masked-then-fill task** can be turned into\nerror-correction objectives, where models are trained to identify and fix logical flaws, thereby improving their self-\n8\nAPREPRINT- NOVEMBER24, 2025\nchecking capability. This pretraining stage therefore supplies more informative and sample-efficient learning signals\nthan relying solely on supervised solutions.\n6 Conclusion\nThis paper presents MR-RLVR, a framework that enriches reinforcement learning from verifiable rewards with\nprocess-level self-supervision. Instead of relying solely on outcome-level rewards derived from final-answer checking,\nMR-RLVR constructs two types of process-level tasks, namely masked-then-fill and step reordering, on mathematical\nreasoning traces, thereby providing dense training signals for intermediate reasoning steps. These tasks encourage the\nmodel not only to produce correct final answers, but also to acquire reusable patterns and structures of reasoning, rather\nthan merely memorizing superficial solution templates.\nWe implement and evaluate MR-RLVR on Qwen2.5-3B and DeepSeek-R1-Distill-Qwen-1.5B, and conduct systematic\nexperiments on a diverse set of mathematical benchmarks, including AIME24, AIME25, AMC23, and MATH500.\nUnder a fixed sampling and decoding budget, MR-RLVR consistently outperforms standard RLVR. This indicates that\nprocess-level self-supervision becomes especially beneficial when problems require long-horizon, multi-step reasoning.\nOur data efficiency analysis further shows that, compared to relying solely on outcome-level rewards, MR-RLVR\nprovides more informative learning signals in low-data regimes.\nFor future work, we first note that the current implementation adopts fixed masking positions for masked-then-fill\ntask and a fixed shuffling scheme for step reordering. An interesting direction is to explore dynamically sampling\nmasking locations and reordering strategies during training, allowing data augmentation and process-level tasks to\nadapt to the model\u2019s current state and further improve sample efficiency. Second, we plan to extend MR-RLVR to\nbroader structured reasoning domains such as program synthesis and formal theorem proving, as well as to multimodal\nreasoning tasks involving images, diagrams, and geometric figures, where rich structure and verifiable signals naturally\narise. In addition to masking and reordering, we aim to design more diverse process-level tasks, such as error correction\ntasks that explicitly require the model to identify and revise incorrect steps in a reasoning chain. Finally, MR-RLVR is\nhighly complementary to explicit process reward models and test-time scaling techniques; integrating these components\nmore tightly may further enhance the reliability and scalability of reasoning-focused language models. We hope that\nMR-RLVR offers a useful starting point for more principled integration of self-supervision and verifiable rewards in the\ntraining of reasoning-oriented large language models.\nReferences\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,\nYuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,\nGirish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser,\nMohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert,\nFotios Chantzis, Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas\nTezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira\nMurati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech\nZaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374 .\nYihe Deng, I-Hung Hsu, Jun Yan, Zifeng Wang, Rujun Han, Gufeng Zhang, Yanfei Chen, Wei Wang, Tomas Pfister,\nand Chen-Yu Lee. Supervised reinforcement learning: From expert trajectories to step-wise reasoning, 2025. URL\nhttps://arxiv.org/abs/2510.25992.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding, 2019. URLhttps://arxiv.org/abs/1810.04805.\nXinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small\nllms can master math reasoning with self-evolved deep thinking, 2025. URL https://arxiv.org/abs/2501.\n04519.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y . Wu, Y . K. Li,\nFuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder: When the large language model meets programming\n\u2013 the rise of code intelligence, 2024. URLhttps://arxiv.org/abs/2401.14196.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang,\nXiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.arXiv preprint\narXiv:2501.12948, 2025.\n9\nAPREPRINT- NOVEMBER24, 2025\nZhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang,\nWenxuan Wang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Deepmath-103k: A\nlarge-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning, 2025. URL\nhttps://arxiv.org/abs/2504.11456.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob\nSteinhardt. Measuring mathematical problem solving with the math dataset.arXiv preprint arXiv:2103.03874, 2021.\nhiyouga. Mathruler.https://github.com/hiyouga/MathRuler, 2025.\nJia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu,\nAlbert Q Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition\nmath problems and solutions.Hugging Face repository, 13(9):9, 2024.\nSiheng Li, Kejiao Li, Zenan Xu, Guanhua Huang, Evander Yang, Kun Li, Haoyuan Wu, Jiajia Wu, Zihao Zheng,\nChenchen Zhang, Kun Shi, Kyrierl Deng, Qi Yi, Ruibin Xiong, Tingqiang Xu, Yuhao Jiang, Jianfeng Yan, Yuyuan\nZeng, Guanghui Xu, Jinbao Xue, Zhijiang Xu, Zheng Fang, Shuai Li, Qibin Liu, Xiaoxue Li, Zhuoyu Li, Yangyu\nTao, Fei Gao, Cheng Jiang, Bo Chao Wang, Kai Liu, Jianchen Zhu, Wai Lam, Wayyt Wang, Bo Zhou, and Di Wang.\nReinforcement learning on pre-training data, 2025. URLhttps://arxiv.org/abs/2509.19249.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman,\nIlya Sutskever, and Karl Cobbe. Let\u2019s verify step by step, 2023. URLhttps://arxiv.org/abs/2305.20050.\nQuang Hieu Pham, Thuy Duong Nguyen, Tung Pham, Anh Tuan Luu, and Dat Quoc Nguyen. Clozemath: Improving\nmathematical reasoning in language models by learning to fill equations, 2025. URL https://arxiv.org/abs/\n2506.03763.\nQwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\nFei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou,\nJunyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu,\nRui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su,\nYichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025.\nURLhttps://arxiv.org/abs/2412.15115.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,\nand Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2023. URL\nhttps://arxiv.org/abs/1910.10683.\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous\ncontrol using generalized advantage estimation.arXiv preprint arXiv:1506.02438, 2015.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li,\nYang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.arXiv\npreprint arXiv:2402.03300, 2024.\nGuangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and\nChuan Wu. Hybridflow: A flexible and efficient rlhf framework. InProceedings of the Twentieth European Conference\non Computer Systems, EuroSys \u201925, page 1279\u20131297. ACM, March 2025. doi: 10.1145/3689031.3696075. URL\nhttp://dx.doi.org/10.1145/3689031.3696075.\nPeiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y . Wu, and Zhifang Sui. Math-shepherd:\nVerify and reinforce llms step-by-step without human annotations, 2024. URL https://arxiv.org/abs/2312.\n08935.\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren\nZhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang.\nQwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. URL https:\n//arxiv.org/abs/2409.12122.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang,\nChenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong\nTang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai\nDang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang,\nQin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren,\nXinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong\nLiu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL\nhttps://arxiv.org/abs/2505.09388.\n10\nAPREPRINT- NOVEMBER24, 2025\nTianyu Yu, Bo Ji, Shouli Wang, Shu Yao, Zefan Wang, Ganqu Cui, Lifan Yuan, Ning Ding, Yuan Yao, Zhiyuan Liu,\nMaosong Sun, and Tat-Seng Chua. Rlpr: Extrapolating rlvr to general domains without verifiers, 2025. URL\nhttps://arxiv.org/abs/2506.18254.\nDan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm self-training via\nprocess reward guided tree search, 2024. URLhttps://arxiv.org/abs/2406.03816.\nZiyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhenwen Liang, Zhuosheng\nZhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Deeptheorem: Advancing llm reasoning for theorem\nproving through natural language and reinforcement learning, 2025. URL https://arxiv.org/abs/2505.23754 .\n11\nAPREPRINT- NOVEMBER24, 2025\nAppendices\nA Prompts\nA.1 Prompts for Data Curation\nPrompt for masked-then-fill data curation\nYou are a helpful assistant.\nTask: Extract the most key formulas or theorem names from the following original answer text and save them in\nJSON format.\nOutput Format: Return the key formulas or theorem names in a JSON object with the following structure:\n{\n\"theorems\": [\n\"Theorem or formula name 1\",\n\"Theorem or formula name 2\",\n\"Theorem or formula name 3\",\n// Continue until all key formulas or theorem names are included\n]\n}\nRequirements:\n\u2022 Extract only the content from the original text without adding new formulas or theorems.\n\u2022 Use standard LaTeX format for all mathematical symbols and expressions.\n\u2022Sort the extracted theorems by importance, placing the most important ones first and the less important\nones later.\n\u2022 The output must comply with JSON format and be ready for use.\nPrompt for step reordering data curation\nYou are a helpful assistant.\nTask: Split the following answer into independent logical steps while maintaining the original meaning of the\ncontent.\nOutput Format: Return the steps in a JSON object with the following structure:\n{\n\"steps\": [\n\"Step 1 description...\",\n\"Step 2 description...\",\n\"Step 3 description...\",\n// Continue until all steps are included\n]\n}\nRequirements:\n\u2022 All steps must be generated from the original answer text without creating new steps or content.\n\u2022 Each step should maintain an independent logical meaning, allowing it to stand alone.\n\u2022The steps should connect logically in a way that reconstructs the original answer when combined\ntogether.\n\u2022 Ensure clarity and conciseness in each step to facilitate understanding.\n\u2022 Use standard LaTeX format for all mathematical symbols and expressions.\nA.2 Prompts for MR-RLVR\nblank\n12\nAPREPRINT- NOVEMBER24, 2025\nPrompt for Masked-Then-Fill task\nSystem:\nA conversation between the User and the Assistant.\nThe User supplies a mathematical statement together with a partial solution in which some formulas or theorems\nare masked with<formula_masked>tags.\nThe Assistant\u2019s task is to complete the missing portions of the solution by replacing the <formula_masked>\ntags with the appropriate mathematical formulas or theorems.\nPlease adhere to the following structured approach:\n1. Begin by performing a comprehensive logical analysis to determine the precise formula\nrequired for each <formula_masked> tag. The objective is to ensure the logical coherence\nand completeness of the entire solution.\n2. Enclose your **detailed logical analysis**, explaining the derivation of each missing\nformula, within<think>tags, formatted as follows:\n<think>\n[Your detailed reasoning process, explaining how each missing formula was de-\nrived.]\n</think>\n3. Finally, upon completion of the analysis and derivation of all missing formulas, provide\n**only** the derived formulas, enclosed within\\boxed{}notation:\n\\boxed{formula_1; formula_2; ...; formula_n}\nThe formulas within the \\boxed{} answer must appear in the same order as their correspond-\ning<formula_masked> tags in the original solution. All mathematical formulas should be\npresented using proper LaTeX notation.\nUser:\nThe user\u2019s statement:\nThe partial solution is:\nPrompt for Step reordering task\nSystem:\nA conversation between the User and the Assistant.\nThe User supplies a mathematical statement and a solution whose steps are out of order (each step is already\nnumbered with \u2019Step i\u2019).\nThe Assistant\u2019s task is to determine the correct logical sequence of these steps.\nPlease adhere to the following structured approach:\n1.Begin by performing a comprehensive logical analysis of the mathematical statement and all given\nsteps to establish their correct sequential order. The objective is to reconstruct a logically sound and\ncomplete solution.\n2.Enclose your **detailed logical analysis**, explaining how you determined the correct sequence,\nwithin<think>tags, formatted as follows:\n<think>\n[Your detailed reasoning process, explaining how the logical sequence of steps was deter-\nmined.]\n</think>\n3.Finally, provide **only** the correct sequence of step numbers, enclosed within \\boxed{} notation:\n\\boxed{n1, n2, n3, ..., nk}\nThe step numbers within the \\boxed{} answer must represent the final, logically ordered sequence of\nthe steps.\nUser:\nThe user\u2019s statement:\nThe shuffled solution:\n13\nAPREPRINT- NOVEMBER24, 2025\nPrompt for Outcome-only Task\nSystem:\nA conversation between User and Assistant.\nThe User provides a question, and the Assistant outputs the answer.\nThe Assistant\u2019s task is to solve the question and provide the final answer.\nPlease adhere to the following structured approach:\n1.Provide a concise solution analysis to determine how to compute the answer and enclose a detailed,\nstep-by-step derivation within<think>tags. Use the following format:\n<think>\n[Your detailed reasoning process analysis, explained through a step-by-step derivation.]\n</think>\n2. Finally, provide only the final result written in standard LaTeX and enclosed within\\boxed{ }.\nUser:\nhe user\u2019s question:\nB Implementation Details\nParameter qwen-3b STAGE I qwen-3b STAGE II\nLearning Rate (lr) 1\u00d710\u221261\u00d710\u22126\nRollout Number 16 16\nRollout Temperature 1.0 1.0\nPrompt Length Token 2048 1024\nResponse Length Token 4096 4096\nTraining Batch Size 512 512\nPPO Mini Batch Size 64 64\nKL Loss Coefficient 0.001 0.001\nTraining Epochs 3 3\nTable 4: The training hyperparameters of MR-RLVR for qwen-3b\nParameter Deepseek-R1-distill-Q wen-1.5b STAGE I Deepseek-R1-distill-Q wen-1.5b STAGE II\nLearning Rate 1\u00d710\u221261\u00d710\u22126\nRollout Number 8 16\nTemperature 1.0 1.0\nPrompt Length Token 2048 1024\nResponse Length Token 8192 4096\nTraining Batch Size 512 512\nPPO Mini Batch Size 64 64\nKL Loss Coefficient 0.001 0.001\nTraining Epochs 3 3\nTable 5: The training hyperparameters of MR-RLVR for deepseek-dis-qwen-1.5b\nC Test Cases\nblank\n14\nAPREPRINT- NOVEMBER24, 2025\nCase of Reordering Steps Task\nProblem:Leibniz Integral Rule under Lebesgue Conditions\nLetf:Rn\u2192R satisfy the conditions of the Lebesgue differentiation theorem, and let S\u2282Rnbe measurable\nwith0<|S|<\u221e.\nProve: For allxsuch that\u2202f\n\u2202xexists a.e. inS,\nd\ndxZ\nSf(x, y)dy=Z\nS\u2202f\n\u2202x(x, y)dy.\nInput:\nStep 0: State that for almost everyy\u2208S, the pointwise limit\nlim\nh\u21920f(x+h, y)\u2212f(x, y)\nh=\u2202f\n\u2202x(x, y)\nexists by hypothesis.\nStep 1: Apply the Dominated Convergence Theorem to interchange limit and integral, yielding\nlim\nh\u21920Z\nSf(x+h, y)\u2212f(x, y)\nhdy=Z\nS\u2202f\n\u2202x(x, y)dy.\nStep 2: DefineF(x) =R\nSf(x, y)dyand state the goal to show\ndF\ndx=Z\nS\u2202f\n\u2202x(x, y)dy.\nStep 3: Conclude that\ndF\ndx=Z\nS\u2202f\n\u2202x(x, y)dy.\nStep 4: Note that\f\f\f\u2202f\n\u2202x(x, y)\f\f\f\u2264g(y)and\n\f\f\f\ff(x+h, y)\u2212f(x, y)\nh\f\f\f\f\u2264g(y)\nfor sufficiently smallh, wheregis Lebesgue integrable overS.\nStep 5: Express the difference quotient forF(x)as\nF(x+h)\u2212F(x)\nh=Z\nSf(x+h, y)\u2212f(x, y)\nhdyforh\u0338= 0.\nOutput:\nStep 0: This is a crucial step in establishing the differentiability offin thex-direction.\nStep 1: This is a key step in proving the differentiability ofF(x).\nStep 2: This is the starting point of the proof.\nStep 3: This is the final result we are aiming to prove.\nStep 4: This is a necessary step to apply the Dominated Convergence Theorem.\nStep 5: This is a crucial step in establishing the differentiability ofF(x).\nBased on this analysis, the correct logical sequence is:\n2\u21925\u21920\u21924\u21921\u21923\nTable 6: Detailed Examination of Reordering Steps in the Proof of the Leibniz Integral Rule under Lebesgue Conditions\n15\nAPREPRINT- NOVEMBER24, 2025\nCase of Masked-Then-Fill Task\nProblem:Find the hexadecimal value ofAsatisfying\n(0x7EFEFEFF+A)\u2295(A\u22950xFFFFFFFF) =0x81010100\nwhere+denotes bitwise addition and\u2295denotes XOR.\nInput:\nStep 1. Simplification\nNote thatA\u22950xFFFFFFFF=\u223cAfor any 32-bit value. Rewrite the equation as:\n<formula_masked>\nStep 2. Binary Analysis\nWrite0x7EFEFEFFin binary:\n0x7EFEFEFF= 01111110 11111110 11111110 11111111\nObserve: zeros at bit positions 31, 24, 16, and 8.\nStep 3. Deriving A\nTo avoid carries during addition,Ashould have 1\u2019s only where0x7EFEFEFFhas 0\u2019s:\n<formula_masked>\nStep 4. Verification\nCompute0x7EFEFEFF+A. Since there is no overlap (no bit is 1 in both numbers), the addition result is:\n<formula_masked>\nThen verify:\n0xFFFFFFFF\u22950x7EFEFEFF=0x81010100\nOutput:\n<formula_masked>\u2192(0x7EFEFEFF+A)\u2295 \u223cA=0x81010100\u2713Correct\n<formula_masked>\u2192A=0x81010100\u2713Correct\n<formula_masked>\u21920x7EFEFEFF\u22950x81010100=0xFFFFFFFF\u00d7Wrong operator! Should be \u2019+\u2019\nExpected:0x7EFEFEFF+0x81010100=0xFFFFFFFF\nTable 7: Detailed Examination of Mask-Then-Fill for Finding the Hexadecimal Value of ABased on Bitwise Operations\n16\n",
    "title": "Masked-and-Reordered Self-Supervision for Reinforcement Learning from Verifiable Rewards",
    "arxiv_id": "2511.17473v1",
    "category": "recent",
    "authors": [
      "Zhen Wang",
      "Zhifeng Gao",
      "Guolin Ke"
    ],
    "published": "2025-11-21",
    "num_pages": 16,
    "num_chars": 53223
  },
  {
    "text": "PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for\nPersonalized LLM\nSiqi Liang1*\u2020, Yudi Zhang2*, Yue Guo3\n1Purdue University\n2Iowa State University\n3Columbia University\nAbstract\nWe propose a novel framework forpersona-basedlanguage\nmodel system, motivated by the need for personalized AI\nagents that adapt to individual user preferences. In our\napproach, the agent embodies the user\u2019s \u201cpersona\u201d (e.g.\nuser profile or taste) and is powered by a large language\nmodel (LLM). To enable the agent to leverage rich contex-\ntual information, we introduce a Knowledge-Graph-enhanced\nRetrieval-Augmented Generation (Graph RAG) mechanism\nthat constructs an LLM-derived graph index of relevant doc-\numents and summarizes communities of related information.\nOur framework generates personalized prompts by combin-\ning: (1) a summary of the user\u2019s historical behaviors and\npreferences extracted from the knowledge graph, and (2) rel-\nevant global interaction patterns identified through graph-\nbased community detection. This dynamic prompt engineer-\ning approach allows the agent to maintain consistent persona-\naligned behaviors while benefiting from collective knowl-\nedge. On the LaMP benchmark, our method improves news\ncategorization F1 by 11.1%, movie tagging F1 by 56.1%, and\nreduces product rating MAE by 10.4% over prior methods.\nOur code is available at https://anonymous.4open.science/r/\nPersonaAgentwGraphRAG-DE6F\nIntroduction\nLarge Language Models (LLMs) have shown strong perfor-\nmance across applications, from recommendation tasks (Xu\nand Zhang 2025; Liang, Zhang, and Wang 2025; Yu et al.\n2025; Said 2025; Lin and et al. 2023) to agent-based sys-\ntems capable of reasoning, dialogue, and tool use (Ruan and\net al. 2023). While earlier work applied LLMs to isolated\ncomponents of recommender systems, recent agent-based\napproaches address more ambitious challenges such as\nlong-horizon decision-making, collaboration, and domain-\nspecific reasoning (Wang et al. 2024), often enhanced with\nmemory, planning, retrieval, and inter-agent communica-\ntion for tasks like tutoring, simulation, and assistant work-\nflows (Zou et al. 2025).\n*These authors contributed equally.\n\u2020Equal contribution (alphabetical order). Corresponding\nauthors: Siqi Liang (lsq950917@gmail.com), Yudi Zhang\n(yudiz@iastate.edu)\nCopyright \u00a9 2026, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.Within this paradigm, persona-driven agents are increas-\ningly important for personalization: in recommender sys-\ntems, an agent may reflect a user\u2019s taste profile; in decision-\nsupport, it may simulate an expert\u2019s reasoning style. By en-\ncoding preferences and behaviors into a natural language\npersona, LLMs can adapt outputs to individual users, achiev-\ning personalized behavior across dialogue, reasoning, and\nrecommendation (Kenan Jiang 2024; Samuel, Zou, and et al.\n2024). However, most prior work relies on static personas or\ntemplates, without dynamically incorporating evolving be-\nhaviors or community knowledge. In domains like movie\nrecommendation or e-commerce, agents must produce de-\ncisions aligned with changing preferences, motivating our\nframework forpersona-based LLM agentsthat ground out-\nputs in both individual and collective knowledge.\nOur system integrates three components: (1) aper-\nsona promptencoding user preferences; (2) aknowledge\ngraphcapturing personal interactions and community pat-\nterns (Chen and et al. 2024; Xu and et al. 2024); and (3)\naGraphRAGmechanism that retrieves and synthesizes rel-\nevant context. Given a query, dense search identifies can-\ndidate nodes, graph traversal collects related user and item\nsignals, and the resulting subgraph is linearized and com-\nbined with the persona prompt for generation. This enables\ngrounding in both user history and community wisdom, sup-\nporting accurate and explainable personalization (Mansour\nand et al. 2024; Zerhoudi and Granitzer 2024).\nTo our knowledge, this is the first system to combine\ngraph-based retrieval with dynamic persona prompting de-\nrived from both individual and community patterns. The re-\nsult is a knowledge-aware, preference-aligned agent that im-\nproves personalization in tasks such as movie tagging and\nproduct rating.\nRelated Work\nPersona-Based LLM Agents\nRecent work has begun to endow LLM agents with ex-\nplicit personas to achieve personalized behavior. Persona\nagents have demonstrated improved contextual and person-\nalized responses across applications such as tutoring, cus-\ntomer support, and gaming (Zhang and et al. 2024). Per-\nsonaGym (Samuel, Zou, and et al. 2024) measures whether\nagents take optimal actions aligned with their personas,arXiv:2511.17467v1  [cs.LG]  21 Nov 2025\nFigure 1: Overview of the PersonaAgent with GraphRAG framework.\nit assesses adherence to persona-specific communication\nstyles, consistency in persona attributes, and avoidance of\nharmful outputs. HARBOR (Kenan Jiang 2024) explores\nhow an agent\u2019s assigned persona affects its bidding behav-\nior, whether agents can accurately profile competitors\u2019 per-\nsonas during auctions. Other studies have shown that per-\nsona prompts allow agents to extrapolate consistent prefer-\nences (e.g., adjusting answers about a tractor differently for\na farmer vs. an urban planner persona). However, most prior\nwork focuses on static personas or template-based personal-\nization, rather than dynamically incorporating user behavior\npatterns and community knowledge.\nMemory and Knowledge Integration in LLM\nSystems\nMemory and knowledge integration are critical for maintain-\ning consistent and informed agent behavior. LLM-memory\nsystems typically combine short-term context windows with\nlong-term external memories (Xu and et al. 2024; Chen and\net al. 2024). For example, Xu et al (Xu and et al. 2024)\nproposes a sophisticated memory architecture with multi-\nple specialized memory types (episodic, semantic, procedu-\nral, etc) to support complex reasoning tasks. More generally,\nsurveys have drawn analogies between human memory sys-\ntems and AI memory modules (Chen and et al. 2024). These\ninsights inform our approach to maintaining user preference\nhistories and behavioral patterns.\nRetrieval-Augmented Generation and Knowledge\nGraphs\nRetrieval-Augmented Generation (RAG) techniques use ex-\nternal knowledge to improve LLM outputs. Classical RAG\napproaches select relevant text passages via sparse term-\nmatching or dense embedding search (Lewis and et al. 2023;\nMansour and et al. 2024). Recent work has extended this\nto graph-based knowledge structures. Graph-based RAG\n(GraphRAG) enriches retrieval with structured knowledgegraphs: after an initial search for relevant entities, the system\ntraverses graph links to gather related information (Man-\nsour and et al. 2024; Zerhoudi and Granitzer 2024). This ap-\nproach grounds LLMs in relational data, improving factual\naccuracy and explainability. Our framework builds on these\nideas by encoding both domain knowledge and user behav-\nior patterns in a knowledge graph, using a combination of\nvector retrieval and graph expansion to construct personal-\nized contexts for the LLM.\nMethodology\nOur PersonaAgent system leverages Knowledge Graph-\nbased GraphRAG to enable personalized content genera-\ntion. The system combines individual user preferences with\nbroader community insights through a structured knowledge\ngraph and personalized prompt generation (see Fig 1).\nKnowledge Graph Construction\nOur system maintains a heterogeneous knowledge graph\nG= (V, E)where nodesVrepresent:\n1.Interaction nodes: Represent user interactions and con-\ntain metadata such as title, text, category, and timestamp.\n2.Concept nodes: Represent extracted named entities and\ndomain-relevant keywords from interaction text. These\nnodes generalize across users and support semantic rea-\nsoning.\n3.Category nodes: Represent high-level content domains,\nlinking interactions with broader thematic structures.\nEdgesEin the graph capture semantic relationships be-\ntween nodes:\n\u2022Interaction\u2194Category: Connects interactions to their\ncategorical domain.\n\u2022Interaction\u2194Concept: Links an interaction to its ex-\ntracted concepts or entities.\n\u2022Concept\u2194Concept: Can be inferred via co-occurrence\nacross interactions or shared categories to enable graph-\nbased community detection.\nFor each new interaction, the system 1) Creates an interac-\ntion node with unique identifier; 2) Extracts relevant con-\ncepts using pattern-based and domain-specific methods; 3)\nEstablishes connections to existing nodes based on semantic\nrelationships.\nGraphRAG Retrieval Mechanism\nThe system employs a dual-source retrieval approach that\ncombines personal and community-based insights:\nUser-Specific RetrievalFor a given useruand queryq,\nwe retrieve relevant interactions from their history:\nIuser(u, q) =TopK(sim(q, i) :i\u2208 H u)\nwhereH urepresents useru\u2019s interaction history and sim\nrefers to the Cosine similarity measured by TF-IDF.\nGlobal RetrievalWe augment personal context with rel-\nevant community interactions:\nIglobal (u, q) =TopK(sim(q, i) :i\u2208 H all\\ Hu)\nThe combined semantic contextC(u, q)includes:\nC(u, q) ={I user(u, q),I global (u, q),(1)\nPcat(u),E concepts (u, q)}(2)\nwhereP cat(u)represents user category preferences and\nEconcepts (u, q)contains relevant concepts.\nPersonalized Prompt Generation\nThe system generates context-rich prompts by combining:\n1) Task-specific instructions and available categories; 2) Re-\ntrieved personal interactions with relevance scores; 3) Re-\nlated global community interactions and patterns; 4) User\npreference distributions; 5) Relevant concept clusters.\nThe prompt construction process follows:\nAlgorithm 1: Personalized Prompt Generation\nRequire:User IDu, Queryq, CategoriesC\nEnsure:Personalized promptP\n1:context\u2190GetSemanticContext(u,q)\n2:content\u2190ExtractTaskContent(q)\n3:P\u2190InitializeBasePrompt(content,C)\n4:P\u2190P+FormatUserIntereaction\n5:P\u2190P+FormatCommunityIntereaction\n6:P\u2190P+FormatPreferencesAndConcepts(context)\n7:returnP\nResults\nData Description\nWe evaluate our framework using the LaMP bench-\nmark (Salemi et al. 2023), focusing on three decision-\nmaking tasks: news categorization, movie tagging, and prod-\nuct rating. These tasks enable us to assess the effectivenessof personalized agents across diverse personalization do-\nmains. Following the data processing procedure described\nin (Zhang and et al. 2024), we construct test sets by selecting\nthe 100 users with the most extensive activity histories from\nthe time-ordered version of the LaMP dataset. In the train-\ning sets which are used to construct the knowledge graph,\nthe news data includes 274 users, the movie data includes\n829 users, and product rating includes 1000 users.\nMetircs Comparison\nTable 1 presents results on three personalized tasks:\nnews categorization (LaMP-2N), movie tagging (LaMP-\n2M), and product rating (LaMP-3). PersonaAgent with\nGraphRAG consistently outperforms all baselines includ-\ning non-personalized LLMs (Liu et al. 2021), retrieval-\naugmented prompting (ReAct) (Yao et al. 2023), memory-\nbased models (MemBank) (Zhong et al. 2023), and the prior\nstate-of-the-art PersonaAgent (Zhang and et al. 2024). On\nLaMP-2N, it achieves 0.804 accuracy and 0.591 F1, improv-\ning over PersonaAgent by 1.0% and 11.1%, respectively.\nOn LaMP-2M, the gains are larger, with accuracy increasing\nfrom 0.513 to 0.653 (+27.3%) and F1 from 0.424 to 0.662\n(+56.1%), demonstrating stronger personalization for sub-\njective behaviors. For LaMP-3, GraphRAG reduces MAE\nfrom 0.241 to 0.216 (\u201310.4%) and RMSE from 0.509 to\n0.484 (\u20134.9%), indicating more precise rating prediction. We\nalso noticed that with our method, small models, such as\nLLaMA3 can perform better than competing methods, for\nexample, on the movie data, accuracy can be improved by\n13.6%. These results highlight the value of integrating struc-\ntured user memory with graph-based retrieval.\nFigure 2: LLMs Comparison on LaMP-2N\nFigure 2 compares five language models\u2014Mistral\nSmall (Jiang and et al. 2023), LLaMA2-7B (Touvron and\net al. 2023), LLaMA3-8B (AI 2024), Claude 3.5 Sonnet,\nand Claude 4 (Anthropic 2024)\u2014on the LaMP-2N per-\nsonalized news categorization task. Mistral Small performs\nworst across all metrics, reflecting its limited capacity for\npersonalization. LLaMA2-7B shows strong results, rivaling\nLLaMA3-8B in Accuracy and Recall despite its smaller size,\nwhile LLaMA3-8B offers more balanced improvements in\nF1 and Recall. Among Claude models, Claude 3.5 Sonnet\nachieves the best overall performance, with the highest F1\nMetrics Non-Personalized ReAct MemBank PersonaAgentPersonaAgent\nwith GraphRAG\nLaMP-2N: Personalized\nNews CategorizationAcc 0.660 0.639 0.741 0.796*0.804\nF1 0.386 0.381 0.456 0.532*0.591\nLaMP-2M: Personalized\nMovie TaggingAcc 0.387 0.450 0.470 0.513*0.653\nF1 0.302 0.378 0.391 0.4240.662\nLaMP-3: Personalized\nProduct RatingMAE 0.295 0.313 0.321 0.241*0.216\nRMSE 0.590 0.590 0.582 0.509*0.484\nTable 1: Performance comparison across different tasks and models\nFigure 3: Case study of PersonaAgent with GraphRAG for personalized classification\nand Recall, highlighting its superior alignment with user-\nspecific content. By contrast, Claude 4 underperforms across\nall metrics, often overcomplicating the task and failing to\nprovide correct answers.\nCase Study\nThe example in Fig 3 demonstrates that incorporating glob-\nally similar interactions from other users into the person-\nalization prompt substantially improves classification ac-\ncuracy by providing a broader contextual grounding be-\nyond a single user\u2019s history. In our example, the LLaMA3-\n8B model misclassified an article about a Parkland shoot-\ning survivor\u2019s essay for Teen V ogue as belonging to the\n\u201cwomen\u201d category when only the user\u2019s personal interaction\nhistory was considered. This error is likely due to the user\u2019s\nstrong historical preference for women-focused protest arti-\ncles, which skewed the model\u2019s prediction. However, when\nwe enriched the prompt with globally similar articles\u2014such\nas those involving youth activism and gun law reform (e.g.,\n\u201cTeen Survivors Of Florida Shooting To March On Wash-\nington\u201d)\u2014the model correctly classified the article as \u201cpol-\nitics\u201d. These globally similar interactions helped steer the\nmodel toward the correct thematic alignment by introducing\nrelevant but more nuanced examples, thus balancing person-\nalization with generalizability. This demonstrates that com-\nmunity context acts as a corrective signal, especially when\na user\u2019s preferences are strongly skewed or lack diversity\nacross topics.Conclusion\nWe introduced PersonaAgent with GraphRAG, a frame-\nwork that integrates persona-driven prompting with graph-\nenhanced retrieval to provide accurate, explainable, and con-\nsistent personalization. By leveraging both user-specific his-\ntories and global community patterns, the system balances\nindividual preferences with collective knowledge, yielding\nimprovements in news categorization, movie tagging, and\nproduct rating.\nLooking forward, two promising directions emerge. First,\nmulti-agent collaboration, where persona agents interact, ne-\ngotiate, and share knowledge, could enhance robustness and\nenable collective intelligence for recommendation, classifi-\ncation, and decision-support. Second, incorporating inverse\nreinforcement learning (IRL) (Beliaev and Sadigh 2024; Ke\net al. 2025; Jeon et al. 2020) would allow agents to infer\nlatent preference signals from behavior, aligning with both\nexplicit histories and implicit reward structures. This could\nproduce agents that better adapt to evolving goals while re-\nmaining consistent with user values.\nReferences\nAI, M. 2024. LLaMA 3 8B.\nAnthropic. 2024. Claude 3.5 Sonnet.\nBeliaev, M.; and Sadigh, D. 2024. Inverse Reinforcement\nLearning by Estimating Expertise of Demonstrators.arXiv\npreprint. Extended version of AAAI publication.\nChen, J.; and et al. 2024. From Persona to Personalization:\nA Survey on Role-Playing Language Agents.arXiv preprint\narXiv:2404.18231.\nJeon, W.; Su, C.-Y .; Barde, P.; and et al. 2020. Regularized\nInverse Reinforcement Learning. arXiv:2010.03691.\nJiang, A. Q.; and et al. 2023. Mistral 7B. arXiv:2310.06825.\nKe, J.; Wu, F.; Wang, J.; and et al. 2025. Inverse Reinforce-\nment Learning with Switching Rewards and History Depen-\ndency. InICLR. Submission 10850.\nKenan Jiang, F. L., Li Xiong. 2024. HARBOR: Explor-\ning Persona Dynamics in Multi-Agent Competition.arXiv\npreprint arXiv:2502.12149.\nLewis, P.; and et al. 2023. Retrieval-Augmented Genera-\ntion for Knowledge-Intensive NLP Tasks: A Survey.arXiv\npreprint arXiv:2303.06519.\nLiang, S.; Zhang, Y .; and Wang, Y . 2025. C-TLSAN:\nContent-Enhanced Time-Aware Long-and Short-Term At-\ntention Network for Personalized Recommendation.arXiv\npreprint arXiv:2506.13021.\nLin, J.; and et al. 2023. How Can Recommender Systems\nBenefit from Large Language Models: A Survey.arXiv\npreprint arXiv:2306.05817.\nLiu, J.; Shen, D.; Zhang, Y .; and et al. 2021. What Makes\nGood In-Context Examples for GPT-3? arXiv:2101.06804.\nMansour, S.; and et al. 2024. PAARS: Persona Aligned\nAgentic Retail Shoppers.arXiv preprint arXiv:2506.13021.\nRuan, J.; and et al. 2023. TPTU: Large Language Model-\nbased AI Agents for Task Planning and Tool Usage.arXiv\npreprint arXiv:2308.03427.\nSaid, A. 2025. On explaining recommendations with Large\nLanguage Models: a review.Frontiers in Big Data, 7:\n1505284.\nSalemi, A.; Mysore, S.; Bendersky, M.; and et al. 2023.\nLaMP: When Large Language Models Meet Personaliza-\ntion. arXiv:2304.11406.\nSamuel, V .; Zou, H. P.; and et al., Y . Z. 2024. PersonaGym:\nEvaluating Persona Agents and LLMs.arXiv preprint\narXiv:2407.18416.\nTouvron, H.; and et al. 2023. Llama 2: Open Foundation and\nFine-Tuned Chat Models. arXiv:2307.09288.\nWang, L.; Ma, C.; Feng, X.; and et al. 2024. A survey on\nlarge language model based autonomous agents.Frontiers\nof Computer Science, 18(6).\nXu, R.; and et al. 2024. Character is Destiny: Can Role-\nPlaying Language Agents Make Persona-Driven Decisions?\narXiv preprint arXiv:2404.12138.\nXu, Z.; and Zhang, Y . 2025. LLM-Enhanced Rerank-\ning for Complementary Product Recommendation.\narXiv:2507.16237.\nYao, S.; Zhao, J.; Yu, D.; and et al. 2023. ReAct:\nSynergizing Reasoning and Acting in Language Models.\narXiv:2210.03629.\nYu, P.; Xu, Z.; Wang, J.; and et al. 2025. The Application\nof Large Language Models in Recommendation Systems.\narXiv preprint arXiv:2501.02178. ArXiv:2501.02178v2.Zerhoudi, S.; and Granitzer, M. 2024. PersonaRAG:\nEnhancing Retrieval-Augmented Generation Systems with\nUser-Centric Agents.arXiv preprint arXiv:2407.09394.\nZhang, W.; and et al. 2024. PersonaAgent: When Large\nLanguage Model Agents Meet Personalization at Test Time.\narXiv preprint arXiv:2506.06254.\nZhong, W.; Guo, L.; Gao, Q.; and et al. 2023. MemoryBank:\nEnhancing Large Language Models with Long-Term Mem-\nory. arXiv:2305.10250.\nZou, H. P.; Huang, W.-C.; Wu, Y .; ; and et al. 2025. LLM-\nBased Human-Agent Collaboration and Interaction Sys-\ntems: A Survey. arXiv:2505.00753.\n",
    "title": "PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM",
    "arxiv_id": "2511.17467v1",
    "category": "recent",
    "authors": [
      "Siqi Liang",
      "Yudi Zhang",
      "Yue Guo"
    ],
    "published": "2025-11-21",
    "num_pages": 5,
    "num_chars": 19012
  },
  {
    "text": "SRA-CP: Spontaneous Risk-Aware Selective Cooperative\nPerception\nJiaxi Liua, Chengyuan Ma*a, Hang Zhoua, Weizhe Tanga, Shixiao Lianga, Haoyang Dingb,\nXiaopeng Liaand Bin Rana\naDepartment of Civil and Environmental Engineering, University of Wisconsin-Madison, Madison, 53706, Wisconsin, United States\nbSchool of Computer, Data and Information Sciences, University of Wisconsin-Madison, Madison, 53706, Wisconsin, United States\nARTICLE INFO\nKeywords:\nCooperative perception\nVehicle-to-Everything\nObject detection\nBlind spot analysis\nSpontaneous Risk-Aware Selective\nCooperative Perception (SRA-CP)ABSTRACT\nCooperative perception (CP) offers significant potential to overcome the limitations of single-\nvehicle sensing by enabling information sharing among connected vehicles (CVs). However,\nexisting generic CP approaches need to transmit large volumes of perception data that are\nirrelevant to the driving safety, exceeding available communication bandwidth. Moreover,\nmost CP frameworks rely on pre-defined communication partners, making them unsuitable\nfor dynamic traffic environments. This paper proposes aSpontaneous Risk-Aware Selective\nCooperative Perception (SRA-CP)framework to address these challenges. SRA-CP introduces\nadecentralizedprotocolwhereconnectedagentscontinuouslybroadcastlightweightperception\ncoverage summaries and initiate targeted cooperation only when risk-relevant blind zones are\ndetected. A perceptual risk identification module enables each CV to locally assess the impact\nof occlusions on its driving task and determine whether cooperation is necessary. When CP\nis triggered, the ego vehicle selects appropriate peers based on shared perception coverage\nand engages in selective information exchange through a fusion module that prioritizes safety-\ncritical content and adapts to bandwidth constraints. We evaluate SRA-CP on a public dataset\nagainstseveralrepresentativebaselines.ResultsshowthatSRA-CPachieveslessthan1%average\nprecision (AP) loss for safety-critical objects compared to generic CP, while using only 20% of\nthe communication bandwidth. Moreover, it improves the perception performance by 15% over\nexisting selective CP methods that do not incorporate risk awareness.\n1. Introduction\nMulti-agent cooperative perception (CP) has emerged as a promising paradigm to overcome the limitations of\nsingle-agent sensing by enabling agents to share information with each other. In road traffic environments, a single\nvehicle\u2019s sensing capability is often obstructed by occlusions, resulting in blind zones that lead to hesitation in\ndecision-making and increased collision risk with surrounding participants. These issues are particularly pronounced\nin scenarios such as unprotected left turns or pedestrians suddenly appearing from behind parked vehicles. With the\nrapid advancement of connected and automated vehicle (CAV) technologies, ensuring safe and complete perception\nbecomes even more critical, especially for autonomous driving in complex environments [Zha et al., 2025]. In such\ncontexts, CP offers valuable potential to enhance safety.\nHowever, most existing CP studies remain limited to simulations or small-scale experimental setups conducted\nunderidealandcontrolledconditions.Achievinglarge-scaledeploymentofCPinreal-worldtrafficstillfacestwomajor\nchallenges.ThefirstchallengeliesinthegapbetweenthemassivevolumeofsensingdatageneratedbyCAVsandthe\nlimitedbandwidthofvehicularcommunicationnetworks[Huetal.,2022].Forexample,intermediatefeaturesextracted\nfrom onboard sensors at a rate of 5\u201320 Hz can produce up to 2MB per frame, translating to a potential transmission\nrate of 300 Mbps. Such data loads are far beyond what even advanced wireless systems (e.g., 5G) can support in\ndenseenvironments,especiallywhenvehiclesattempttotransmitfullperceptiondatasimultaneously,astheGeneric\nCPshown in Figure 1 (a). Moreover, the problem is exacerbated in real-world traffic, where the number of dynamic\nagents is large and constantly changing. If every pair of agents were required to maintain real-time communication,\nthe bandwidth burden would grow quadratically with the number of agents. In reality, most of the information being\nshared is unnecessary\u2014an individual vehicle\u2019s local perception is often sufficient for safe driving in the majority of\n\u2217Corresponding author\nORCID(s):0009-0001-2749-6435(J. Liu);0000-0002-6337-0450(C. Ma*);0000-0003-3286-341X(H. Zhou);\n0000-0002-5264-3775(X. Li)\nJiaxi Liu:Preprint submitted to ElsevierPage 1 of 22arXiv:2511.17461v1  [cs.AI]  21 Nov 2025\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\nFigure 1:Comparison between(a) Generic CPwith full-time information exchange VS the proposedRisk-aware selective\nCPactivated by risky blind-spot events; and(b) Pre-arranged CPconstrained by predefined communication partners VS\nSpontaneous CPenabling dynamic ad-hoc cooperation in arbitrary encounter situations.\nsituations. Even in CP-required cases, not all detected elements need to be transmitted. A more efficient strategy is\nto share only the information that is both unseen by the receiving vehicle and potentially hazardous to its driving\ndecisions. This observation motivates the concept ofRisk-aware selective CP, where each vehicle evaluates the risk\nlevelofobjectsitperceivesandonlytransmitsthosethatsatisfytwoconditionssimultaneously:(i)theobjectlieswithin\nanothervehicle\u2019sblindzone,and(ii)theobjectposesapotentialsafetyrisk.Forinstance,inFigure1,aparkedroadside\nvehicle obstructs a portion of the scene, and the oncoming vehicle provides supplementary information to complete\nperception.WhilerecentstudieshaveexploredselectiveCPstrategiesthatonlyshareblind-zonecontent\u2014resultingin\nsignificantcommunicationreductioncomparedtogenericCP[Qiuetal.,2025]\u2014theystilldonotaccountforthetraffic\nrisk relevance of shared content. Our previous work validated that only a small fraction (0.1%) of driving scenarios\nactuallyrequireCP[Maetal.,2025].Thus,theinsightbehindrisk-awareselectiveCPservesasthefoundationforthe\ncommunication-efficient strategy proposed in this study.\nThesecondchallengeconcernshowtoconstructcommunicationpathwaysinascalableanddynamictrafficsystem.\nTothebestofourknowledge,mostexistingCPstudiesareconductedwithinPre-arrangedcommunicationzonesand\namong predefined partner vehicles, as illustrated in Figure 1(b). While pre-arranged communication enables stable\npoint-to-point connections under idealized network assumptions\u2014and even allows for global optimization of com-\nmunicationtopologiesandgroupingstrategies[Dongetal.,2022]\u2014thesesetupsaretypicallylimitedtoexperimental\ntestbeds with a fixed number of specified agents. They fail to generalize to open-world traffic environments, where a\nsmallnumberofunfamiliarconnectedvehiclesmayencountereachotherspontaneouslyacrosslargespatialareasand\natunpredictabletimes.Thislimitationhighlightstheurgentneedforadecentralizedandself-organizedcommunication\nmechanism. To address this, we propose aSpontaneous CPframework that builds upon the selective CP principle.\nEachconnectedvehicleoperatesindependently,broadcastingminimalinformationaboutitsownperceptioncoverage.\nOnly when a risk-relevant blind zone is detected does it initiate a CP request. Neighboring vehicles, upon receiving\nthe request, respond cooperatively if they are capable of contributing, as illustrated in Figure 1(b). This mechanism\nleverages a key principle: connected vehicles can identify whether they can be assisted in blind-zone completion by\nevaluating other vehicles\u2019 relative positions and their shared perception coverage. As a result, the handshake process\nis realized through lightweight broadcasting during regular operation and precise, selective information exchange\ntriggered only when necessary.\nTobridgetheabovetwogaps,weproposeaSpontaneousRisk-AwareSelectiveCooperativePerception(SRA-\nCP)framework.Itintroducesaspontaneouscollaborationmechanismcomposedoftwomodes:aroutinemode,where\nvehicles continuously broadcast only their perception coverage maps; and a triggered mode, where a vehicle initiates\nCP only when it detects a risk-relevant blind zone, and neighboring connected agents are capable of assisting. Built\nupon this mechanism, we develop arisk-aware hierarchical perception fusion modelthat ensures efficient CP by\nadaptively prioritizing critical information within any available communication bandwidth. The model consists of\nfourkeycomponents:asharedfeatureencoder,arisk-awarecommunicationmodule,adual-attentionfusionnetwork,\nandamulti-taskdecoder.Thisarchitectureenablesvehiclestoselectivelyfusethemostimportantperceptualfeatures\nbasedonspatialocclusionandsafetyrelevanceunderanygivencommunicationconstraints.Weevaluatetheproposed\nJiaxi Liu:Preprint submitted to ElsevierPage 2 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\nframework on a public dataset by comparing it against three baselines: generic CP, a state-of-the-art selective CP\nmethod without risk awareness, and a no-CP setup, in terms of communication cost and Average Precision (AP) for\nkey-objectdetection.ResultsdemonstratethatSRA-CPachievescomparableorsuperiorperformancewithsignificantly\nreducedcommunicationoverhead.Specifically,comparedtogenericCP,SRA-CPreducesthetransmissionvolumeto\n80% while incurring only a 0.1 drop in AP. Compared to the selective CP baseline without risk modeling, SRA-CP\nimproves AP for critical objects by 15% under the same communication budget, showcasing its potential for scalable\ndeployment in large-scale, dynamic traffic environments.\nThe main contributions of this paper are as follows:\n\u2022Weaddressthebandwidthbottleneckinmulti-agentCPbyproposingarisk-awareselectiveCPstrategy,which\nprioritizesthetransmissionofperceptualelementsbasedontheirimpactondrivingsafety.Thisapproachenables\nefficient use of limited communication resources under varying bandwidth constraints.\n\u2022We propose theSpontaneous Risk-Aware Selective Cooperative Perception (SRA-CP)framework, which\nsupports dynamic, on-demand handshakes between agents without predefined regions or communication\npartners.Thisdesignenablesscalableandlow-costCPinlarge-scale,real-worldtrafficenvironmentswithself-\norganizing connected agents.\n\u2022We validate the proposed framework on a public dataset and show thatSRA-CPachieves comparable\nperformance to generic CP while using only 20% of the communication volume, with less than 1% drop in\nAP. Compared to a cutting-edge selective CP baseline that does not consider driving risk, SRA-CP improves\ncritical object detection accuracy by 15%.\n2. Related work\n2.1. Cooperative Perception (CP)\nCP allows multiple agents to share perceptual information to achieve a more complete understanding of their\nsurroundings.Thisparadigmaddresseskeylimitationsofsingle-agentperceptionsuchasocclusionandlimitedsensing\nrange [Chen et al., 2019a, Liu et al., 2020]. There could be different downstream perception tasks to be fulfilled with\nCP, such as 3D object detection [Xu et al., 2023, Li and Pei, 2024, Xiang et al., 2024, Yu et al., 2023], lane detection\n[Jahnetal.,2024,ElBoukilietal.,2025],objecttracking[Chiuetal.,2024,Zimmeretal.,2024,Zhongetal.,2025].\nCP has been implemented through various fusion schemes, including early [Chen et al., 2019b, Yang et al., 2025],\nintermediate [Liu et al., 2020, Wang et al., 2020, Xu et al., 2022a], and late fusion [Liu et al., 2024, Sarlak et al.,\n2025]. Early fusion directly shares raw sensor data (e.g., LiDAR point clouds or images), aligns them in a common\ncoordinate frame, and jointly processes the fused measurements through a single perception network. Intermediate\nfusion exchanges intermediate feature maps extracted by each agent\u2019s backbone. These features are spatially aligned\nandcombinedbeforethedetectionhead.Latefusiontransmitsonlyhigh-levelperceptionoutputssuchasobjectboxes\nortracks,whicharethenassociatedandmergedatthecooperativelayer,offeringlowcommunicationcostattheexpense\nof reduced ability to recover missed local detections.\nAlthough these schemes achieve a certain trade-off between communication cost and perception fusion perfor-\nmance,thewidespreadtransmissionofperceptualinformationinmulti-agentscenariosremainsasignificantchallenge.\n2.2. Selective Information Sharing\nAmajorchallengeinCPisreducingthecommunicationburdenwhilemaintainingperceptionquality.Recentwork\nsuchasWhere2comm[Huetal.,2022]hasproposedtousespatialconfidencemapstoidentifyperceptuallyimportant\nregions and selectively transmit only the features from those areas. This strategy improves the perception accuracy\nunder limited bandwidth by avoiding indiscriminate sharing of all information. There are also other CP works that\nsharethesamethoughts[Yangetal.,2023a,Liuetal.,2020,Yangetal.,2023b].However,perceptualaccuracyalone\nis not a sufficient criterion in driving scenarios. In practice, much of the perceptual improvement may not contribute\nto driving decisions or safety [Ma et al., 2025, Van Brummelen et al., 2018, Pan et al., 2024, Gao et al., 2024]. For\nexample, perceiving a distant vehicle with higher precision may not change the ego vehicle\u2019s behavior. Thus, a key\nlimitationofcurrentselectivestrategiesisthelackofrisk-awareness.Theyfailtodistinguishbetweeninformationthat\nisperceptuallyusefulandinformationthatissafety-critical.Arisk-awareselectionmechanismisneededtoensurethat\nperception elements that are both unseen and pose potential safety risks are shared.\nJiaxi Liu:Preprint submitted to ElsevierPage 3 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\nFigure 2:Spontaneous Risk-Aware Selective Cooperative Perception (SRA-CP)\n2.3. Communication Paradigms for CP\nThecommunicationparadigmforCPisalsoanimportanttopicinCP.Inmostcases,priorstudiesassumealimited\nand fixed set of collaborators operating within a bounded area. These assumptions simplify the design of interaction\nprotocols and enable direct coordination [Feng et al., 2018, Yu et al., 2019]. Most existing CP frameworks also adopt\nsuch settings [Chen et al., 2019a, Liu et al., 2020]. However, these conditions are difficult to satisfy in real-world\ntraffic environments, where agents are numerous, highly dynamic, and distributed across large spatial regions. In\naddition, only a small fraction of encounters truly require cooperation, and participating agents are often unfamiliar\nwithoneanother.Theselimitationshighlighttheneedforascalable,deployable,andself-organizedCPcommunication\nparadigm. Recent work in agentic AI has begun to explore the notion ofspontaneous cooperation[Wu et al., 2024,\nGodhwanietal.,2025,Mirskyetal.,2022],wherecollaborationemergesdynamicallybasedonlocalcontextandshared\nobjectives. Building on this insight, our proposed SRA-CP framework leverages the property that connected vehicles\ncan share their perception coverage, allowing other agents to identify opportunities to fulfill blind-zone completion\nneeds. This enables the spontaneous formation of cooperation links without prior coordination or global knowledge.\n3. Problem Description\nAs illustrated in Figure 2, we consider a dynamic road network with multiple Connected Vehicle (CV) agents\nindexed by\ud835\udc52,\ud835\udc57,\ud835\udc58, in which\ud835\udc52denotes the ego vehicle. At a certain time\ud835\udc61(all variables hereafter are defined at time\n\ud835\udc61unless otherwise specified), each vehicle has a physical state represented by its position\ud835\udc29\ud835\udc52= (\ud835\udc65\ud835\udc52,\ud835\udc66\ud835\udc52)and velocity\n\ud835\udc2f\ud835\udc52=(\ud835\udc63\ud835\udc65\n\ud835\udc52,\ud835\udc63\ud835\udc66\n\ud835\udc52), taking ego vehicle as an example. For a given connected agent\ud835\udc52, its perception range from a bird\u2019s-eye\nview (e.g., the spatial coverage of LiDAR) is denoted by\ue23c\ud835\udc52, with the corresponding sensed information\ud835\udebd(\ue23c\ud835\udc52). The\nblind zone\u2014areas not observable by the agent\u2014is denoted as\ue23b\ud835\udc52. In most cases, such blind zones have a negligible\nimpactondrivingsafety,asillustratedby\ue23b\ud835\udc57and\ue23b\ud835\udc58inFigure2.However,insomecases,suchasavehicleapproaching\nfrom the opposite direction within\ue23b\ud835\udc52that affects a left-turn decision, the blind zone can pose a significant risk. This\nstudyfocusesonidentifyingsuchriskyblindzonesandselectivelycompletingthemviaCPwithlimitedcommunication\nbandwidth.\nWedesigntheSRA-CPframeworkinwhichtheegoconnectedvehicle\ud835\udc52continuouslybroadcastsitsownposition\n\ud835\udc29\ud835\udc52, velocity\ud835\udc2f\ud835\udc52, and perception coverage\ue23c\ud835\udc52(requiring only low bandwidth). It then receives broadcasted data from\nnearbyvehicleswithinitscommunicationzone\ue246\ud835\udc52\u2014acircularareaofradius\ud835\udc59\ud835\udc50\u2014whichincludesthesetofneighboring\nagents\ue243\ud835\udc52. Based on this shared information, the agent determines whether it has a risky blind zone that can be\nsupplemented by any surrounding vehicle, and if so, initiates a spontaneous CP handshake and performs cooperative\nJiaxi Liu:Preprint submitted to ElsevierPage 4 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\nFigure 3: End-to-end architecture of Selective Information Sharing and Fusion.Each co-operative vehicle\ud835\udc56\u2208 {\ud835\udc52,\ud835\udc58,\ud835\udc57}\nprojectsitsrawpointcloud\ud835\udebd(\ue23c\ud835\udc56)totheegoBird\u2019s-Eye-View(BEV)frameandencodesitthroughasharedfeature-encoder,\nyielding\ud835\udc39\ud835\udc56.Risk-aware communication(Sec. 4.3.2) attaches two light-weight masks\u2014the spatial mask\ud835\udc46\ud835\udc56and the risk\nmask\ud835\udc45\ud835\udc56\u2014to the feature map and broadcasts only these three tensors, avoiding transmission of raw point clouds. The ego\ncar receives the partner streams and performsdual-attention feature fusion(Sec. 4.3.3): a safety-focused selector prunes\npartner features with(\ud835\udc46\ud835\udc56,\ud835\udc45\ud835\udc56)and a location-wise multi-head attention block aligns the surviving cells with the ego map\n\ud835\udc39\ud835\udc52, producing \u0303\ud835\udc39\ud835\udc52. Finally, two heads operate on \u0303\ud835\udc39\ud835\udc52: (i) a Risk Decoder refines a dense risk heat-map, and (ii) a Detection\nDecoder outputs 3-D bounding boxes.\nfusion. Specifically, the ego connected vehicle\ud835\udc52first evaluates the risk level of its perception blind zones using the\nproposedperceptual risk identificationmodel, resulting in a perceptual risk matrix\ue23e\ud835\udc52. If no risky blind zones are\ndetected (e.g., as in the case of other CVs\ud835\udc57and\ud835\udc58), the process at\ud835\udc61terminates. If risky blind zones are identified,\nthe vehicle proceeds to select an appropriate target connected agent. Based on the shared perception coverage from\nneighboring agents (e.g.,\ue23c\ud835\udc57and\ue23c\ud835\udc58), the vehicle determines whether any connected agent can compensate for its\noccludedregions(e.g.,agent\ud835\udc57intheillustratedcase).Notethatifnosuitableconnectedagentsareavailabletoprovide\nblind-zone compensation, the ego vehicle relies solely on its own onboard perception for decision-making\u2014e.g., by\nstopping to continue observation. Once a candidate is selected, ego vehicle\ud835\udc52sends a CP request to agent\ud835\udc57, including\nits current position\ud835\udc29\ud835\udc52, blind zone\ue23b\ud835\udc52, and the computed risk matrix\ue23e\ud835\udc52. Upon receiving the request, agent\ud835\udc57invokes\nthe proposedselective information sharingmodel, which, under the given bandwidth constraint\ud835\udc35bytes, selects and\ntransmitsthemostinformativefeatures\ud835\udc39\ud835\udc57,\ud835\udc52tosupplementagent\ud835\udc52\u2019sperception.Finally,agent\ud835\udc52performscooperative\nfusionviaadual-attentionfeaturefusionmodeltointegratethereceivedfeaturesandcompleteitsunderstandingof\nthe occluded region.\nIn this study, we focus on CP using LiDAR data, which provides accurate geometric structure, consistent\nperformance under varying illumination, and reliable spatial measurements for dynamic traffic environments. These\nproperties make LiDAR particularly suitable for blind-zone estimation and risk-aware perception. It is worth noting\nthat the proposed framework is modality-agnostic. Although our implementation uses LiDAR as the primary sensing\nmodality, the methodology can be extended to other perception inputs, such as video data.\n3.1. Notation and Symbols\nJiaxi Liu:Preprint submitted to ElsevierPage 5 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\nTable 1: Notation used throughout the paper (unified).\nSymbol Type Meaning / Unit\nSets, indices, regions\n\ud835\udc61scalar Time index.\n\ud835\udc56,\ud835\udc57,\ud835\udc58index Generic agent indices.\n\ud835\udc52index Ego agent.\n\ud835\udc29\ud835\udc56=(\ud835\udc65\ud835\udc56,\ud835\udc66\ud835\udc56)vector 2D position of agent\ud835\udc56in BEV/ego frame (at time\ud835\udc61, unless stated otherwise).\n\ud835\udc2f\ud835\udc56=(\ud835\udc63\ud835\udc65\n\ud835\udc56,\ud835\udc63\ud835\udc66\n\ud835\udc56)vector 2D velocity of agent\ud835\udc56.\n\ue23c\ud835\udc56 set Perception coverage (field of view, FoV) of agent\ud835\udc56.\n\ud835\udebd(\u22c5)map Region\u2192perceived sensory information; e.g.,\ud835\udebd(\ue23c\ud835\udc56)is the perception information of\nagent\ud835\udc56.\n\ue23b\ud835\udc56 set Blind zone of agent\ud835\udc56.\n\ue246\ud835\udc56 set Local communication region of agent\ud835\udc56(disk of radius\ud835\udc59\ud835\udc50).\n\ud835\udc59\ud835\udc50 scalar Communication radius defining\ue246\ud835\udc56(m).\n\ue243\ud835\udc56 set Neighboring connected agents within\ue246\ud835\udc56.\nSRA-CP protocol\n\ud835\udf0c\ud835\udc56,\ud835\udc57 scalar Pairwise collision risk score between\ud835\udc56and\ud835\udc57.\n\ud835\udf0f\ud835\udc5f scalar Threshold on\ud835\udf0cto trigger risk-aware sharing.\n\ue23ematrix Risk matrix collecting\ud835\udf0c\ud835\udc56,\ud835\udc57.\n\ud835\udc45\ud835\udc56, \ud835\udc45(\ud835\udc51)\n\ud835\udc56, \ud835\udc45(\ud835\udc60)\n\ud835\udc56, \ud835\udc45\ud835\udc5b\n\ud835\udc56scalar Object\ud835\udc56\u2019s total risk and distance/speed/intersection components.\n\u0302\ud835\udc45\ud835\udc56 scalar Clipped/normalized risk of object\ud835\udc56in[0,1].\n\ud835\udc45gt map Ground-truth risk heatmap.\n\ue230\ud835\udc52 set Potentially dangerous agents for ego\ud835\udc52, selected from\ue243\ud835\udc52using\ue23e.\nPerceptual risk identification model\n\ud835\udc2e=(\ud835\udc65,\ud835\udc66)vector 2D BEV grid cell center in ego coordinates.\n\ue233\ud835\udc2e grid BEV grid cell centered at\ud835\udc2e=(\ud835\udc65,\ud835\udc66).\n\ud835\udc5c(\ud835\udc2e)\u2208[0,1]field Occupancy at BEV cell\ud835\udc2e.\n\u03a0BEV op 3D\u2192BEV projection operator.\n\ud835\udf05,\ud835\udf0e(\u22c5)func Smoothing kernel; squashing function for occupancy.\n\ud835\udf17angle Ray azimuth (rad).\n\ud835\udc5fscalar Range of BEV grid cell (m).\n\ud835\udc2b(\ud835\udc60;\ud835\udf17)curve Ray parameterization along azimuth\ud835\udf17.\n\ud835\udc47(\ud835\udc2e)scalar Line-of-sight transmittance to\ud835\udc2e.\n\ud835\udf06,\u0394\ud835\udc60,\ud835\udc3escalars Beer\u2013Lambert attenuation; step; number of samples along a ray.\n\ud835\udf12fov(\ud835\udc2e)gate FoV gate{0,1}.\n\ud835\udc43occ(\ud835\udc2e)prob Occlusion probability at\ud835\udc2e.\n\ud835\udf0focc, \ud835\udc3e\ud835\udc61, \ud835\udf0f\ud835\udc61 scalars Occlusion threshold; number of temporal frames; temporal consensus threshold.\n\ue23be(\ud835\udc2e),\u0304\ue23be mask Instantaneous and stabilized blind-zone masks.\n\ud835\udc13\ud835\udc52\u2190\ud835\udc64 matrix Rigid transform from world to ego frame.\n\ud835\udc67scalar Vertical coordinate (height) in the ego frame.\nSelective information sharing and fusion\n\ud835\udc39\ud835\udc56\u2208\u211d\ud835\udc36\u00d7\ud835\udc3b\u00d7\ud835\udc4atensor BEV feature map of agent\ud835\udc56;\ud835\udc36channels,\ud835\udc3b\u00d7\ud835\udc4athe height/width of grid.\n\ud835\udc36\ud835\udc60,\ud835\udc57,\ud835\udc36\ud835\udc5f,\ud835\udc57 map Spatial- and risk-confidence maps on partner\ud835\udc57.\n\ud835\udc46\ud835\udc57, \ud835\udc45\ud835\udc57\u2208{0,1}\ud835\udc3b\u00d7\ud835\udc4amask Spatial / risk masks (binary).\n\u0303\ud835\udc39\ud835\udc56 tensor Masked feature patch to transmit from partner\ud835\udc56.\n\ud835\udc39\ud835\udc57,\ud835\udc52 tensor Partner\ud835\udc57\u2019s selected feature patch transmitted to ego\ud835\udc52.\n\ud835\udc53enc, \ud835\udc53dec net Shared encoder; multi-task decoder.\n(\u0302\ud835\udc36,\u0302\ud835\udc35,\u0302\ud835\udc45)out Class scores, 3D boxes, refined risk heatmap.\n\ud835\udc3esel scalar Top-\ud835\udc3eselected cells for transmission.\n\ud835\udc54sp(\ud835\udc2e), \ud835\udc54risk(\ud835\udc2e)score Spatial/risk gains used for selection.\n\ud835\udc54(\ud835\udc2e), \ud835\udefcscore Combined gain and its mixing weight\ud835\udefc\u2208[0,1].\nContinued on next page\nJiaxi Liu:Preprint submitted to ElsevierPage 6 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\nTable 1 (continued).\nSymbol Type Meaning / Unit\n\ud835\udc43\ud835\udc52 path Planned trajectory used by the risk head of ego agent\ud835\udc52.\nTraining objective and evaluation\n\ue238total loss Total training loss.\n\ue238det loss Detection loss.\n\ud835\udf06risk, \ud835\udf06comm weight Weights for risk regression and communication penalty.\n\ud835\udf19(usage;target)penalty Hinge-style penalty on over-usage of bytes.\n\ud835\udc35bytes bytes Target per-link byte budget.\n\u210ehdr bytes Header/metadata overhead per message.\n\ud835\udc4fidx, \ud835\udc4ffeat, \ud835\udc4fcell bytes Bytes per cell index / per feature value / per cell (\ud835\udc4fcell=\ud835\udc4fidx+\ud835\udc36\ud835\udc4ffeat).\n\ud835\udc48bytes Actual bytes used in a batch.\n\ud835\udc35batch, \ud835\udc3f\ud835\udc4f count Batch size; number of agents in sample\ud835\udc4f.\n\ud835\udc40(\ud835\udc4f)\n\ud835\udc59,\ud835\udc56,\ud835\udc57\u2208{0,1}mask Binary selection mask for sample\ud835\udc4f.\nAP,3DAP(\ud835\udf03)metric AP and 3D AP at IoU threshold\ud835\udf03.\n\ud835\udc47\ud835\udc43(\ud835\udf03), \ud835\udc39\ud835\udc43(\ud835\udf03), \ud835\udc39\ud835\udc41(\ud835\udf03)count True/false positives and false negatives at\ud835\udf03.\n\ue235risk(\ud835\udf0f)set Subset filtered by risk threshold\ud835\udf0ffor Risk-AP.\n\u0394Risk-AP\u2215KB metric\u0394Risk-AP per KB (\u0394Risk-AP\u2215KB).\nRisk label generation\n\ud835\udefc\ud835\udc51,\ud835\udefc\ud835\udc60,\ud835\udefc\ud835\udc5b scalar Weightsfordistance-,speed-,andintersection-basedriskcomponentsintheoverallrisk\nscore\ud835\udc45.\n\ud835\udf06\ud835\udc51,\ud835\udf06\ud835\udc5b scalar Decay rates for distance-based and intersection-based risk terms.\n\ud835\udc5aindex index of intersections.\n\ud835\udc2a\ud835\udc5a vector Center location of the\ud835\udc5a-th intersection.\n\ue23dset Set of all intersection center locations.\n\ud835\udc63\ud835\udc56 scalar Speed magnitude of object\ud835\udc56.\n\ud835\udf16scalar Smallpositiveconstanttoavoiddivisionbyzerointhespeed-basedrisknormalization.\n4. Methodology\nAsmentionedintheprevioussection,theproposedSRA-CPframeworkisdesignedtooperateintwophases:during\nnormal operation, each vehicle broadcasts basic perception coverage information with minimal bandwidth; when a\nrisk-relevant blind zone is detected, it initiates a targeted CP link and transmits only the most critical information\nwithin the available communication bandwidth. The framework relies on aperceptual risk identification modelto\nassess the risk level of blind zones. Upon identifying a suitable cooperative agent, the responder employs aselective\ninformationsharingmodeltodeterminewhichfeaturestotransmitunderbandwidthconstraints.Thereceivingagent\nthen performs cooperative fusion using adual-attention feature fusion modelto produce an enhanced perception\nresult. The following subsections detail each of these four key components.\n4.1. SRA-CP Protocol\nThe core idea of the proposed SRA-CP protocol is as follows: at each time\ud835\udc61, the ego vehicle\ud835\udc52periodically\nbroadcasts a compact coverage map of\ue23c\ud835\udc52to all nearby agents\ue243\ud835\udc52within\ue246\ud835\udc52. This map summarizes which BEV cells\nare currently visible and which are likely occluded (Sec. 4.2), without exposing raw sensor data. Each neighbor does\nthe same, enabling every agent to infer who can potentially compensate for its blind zones. When a risky blind area\nis detected, the ego triggers an on-demand handshake with one suitable partner and proceeds with selective sharing\nand fusion under the current byte budget. In practice, a risk threshold\ud835\udf0f\ud835\udc5fdetermines whether the detected blind-zone\nrisk warrants initiating the cooperative handshake. Then, based on the received coverage maps, each agent constructs\nan inter-object risk matrix\ue23e= [\ud835\udf0c(\ud835\udc52,\ud835\udc56)|\ud835\udc56\u2208\ue243e]by evaluating pairwise risks. From this vector, the potentially\ndangerous set\ue230eis identified. If an agent\ud835\udc56\u2208\ue230ealso lies in\ue23be, the partner transmits only the features covering that\nregion to assist perception completion.\nFor example, as illustrated in the intersection scenario in Figure 2, there are six vehicles, among which\ud835\udc56,\ud835\udc57, and\n\ud835\udc58are connected agents. In the first step, each connected agent broadcasts its local perception coverage\ue23c\ud835\udc56,\ue23c\ud835\udc57, and\nJiaxi Liu:Preprint submitted to ElsevierPage 7 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\n\ue23c\ud835\udc58to others. Since only coverage maps are shared\u2014without detailed perception content\u2014this step incurs negligible\ncommunication overhead.\nNext, each agent performs an inter-object risk estimation over the observed objects in the scene and generates a\nrisk vector\ue23eto estimate whether they need external information from other agents to help with their perception. In\nthisscenario,agent\ud835\udc58andagent\ud835\udc57findnoriskyblindspot,sotheydonotneedfurtherexternalinformationfromother\nagents. However, agent\ud835\udc56finds it can not see the potentially risky objects in the blind zone of the black vehicle, which\nis risky to its driving intention, and agent\ud835\udc57finds it can help the detection of agent i. Therefore, agent\ud835\udc57sends the\ninformation of the potentially risky zone to agent\ud835\udc56to complete its perception.\n4.2. Perceptual risk identification model\nThePerceptual Risk Identification Modeltakes the individual perception\ud835\udebd(\ue23c\ud835\udc56)as input and produces a risk\nmatrix\ue23e\ud835\udc56over the blind zone\ue23b\ud835\udc56, indicating the safety-critical importance of each location with respect to the ego\nvehicle\u2019s driving decisions.\nSRA-CP requires a light-weight, geometry-based estimate of the ego vehicle\u2019s blind zones to prioritize compen-\nsation from partners. We adopt a BEV visibility model that is fast, rule-based, and admits a continuous formulation\nfor analysis. Let\ue233\ud835\udc2edenote a BEV grid with cell centers\ud835\udc2e=(\ud835\udc65,\ud835\udc66)in ego coordinates, z is the vertical coordinate of\n\ud835\udc2ein the ego frame used for 2.5D occupancy computation, ego pose\ud835\udc13\ud835\udc52\u2190\ud835\udc64(world\u2192ego), and a 2.5D occupancy field\n\ud835\udc5c(\ud835\udc2e)\u2208[0,1]obtained from the LiDAR sweep\ud835\udebd(\ue23c\ud835\udc56)by height-thresholding and kernel density aggregation:\n\ud835\udc5c(\ud835\udc2e) =\ud835\udf0e(\nmax\n\ud835\udc67\u2208[\ud835\udc67min,\ud835\udc67max]\ud835\udf05\u2217\u2211\n\ud835\udc29\u2208\ue238\ud835\udeff(\u03a0BEV(\ud835\udc13\ud835\udc52\u2190\ud835\udc64\ud835\udc29)\u2212(\ud835\udc2e,\ud835\udc67)))\n,(1)\nwhere\u03a0BEVprojects 3D points to the BEV cell,\ud835\udf05is a spatial smoothing kernel, and\ud835\udf0eis a squashing function\nensuring\ud835\udc5c\u2208 [0,1](e.g.,\ud835\udf0e(\ud835\udc4e) = 1\u2212\ud835\udc52\u2212\ud835\udc4e). For a BEV direction\ud835\udf17= atan2(\ud835\udc66,\ud835\udc65)and range\ud835\udc5f=\u2016\ud835\udc2e\u20162, define\nthe ray parameterization\ud835\udc2b(\ud835\udc60;\ud835\udf17) =\ud835\udc60[cos\ud835\udf17,sin\ud835\udf17]\u22a4,\ud835\udc60\u2208 [0,\ud835\udc5f]. The line-of-sight transmittance to\ud835\udc2eis modeled by a\nBeer\u2013Lambert integral over occupancy:\n\ud835\udc47(\ud835\udc2e) = exp(\n\u2212\u222b\ud835\udc5f\n0\ud835\udf06\ud835\udc5c(\ud835\udc2b(\ud835\udc60;\ud835\udf17))d\ud835\udc60)\n, \ud835\udf06>0,(2)\nwith discrete approximation on grid steps\u0394\ud835\udc60:\n\ud835\udc47(\ud835\udc2e)\u2248exp(\n\u2212\ud835\udf06\u0394\ud835\udc60\ud835\udc3e\u2211\n\ud835\udc58=0\ud835\udc5c(\ud835\udc2b(\ud835\udc58\u0394\ud835\udc60;\ud835\udf17)))\n, \ud835\udc3e\u0394\ud835\udc60\u2248\ud835\udc5f.(3)\nCells outside the sensor field-of-view (FOV) or range are treated as fully occluded by an FOV gate\ud835\udf12fov(\ud835\udc2e)\u2208{0,1};\nwe define the occlusion probability and the binary blind-zone mask as\n\ud835\udc43occ(\ud835\udc2e) = 1\u2212\ud835\udf12fov(\ud835\udc2e)\ud835\udc47(\ud835\udc2e),\ue23be(\ud835\udc2e) =\ud835\udd40[\ud835\udc43occ(\ud835\udc2e)>\ud835\udf0focc],(4)\nwiththreshold\ud835\udf0focc\u2208(0,1).Toreduceflicker,wetemporallystabilizethemaskbywarpingthelast\ud835\udc3e\ud835\udc61framesintothe\ncurrent ego frame using odometry and taking a robust union:\n\u0304\ue23be(\ud835\udc2e) =\ud835\udd40[\n1\n\ud835\udc3e\ud835\udc61\ud835\udc61\u2211\n\ud835\udc61\u2032=\ud835\udc61\u2212\ud835\udc3e\ud835\udc61+1\ue23b(\ud835\udc61\u2032)\ne(\ud835\udc13\ud835\udc52\u2190\ud835\udc52(\ud835\udc61\u2032)(\ud835\udc2e))> \ud835\udf0f\ud835\udc61]\n.(5)\nThe mask\u0304\ue23beis used as a compressed coverage summary and to increase selection gains in risky blind zones.\nSpecifically, let\ud835\udc54sp(\ud835\udc2e)and\ud835\udc54risk(\ud835\udc2e)are spatial/risk scores (Sec. 4.3.2), the budgeted gain can be\n\ud835\udc54(\ud835\udc2e) =\ud835\udefc\ud835\udc54sp(\ud835\udc2e)\ud835\udc54risk(\ud835\udc2e) + (1\u2212\ud835\udefc) \u0304\ue23be(\ud835\udc2e)\ud835\udc54risk(\ud835\udc2e), \ud835\udefc\u2208[0,1],(6)\nwhich prioritizes risky and occluded cells under a rate/byte budget.\nJiaxi Liu:Preprint submitted to ElsevierPage 8 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\n4.3. Selective Information Sharing and Fusion\nTheSelectiveInformationSharingandFusionModeldescribesthefullpipelinefromselectingthetargetagent\nfor cooperation, to determining which features to share, and finally to integrating the received features on the ego\nvehicle. The overall framework is illustrated below.\nSelective Information Sharing and Fusion is the model layer that operationalizes the SRA-CP contract: given the\nneighbors and a per-link budget, it learns what to communicate and how to fuse. Concretely, Selective Information\nSharing and Fusion produces lightweight spatial and risk masks, sparsifies partner features under a given communi-\ncation budget, and performs risk-aware fusion on the ego agent using the fused ego map (Figure 3). This converts\ncommunication bandwidth into safety-relevant detections by prioritizing risky\u00d7occluded regions. The pipeline has\nfour building blocks:\n1.Shared feature encoder\ud835\udc53enc(\u22c5)that transforms each LiDAR sweep\ud835\udebd(\ue23c\ud835\udc56)into a BEV feature tensor\ud835\udc39\ud835\udc56\u2208\n\u211d\ud835\udc36\u00d7\ud835\udc3b\u00d7\ud835\udc4a;\n2.Risk-aware communication module(Figure 4) that derives a spatial mask\ud835\udc46\ud835\udc56and a risk mask\ud835\udc45\ud835\udc56from\ud835\udc39\ud835\udc56and\nthese masks will be used as a reference in the Dual-attention feature fusion process to decide which features\nshould be shared;\n3.Dual-attentionfeaturefusionmodule(Figure5)thatselectsthefeatures{ \u0303\ud835\udc39\ud835\udc57}\ud835\udc57\u2260\ud835\udc52tobesharedtotheegovehicle\nbased on the spatial mask\ud835\udc46\ud835\udc57and the risk mask\ud835\udc45\ud835\udc57and transmits{ \u0303\ud835\udc39\ud835\udc57}\ud835\udc57\u2260\ud835\udc52to the ego vehicle and merges them\nwith the ego feature map\ud835\udc39\ud835\udc52and outputs the fused representation \u0303\ud835\udc39\ud835\udc52in the ego vehicle\u2019s coordinate system;\n4.Multi-task decoderthat predicts both 3D bounding boxes and a dense risk heat-map.\n4.3.1. Feature Encoding\nDuringthetrainingprocesseachCV\ud835\udc56encodesitsLiDARsweep\ud835\udebd(\ue23c\ud835\udc56)withasharedPointPillarBEVencoderLang\netal.[2019]inthesamestructure,yielding\ud835\udc39\ud835\udc56\u2208\u211d\ud835\udc36\u00d7\ud835\udc3b\u00d7\ud835\udc4a.FeaturesareexpressedinacommonegoBEVframeusing\nthe known pairwise poses which is transmitted with the coverage map. The backbone within the same structure feeds\ntwo light heads to derive a spatial confidence map and a risk map. The spatial confidence map stores the confidence\nscoreofthefeaturesfromthespatialperspective,whichmeanswhichfeatureisspatiallyimportantforperception.And\ntheriskconfidencemapstorestheconfidencescoreofthefeaturesfromthetrafficriskperspective,whichmeanswhich\nfeature is essential in terms of traffic importance. Both of these two confidence maps will guide the communication\nprocess to choose which features to communicate and the later fusion.\n4.3.2. Risk-Aware Communication\nTheaimofthismoduleistoreducesthecommunicationbandwidthwhilepreservingthebalanceofsafetyrelevance\nand spatial relevance. Each partner summarizes where its features are informative (spatial saliency) and where they\nare safety-critical for the ego (risk), then the CVs will combine the scores together to select which features are more\nimportant for the ego vehicle and they will send only the most important parts under a given budget.\nOn each partner\ud835\udc57, two lightweight heads process\ud835\udc39\ud835\udc57(Figure 4):\n\u2022Spatial-confidence head outputs\ud835\udc36\ud835\udc60,\ud835\udc57.\n\u2022Risk-confidence head outputs\ud835\udc36\ud835\udc5f,\ud835\udc57.\nUnder a given communication budget, adaptive sampling will preform Top-K selection over non-ego grid cells based\non their spatial and risk scores separately in the scene and then it will produces binary masks\ud835\udc46\ud835\udc57,\ud835\udc45\ud835\udc57\u2208 {0,1}\ud835\udc3b\u00d7\ud835\udc4a.\nUnderagivenper-linkbudget,SRA-CPcombinesthetwocues(union)andserializesonlycellswithinthemaskasin\nthe safety-focus feature selection part in the Figure 5. In practice this masks the feature map:\n\u0303\ud835\udc39\ud835\udc57=\ud835\udc39\ud835\udc57\u2299(\ud835\udc46\ud835\udc57\u2228\ud835\udc45\ud835\udc57),\nsoonlyareasthatarespatiallysalientandsafety-criticalaretransmitted.Thisconcentratescommunicationonoccluded\norriskyregionsthatmatterfordecision-making,keepsprivacybyavoidingrawpoints,andgracefullyadaptstotighter\nbudgets by shrinking the selected area.\nJiaxi Liu:Preprint submitted to ElsevierPage 9 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\nFigure 4: Risk-aware communication pipeline executed on each partner vehicle\ud835\udc57.The shared feature map\ud835\udc39\ud835\udc57is processed\nby two light-weight heads:(i) Spatial-confidence map generatorproduces a spatial confidence map\ud835\udc36\ud835\udc60,\ud835\udc57that highlights\nsemantically important cells; an adaptive sampling module is used to select a sparse binary spatial mask based on scenario\n\ud835\udc46\ud835\udc57for transmission.(ii) Risk-confidence map generatoruses\ud835\udc39\ud835\udc57together with the ego planned trajectory\ud835\udc43\ud835\udc52and speed\ud835\udc63\ud835\udc52\nto compute a risk map\ud835\udc36\ud835\udc5f,\ud835\udc57. Adaptive sampling converts it into a binary risk mask\ud835\udc45\ud835\udc57. Both masks(\ud835\udc46\ud835\udc57,\ud835\udc45\ud835\udc57)are sent to the\nego vehicle, while a miniature Risk Decoder can optionally convert\ud835\udc36\ud835\udc5f,\ud835\udc57into a dense risk heat-map for supervision training.\nFigure 5: Dual-attention feature fusion.Remote feature tensors\ud835\udc39\ud835\udc58and\ud835\udc39\ud835\udc57are first filtered by a Safety-focused Feature\nSelection block that combines each partner\u2019s spatial mask\ud835\udc46\ud835\udc56and risk mask\ud835\udc45\ud835\udc56, yielding sparsified maps \u0303\ud835\udc39\ud835\udc58and\u0303\ud835\udc39\ud835\udc57. The\nego map\ud835\udc39\ud835\udc52and the sparsified partner maps are then fused by a location-wise multi-head attention module that performs\nper-cell key\u2013query interactions, producing an enriched representation \u0303\ud835\udc39\ud835\udc52. This two-stage design discards bandwidth-hungry,\nlow-value regions before attention, so both communication and computation focus on areas that are simultaneously\nsafety-critical and semantically informative. During this process, only three low-bandwidth tensors( \u0303\ud835\udc39\ud835\udc57,\ud835\udc46\ud835\udc57,\ud835\udc45\ud835\udc57)leave the\nvehicle, preserving privacy and saving channel capacity.\n4.3.3. Dual-Attention Feature Fusion\nAt the ego agent, the masked partner maps{ \u0303\ud835\udc39\ud835\udc57}and the local map\ud835\udc39\ud835\udc52are fused in two stages (Figure 5). First, a\nsafety-focused selector re-applies(\ud835\udc46\ud835\udc57,\ud835\udc45\ud835\udc57)to suppress any residual clutter and enforce budget consistency. Second,\nWe fuse ego and partner features in a location-wise manner, for each BEV cell\ud835\udc2e, the ego feature provides the query,\nwhile only partners that selected this cell contribute keys and values. This yields an attention distribution over the\nJiaxi Liu:Preprint submitted to ElsevierPage 10 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\nrelevantcollaborators,ensuringthatinformationisaggregatedonlywherecommunicationactuallyprovidedfeatures.\nA residual update then produces the fused representation \u0303\ud835\udc39\ud835\udc52.\nTo handle small spatial misalignment, the module can optionally attend within a local window around\ud835\udc2e, but still\nrestricts computation to cells indicated by partner selection. This keeps the complexity proportional to the number of\ncommunicated cells, making the fusion efficient under sparse CP.\nThiskindofdesignlimitscomputationtoasmallsetofsafety-relevantcells,improvesalignmentunderocclusion,\nandavoidsfloodingthedecoderwithlow-valueregions.Whennopartnerdataarrives,themodulenaturallyfallsback\nto the ego features without architectural changes.\n4.3.4. Budgeted Selection and Training Objective\nThe fused tensor is decoded as( \u0302\ud835\udc36,\u0302\ud835\udc35,\u0302\ud835\udc45) =\ud835\udc53dec(\u0303\ud835\udc39\ud835\udc52),where\u0302\ud835\udc36are class scores, \u0302\ud835\udc35are 3-D boxes, and \u0302\ud835\udc45is the\nrefined risk heat-map.\nBudgeted selection.Given a budget per link, Selective Information Sharing and Fusion Model ranks non-ego BEV\ncells by the gain\ud835\udc54(\ud835\udc2e)(Sec. 4.2) and selects the top\ud835\udc3eselcells subject to the budget. Let the per-cell byte cost be\n\ud835\udc4fcell=\ud835\udc4fidx+\ud835\udc36\ud835\udc4ffeatand header overhead\u210ehdr. For a byte budget\ud835\udc35bytes, the capacity in cells is\n\ud835\udc3esel= max(\n0,\u230a\ud835\udc35bytes\u2212\u210ehdr\n\ud835\udc4fcell\u230b)\n,(7)\nwhere,\u210ehdris a fixed header cost (bytes).\nBudget-aware training.To make the bandwidth\u2013accuracy trade-off controllable at training time, we add a com-\nmunication regularizer that penalizes over-usage relative to a target budget; this does not change runtime budget, but\nshapes the model\u2019s selection behavior. The total loss denoted by\ue238totalis as follows:\n\ue238total=\ue238det+\ud835\udf06risk\u2016\u0302\ud835\udc45\u2212\ud835\udc45gt\u20162\n2+\ud835\udf06comm\ud835\udf19(\ud835\udc48;\ud835\udc35bytes).(8)\nwhere:\n\u2022Detection loss.\ue238det=\ue238conf+\ue238regis the standard detection loss. The classification term\ue238confis a focal loss\nwith\ud835\udefc=0.25,\ud835\udefe=2.0, computed on BEV anchors and normalized by the number of positives. The regression\nterm\ue238regis a weighted Smooth-L1 loss over 7 box codes per anchor.\n\u2022Risk regression.The risk loss is a mean-squared error between predicted and ground-truth BEV risk maps:\n\u2016\u0302\ud835\udc45\u2212\ud835\udc45gt\u20162\n2.\n\u2022Communication over-usage penalty.The term\ud835\udf19(\ud835\udc48;\ud835\udc35bytes) = max(0, \ud835\udc48\u2215\ud835\udc35bytes\u2212 1).penalizes only\ncommunicationabovethe target budget, aligning learned masks with the desired budget without changing the\nruntime protocol.\nThe usage definitions can be calculated from:\n\ud835\udc48=\ud835\udc35\ud835\udc4f\ud835\udc4e\ud835\udc61\ud835\udc50\u210e\u22c5\u210ehdr+(\ud835\udc35\ud835\udc4f\ud835\udc4e\ud835\udc61\ud835\udc50\u210e\u2211\n\ud835\udc4f=1\ud835\udc3f\ud835\udc4f\u2211\n\ud835\udc59=2\u2211\n\ud835\udc56,\ud835\udc57\ud835\udc40(\ud835\udc4f)\n\ud835\udc59,\ud835\udc56,\ud835\udc57)\n\u22c5(\ud835\udc4fidx+\ud835\udc36\u22c5\ud835\udc4ffeat),(9)\nwhere\ud835\udc35\ud835\udc4f\ud835\udc4e\ud835\udc61\ud835\udc50\u210eis the batch size of this training,\ud835\udc40(\ud835\udc4f)\n\ud835\udc59,\ud835\udc56,\ud835\udc57\u2208 {0,1}is the non-ego mask (for all the masks of ego is\n\ud835\udc59=1),\u210ehdris a fixed header bytes cost,\ud835\udc4fidxis per-cell index bytes cost,\ud835\udc36is the channel dimension, and\ud835\udc4ffeatis\nbytes per feature value.\n5. Experimental Setup\n5.1. Datasets\nWeusetheOPV2Vdataset[Xuetal.,2022b]asthebasedataset.OPV2Visasyntheticmulti-vehicleCPbenchmark\ngeneratedbytheOpenCDAco-simulationofSUMO[Krajzewiczetal.,2012]andCARLA[Dosovitskiyetal.,2017].\nJiaxi Liu:Preprint submitted to ElsevierPage 11 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\nOPV2V contains 73 scenarios (average\u223c25s) across multiple CARLA towns, where2 \u223c 7connected vehicles\nrecord 64-channel LiDAR from their own viewpoints. We follow the standard frame-level counts of 6765/1981/2170\nfor train/val/test, respectively. Importantly, OPV2V natively covers a diverse set of driving situations without any\nadditional sampling from our side. The included situations comprise:\n\u2022Overtaking / Lane Change: fast lateral maneuvers with transient occlusions.\n\u2022Left-turnandRight-turn Intersections: cross-traffic under partial observability (pedestrians/cyclists may\nemerge from blind zones).\n\u2022On-ramp Merging: gap selection and speed adjustment with strong temporal risk.\n\u2022Unprotected Crossroads: multiple agents with conflicting trajectories.\n\u2022Head-on Encounters: close-range opposing traffic forming highly critical regions.\n\u2022Straight Driving (Low-risk Baseline): low-complexity scenes for calibration.\n\u2022Multi-agent Cooperation:\u22653 vehicles jointly negotiating maneuvers.\nTo illustrate why risk-aware cooperation is meaningful, we provide illustrative exemplars from OPV2V for the above\nsituations in Figure 6. These thumbnails are for visualization only and do not change the dataset composition.\nTo strengthen generalization and avoid leakage, we keep the natural scenario composition of OPV2V, but ensure\nthat train/val/test have comparable proportions of each situation (e.g., intersections, merging, head-on). The unit\nof assignment is the entire scenario (all its frames stay in one split), preventing temporal leakage while reducing\ndistributional drift between splits.\nWe control the number of agents per frame (2\u20137) by matching their histograms across splits within\u00b15%. The\nqualitativeexemplarsinFigure6showsthedifferentscenariosthatareinherentlycoveredbythedatasetorganizedby\nus.\n5.2. Risk label generation\nTo facilitate risk-aware CP using the OPV2V dataset [Xu et al., 2022b], we generate risk annotations based on\nspatial, kinematic, and traffic-contextual information, further refined by expert domain knowledge\u2014particularly in\ncomplexenvironmentssuchasintersections.Thefinalriskscoreforeachobjectiscomputedasaweightedcombination\nof three sub-components:\n\ud835\udc45\ud835\udc56=\ud835\udefc\ud835\udc51\ud835\udc45(\ud835\udc51)\n\ud835\udc56+\ud835\udefc\ud835\udc60\ud835\udc45(\ud835\udc60)\n\ud835\udc56+\ud835\udefc\ud835\udc5a\ud835\udc45(\ud835\udc5b)\n\ud835\udc56,(10)\nwhere\ud835\udc45\ud835\udc56denotestheoverallriskscoreforobject\ud835\udc56,and\ud835\udc45(\ud835\udc51)\n\ud835\udc56,\ud835\udc45(\ud835\udc60)\n\ud835\udc56,and\ud835\udc45(\ud835\udc5b)\n\ud835\udc56correspondtodistance-based,speed-based,\nand intersection-based risk scores, respectively. The weights\ud835\udefc\ud835\udc51=0.5,\ud835\udefc\ud835\udc60=0.3, and\ud835\udefc\ud835\udc5b=0.2were selected based on\nempirical tuning and expert input.\n\u2022Distance-BasedRisk:Objectslocatedclosertotheegovehiclearemorelikelytoposeanimmediatethreat.We\nquantify this via an exponential decay function of the Euclidean distance:\n\ud835\udc45(\ud835\udc51)\n\ud835\udc56=exp(\u2212\ud835\udf06\ud835\udc51\u22c5\u2016\ud835\udc29\ud835\udc56\u2212\ud835\udc29\ud835\udc52\u20162),(11)\nwhere\ud835\udc29\ud835\udc56and\ud835\udc29\ud835\udc52denotethepositionsofobject\ud835\udc56andtheegovehicle,respectively.Theparameter\ud835\udf06\ud835\udc51controlsthe\ndecay rate of risk with distance.\n\u2022Speed-BasedRisk:Rapidlyapproachingvehiclesorthosewithhighrelativespeedintroducedynamichazards.\nWe model this component as:\n\ud835\udc45(\ud835\udc60)\n\ud835\udc56=|\ud835\udc63\ud835\udc56\u2212\ud835\udc63\ud835\udc52|\nmax\ud835\udc57|\ud835\udc63\ud835\udc57\u2212\ud835\udc63\ud835\udc52|+\ud835\udf16,(12)\nwhere\ud835\udc63\ud835\udc56is the velocity of object\ud835\udc56,\ud835\udc63\ud835\udc52is the ego vehicle\u2019s speed, and\ud835\udf16is a small constant to avoid division by\nzero. This formulation emphasizes relative speed normalized across the scene.\nJiaxi Liu:Preprint submitted to ElsevierPage 12 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\nFigure 6:Representative exemplars from OPV2V illustrating scenarios that are inherently covered by the dataset and\norganized by us.\n\u2022Intersection-BasedRisk:Intersectionsareinherentlyhigh-riskregionsduetocomplextrafficflows,occlusion,\nsignal compliance issues, and the presence of vulnerable road users. We begin by measuring proximity to\nintersections:\n\ud835\udc45(\ud835\udc5b)\n\ud835\udc56=exp(\n\u2212\ud835\udf06\ud835\udc61\u22c5min\n\ud835\udc2a\ud835\udc5a\u2208\ue23d\u2016\ud835\udc29\ud835\udc56\u2212\ud835\udc2a\ud835\udc5a\u20162)\n,(13)\nwhere\ue23d={\ud835\udc2a\ud835\udc5a}denotesknownintersectioncoordinatesand\ud835\udf06\ud835\udc5aadjuststhedecaywithdistancetointersections.\nNormalization:Finally, we clip the combined risk score to the range[0,1]for stable learning:\n\u0302\ud835\udc45\ud835\udc56=min(1,max(0,\ud835\udc45\ud835\udc56)).(14)\n5.3. Baselines\nTocontextualizethestandardAPresults,wecomparethefollowingbaselinesunderthesamebackbone,BEVgrid,\nIoU thresholds, synchronization window, and quantization:\n\u2022Where2Comm(Spatial-onlybaseline)[Huetal.,2022].Arepresentativespatial-communicationmethodthat\nlearnswheretocommunicatebasedsolelyonspatialsaliencywithoutexplicitriskortask-awareweighting.Each\nagentpredictsabinarymaskindicatinginformativeBEVcells,andonlythoseregionsaretransmittedforfeature\nfusion. This baseline captures the benefit of geometry-aware but task-agnostic cooperation.\n\u2022Upper Bound (fully connected).Fully connected communication with no budget, transmitting all partner\nfeatures for fusion; serves as a performance ceiling.\nJiaxi Liu:Preprint submitted to ElsevierPage 13 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\n\u2022Lower Bound (single-agent).No cooperative communication. It measures the capability of the ego-only\ndetector.\n\u2022Fixed-Neighbor (equal-budget).The total communication budget is equally divided among all non-ego\nneighbors. Within each neighbor, features (e.g., grid cells or point clusters) are uniformly sampled at random.\nThis baseline isolates the effect of adaptive link-wise budget allocation from uniform distribution.\n\u2022Random-Cell.A global uniform sampler randomly selects exactly\ud835\udc3efeature cells from all non-ego agents,\nregardless of their spatial location or risk relevance. This baseline evaluates the effectiveness of our selective\ncontent transmission compared to random feature selection under the same bandwidth constraint.\n5.4. Implementation details\nModelandfeatureencoding.WeadoptaPointPillarBEVbackbone.ThePillarVFEuses64channels.TheBEV\nbackbone has 3/5/8 blocks with filters 64/128/256 and deconvs of 128 channels; a shrink header downsamples to 256\nchannelsforheads.Weattachthreelightweightheads:classification(percell2anchors),regression(7parametersper\nanchor), and a risk head (1 per anchor) to produce dense risk heatmaps.\nVoxel/gridandanchors.Voxelsizeis0.4\u00d70.4\u00d74mwithLiDARrange[\u2212140.8,\u221238.4,\u22123,140.8,38.4,1]m.\nThe BEV grid is\ud835\udc3b=192,\ud835\udc4a=704(feature stride 4). Anchors follow(\ud835\udc59,\ud835\udc64,\u210e)=(3.9,1.6,1.56)with yaw{0\u25e6,90\u25e6};\n\ud835\udc41\ud835\udc40\ud835\udc46=0.15, and the positive, negative thresholds are0.6and0.45separately.\nTraining setup.Optimizer: We select Adam with leanring rate=2\u00d710\u22124as the optimizer and the selection of\nweight_decay is 0.01 andeps=1e-10. For the learning rate schedule, we set cosine annealing for 50 epochs with 10-\nepochwarmup(withthewarmuplearningrate=2\u00d710\u22125,andtheminimallearningrate=5\u00d710\u22126).Duringthetraining\nofallthemodels,wesetthebatchsizeas8.Intermsofconnectingagentnumbers,wecutthenumberofagentsupto\n5 CAVs. Data augmentation includes x-axis flip, random rotation (\u00b145\u25e6), and scaling (0.95\u20131.05). Voxelization caps\nare 32 points/voxel, with train/test voxel maxima as 32k/70k separately.\nInference and post-processing.We decode detection and risk heatmaps after fusion. Evaluation uses IoU\u2208\n{0.3,0.5,0.7}; risk-aware AP uses\ud835\udf0f\u2208{0.2,0.3,0.4}. We log per-frame communication rate and bytes for the report.\n5.5. Evaluation protocols and metrics.\nWe use 3D Average Precision (3DAP) to assess object detection performance. Given a detection is considered\ncorrect if the Intersection over Union (IoU) between the predicted and ground-truth 3D bounding box exceeds a\nthreshold\ud835\udf03, the AP is computed based on the precision-recall curve.\nWe report 3DAP under three IoU thresholds:\n\ud835\udf03\u2208{0.3,0.5,0.7},\ncorresponding to different levels of localization strictness.\nLet\ud835\udc47\ud835\udc43(\ud835\udf03),\ud835\udc39\ud835\udc43(\ud835\udf03),and\ud835\udc39\ud835\udc41(\ud835\udf03)bethenumberoftruepositives,falsepositives,andfalsenegativesunderthreshold\n\ud835\udf03, respectively. Precision and recall are defined as:\nPrecision(\ud835\udf03)=\ud835\udc47\ud835\udc43(\ud835\udf03)\n\ud835\udc47\ud835\udc43(\ud835\udf03)+\ud835\udc39\ud835\udc43(\ud835\udf03),Recall(\ud835\udf03)=\ud835\udc47\ud835\udc43(\ud835\udf03)\n\ud835\udc47\ud835\udc43(\ud835\udf03)+\ud835\udc39\ud835\udc41(\ud835\udf03).(15)\n3DAP is then computed as:\n3DAP(\ud835\udf03)=\u222b1\n0Precision(\ud835\udf03,\ud835\udc5f)\ud835\udc51\ud835\udc5f,(16)\nwhere Precision(\ud835\udf03,\ud835\udc5f)is interpolated at recall level\ud835\udc5f.\nTo assess the influence of risk understanding on perception, we compute 3DAP selectively over high-risk regions\ndetermined by thresholding the predicted risk map.\nLet\ue235\ud835\udc5f\ud835\udc56\ud835\udc60\ud835\udc58(\ud835\udf0f)={\ud835\udc56|\u0302\ud835\udc45\ud835\udc56>\ud835\udf0f}be the set of objects or regions identified as risky with a risk threshold\ud835\udf0f. We evaluate\ndetection performance on this subset, denoted as 3DAP\ud835\udc5f\ud835\udc56\ud835\udc60\ud835\udc58(\ud835\udf03,\ud835\udf0f):\n3DAP\ud835\udc5f\ud835\udc56\ud835\udc60\ud835\udc58(\ud835\udf03,\ud835\udf0f)=3DAP evaluated on\ue235\ud835\udc5f\ud835\udc56\ud835\udc60\ud835\udc58(\ud835\udf0f),with IoU threshold\ud835\udf03.(17)\nJiaxi Liu:Preprint submitted to ElsevierPage 14 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\nWe report results for:\n\ud835\udf03\u2208{0.3,0.5,0.7}, \ud835\udf0f\u2208{0.2,0.3,0.4}.\nThis metric captures how well the model perceives objects in scenarios that are potentially dangerous or require\nimmediate attention, reflecting the synergy between risk assessment and spatial awareness.\nWe evaluate under two protocols:\n\u2022P1: Fixed-bandwidth.Per-link budget\ud835\udc35bytes\u2208 {0.5,0.7,1,2,3,5,10}KB/frame. Each method is tuned per\n\ud835\udc35bytes; we report Risk-AP,\u0394Risk-AP\u2215KB, and data transmission volume. This stresses efficiency at scarce\nbandwidth.\n\u2022P2: Fixed-performance.Given a target Risk-AP (e.g.,\u2265\ud835\udc4b), we report minimal bytes and latency to reach it.\nThis answers how much bandwidth is necessary for a safety line.\n6. Results and Discussion\n6.1. Main Results (Standard AP)\nWe report standard detection AP at IoU 0.3/0.5/0.7 denoted as AP30, AP50 and AP70 separately, across\nbaselines and our method. See Table 2 for overall comparison. It should be noted that the communication budget\nofWhere2comm,Fixed-Neighbor,Random-Cellandoursare20%ofthefullyconnectedsituationlikethesettingsof\nthemethodUpperBound.AndthereisnocommunicationinLowerBoundmethod.Asshowninthetable,ourmodel\nachieves consistently competitive performance across all IoU thresholds, with only marginal differences compared\nto the strongest baselines, while remaining close to the upper bound. This indicates that both our approach and the\nbaseline methods are able to effectively leverage the advantages of CP.\n6.2. Risk-Aware Evaluation\nWefurtherevaluateRisk-AwareAPbyfilteringground-truthsaboveriskthresholds\ud835\udf0f\u2208{0.2,0.3,0.4}.Resultsare\nsummarized in Tables 3. Compared with the overall AP results, where our model and the baselines perform similarly\nin Table 2, the risk-aware evaluation reveals a clearer distinction. As shown in Table 3, our method consistently\noutperforms the baselines across all IoU thresholds, especially under higher risk conditions (\ud835\udf0f= 0.3,0.4). The\nperformance of our model remains close to the upper bound while the spatial-only baseline drops significantly as\nriskincreases.Thisdemonstratesthatourdesignbetterpreservesdetectionrobustnesswhenencounteringhigh-riskor\nsafety-criticalobjects,validatingtheeffectivenessofthecommunicationprotocolandSRA-CPcoordination.Inother\nwords, although both methods achieve comparable aggregate perception accuracy, our framework exhibits stronger\nrisksensitivityandresilience,whichareessentialforsafety-orientedCP.Weadditionallyvisualizerisk-awareexample\nheatmaps (Sec. 6.6).\n6.3. P1: Pareto efficiency under fixed bandwidth\nTo further examine model performance under resource-constrained conditions, we plot AP30/AP50/AP70 vs.\ncommunication cost (KB/frame) in Figure 7 and Risk-AP30/AP50/AP70 vs. communication cost (KB/frame) in\nFigure 8. These plots provide a quantitative view of how perception accuracy scales with bandwidth usage.\nAcross the 0.5\u201310 KB/frame regime, our proposed SRA-CP configuration consistently dominates the Pareto\nfrontier, achieving higher safety-aware gains per byte compared to baseline methods. For example, at 5 KB/frame,\nour approach yields approximately +4.7% Risk-AP50 improvement over the baseline while maintaining comparable\ncommunicationoverhead.Thisdemonstratesthatthemethod\u2019scommunicationsparsificationandtheinferencefusion\njointly enable efficient and safety-preserving cooperation.\n6.4. P2: Minimal cost to reach a safety line\nFigure 9 reports the minimum bandwidth (KB/frame) required to achieve specific Risk-AP30/AP50/AP70 targets\nunder different risk thresholds. Across all nine subplots in Figure 9, our method consistently requires fewer bytes\nper frame to reach the same Risk-AP target compared to the baseline, demonstrating superior efficiency across all\nthresholds\ud835\udf0f\u2208{0.2,0.3,0.4}.\nIt is worth noting that our target values for Risk-AP were determined in a principled way: for each risk threshold\n\ud835\udf0f, we set the target AP values (for AP30, AP50, and AP70) to 0.9\u00d7, 0.8\u00d7, and 0.7\u00d7of the upper bound performance,\nJiaxi Liu:Preprint submitted to ElsevierPage 15 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\nFigure 7:Comparison of perception accuracy (AP30, AP50, AP70) under varying communication costs (KB/frame) across\nall objects.\nFigure 8:Comparison of perception accuracy (AP@30, AP@50, AP@70) under varying communication costs (KB/frame)\nfor objects with different risk levels, defined by risk thresholds\ud835\udf0f\u2208{0.2,0.3,0.4}.\nrespectively. Thisprovides areasonable andbalanced targetscale\u2014stringent enough tochallenge thecommunication\nstrategy, yet attainable for well-designed cooperative frameworks.\nForexample,at\ud835\udf0f=0.2andAP50=0.75,ourmethodreachesthetargetRisk-APusingonly1.3KB/frame,compared\ntothebaseline\u2019s3.3KB/frame.Theadvantagebecomesevenmorepronouncedunderhigherriskthresholds:at\ud835\udf0f=0.4\nand AP70=0.42/0.38, the baseline fails to achieve the required AP target across all IoU levels (Figure 9(c, f)), while\nour method maintains strong performance with 5.1 and 9 KB/frame. This indicates that when perception becomes\nsafety-critical, the baseline communication policy saturates its bandwidth without sufficient accuracy gain, whereas\nour SRA-CP-driven policy continues to deliver usable, risk-aware perception outputs.\nJiaxi Liu:Preprint submitted to ElsevierPage 16 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\nFigure 9:Comparison of different CP methods in terms of the minimum communication bandwidth (KB/frame) required\nto achieve specific perception accuracy levels (AP30, AP50, AP70) for objects with varying risk levels, categorized by risk\nthresholds\ud835\udf0f\u2208{0.2,0.3,0.4}.\nTable 2\nDetection performance comparison (the communication transmission volume of all the baselines are 20% of the Upper\nBound) (AP score higher is better).\nMethod AP30 AP50 AP70\nUpper Bound 0.9057 0.8955 0.7996\nOurs 0.89200.87310.7979\nWhere2comm (spatial-only) 0.89020.87910.7928\nFixed-Neighbor (equal-budget, ours-union) 0.8341 0.8159 0.6857\nRandom-Cell (ours-union) 0.8337 0.8156 0.6861\nLower Bound 0.8190 0.7908 0.6263\n6.5. Ablation Studies\nWeablatekeycommunicationchoiceslikegatetype(S-only,R-only,Union),blind-zoneestimation(on/off)tosee\nwhether the modules of our method are actually working.\nGate Mode AnalysisWe compare three gate configurations under the same 5kB/frame bandwidth: spatial-only (S-\nonly),risk-only(R-only),andourhybridUniongatethatintegratesbothspatialandriskcuesasshowninTable4.The\nresults in Table 4 report Risk-Aware AP at IoU=0.3/0.5/0.7 across\ud835\udf0f\u2208{0.2,0.3,0.4}.\nAcross all thresholds and IoU levels, the proposed Union gate consistently outperforms both S-only and R-only\nvariants. At\ud835\udf0f=0.3, for instance, Union improves AP50 from 0.6636 (S-only) and 0.6722 (R-only) to 0.6959, while at\n\ud835\udf0f=0.4the gap widens to over +4.2% compared with S-only. Similarly, the AP70 metric rises from 0.3302 (S-only)\nJiaxi Liu:Preprint submitted to ElsevierPage 17 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\nTable 3\nRisk-aware detection performance across risk thresholds (\ud835\udf0f=0.2\u22150.3\u22150.4). Higher is better.\nMethod Risk\ud835\udf0fAP30 AP50 AP70\nUpper Bound0.2 0.8461 0.8411 0.7745\n0.3 0.7659 0.7632 0.6962\n0.4 0.5003 0.4994 0.4704\nOurs0.20.8365 0.8315 0.7667\n0.30.7642 0.7622 0.6998\n0.40.4963 0.4955 0.4702\nWhere2comm (spatial-only)0.2 0.8203 0.8136 0.7512\n0.3 0.7412 0.7354 0.6807\n0.4 0.4701 0.4553 0.4177\nFixed-Neighbor (equal-budget)0.2 0.7644 0.7519 0.6519\n0.3 0.6640 0.6553 0.5705\n0.4 0.3610 0.3565 0.3171\nRandom-Cell0.2 0.7641 0.7511 0.6505\n0.3 0.6670 0.6578 0.5723\n0.4 0.3737 0.3685 0.3238\nLower Bound0.2 0.7531 0.7357 0.6191\n0.3 0.6483 0.6374 0.5381\n0.4 0.3631 0.3581 0.3111\nTable 4\nRisk-aware AP at IoU=0.3/0.5/0.7 for different gate modes (5k budget) across risk thresholds\ud835\udf0f.\nGate Metric\ud835\udf0f=0.2\ud835\udf0f=0.3\ud835\udf0f=0.4\nS-onlyAP30 0.7763 0.6714 0.3716\nAP50 0.7608 0.6636 0.3661\nAP70 0.6725 0.5963 0.3302\nR-onlyAP30 0.7861 0.6811 0.3959\nAP50 0.7731 0.6722 0.3894\nAP70 0.6795 0.6002 0.3544\nUnion (ours)AP300.7981 0.7032 0.4128\nAP500.7866 0.6959 0.4082\nAP700.7097 0.6308 0.3742\nand 0.3544 (R-only) to 0.3742. These gains demonstrate that combining spatial coverage with risk awareness yields\ncomplementary benefits\u2014risk-only gating favors safety-critical regions but may miss peripheral context, whereas\nspatial-only gating ensures broader coverage but wastes bandwidth on low-risk areas.\nByunifyingbothcriteria,theUniongateadaptivelyallocatestransmissionprioritybasedonspatialrelevanceand\nestimated collision risk, effectively balancing perception completeness and communication efficiency. This hybrid\ngating thus provides a more stable and risk-sensitive communication policy, enabling the system to maintain higher\ndetection performance even as\ud835\udf0fincreases.\nBlind-Zone EstimationTo examine whether the model benefits from explicitly prioritizing safety-critical blind\nareas, we conduct an ablation study on the Union gating scheme with and without blind-zone weighting under a\nfixed 5kB/frame communication budget. The results in Table 5 report Risk-Aware AP at IoU=0.3/0.5/0.7 across risk\nthresholds\ud835\udf0f\u2208{0.2,0.3,0.4}.\nJiaxi Liu:Preprint submitted to ElsevierPage 18 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\nTable 5\nRisk-aware AP at IoU=0.3/0.5/0.7 for Union gate with/without blind-zone weighting (5k budget) across\ud835\udf0f.\nSetting Metric\ud835\udf0f=0.2\ud835\udf0f=0.3\ud835\udf0f=0.4\nUnion (no blind)AP30 0.7912 0.6877 0.3973\nAP50 0.7803 0.6778 0.3912\nAP70 0.6948 0.6104 0.3542\nUnion (blind on, ours)AP300.7981 0.7032 0.4128\nAP500.7866 0.6959 0.4082\nAP700.7097 0.6308 0.3742\nAcross all IoU and risk thresholds, enabling blind-zone weighting consistently improves detection performance.\nComparedtothevanillaUniongate,ourmethodachievesanaveragegainof+0.7%,+1.2%,and+1.6%forAP30,AP50\nand AP70, respectively. The improvement becomes more pronounced as the risk threshold increases. For instance, at\n\ud835\udf0f=0.4,theRisk-AP70risesfrom0.3542to0.3742,representingarelativegainof+5.6%.Thispatternsuggeststhatthe\nproposedweightingmechanismeffectivelyallocatescommunicationbandwidthtowardregionswithhigherocclusion\nand potential collision risk.\nQualitatively, this mechanism acts as a \u201csafety amplifier\u201d: when cooperative views overlap poorly or when agents\nobserve asymmetric blind spots, the weighting function adaptively increases the transmission priority of uncertain\nspatial zones. As a result, even under the same bandwidth constraint, more informative features are propagated to\nneighboring vehicles, enhancing risk-aware perception robustness in safety-critical scenarios.\n6.6. Visualization & Case Study\nFigure 10 illustrates a challenging unprotected left-turn scenario with dense cross-traffic. The ego vehicle intends\nto turn left, yet its LiDAR alone cannot observe the incoming traffic hidden behind other vehicles\u2019 occlusions. These\nblind-zone regions coincide with locations where high-risk background vehicles are approaching, making the timely\nrestoration of occluded agents crucial for safe maneuver planning.\nWe compare four communication strategies: a random-cell baseline, spatial-only, risk-only, and our Union (SRA-\nCP) method. The spatial-only, risk-only, and Union methods all operate under the same fixed communication budget,\nwhereas the random baseline uses a significantly higher budget, illustrating how communication volume alone does\nnot guarantee performance.\nOurs vs. Spatial-only and Risk-only.Despite using the same byte budget, the three strategies prioritize cells\ndifferently:\nSpatial-onlyfocusessolelyongeometricvisibilitydifficulty.Itsuccessfullyidentifiescellsthatarehardtoperceive\nbut often fails to emphasize high-risk agents located in traffic-conflict regions. As a result, it may transmit cells that\nare geometrically interesting yet irrelevant for imminent collision risk, while missing the truly dangerous ones.\nRisk-onlyallocatesnearlyallbandwidthtothehigh-riskregion.Thisimprovesawarenessofhazardousagentsbut\nignores spatial fusion quality, often leading to incomplete or noisy reconstructions because difficult-to-fuse regions\nreceive insufficient coverage.\nUnion (Ours) balances both spatial fusion difficulty and collision risk. It means SRA-CP suppresses low-value\nregions and forms a dense transmission corridor aligned with the ego\u2013background conflict path, precisely where the\noccluded vehicle lies. As shown in the detection overlays, Union restores the hidden vehicle more reliably and aligns\ncloser with the ground truth than either single-objective method.\nOurs vs. Random-cell Communication.Even with a much larger number of transmitted cells, the random-cell\nmethod performs poorly. Because cells are sampled uniformly at random, it often allocates bandwidth to irrelevant\nfree-spaceareaswhilefailingtocoverthecriticalblind-zoneregionatthecorrectmoment.Consequently,therecovered\ndetection remains incomplete or inconsistent despite the inflated budget.\nIncontrast,Union(SRA-CP)pinpointsandtransmitsonlytheessentialcells\u2014thosethatinfluencecollisionriskor\nimprovemulti-agentfusionquality\u2014andthusreconstructsthecriticaloccludedvehiclewithdramaticallyfewerbytes.\nJiaxi Liu:Preprint submitted to ElsevierPage 19 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\nWhat Gets Transmitted (Per-cell Transmission Maps).The transmission maps further confirm each method\u2019s\nbehavior:\nSpatial-onlyspreadsbytesbroadlyacrossmanycells\u2014highcoveragebutlowefficiency.Risk-onlyover-concentrates\ninacompactregion\u2014highfocusbutweakcontextualsupport.Randomshowsnoisy,unstructuredcoverageevenwitha\nlargebudget\u2014nosemanticprioritization.Union(Ours)exhibitsanintelligent,elongatedhigh-densitybandthattracks\nthe potential collision trajectory while maintaining minimal peripheral context.\nThis pattern matches the ablation findings in Table 4: combining spatial difficulty and risk factors yields the most\nefficient allocation strategy.\nInsummary,ourexperimentsshowthatSRA-CPconsistentlydominatesexistingcooperative-perceptionbaselines\nin the communication\u2013safety trade-off. Under the same communication budget, SRA-CP matches or exceeds the\ncutting-edgespatial-onlyselectivemethodinperceptionaccuracy,whiledeliveringnotablyhigherperceptionaccuracy\nfor safety-critical objects. When sweeping the per-link budget, our method traces the Pareto frontier: for any given\nbandwidthitattainsthebestrisky-objectdetection,andforanytargetperceptionaccuracyitrequiressubstantiallyfewer\ntransmitted bytes than competing schemes. Qualitative case studies at unprotected intersections further illustrate that\nSRA-CPautomaticallyconcentratesmessagesonriskyblind-zonecells,allowingtheegovehicletorecoveroccluded,\ndangerous agents earlier and more reliably during driving.\n7. Conclusion\nThispaperpresentsanovelSpontaneousRisk-AwareSelectiveCooperativePerception(SRA-CP)frameworktoad-\ndressthescalabilityandbandwidthchallengesofmulti-agentcooperativeperceptionindynamictrafficenvironments.\nWe first design a protocol in which connected agents continuously broadcast their perception coverage with very low\ncommunication cost and initiate on-demand handshakes when risk-relevant blind zones are detected. For a certain\nconnected agent, we propose a perceptual risk identification model to detect and quantify risk-critical occlusions,\na selective information sharing model to determine which features to transmit under bandwidth constraints, and a\ndual-attention feature fusion model to integrate received features into the ego agent\u2019s perception output.\nExtensive evaluations on a public dataset were conducted against five baseline methods, each targeting a different\naspect of the problem. These include a cutting-edge selective CP method, a fully connected CP setting as an upper\nbound, a no-CP setup as a lower bound, and another 2 methods: fixed neighbor allocation and random feature\nSamplingtoevaluatetheeffectsofcommunicationtargetselectionandcontent-levelfeatureprioritization,respectively.\nExperimental results show that SRA-CP achieves less than 1% loss for safety-critical objects compared to generic\nCP, while using only 20% of the communication bandwidth. Moreover, compared to the cutting-edge selective CP\nmethod, SRA-CP improves the AP for critical objects by 15% under the same bandwidth budget, demonstrating its\ncommunication efficiency and risk-awareness advantage.\nAs future work, we are collecting real-world driving data using our lab\u2019s connected vehicles. We plan to further\nevaluate the framework on this in-house dataset and conduct field tests to assess its real-world applicability and\nrobustness.\nReferences\nQi Chen, Xu Ma, Sihai Tang, Jingda Guo, Qing Yang, and Song Fu. F-cooper: Feature based cooperative perception for autonomous vehicle\nedge computing system using 3d point clouds. InProceedings of the 4th ACM/IEEE Symposium on Edge Computing (SEC), pages 88\u2013100.\nACM/IEEE, 2019a.\nQi Chen, Sihai Tang, Qing Yang, and Song Fu. Cooper: Cooperative perception for connected autonomous vehicles based on 3d point clouds. In\nProceedings of the 2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS), pages 514\u2013524. IEEE, 2019b. doi:\n10.1109/ICDCS.2019.00058.\nHsu-kuangChiu,Chien-YiWang,Min-HungChen,andStephenF.Smith. Probabilistic3dmulti-objectcooperativetrackingforautonomousdriving\nvia differentiable multi-sensor kalman filter. InProceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages\n18458\u201318464. IEEE, 2024. doi: 10.1109/ICRA57147.2024.10610487.\nLiang Dong, Zheng Yang, Xinjun Cai, Yi Zhao, Qiang Ma, and Xin Miao. Wave: Edge-device cooperated real-time object detection for open-air\napplications.IEEE Transactions on Mobile Computing, 22(7):4347\u20134357, 2022.\nAlexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. InConference\non robot learning, pages 1\u201316. PMLR, 2017.\nBrahim El Boukili, Mohammed-Hicham Zaggaf, and Lhoussain Bahatti. Cooperative lane keeping assist: Design and evaluation of a v2v lane\nperception sharing approach.Journal of Robotics and Control, 6(5):2239\u20132248, 2025. doi: 10.18196/jrc.v6i5.26784.\nJiaxi Liu:Preprint submitted to ElsevierPage 20 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\nYihengFeng,ChunhuiYu,andHenryX.Liu.Spatiotemporalintersectioncontrolinaconnectedandautomatedvehicleenvironment.Transportation\nResearch Part C: Emerging Technologies, 89:364\u2013383, 2018. doi: 10.1016/j.trc.2018.02.001.\nBolin Gao, Jiaxi Liu, Hengduo Zou, Jiaxing Chen, Lei He, and Keqiang Li. Vehicle-road-cloud collaborative perception framework and key\ntechnologies: A review.IEEE Transactions on Intelligent Transportation Systems, 2024.\nKirin Godhwani, Adam S. R. Parker, Matthew E. Taylor, William Yeoh, and Reuth Mirsky. Towards spontaneous cooperation in multi-agent\nreinforcement learning using explicit goal recognition. InRLC 2025 Workshop on Cooperative and Competitive Multi-Agent Reinforcement\nLearning (CoCoMARL), 2025. Poster paper.\nYue Hu, Shaoheng Fang, Zixing Lei, Yiqi Zhong, and Siheng Chen. Where2comm: Communication-efficient collaborative perception via spatial\nconfidence maps. InAdvances in Neural Information Processing Systems, volume 35, pages 4874\u20134886, 2022.\nLennart Lorenz Freimuth Jahn, Seongjeong Park, Yongseob Lim, Jinung An, and Gyeungho Choi. Enhancing lane detection with a lightweight\ncollaborative late fusion model.Robotics and Autonomous Systems, 175:104680, 2024.\nDaniel Krajzewicz, Jakob Erdmann, Michael Behrisch, Laura Bieker, et al. Recent development and applications of sumo-simulation of urban\nmobility.International journal on advances in systems and measurements, 5(3&4):128\u2013138, 2012.\nAlex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from\npoint clouds. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12697\u201312705, 2019.\nRongsong Li and Xin Pei. Multi-V2X: A large scale multi-modal multi-penetration-rate dataset for cooperative perception, 2024.\nJiaxiLiu,BolinGao,WeiZhong,YanboLu,andShuoHan. Adaptiveoptimizationstrategyandevaluationofvehicle-roadcollaborativeperception\nalgorithm in real-time settings.Computers and Electrical Engineering, 120:109785, 2024.\nYen-Cheng Liu, Junjiao Tian, Nathaniel Glaser, and Zsolt Kira. When2com: Multi-agent perception via communication graph grouping. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4106\u20134115, 2020.\nChengyuanMa,HangyuLi,KekeLong,HangZhou,ZhaohuiLiang,PeiLi,HongkaiYu,andXiaopengLi. Real-timeidentificationofcooperative\nperception necessity in road traffic scenarios.Available at SSRN 4973353, 2025.\nReuthMirsky,IgnacioCarlucho,ArrasyRahman,ElliotFosong,WilliamMacke,MohanSridharan,PeterStone,andStefanoV.Albrecht. Asurvey\nof ad hoc teamwork: Definitions, methods, and open problems.arXiv preprint arXiv:2202.10450, 2022.\nFenglian Pan, Yinwei Zhang, Jian Liu, Larry Head, Maria Elli, and Ignacio Alvarez. Reliability modeling for perception systems in autonomous\nvehicles:Arecursiveevent-triggeringpointprocessapproach.TransportationResearchPartC:EmergingTechnologies,169:104868,2024. doi:\n10.1016/j.trc.2024.104868.\nHuan Qiu, Jian Zhou, Bijun Li, Qin Zou, Youchen Tang, and Man Luo. Map4comm: A map-aware collaborative perception framework with\nefficient-bandwidth information fusion.Information Fusion, page 103567, 2025.\nAhmad Sarlak, Rahul Amin, and Abolfazl Razi. Extended visibility of autonomous vehicles via optimized cooperative perception under imperfect\ncommunication.Transportation Research Part C: Emerging Technologies, 180:105350, 2025. doi: 10.1016/j.trc.2025.105350.\nJessica Van Brummelen, Marie O\u2019Brien, Dominique Gruyer, and Homayoun Najjaran. Autonomous vehicle perception: The technology of today\nand tomorrow.Transportation Research Part C: Emerging Technologies, 89:384\u2013406, 2018. doi: 10.1016/j.trc.2018.02.012.\nTsun-Hsuan Wang, Sivabalan Manivasagam, Ming Liang, Bin Yang, Wenyuan Zeng, and Raquel Urtasun. V2VNet: Vehicle-to-vehicle commu-\nnication for joint perception and prediction. InComputer Vision \u2013 ECCV 2020, volume 12347 ofLecture Notes in Computer Science, pages\n605\u2013621. Springer, 2020.\nZengqingWu,RunPeng,ShuyuanZheng,QianyingLiu,XuHan,BrianI.Kwon,MakotoOnizuka,ShaojieTang,andChuanXiao. Shallweteam\nup:ExploringspontaneouscooperationofcompetingLLMagents. InFindingsoftheAssociationforComputationalLinguistics:EMNLP2024,\npages 5163\u20135186, Miami, Florida, USA, 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.297. URL\nhttps://aclanthology.org/2024.findings-emnlp.297/.\nHao Xiang, Zhaoliang Zheng, Xin Xia, Runsheng Xu, Letian Gao, Zewei Zhou, Xu Han, Xinkai Ji, Mingxi Li, Zonglin Meng, Li Jin, Mingyue\nLei, Zhaoyang Ma, Zihang He, Haoxuan Ma, Yunshuang Yuan, Yingqian Zhao, and Jiaqi Ma. V2X-Real: A large-scale dataset for vehicle-to-\neverything cooperative perception. InComputer Vision \u2013 ECCV 2024, 2024.\nRunshengXu,HaoXiang,ZhengzhongTu,XinXia,Ming-HsuanYang,andJiaqiMa. V2X-ViT:Vehicle-to-everythingcooperativeperceptionwith\nvision transformer. InComputer Vision \u2013 ECCV 2022, volume 13699 ofLecture Notes in Computer Science, pages 107\u2013124. Springer, 2022a.\nRunsheng Xu, Hao Xiang, Xin Xia, Xu Han, Jinlong Li, and Jiaqi Ma. Opv2v: An open benchmark dataset and fusion pipeline for perception\nwithvehicle-to-vehiclecommunication. In2022IEEEInternationalConferenceonRoboticsandAutomation(ICRA),pages2583\u20132589.IEEE,\n2022b.\nRunsheng Xu, Xin Xia, Jinlong Li, Hanzhao Li, Shuo Zhang, Zhengzhong Tu, Zonglin Meng, Hao Xiang, Xiaoyu Dong, Rui Song, Hongkai Yu,\nBolei Zhou, and Jiaqi Ma. V2V4Real: A real-world large-scale dataset for vehicle-to-vehicle cooperative perception. InProceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13712\u201313722, 2023.\nDingkang Yang, Kun Yang, Yuzheng Wang, Jing Liu, Zhi Xu, Rongbin Yin, Peng Zhai, and Lihua Zhang. How2comm: Communication-efficient\nand collaboration-pragmatic multi-agent perception.Advances in Neural Information Processing Systems, 36:25151\u201325164, 2023a.\nKunYang,DingkangYang,JingyuZhang,HanqiWang,PengSun,andLiangSong. What2comm:Towardscommunication-efficientcollaborative\nperception via feature decoupling. InProceedings of the 31st ACM international conference on multimedia, pages 7686\u20137695, 2023b.\nWenbin Yang, Hang Yu, Xiangfeng Luo, and Shaorong Xie. Density-aware early fusion for vehicle collaborative perception.IEEE Intelligent\nTransportation Systems Magazine, 17(2):33\u201347, 2025.\nChunhui Yu, Yiheng Feng, Henry X. Liu, Wanjing Ma, and Xiaoguang Yang. Corridor level cooperative trajectory optimization with connected\nand automated vehicles.Transportation Research Part C: Emerging Technologies, 105:405\u2013421, 2019. doi: 10.1016/j.trc.2019.06.002.\nHaibao Yu, Wenxian Yang, Hongzhi Ruan, Zhenwei Yang, Yingjuan Tang, Xu Gao, Xin Hao, Yifeng Shi, Yifeng Pan, Ning Sun, Juan Song, Jirui\nYuan, Ping Luo, and Zaiqing Nie. V2X-Seq: A large-scale sequential dataset for vehicle-infrastructure cooperative perception and forecasting.\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\nJiaxi Liu:Preprint submitted to ElsevierPage 21 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\nYuanyuan Zha, Wei Shangguan, Junjie Chen, Linguo Chai, Weizhi Qiu, and Antonio M L\u00f3pez. Heterogeneous multiscale cooperative perception\nfor connected autonomous vehicles via v2x interaction.IEEE Internet of Things Journal, 2025.\nJiaruZhong,JiahaoWang,JiahuiXu,XiaofanLi,ZaiqingNie,andHaibaoYu. Cooptrack:Exploringend-to-endlearningforefficientcooperative\nsequentialperception. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision(ICCV),2025. doi:10.48550/arXiv.2507.\n19239. Highlight paper.\nWalter Zimmer, Gerhard Arya Wardana, Suren Sritharan, Xingcheng Zhou, Rui Song, and Alois C. Knoll. Tumtraf V2X cooperative perception\ndataset. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\nJiaxi Liu:Preprint submitted to ElsevierPage 22 of 22\nSRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception\n(a) Predictions vs. ground truth in BEV with risk overlay.\n(b) Risk heatmap and per-cell transmission (Union vs. baselines).\nFigure 10:Qualitative example at an unprotected intersection. Our method prioritizes risky blind-zone cells, recovering\noccluded targets with fewer transmission bytes.\nJiaxi Liu:Preprint submitted to ElsevierPage 23 of 22\n",
    "title": "SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception",
    "arxiv_id": "2511.17461v1",
    "category": "recent",
    "authors": [
      "Jiaxi Liu",
      "Chengyuan Ma",
      "Hang Zhou",
      "Weizhe Tang",
      "Shixiao Liang",
      "Haoyang Ding",
      "Xiaopeng Li",
      "Bin Ran"
    ],
    "published": "2025-11-21",
    "num_pages": 23,
    "num_chars": 72467
  },
  {
    "text": "Planning with Sketch-Guided Verification for Physics-Aware Video Generation\nYidong Huang1Zun Wang1Han Lin1Dong-Ki Kim2Shayegan Omidshafiei2\nJaehong Yoon3Yue Zhang1Mohit Bansal1\n1UNC Chapel Hill2FieldAI3Nanyang Technological University\nhttps://sketchverify.github.io/\nAbstract\nRecent video generation approaches increasingly rely on\nplanning intermediate control signals such as object tra-\njectories to improve temporal coherence and motion fi-\ndelity. However, these methods mostly employ single-\nshot plans that are typically limited to simple motions,\nor iterative refinement which requires multiple calls to\nthe video generator, incuring high computational cost.\nTo overcome these limitations, we proposeSketchVerify,\na training-free, sketch-verification-based planning frame-\nwork that improves motion planning quality with more dy-\nnamically coherent trajectories (i.e., physically plausible\nand instruction-consistent motions) prior to full video gen-\neration by introducing a test-time sampling and verification\nloop. Given a prompt and a reference image, our method\npredicts multiple candidate motion plans and ranks them\nusing a vision-language verifier that jointly evaluates se-\nmantic alignment with the instruction and physical plausi-\nbility. To efficiently score candidate motion plans, we ren-\nder each trajectory as a lightweight video sketch by com-\npositing objects over a static background, which bypasses\nthe need for expensive, repeated diffusion-based synthesis\nwhile achieving comparable performance. We iteratively\nrefine the motion plan until a satisfactory one is identified,\nwhich is then passed to the trajectory-conditioned genera-\ntor for final synthesis. Experiments on WorldModelBench\nand PhyWorldBench demonstrate that our method signifi-\ncantly improves motion quality, physical realism, and long-\nterm consistency compared to competitive baselines while\nbeing substantially more efficient. Our ablation study fur-\nther shows that scaling up the number of trajectory candi-\ndates consistently enhances overall performance.\n1. Introduction\nImage-to-Video (I2V) generation has demonstrated strong\npotential across a wide range of applications, includingrobotic manipulation, autonomous driving, and game con-\ntent creation. While modern video generative models [4,\n12, 16, 34, 37] have enabled impressive visual quality and\nsemantic alignment, producing videos with physically re-\nalistic and temporally consistent motion remains challeng-\ning. In particular, these models often fail to interpret fine-\ngrained motion instructions and struggle to generate se-\nquences that adhere to plausible physical dynamics [8, 22].\nRecent studies have introduced intermediate object-level\nlayout [2, 40] or trajectory planning [9, 15, 23] with large\nlanguage models (LLMs) to guide video generation, aim-\ning to improve motion fidelity and controllability. How-\never, most existing approaches adopt a single-shot planning\nparadigm (Fig. 1a), where a single control sequence is gen-\nerated per prompt. While this design is straightforward,\nit is vulnerable to inaccuracies or noise in the predicted\nplan, which can propagate through the generation process\nand result in inconsistent or implausible object motions.\nTo mitigate the instability inherent in single-shot planning,\nan alternative line of research explores iterative refinement\n(Fig. 1b), where the prompt or control signals are progres-\nsively updated over multiple steps [7, 20, 45]. Although\nsuch methods can enhance visual realism through feedback-\nbased correction, they incur substantial computational over-\nhead due to repeated diffusion calls during generation.\nTo address these limitations, we proposeSketchVerify,\na test-time planning framework that iteratively refines mo-\ntion plans using verification on lightweight video sketches\ninstead of costly full video synthesis (Fig. 1c). SketchVer-\nify integrates a multimodal verifier with a test-time search\nprocedure to automatically detect and correct semantic or\nphysical inconsistencies, compensating for the lack of self-\ncorrection in one-shot planning. By decoupling refinement\nfrom the diffusion backbone and verifying motion at the\nsketch or layout level, SketchVerify avoids the heavy over-\nhead of full-generation\u2013based iterative updates, enabling ef-\nficient test-time search in about five minutes\u2014over an order\nof magnitude faster than baselines requiring full generation.\nSpecifically, given a text prompt and an initial image,\n1arXiv:2511.17450v1  [cs.CV]  21 Nov 2025\nText Prompt\n& Image\nMLLM\nPlanner\nObject Layout \n& Trajectory\nTI2V\nVideo\n(a)One-shot Planning: Errors accumulate due to lack of correction.\nPrompt \nExtension\nTI2V\nMLLM\nVerifierVideo\nText Prompt\n& Image (b)Iterative Generation: Inefficient repeated generations.\nText Prompt\n& Image\nMLLM\nPlanner\nObject Layout \n& Trajectory\nMLLM\nVerifier\n\u2026\n TI2V\nVideo\nSketchesVerified  \nObject Layout & \nTrajectoryVideo\n(c)SketchVerify: A multimodal verifier ranks control plans using warped video sketches before synthesis.\nFigure 1. Comparison of SketchVerify with other MLLM planning based video generation pipelines. Existing methods either rely on\none-shot planning, which lacks correction, or iterative refinement, which requires repeated generation. Our method addresses both issues\nby selecting high-quality control plans using a multimodal verifier prior to synthesis.\nour approach first constructs a high-level motion plan com-\nposed of sequential sub-instructions (e.g., \u201capproach the\nball,\u201d \u201cpick it up,\u201d \u201cplace it on the table\u201d) and identi-\nfies the corresponding movable objects through segmenta-\ntion. Then, it sequentially generates a trajectory plan for\neach sub-instruction over time. In particular, given a sub-\ninstruction and the context image derived from the previ-\nous step, SketchVerify samples multiple candidate trajec-\ntory plans represented as a sequence of bounding boxes cap-\nturing the object\u2019s location at each frame. To efficiently vi-\nsualize and verify these motion candidates, we render each\ntrajectory as a lightweightvideo sketch. Rather than syn-\nthesizing full videos, the framework crops the segmented\nobject from the first frame and composites it onto a static\nbackground. This lightweight video sketch preserves the es-\nsential spatial and temporal structure of the scene, allowing\nsignificantly faster verification while maintaining compara-\nble content information and verification quality to full video\ngeneration. A vision\u2013language verifier then evaluates each\nsketch along two complementary dimensions. First, it as-\nsesses semantic alignment by comparing the sketch with the\ncorresponding sub-instruction, ensuring that the depicted\nmotion fulfills the described intent (e.g., whether the ob-\nject indeed \u201cmoves toward the basket\u201d or \u201cpicks up the\nball\u201d). Second, it evaluates physical plausibility through\nstructured reasoning over several motion principles, includ-\ning Newtonian consistency, non-penetration with scene el-\nements, gravity-coherent vertical motion, and shape stabil-\nity across frames. The trajectory achieving the highest ag-\ngregated verification score is selected as the final motion\nplan for that sub-instruction. After all sub-instructions are\nprocessed, their verified trajectories are merged into a uni-fied plan, which is passed to a trajectory-conditioned diffu-\nsion model for final video synthesis. By conducting this\nstructured planning and verification entirely at test time,\nour approach produces semantically coherent and physi-\ncally grounded videos without requiring additional training\nor costly iterative refinement.\nWe evaluate SketchVerify on WorldModelBench and\nPhyWorldBench, two large-scale benchmarks designed to\nassess instruction compliance, physical reasoning, and tem-\nporal coherence in generative video models. Our method\nconsistently outperforms state-of-the-art open-source I2V\nmodels across instruction following, physical law adher-\nence, and commonsense consistency, while reducing over-\nall planning cost by nearly an order of magnitude compared\nto iterative refinement pipelines. Ablation studies further\nshow that (i) multimodal verification markedly strengthens\nspatial and physical reasoning compared to language-only\nvariants, (ii) scaling the verifier improves trajectory plau-\nsibility, (iii) sketch-based verification matches the quality\nof full video\u2013based verification with nearly a tenfold effi-\nciency gain, and (iv) increasing the number of sampled tra-\njectories yields steady performance gains.\n2. Related Works\nMLLM Planning for Video Generation.Recent work\nincreasingly leverages LLMs and MLLMs to provide\nstructured planning for video generation. GPT-style models\nexpand sparse text prompts into \u201cvideo plans\u201d\u2014including\nbounding-box trajectories [23, 25, 43], scene-level\nkeyframes [15], or motion-aware sketches [23]\u2014which\nare then used for layout-guided diffusion synthe-\nsis [11, 24, 25, 49\u201353]. However, these methods rely\n2\non a single-pass plan that often remains coarse or phys-\nically inconsistent. In contrast, our approach performs\niterative, verifier-guided refinement, repeatedly scoring\nand updating candidate trajectories to produce plans with\nstronger spatial constraints and physical plausibility.\nIterative Refinement for Visual Generation.Iterative re-\nfinement is widely used to improve consistency and control-\nlability in visual generation. Methods such as RPG [46],\nPhyT2V [45], VideoRepair [20], and VISTA [29] refine\nprompts or layouts by repeatedly evaluating fully generated\nvideos, which is time-consuming (whole process usually\nneeds over 30 minutes). In contrast, we refine during the\nplanning stage by scoring lightweight sketch-based simu-\nlations with a multimodal verifier, avoiding repeated video\nsynthesis and enabling efficient test-time optimization with-\nout harming the verification performance.\nPhysics-Aware Video Generation.Recent work reveals\nthat state-of-the-art video diffusion models often violate\neven basic physical laws [3, 31]. To address this, prior\nstudies explored a wide range of physics-aware enhance-\nments, including physics-simulator\u2013guided motion [28, 30,\n35, 42], physics-driven post-training and reward optimiza-\ntion [21, 26, 41], and vision\u2013language reasoning or force-\nbased conditioning that inject implicit physical priors [6,\n10, 39, 47]. Instead of relying on heavy simulation, spe-\ncialized datasets, or extensive finetuning, we focus on plan-\nlevel iterative refinement with our SketchVerify framework,\nachieving zero-shot generalization across diverse physical\nscenarios.\n3. Method \u2013 SketchVerify\nOur approach performs lightweight, verifier-guided refine-\nment entirely before generation, scoring and improving mo-\ntion plans at test time without any additional training or re-\npeated video synthesis. We organize the method section\nas follows. Section 3.1 introduces the problem formula-\ntion and provides a high-level overview of our framework.\nSection 3.2 describes the high-level plan decomposition\nand object/background extraction process from the input\nprompt and image. Section 3.3 presents our proposed visual\ntest-time planning module (SketchVerify), which performs\ntrajectory sampling and multimodal verification based on\nsketch-based surrogates. Section 3.4 describes the final\nvideo synthesis process using a trajectory-conditioned dif-\nfusion model guided by the selected motion plan.\n3.1. Overview\nGiven a natural language promptPand an initial imageI 0,\nour method produces a temporally coherent and physically\nplausible videoV={I 1, . . . ,I T}. As shown in Fig. 2, the\nframework is composed of three key modules:\n1.High Level Planning and Object Parsing:This mod-\nule interprets the prompt\u2019s high-level narrative intent andgenerates a sequence of actionable sub-goals. Concur-\nrently, it parses the initial scene to isolate the dynamic\ntarget object from the static \u201cstage\u201d (the background),\nthereby defining a clear, structured problem for the sub-\nsequent motion planning module.\n2.Test-Time Planning:This module constitutes the\ncore contribution of our method. Instead of perform-\ning expensive trial-and-error with diffusion models,\nSketchVerify conducts an efficient test-time search for\noptimal motion trajectories. It samples lightweight mo-\ntion candidates (video sketches) and scores them with\na multimodal verifier that assessessemantic alignment\nwith the instruction andphysical plausibilitybased on\nreal-world motion priors. By verifying motion qual-\nity before synthesis, SketchVerify decouples reasoning\nabout object dynamics from the computationally inten-\nsive generation process.\n3.Trajectory-Conditioned Video Generation:The ver-\nified motion plan is passed to a diffusion-based video\ngenerator. Because the generator receives a pre-verified,\nhigh-quality motion plan, this stage focuses solely on vi-\nsual fidelity, producing semantically coherent and phys-\nically consistent video sequences.\n3.2. High-Level Planning and Object Parsing\nHigh-Level Planning.We begin by generating a struc-\ntured plan of sub-instructionsP 1, . . . ,P M(e.g., \u201capproach\nthe ball\u201d, \u201cpick up the ball\u201d) from the natural language\npromptPusing an MLLM, thereby mitigating the difficulty\nof long-horizon planning.\nObject and Background Extraction.To enable motion\nplanning over a clean static canvas, we first identify objects\ninvolved in motion. Specifically, given the promptPand\ninitial frameI 0, we use an MLLM to extract a list of ob-\nject names expected to move according to the described ac-\ntions. Next, we apply a detector\u2013segmenter pair, Ground-\nedSAM [17, 27, 33], for precise mask extraction to localize\nthe mentioned objects. This results in a set of object masks\nM={m 1, . . . , m N}corresponding to the moving entities,\nwhereNis the number of moving objects proposed. To\nobtain a clean, static background for compositing, we re-\nmove the masked object regions fromI 0and fill them using\nOmnieraser [44], a background inpainting model fine-tuned\nfrom FLUX [19]. The output is a static background image\nB, which serves as the canvas for video sketch rendering in\nsubsequent stages (Step 1 in Fig. 2).\n3.3. Test-Time Planning\nTo improve trajectory quality without incurring the com-\nputational cost of iterative synthesis, we propose a sketch-\nverification guided test-time planning module that samples\nand verifies object-level motion plans before video genera-\ntion (Step 2 in Fig. 2).\n3\nFrames 12 -25: \nThe robotic arm \npicks up the carrot \nand moves to the \nmetal bowl.Frames 1 -12: \nThe robotic arm \napproaches the \ncarrot.Step 1:  High -Level Planning\n\u201cThe robotic arm \nplaces the carrot \ninto the metal bowl\u201d\nInitial\nPlansMoving Objects &\nBackground\nRobotic arm, \nCarrot\nSegmenter\nMLLM\nPlannerStep 2:  Test-Time Planning\nFrames 1 -12\nSketchVerify\nFrames 12 -25\nSketchVerify\nFrames 1 -12: \nThe robotic arm \napproaches the \ncarrot.\nMLLM\nPlanner\nMLLM\nVerifier\nMLLM\nVerifier\nMLLM\nVerifierScore: 0.3\nScore: 0.1\nScore: 0.2Above\nThreshold\n?\nNoObject Layouts \n& TrajectoriesVideo \nSketches\nYes\nStep 3:  Trajectory -Conditioned Video Generation\nFrames 12 -25 Frames 1 -12\u2026\nConditioned\nTI2V\nFinal Video Based on Verified  Object Layout & TrajectoryFigure 2.Overview of our framework.Given a prompt and initial frame, we (1) decompose instructions and segment movable objects,\n(2) sample and verify candidate trajectories using lightweight video sketches scored by a multimodal verifier and (3) synthesize the final\nvideo using a trajectory-conditioned diffusion model. We provide more detail about the MLLM verifier in Fig. 3.\nTrajectory Sampling.For each sub-instructionP i, the goal\nis to generate a set of candidate trajectories that guide the\nmoving objectOaccording to the intended action. The sam-\npling process is conditioned on the current visual context\nCi, which provides spatial grounding for planning. We use\nMLLM PlannerFto generateKcandidate trajectories:\nn\n\u03a0(1)\ni, . . . ,\u03a0(K)\nio\n=F(P i,O,C i),\nwhere each\u03a0(k)\niis a list of bounding boxes overT iframes:\n\u03a0(k)\ni={b(k,t)\ni= (x(t)\nmin, y(t)\nmin, x(t)\nmax, y(t)\nmax)}Ti\nt=1.\nHere,b(k,t)\nidenotes the bounding box of the object at frame\nt, with(x(t)\nmin, y(t)\nmin)and(x(t)\nmax, y(t)\nmax)being the upper-left\nand lower-right coordinates, respectively. For example, if\nPiis \u201cmove the apple toward the basket,\u201d then\u03a0(k)\nican\ndefine a smooth horizontal motion of the apple acrossT i\nframes toward the location of the basket. The visual context\nCiis initialized as the reference imageI 0wheni= 1, and\nupdated at each subsequent step to the last frame of the se-\nlected sketchS\u2217\ni\u22121, preserving temporal continuity through-\nout the planning process.\nVideo Sketch Rendering.To enable assessing plans with-\nout incurring the cost of full video generation, we render alightweight video sketch that visualizes only the intended\nobject motions. Specifically, each trajectory\u03a0(k)\niis con-\nverted into a sketchS(k)\niby cropping the segmented object\nregion from the initial frameI 0using its predicted mask and\ncompositing it frame by frame onto the static backgroundB\naccording to the bounding box coordinates in\u03a0(k)\ni. These\nsketches provide a faithful, layout-preserving approxima-\ntion of the planned motion, enabling efficient test-time ver-\nification focused on spatial\u2013temporal coherence rather than\nappearance-level generation (see Sec. 5.2).\nVerifier-Guided Scoring.As is shown in Fig. 3, we per-\nform a two-stage evaluation strategy combining semantic\nalignment and physics-aware verification to assess the qual-\nity of each trajectory candidate. First, we compute a se-\nmantic score using an MLLM. Given the sub-instructionP i\nand corresponding sketchS(k)\ni, the MLLMV semreturns a\nscalar compatibility score:ssem\nk=V sem(S(k)\ni,Pi),which\nreflects how well the proposed motion aligns with the in-\ntended behavior. In parallel, we evaluate the physical plau-\nsibility of each sketch using structured prompts and few-\nshot in-context learning to probe four physical laws:\n\u2022Newtonian Consistency:Acceleration and deceleration\nshould reflect plausible physical dynamics.\n\u2022Penetration Violation:Moving objects should not pass\n4\nSub-instr. :\nthe robotic arm \napproaches the \ncarrot. \nMLLM Verifier \nSemantic Physics Newton  \u274c\nPenetration \u2705\n Deformation  \u2705\n Gravity  \u274c\nSemantic \nScore=0.3 Physics \nScore=0.5 Frame \n0 Frame \n6Frame \n12 \nThe carrot floats up by itself. \nRobotic arm  \u2705\nCarrot  \u2705\nContact  \u274c\nFigure 3. Illustration of MLLM verifier. Given a video sketch and\nsub-instruction, the MLLM outputs semantic and physics scores\nused to rank candidate trajectories.\nthrough static scene elements.\n\u2022Gravitational Coherence:Vertical motion should follow\nrealistic arcs consistent with gravity.\n\u2022Deformation Consistency:Object size and shape should\nremain stable throughout the sequence.\nEach response from the MLLM is parsed into a scalar\nscores(l)\nk, wherel\u2208 L={Newton, Penetration, Gravity,\nDeformation}. We map descriptive outputs (e.g., \u201cvery con-\nsistent\u201d) to numerical values using predefined rules (e.g.,\n\u201cvery consistent\u201d\u21921.0, \u201csomewhat inconsistent\u201d\u21920.7).\nDetailed prompt templates and mapping rules are provided\nin Appendix A.4. The final trajectory is selected by maxi-\nmizing a weighted combination of semantic alignment and\nphysical plausibility scores:\n\u03a0\u2217\ni= arg max\nk \n\u03bbsemssem\nk+X\nl\u2208L\u03bbls(l)\nk!\n,\nwhereLdenotes the set of physical law dimensions and the\n\u03bbcoefficients balance their relative importance.\nIterative Trajectory Selection.To ensure trajectory qual-\nity, we adopt an iterative refinement strategy. If all candi-\ndate scores fall below a quality threshold\u03c4, the entire set is\ndiscarded. A new batch of trajectories is then sampled by\nprompting the planner with an augmented instruction that\nincorporates feedback on previous failure cases until a valid\nplan is found or a retry limit is reached. Finally, we set the\nlast frame ofS\u2217\nias the new context frameC i+1for a new\nround of SketchVerify on the next sub-instruction.\n3.4. Trajectory-Conditioned Video Generation\nOnce the full instruction plan has been verified and selected,\nwe obtain a verified motion plan\u03a0\u2217={\u03a0\u2217\n1, . . . ,\u03a0\u2217\nM},\nwhere each sub-plan\u03a0\u2217\niis a sequence of bounding boxes\n{bi,1, . . . , b i,Ti}overT iframes. For each boxb i,t\u2208R4,\nwe extract a representative pointp i,t\u2208R2(e.g., the cen-\nter), resulting in a sparse trajectoryP i={p i,1, . . . , p i,Ti}.The full object path is formed by concatenating all sub-\ntrajectories:P\u2217=P 1\u2225. . .\u2225P M. We temporally inter-\npolate this sequence to produce a dense trajectory \u00afP=\n{q1, . . . , q T}overTframes, where eachq t\u2208R2specifies\nthe object position at timet.\nWe adopt a pre-trained trajectory-conditioned image-to-\nvideo diffusion model for video generation. It encodes the\ninitial imageI 0into latent features and modulates the de-\nnoising process by injecting object trajectory latents. This\ninjection follows the planned trajectory\u03a0\u2217and guides the\ngeneration process to produce coherent motion consistent\nwith both appearance and spatial control. The result is a\nT-frame videoVthat exhibits faithful motion behavior,\naligned with the high-level prompt and semantically and\nphysically verified trajectory (Step 3 in Fig. 2).\n4. Experimental Results\n4.1. Benchmarks and Evaluation Metrics\nBenchmarks.We evaluate our method on two recent large-\nscale benchmarks for visual world models:\n\u2022 WorldModelBench [22], which evaluates instruction fol-\nlowing, physical plausibility, and commonsense across 7\ndomains using a benchmark-provided MLLM scorer.\n\u2022 PhyWorldBench [8], which tests fine-grained physical re-\nalism over 350 prompts. Since it is a text-to-video bench-\nmark, we generate the first frame using FLUX [19] and\nthen perform I2V .\nEvaluation Metrics.Across these two benchmarks, we\nevaluate instruction following, physical law coherency\n(Newtonian motion, deformation, fluid, penetration, grav-\nity), commonsense consistency (frame and temporal), and\nefficiency (planning time and generation cost), using the\nbenchmark-provided MLLM scorer for all assessments; full\nmetric definitions are included in the Appendix A.2.2.\n4.2. Implementation Details\nWe use the multimodal version of GPT-4.1 [32] as the de-\nfault planner to generate five candidate trajectories of all\nmoving objects, where each trajectory records the coordi-\nnates of the top-left and bottom-right corners of the bound-\ning box at every frame. We construct lightweight video\nsketches by pasting foreground objects (segmented with\nGroundedSAM [33]) onto static backgrounds to render ob-\nject motion direction. These sketches are scored in the\nrange[0,1]by Gemini 2.5 [5]. This vision-language veri-\nfier uses prompt-based queries to assess semantic alignment\nand physical laws, including Newtonian consistency, object\npenetration, gravitational coherence, and deformation con-\nsistency. We independently assess each law using in-context\nprompts with positive and negative trajectories and aggre-\ngate the resulting plan scores for the final ranking. We use\nATI-14B model [38] to generate 81 frame 480p videos. We\n5\nTable 1. Comparison on WORLDMODELBENCH[22]. We report scores for instruction following, physical law coherence (grouped under\n\u201cPhysics\u201d), and commonsense consistency (\u201cFrame\u201d and \u201cTemporal\u201d), along with an overall sum score aggregating all metrics. Best results\nin each column are highlighted in bold.\nModelInstruction Physics CommonsenseSum\u2191 Plan Time (min)\u2193\nFollow\u2191 Newton\u2191Deform\u2191Fluid\u2191Penetr.\u2191Gravity\u2191 Frame\u2191Temporal\u2191\nOpen-Source Video Models\nHunyuan-Video [18]1.18 1.00 0.801.00 0.92 1.00 0.64 0.70 7.24 \u2013\nCogVideoX [48]1.46 0.99 0.70 0.99 0.77 0.96 0.860.94 7.67 \u2013\nWan-2.1 [36]1.88 1.00 0.76 0.99 0.81 0.99 0.96 0.82 8.21 \u2013\nCosmos [1]2.06 1.00 0.84 0.990.92 1.00 0.92 0.90 8.63 \u2013\nOpen-Sora [13]1.64 0.98 0.821.000.911.00 0.80 0.84 7.99 \u2013\nSTEP-Video [14]1.04 1.00 0.751.000.891.00 0.50 0.71 6.89 \u2013\nSingle-Shot Planning\nVideoMSG [23]1.46 0.99 0.79 0.99 0.83 0.94 0.96 0.82 7.78 1.33\nIterative Planning\nPhyT2V [45]1.97 1.00 0.82 0.96 0.82 1.00 0.81 0.81 8.19 61.86\nSketchVerify (Ours)2.08 1.00 0.89 1.00 0.92 1.00 0.960.86 8.71 4.71\nshow all the implementation detail in Appendix A.1.1.\n5. Quantitative Results\nEvaluation on WorldModelBench.Table 1 compares our\nmethod with strong open-source video generation mod-\nels, including CogVideoX [48], Cosmos [1], Hunyuan-\nVideo [18], Open-Sora [13], Wan-2.1 [36], and STEP-\nVideo [14], as well as planning-based baselines such as\nVideoMSG [23] (single-shot) and PhyT2V [45] (multi-step\nrefinement). Our approach achieves the strongest perfor-\nmance across all major evaluation dimensions, including in-\nstruction following (2.08), physical law coherence (gravity,\npenetration, deformation), and overall commonsense con-\nsistency. Compared to the base model Wan-2.1 [36], our\nmethod improves instruction-following accuracy by 10.6%\nand increases overall physics coherence by 6%, including\na 17% reduction in deformation-related violations. While\nmulti-step pipelines such as PhyT2V provide gains over\none-shot planners, they rely on repeated, computationally\nexpensive synthesis cycles (typically requiring about 12.5\nminutes for planning and 70 minutes for full video gener-\nation). In contrast, our verifier-guided sampling performs\nhigh-quality trajectory selection within a single planning\nstage, reducing generation time to just 4.7 minutes, corre-\nsponding to a 93% speed-up. We present a detailed per-\ncomponent runtime analysis in Appendix A.1.3.\nEvaluation on PhyWorldBench.As shown in Table 2,\nour verifier-guided framework achieves the highest overall\nscore (19.84) and the strongest performance on the physical\nstandard category (23.52), demonstrating superior physical\nconsistency and object stability. While Cosmos [1] achieves\na slightly higher object\u2013event score (48.29 vs. 43.11), its\noverall and physical-standard scores are substantially lower,\nsuggesting weaker temporal physical consistency. Relative\nto the base model Wan-2.1 [36], our method boosts ob-\nject\u2013event accuracy by 22% and physical accuracy by 18%.Table 2. Evaluation on PHYWORLDBENCH. We report category-\nwise pass rates for object and event (Obj+Evt), physical standard\n(Phys. Std), and the combined overall score (All). Best results are\nshown inbold, and second-best results are underlined .\nModel Obj+Evt\u2191Phys. Std\u2191All\u2191\nCogVideoX [48] 41.62 21.68 17.34\nWan-2.1 [36] 35.34 19.83 15.52\nOpenSora [13] 36.86 17.43 14.00\nCosmos [1]48.2915.71 14.00\nHunyuan-Video [18] 24.86 14.16 10.12\nSTEP-Video [14] 29.51 16.33 12.89\nSketchVerify (Ours)43.11 23.52 19.84\nThese results highlight that our test-time verification not\nonly preserves object-level realism but also improves causal\nand physical coherence across diverse scenarios.\n5.1. Qualitative Results\nFig. 4 presents qualitative comparisons across four domains\nfrom WorldModelBench: Human, Natural, Video Game,\nand Robotics. Existing baselines such as CogVideoX [48],\nCosmos [1], and Wan-2.1 [36] frequently exhibit visible ar-\ntifacts. For example, in the Human domain, baseline models\noften produce body parts that stretch unnaturally or remain\nsuspended mid-air during jumping motions, whereas our\nverifier-guided approach generates smooth forward jumps\nwith realistic limb coordination and consistent gravity re-\nsponse. In the Natural domain, competing models fail to\nfollow the instruction, where the snow never seem to move,\nwhile our method maintains a continuous downhill flow that\nadheres to slope geometry. In the Video Game scenes,\nbaselines tend to misalign collisions (e.g., football play-\ners phasing through each other or even merging into one),\nwhile our results preserve accurate object contact. Finally,\nin the Robotics domain, previous models often cause grip-\nper\u2013object misalignment or floating artifacts during manip-\n6\nDomain: Human \nThe man in red shorts jumps forward on the beach. CogVideox Cosmos Wan Ours CogVideox Cosmos Wan Ours Domain: Natural \nSnow melts and slides down the mountain cliff in the scenic landscape. \nDomain: Robotics \nThe robotic arm picks up the purple tool from the toolbox. Domain: Video Game \nTwo football players tackle an opponent near the end zone in a football simulation game. \nFrame 0 Frame 26 Frame 80 Frame 53 Frame 0 Frame 26 Frame 80 Frame 53 Figure 4. Qualitative comparison on four representative domains from WORLDMODELBENCH: Human, Natural, Video Game, and\nRobotics. Each group shows sampled frames from competing models given the same text prompt. Frames are uniformly sampled from\neach generated 81-frame video.\nulation, whereas our planner enables stable grasping and\nlifting trajectories. Together, these examples demonstrate\nthat verifier-guided planning effectively reduces physical\nimplausibilities and enhances temporal coherence across di-\nverse environments. We show more qualitative results in\nAppendix A.3.\n5.2. Ablation Study\nWe conduct ablation studies on WorldModelBench to exam-\nine how verifier type and test-time sampling affect motion\nplanning quality.Verifier Modality.We evaluate the impact of verifier\nmodality in Table 3. Relying solely on language-based\nplanning, such as VideoMSG [23], leads to suboptimal mo-\ntion control, as the generated trajectories often lack spatial\ncoherence and violate basic physical constraints. Adding a\nlanguage-only verifier slightly improves overall scores by\nproviding textual feedback, yet it remains limited in captur-\ning fine-grained motion cues, as language models struggles\nto directly perceive object geometry, depth, or trajectory\nsmoothness without visual context. In contrast, our mul-\ntimodal verifier can directly visually assess motion consis-\n7\nSingle Shot \nPlanning Output Video Verified \nPlanning Output Video \nPrompt: The robotic arm moves the rubber chicken into the metal bowl. \n\u2705\nInstruction following  \u274c\nPhysical Law: Gravity Violation \n\u2705\nInstruction following  \u2705\nPhysical Law Figure 5. Ablation study on verifier modality. Introducing visual input to the verifier significantly improves both instruction following and\nphysical plausibility, highlighting the importance of multimodal grounding for reliable trajectory evaluation.\nTable 3. Ablation study on WORLDMODELBENCHshowing the ef-\nfect of verifier guidance and test-time sampling. \u201cLang-Only\u201d uses a\nlanguage-only verifier over trajectory descriptions, while \u201cOurs\u201d adds\nsampling-based trajectory selection.\nVariant Instr. Follow\u2191Phys. Score\u2191\nSingle-Shot (no verifier)1.46 4.55\nLang-Only Verifier1.49 4.76\nOurs (MLLM verifier)2.08 4.81Table 4. Ablation of different verifier scale. Using stronger and\nlarger multimodal verifiers improves semantic and physical rea-\nsoning during motion plan selection. All experiments share the\nsame underlying inference pipeline.\nVerifier Instr. Follow\u2191Phys. Score\u2191\nQwen2.5-VL-3B1.62 4.68\nQwen2.5-VL-32B1.83 4.72\nGemini (Default)2.08 4.81\ntency and interactions, thereby offering stronger and more\nphysically grounded guidance during test-time planning. As\nillustrated in Fig. 5, the multimodal verifier effectively iden-\ntifies and rejects physically implausible trajectories (e.g.,\ngravity violations), leading to more realistic and physically\nconsistent motion generation.\nEffect of Different Verifier Choices.We compare different\nMLLMs as verifiers in the plan ranking loop in Table 4. Us-\ning a smaller model such as Qwen-VL-3B provides limited\nimprovements due to weaker spatial reasoning. In contrast,\na stronger model like Gemini-2.5 yields more accurate mo-\ntion selection and higher overall quality, highlighting the\nclear benefit of scaling the verifier\u2019s reasoning capacity.\nEffect of Different Planner Choices.We compare dif-\nferent MLLMs as verifiers in the plan-ranking loop (Ta-\nble 5). Smaller models such as Qwen-VL-3B yield limited\ngains due to weaker spatial reasoning and poorer ground-\ning of object dynamics. In contrast, stronger verifiers likeTable 5. Ablation of different planner choices. Using stronger\nMLLM planner improves semantic and physical reasoning during\nmotion plan selection. All methods use the same pipeline.\nPlanner Instr. Follow\u2191Phys. Score\u2191\nQwen2.5-VL-3B1.23 4.50\nQwen2.5-VL-72B1.59 4.57\nGPT-4.1(Default)2.08 4.81\nGPT-4.1 provide more reliable assessments of motion qual-\nity, enabling more accurate trajectory selection. Moreover,\nweaker open-source VLMs exhibit insufficient instruction-\nfollowing ability, which further limits their effectiveness on\nmulti-step, numerically precise planning tasks.\nEffect of Sampling BudgetK.Fig. 6 examines how the\nnumber of sampled candidate trajectoriesKinfluences the\nfinal motion quality. When there is no iterative genera-\n8\nTable 6. Ablation study comparing different verification strategies\non WORLDMODELBENCH. The reportedplan timemeasures only\nthe duration of motion planning and verification before the final\ndiffusion-based video generation.\nVerification Strategy Instruction\u2191Physics\u2191Plan time (min)\u2193\nUnverified1.52 4.560\nGeneration-based1.92 4.62 38.99\nSketch-based1.90 4.664.08\n0 1 3 5\nTrajectory samples K1.41.61.82.0Instr. score\n1.461.901.982.08\n4.504.554.604.654.704.754.804.85\nPhysics score\n4.554.664.734.81\nInstr. score\nPhysics score\nFigure 6. Ablation on the number of sampled trajectoriesKdur-\ning planning. LargerKvalues enable stronger verifier-guided se-\nlection.K= 0denotes the setting without a verifier, which is\nidentical to the VideoMSG baseline.\ntion (equivalent to VideoMSG [23]), the planner commits\nto a single trajectory without verification, resulting in lim-\nited instruction adherence (1.46) and lower physical con-\nsistency (4.55). Introducing even a small sampling budget\n(K= 1) with refinement from yields noticeable gains, as\nthe verifier can reject implausible motions and select im-\nproved alternatives. Performance continues to increase with\nlargerK, reflecting the benefit of exploring a broader tra-\njectory space. Our full configuration (K= 5) achieves\nthe strongest results across both instruction following and\nphysics coherence, demonstrating that moderate test-time\nsampling is sufficient for robust trajectory selection without\ncompromising efficiency.\nVerification Strategy.To assess the efficiency of verify-\ning on lightweight sketches versus fully generated videos,\nwe compare our sketch-based verification with two alter-\nnatives: (1) Generation-based verification, which evalu-\nates semantic and physical quality after full diffusion-based\nvideo synthesis, and (2) Verifier\u2013regeneration, which re-\ngenerates videos based on verifier feedback from sketches.\nAll methods use the same verifier model and multimodal\nplanner. In both alternatives, the planner follows a gener-\nate\u2013render\u2013verify\u2013refine loop to update the trajectory. Be-\ncause full video generation is extremely expensive (ren-\ndering all\u223c100 candidates require over 30 GPU-hours),\nwe adopt a practical two-round verify\u2013regenerate scheme,where the planner generates one trajectory per round, re-\nceives verifier feedback, then refines it into one updated tra-\njectory in the next round. As shown in Table 6, verification\nimproves motion quality across all settings, and our sketch-\nbased approach matches or surpasses full video verification\nwhile achieving a nearly 10\u00d7speedup. Unlike full videos\nwhich are expensive to render and often contain diffusion\nartifacts that mislead the verifier, sketches isolate motion\nfrom appearance, enabling cleaner and more reliable spa-\ntial\u2013temporal evaluation.\n6. Conclusion\nWe introduced a verifier-guided test-time planning frame-\nwork for physically grounded video generation that decou-\nples motion planning from synthesis. By integrating a mul-\ntimodal verifier into the trajectory sampling loop and em-\nploying sketch-based proxy rendering, our approach en-\nables efficient and reliable evaluation of candidate motions\nprior to video synthesis. This design allows the model to\ngenerate semantically coherent, physically plausible, and\ntemporally smooth videos while reducing planning cost by\nnearly an order of magnitude compared to iterative refine-\nment methods. Comprehensive experiments on WORLD-\nMODELBENCHand PHYWORLDBENCHdemonstrate sub-\nstantial improvements in instruction following, physical law\nadherence, and motion realism over SoTA baselines.\n7. Acknowledgements\nWe thank Justin Chih-Yao Chen, Jaemin Cho, Elias Stengel-\nEskin, Zaid Khan, and Shoubin Yu for their helpful feed-\nback. This work was supported by NSF-AI Engage Insti-\ntute DRL-2112635, ARO Award W911NF2110220, ONR\nGrant N00014-23-1-2356, DARPA ECOLE Program No.\nHR00112390060, and a Capital One Research Award. The\nviews contained in this article are those of the authors and\nnot of the funding agency.\nReferences\n[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji,\nErik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin\nChen, Yin Cui, Yifan Ding, et al. Cosmos world foun-\ndation model platform for physical ai.arXiv preprint\narXiv:2501.03575, 2025. 6, 12\n[2] Pierfrancesco Ardino, Marco De Nadai, Bruno Lepri, Elisa\nRicci, and St \u00b4ephane Lathuili `ere. Click to move: Control-\nling video generation with sparse motion. InProceedings of\nthe IEEE/CVF International Conference on Computer Vision\n(ICCV), page 14749\u201314758, 2021. 1\n[3] Hritik Bansal, Clark Peng, Yonatan Bitton, Roman Golden-\nberg, Aditya Grover, and Kai-Wei Chang. Videophy-2: A\nchallenging action-centric physical commonsense evaluation\nin video generation, 2025. 3\n9\n[4] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,\nXiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,\nQifeng Chen, Xintao Wang, Chao Weng, and Ying Shan.\nVideocrafter: Open diffusion models for high-quality video\ngeneration.arXiv preprint arXiv:2310.19512, 2023. 1\n[5] Google AI / Google DeepMind. Gemini 2.5 (stable release).\nhttps://ai.google.dev/models/gemini, 2025.\nMultimodal large language model, model code: gemini-2.5-\npro / gemini-2.5-flash. 5\n[6] Nate Gillman, Charles Herrmann, Michael Freeman, Daksh\nAggarwal, Evan Luo, Deqing Sun, and Chen Sun. Force\nprompting: Video generation models can learn and gen-\neralize physics-based control signals.arXiv preprint\narXiv:2505.19386, 2025. 3\n[7] Jiaxi Gu, Shicong Wang, Haoyu Zhao, Tianyi Lu, Xing\nZhang, Zuxuan Wu, Songcen Xu, Wei Zhang, Yu-Gang\nJiang, and Hang Xu. Reuse and diffuse: Iterative\ndenoising for text-to-video generation.arXiv preprint\narXiv:2309.03549, 2023. 1\n[8] Jing Gu, Xian Liu, Yu Zeng, Ashwin Nagarajan, Fangrui\nZhu, Daniel Hong, Yue Fan, Qianqi Yan, Kaiwen Zhou,\nMing-Yu Liu, et al. \u201d phyworldbench\u201d: A comprehensive\nevaluation of physical realism in text-to-video models.arXiv\npreprint arXiv:2507.13428, 2025. 1, 5, 13\n[9] Lin Han, Abhay Zala, Jaemin Cho, and Mohit Bansal.\nVideodirectorgpt: Consistent multi-scene video generation\nvia llm-guided planning.arXiv preprint arXiv:2309.15091,\n2023. 1\n[10] Yutong Hao, Chen Chen, Ajmal Saeed Mian, Chang Xu,\nand Daochang Liu. Enhancing physical plausibility in video\ngeneration by reasoning the implausibility.arXiv preprint\narXiv:2509.24702, 2025. 3\n[11] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun,\nYuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao\nWeng, Ying Shan, et al. Animate-a-story: Storytelling\nwith retrieval-augmented video generation.arXiv preprint\narXiv:2307.06940, 2023. 2\n[12] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben\nPoole, Mohammad Norouzi, David J. Fleet, and Tim Sali-\nmans. Imagen video: High definition video generation with\ndiffusion models.arXiv preprint arXiv:2210.02303, 2022. 1\n[13] hpcaitech. Open-sora: Democratizing efficient video pro-\nduction for all, 2024. 6, 12\n[14] Haoyang Huang, Guoqing Ma, Nan Duan, Xing Chen,\nChangyi Wan, Ranchen Ming, Tianyu Wang, Bo Wang,\nZhiying Lu, Aojie Li, et al. Step-video-ti2v technical re-\nport: A state-of-the-art text-driven image-to-video genera-\ntion model.arXiv preprint arXiv:2503.11251, 2025. 6, 12\n[15] Ziqi Huang, Ning Yu, Gordon Chen, Haonan Qiu, Paul\nDebevec, and Ziwei Liu. Vchain: Chain-of-visual-\nthought for reasoning in video generation.arXiv preprint\narXiv:2510.05094, 2025. 1, 2\n[16] Sangwon Jang, Taekyung Ki, Jaehyeong Jo, Jaehong Yoon,\nSoo Ye Kim, Zhe Lin, and Sung Ju Hwang. Frame guidance:\nTraining-free guidance for frame-level control in video dif-\nfusion models.arXiv preprint arXiv:2506.07177, 2025. 1[17] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. InProceedings of the IEEE/CVF international confer-\nence on computer vision, pages 4015\u20134026, 2023. 3, 12\n[18] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai,\nJin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang,\net al. Hunyuanvideo: A systematic framework for large video\ngenerative models.arXiv preprint arXiv:2412.03603, 2024.\n6, 12\n[19] Black Forest Labs. Flux.https://github.com/\nblack-forest-labs/flux, 2024. 3, 5, 12, 13\n[20] Daeun Lee, Jaehong Yoon, Jaemin Cho, and Mohit Bansal.\nVideorepair: Improving text-to-video generation via mis-\nalignment evaluation and localized refinement. InProceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2025. 1, 3\n[21] Chenyu Li, Oscar Michel, Xichen Pan, Sainan Liu, Mike\nRoberts, and Saining Xie. Pisa experiments: Exploring\nphysics post-training for video diffusion models by watch-\ning stuff drop.arXiv preprint arXiv:2503.09595, 2025. 3\n[22] Dacheng Li, Yunhao Fang, Yukang Chen, Shuo Yang, Shiyi\nCao, Justin Wong, Michael Luo, Xiaolong Wang, Hongxu\nYin, Joseph E Gonzalez, et al. Worldmodelbench: Judging\nvideo generation models as world models.arXiv preprint\narXiv:2502.20694, 2025. 1, 5, 6, 12\n[23] Jialu Li, Shoubin Yu, Han Lin, Jaemin Cho, Jaehong Yoon,\nand Mohit Bansal. Training-free guidance in text-to-video\ngeneration via multimodal planning and structured noise ini-\ntialization.arXiv preprint arXiv:2504.08641, 2025. 1, 2, 6,\n7, 9\n[24] Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, and\nBoyi Li. Llm-grounded video diffusion models.arXiv\npreprint arXiv:2309.17444, 2023. 2\n[25] Han Lin, Abhay Zala, Jaemin Cho, and Mohit Bansal.\nVideodirectorgpt: Consistent multi-scene video generation\nvia llm-guided planning. InFirst Conference on Language\nModeling, 2024. 2\n[26] Wang Lin, Liyu Jia, Wentao Hu, Kaihang Pan, Zhongqi\nYue, Wei Zhao, Jingyuan Chen, Fei Wu, and Hanwang\nZhang. Reasoning physical video generation with diffusion\ntimestep tokens via reinforcement learning.arXiv preprint\narXiv:2504.15932, 2025. 3\n[27] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection.arXiv preprint\narXiv:2303.05499, 2023. 3, 12\n[28] Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shen-\nlong Wang. Physgen: Rigid-body physics-grounded image-\nto-video generation. InEuropean Conference on Computer\nVision, pages 360\u2013378. Springer, 2024. 3\n[29] Do Xuan Long, Xingchen Wan, Hootan Nakhost, Chen-Yu\nLee, Tomas Pfister, and Sercan \u00a8O Ar\u0131k. Vista: A test-\ntime self-improving video generation agent.arXiv preprint\narXiv:2510.15831, 2025. 3\n10\n[30] Antonio Montanaro, Luca Savant Aira, Emanuele Aiello,\nDiego Valsesia, and Enrico Magli. Motioncraft: Physics-\nbased zero-shot video generation.Advances in Neural Infor-\nmation Processing Systems, 37:123155\u2013123181, 2024. 3\n[31] Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini,\nand Robert Geirhos. Do generative video models understand\nphysical principles?, 2025. 3\n[32] OpenAI. Gpt-4.1.https://openai.com/research/\ngpt-4, 2024. Large language model. 5\n[33] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kun-\nchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen,\nFeng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang,\nHongyang Li, Qing Jiang, and Lei Zhang. Grounded sam:\nAssembling open-world models for diverse visual tasks,\n2024. 3, 5, 12\n[34] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman.\nMake-a-video: Text-to-video generation without text-video\ndata. InInternational Conference on Learning Representa-\ntions (ICLR), 2023. 1\n[35] Xiyang Tan, Ying Jiang, Xuan Li, Zeshun Zong, Tianyi\nXie, Yin Yang, and Chenfanfu Jiang. Physmotion: Physics-\ngrounded dynamics from a single image.arXiv preprint\narXiv:2411.17189, 2024. 3\n[36] Wan Team. Wan: Open and advanced large-scale video gen-\nerative models, 2025. 6, 12\n[37] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao,\nChen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianx-\niao Yang, et al. Wan: Open and advanced large-scale video\ngenerative models.arXiv preprint arXiv:2503.20314, 2025.\n1\n[38] Angtian Wang, Haibin Huang, Jacob Zhiyuan Fang, Yid-\ning Yang, and Chongyang Ma. Ati: Any trajectory in-\nstruction for controllable video generation.arXiv preprint\narXiv:2505.22944, 2025. 5, 12\n[39] Chen Wang, Chuhao Chen, Yiming Huang, Zhiyang Dou,\nYuan Liu, Jiatao Gu, and Lingjie Liu. Physctrl: Generative\nphysics for controllable and physics-grounded video genera-\ntion.arXiv preprint arXiv:2509.20358, 2025. 3\n[40] Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang,\nKa Leong Cheng, Qifeng Chen, Yujun Shen, and Limin\nWang. Levitor: 3d trajectory oriented image-to-video syn-\nthesis.arXiv preprint arXiv:2412.15214, 2024. 1\n[41] Peiyao Wang, Weining Wang, and Qi Li. Physcorr: Dual-\nreward dpo for physics-constrained text-to-video genera-\ntion with automated preference selection.arXiv preprint\narXiv:2511.03997, 2025. 3\n[42] Zun Wang, Jaemin Cho, Jialu Li, Han Lin, Jaehong Yoon,\nYue Zhang, and Mohit Bansal. Epic: Efficient video camera\ncontrol learning with precise anchor-video guidance.arXiv\npreprint arXiv:2505.21876, 2025. 3\n[43] Zun Wang, Jialu Li, Han Lin, Jaehong Yoon, and Mohit\nBansal. Dreamrunner: Fine-grained storytelling video gen-\neration with retrieval-augmented motion adaptation. InThe\nAAAI Conference on Artificial Intelligence, 2026. 2\n[44] Runpu Wei, Zijin Yin, Shuo Zhang, Lanxiang Zhou, Xueyi\nWang, Chao Ban, Tianwei Cao, Hao Sun, Zhongjiang He,Kongming Liang, et al. Omnieraser: Remove objects and\ntheir effects in images with paired video-frame data.arXiv\npreprint arXiv:2501.07397, 2025. 3, 12\n[45] Qiyao Xue, Xiangyu Yin, Boyuan Yang, and Wei Gao.\nPhyt2v: Llm-guided iterative self-refinement for physics-\ngrounded text-to-video generation. InProceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2025. 1, 3, 6\n[46] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Ste-\nfano Ermon, and Bin Cui. Mastering text-to-image diffu-\nsion: Recaptioning, planning, and generating with multi-\nmodal llms. InForty-first International Conference on Ma-\nchine Learning, 2024. 3\n[47] Xindi Yang, Baolu Li, Yiming Zhang, Zhenfei Yin, Lei Bai,\nLiqian Ma, Zhiyong Wang, Jianfei Cai, Tien-Tsin Wong,\nHuchuan Lu, et al. Vlipp: Towards physically plausible\nvideo generation with vision and language informed phys-\nical prior.arXiv preprint arXiv:2503.23368, 2025. 3\n[48] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer.arXiv preprint\narXiv:2408.06072, 2024. 6, 12\n[49] Jaehong Yoon, Shoubin Yu, and Mohit Bansal. Raccoon:\nA versatile instructional video editing framework with auto-\ngenerated narratives. InConference on Empirical Methods\nin Natural Language Processing, 2025. 2\n[50] Shoubin Yu, Jacob Zhiyuan Fang, Jian Zheng, Gunnar Sig-\nurdsson, Vicente Ordonez, Robinson Piramuthu, and Mohit\nBansal. Zero-shot controllable image-to-video animation via\nmotion decomposition. InProceedings of the 32nd ACM\nInternational Conference on Multimedia, pages 3332\u20133341,\n2024.\n[51] Shoubin Yu, Difan Liu, Ziqiao Ma, Yicong Hong, Yang\nZhou, Hao Tan, Joyce Chai, and Mohit Bansal. Veggie:\nInstructional editing and reasoning of video concepts with\ngrounded generation.arXiv preprint arXiv:2503.14350,\n2025.\n[52] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi\nFeng, and Qibin Hou. Storydiffusion: Consistent self-\nattention for long-range image and video generation. In\nThe Thirty-eighth Annual Conference on Neural Information\nProcessing Systems, 2024.\n[53] Shaobin Zhuang, Kunchang Li, Xinyuan Chen, Yaohui\nWang, Ziwei Liu, Yu Qiao, and Yali Wang. Vlogger: Make\nyour dream a vlog. InProceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n8806\u20138817, 2024. 2\n11\nA. Appendix\nA.1. Detailed Implementations\nA.1.1. SketchVerify Pipeline Specification\nIn this section, we detail the full SketchVerify implemen-\ntation pipeline, including high-level planning, object detec-\ntion and masking, background extraction, test-time trajec-\ntory search, sketch rendering, multimodal verification, and\nfinal video generation.\nHigh-Level Planning.Given the input text prompt, we\nfirst perform a high-level decomposition of the described\naction into a sequence of structured sub-instructions. This\nstep is carried out by GPT-4.1 (multimodal version) us-\ning a constrained prompt that requires the model to output:\n(a) a list of action segments, (b) their temporal ordering,\nand (c) the moving objects involved in each segment (de-\ntailed prompt in Fig. 9). The planner is asked to produce\nMsub-instructions (M\u2208[1,4]), depending on the com-\nplexity of the prompt. Each sub-instruction corresponds to\nan independent phase of motion planning and receives its\nown temporal budgetT i, with all phase lengths summing to\nthe fixedPM\ni=1Ti= 41total frames used throughout the\npipeline. All high-level plans are parsed through a strict\nJSON schema that enforces the required fields (action, du-\nration, object ids). Malformed or under-specified outputs\nare automatically rejected, and the planner is re-sampled.\nThis guarantees that downstream modules always operate\non well-structured, machine-readable action plans.\nObject Detection and Masks.We use Ground-\ning DINO [27, 33] for text-conditioned object detection\n(based on the detected objects in high-level planning) with a\nconfidence threshold of0.3, followed by SAM-HQ [17] for\nsegmentation. For each detected moving object, we retain\nthe highest-scoring instance mask and compute the corre-\nsponding bounding box. Boxes are normalized to[0,1]2\nfollowing the image coordinate system.\nBackground Removal.To obtain a clean static back-\nground, we use FLUX.1-dev [19] with the Omnieraser [44]\nLoRA. All moving-object masks are combined into a single\ninpainting mask. The background is generated at the same\nresolution as the input image. We generate the background\nimage with 28 diffusion steps andcfg= 3.5.\nTest-Time Search.We adopt GPT-4.1 as the default mul-\ntimodal planner. Each planning call producesK= 5can-\ndidate trajectories, each of lengthT i, where each trajectory\ncontains per-frame bounding box coordinates for all mov-\ning objects. All planner outputs are validated through a\nstructure-enforcing JSON parser, and malformed samples\nare automatically re-sampled. We set the temperature to\n1.0. Diversity filtering is enforced by requiring an\u2113 2dis-\ntance of at least0.05(in normalized coordinates) between\ntrajectories.\nSketch Rendering.For each candidate trajectory, we gen-erate a lightweight video sketch ofT iframes using object\nsprites cropped from the first frameI 0and composited onto\nthe static background. All sketches are rendered at the input\nimage resolution and saved as MP4 at 4 fps for verification.\nMultimodal Verification.We use Gemini 2.5-Flash as the\ndefault verifier. Two scores are produced per candidate:\n\u2022 a semantic alignment score from the first/last-frame com-\nparison;\n\u2022 a physics plausibility score from the full sketch video.\nThe planner uses a weighted combination with default\nweights(\u03bb sem, \u03bbphys) = (0.5,0.5). For all four phys-\nical laws, we also set\u03bb l= 0.25for alll\u2208 L=\n{Newton,Penetration,Gravity,Deformation}.\nFor scoring, we use the following criteria:\n\u2022 1.0: Perfect plan alignment and physical-law coherence.\n\u2022 0.7\u20130.9: Good alignment with minor deviations.\n\u2022 0.4\u20130.6: Partial alignment with some correct aspects.\n\u2022 0.0\u20130.3: Poor alignment; does not achieve the goal or di-\nrectly breaks physical laws.\nVideo Generation Model.We use the ATI-14B model [38]\nto generate 81-frame 480p videos. We generate with 40\nsteps andcfg= 5.0. The conditions include the input im-\nage, the input text prompt, and the trajectory plan obtained\nfrom the planner.\nA.1.2. Baseline and Hardware Specifications\nWe use Wan2.1-14B-480p-I2V [36], CogVideoX-5B [48],\nCosmos-Predict2-2B [1], HunyuanVideo-I2V [18],\nOpenSora-I2V [13], and Step-Video-I2V [14] as baseline\nopen-source TI2V models. For these models, we directly\nuse the official implementations and sample videos at 480p\nwith 81 frames using 50 diffusion steps. For PhyT2V and\nVideoMSG, we replace the backbone video generation\nmodel with Wan2.1 for fair comparison. All experiments\nare conducted on NVIDIA A100 80G and NVIDIA RTX\nA6000 GPUs.\nA.1.3. Per-step Runtime\nOn average, high-level planning takes 14.16 s. Object de-\ntection, segmentation, and background inpainting require\n108 s. For each sub-instruction, test-time planning takes\n72.5 s on average, consisting of 20.3 s for trajectory sam-\npling and 52.2 s for multimodal verification. All timings\nare measured on a single NVIDIA A100 GPU using the\nstandard-speed APIs of GPT-4.1 and Gemini-2.5.\nA.2. Benchmark Details and Metric Definitions\nA.2.1. Benchmark Details\nWorldModelBench[22] evaluates video generation mod-\nels as world models, focusing on instruction following,\nphysical plausibility, and commonsense temporal behav-\nior. It contains 7 domains and 56 subdomains across 350\nimage/text-conditioned tasks. It supports both I2V and T2V\nsettings (we use I2V).\n12\nPhyWorldBench[8] focuses on fine-grained physical re-\nalism, testing whether videos obey Newtonian laws, grav-\nitational motion, and object\u2013interaction constraints. This\nbenchmark includes 350 text prompts describing physically\ngrounded events. It is a T2V benchmark; therefore, we gen-\nerate a first frame using FLUX [19] and then perform I2V\ngeneration.\nA.2.2. Metric Details\nWorldModelBenchprovides a vision\u2013language model\n(MLLM) scorer trained on 67K human annotations. Each\ngenerated video outputs scalar scores in three categories:\n\u2022 Instruction Following: Measures how well the generated\nmotion follows the input instruction. Scores range from1\nto3:3 =correct motion,2 =partially correct,1 =incor-\nrect. The score is produced by the MLLM via a trained\ntextual comparison head.\n\u2022 Physics Coherence: Evaluates adherence to natural physi-\ncal priors across six dimensions, each in[0,1]: Newtonian\nmotion, deformation consistency, fluid dynamics, object\npenetration, gravity coherence, and frame-level physics\nconsistency.\n\u2022 Commonsense Consistency: Includes per-frame visual\nrealism and motion smoothness/continuity, both in[0,1].\nAll metrics are computed by the MLLM via prompt-\ndriven scoring heads.\nPhyWorldBenchuses its own MLLM-based evaluator and\nreports three pass-rate metrics. For each video, eight frames\nare sampled uniformly and passed to a proprietary SoTA\nMLLM. Questions about the following criteria are asked:\n\u2022 Obj+Evt: whether the described objects appear and the\nevent occurs.\n\u2022 Phys. Std: whether motion follows expected physical\nlaws (gravity, collision response, continuous motion, no\npenetration).\n\u2022 All: counted as correct only if both Obj+Evt and\nPhys. Std pass.\nEach is computed as a binary decision per prompt and av-\neraged into a percentage. We use GPT-5 as the proprietary\nMLLM evaluator.\nA.3. Extra Qualitative Results\nWe provide additional qualitative examples in Fig. 7\nand Fig. 8. These examples highlight the consistency of\nSketchVerify across diverse scenes and motion types. On\nWorldModelBench tasks (Fig. 7), our trajectories produce\nsmoother and more semantically aligned motions than base-\nline models. On PhyWorldBench (Fig. 8), our method more\nreliably maintains physical plausibility, avoiding common\nfailure modes such as objects floating against gravity or\nmoving on their own in violation of Newton\u2019s first law.A.4. Prompt Template for SketchVerify Pipeline\nWe provide the full set of prompt templates used in our\nsystem, covering high-level planning in Fig. 9, object pro-\nposal in Fig. 10, trajectory generation in Fig. 11, semantic\nalignment verification in Fig. 12, and physical plausibility\nchecking in Fig. 13. These prompts define the behavior of\neach module and ensure consistent outputs across tasks and\nbenchmarks.\nA.5. Limitations\nWhile SketchVerify substantially improves motion plan-\nning quality, several limitations remain. Our verification\nmodule primarily evaluates coarse object motion and high-\nlevel physical plausibility; however, capturing fine-grained\nphysics, such as frictional forces, collision responses, or\nother continuous dynamics that typically require differen-\ntiable simulation, would require additional modeling be-\nyond our current verifier-based design. Moreover, because\nboth the planner and verifier are external MLLMs, they may\noccasionally produce incorrect judgments, which can lead\nto suboptimal candidate selection. Finally, since motion is\nrepresented through 2D bounding boxes, the framework can\nstruggle with fine-grained 3D interactions such as detailed\naffordances or fluid-like behavior, and the realism of the fi-\nnal video remains bounded by the capability of the underly-\ning video generation model. We expect these limitations to\ndiminish as stronger video generators and verification mod-\nels continue to improve.\n13\nDomain: Robotics \nThe robotic arm put the toy carrot into the metal bowl CogVideox Cosmos Wan Ours Domain: Industry \nThe robotic arm positions the engine under the car chassis during assembly. \nFrame 0 Frame 26 Frame 80 Frame 53 Frame 0 Frame 26 Frame 80 Frame 53 \nCogVideox Cosmos Wan Ours Domain: Natural \nThe herd of antelopes stands alert in the savannah. Domain: Video Game \nThe character in the red outfit jumps onto a series of shipping containers. \nCogVideox Cosmos Wan Ours Domain: Industry \nThe worker uses a press brake to cutthe metal sheet. Domain: human \nThe scoop drops pink batter into the liner Figure 7. Qualitative comparison on WorldModelBench.\n14\nA soccer player kicks the ball in a high lob. CogVideox Cosmos Wan Ours A tetherball swings around the pole after being hit. \nFrame 0 Frame 26 Frame 80 Frame 53 Frame 0 Frame 26 Frame 80 Frame 53 \nCogVideox Cosmos Wan Ours A child uses a top on the floor. A helicopter takes off. \nCogVideox Cosmos Wan Ours A slack rope is used to pull a box. A basketball falls into a hoop. Figure 8. Qualitative comparison on PhyWoldBench.\n15\nGlobal Movement Planning Prompt (GPT-4.1 with Tool Calling)\nSystem message:\nYou are a video motion planning expert. You have EXACTLY{total frames}frames to\ncomplete the ENTIRE task.\nUser message:\nText prompt: \"{text prompt}\"\nAvailable frames: EXACTLY{total frames}.\nCurrent objects in frame 1:\n-{label 1}: currently at [x min, y min, x max, y max]\n-{label 2}: currently at [...] (etc.)\nReturn ONLY a call to submit movement plan with a complete plan that finishes by frame\n{total frames}.\nFunction schema (submit movement plan):\n{\n\"task_breakdown\": {\n\"complete_objective\",\n\"phase_1\", \"phase_2\", \"phase_3\",...\n\"success_criteria\"\n},\n\"frame_allocation\": {\n\"phase_1\",\n\"phase_2\",\n\"phase_3\",...\n},\n\"moving_objects\": [...],\n\"static_objects\": [...],\n\"detailed_timeline\": {\nframe_1, frame_3, frame_6, frame_8,\nframe_10, frame_12, frame_15, frame_18,...\nframe_{total_frames}\n},\n\"movement_plans\": {\n\"<object_name>\": {\n\"movement_type\",\n\"total_distance\",\n\"movement_phases\": {\n\"phase_1_frames\",\n\"phase_2_frames\",\n\"phase_3_frames\",...\n},\n}\n},\n\"completion_verification\"\n}\nFigure 9. Prompt used for high-level planning\n16\nObject Proposal Prompt (System + User)\nSystem Message:\nYou are an expert in video generation and object-centric scene analysis.\nGiven the first frame of a video and a text description, determine which\nobjects should be added, moved, or animated to fulfill the described action.\nYour responsibilities:\n1. Analyze the given frame and identify all existing objects.\n2. Based on the text prompt, determine which additional objects (if any)\nmust be introduced to achieve the described event.\n3. Identify which objects|either existing or newly added|must move or\nanimate to satisfy the prompt.\n4. Ensure that proposed object placement and motion are physically\nplausible and consistent with real-world interactions.\n5. Focus on major objects that materially affect the scene. If multiple\nparts form a single rigid object, treat them as one entity.\n6. For each object name, use minimal wording (1{3 words), concise and\nunambiguous.\nReturn the result strictly in the following JSON format:\n{\n\"scene_analysis\": \"Brief description of the current frame\",\n\"existing_objects\": [\"...\"],\n\"objects_to_add\": [\n{\n\"name\": \"object_name\",\n\"reasoning\": \"why this object is required\",\n\"movement_type\": \"static / linear / curved / complex\",\n\"priority\": \"high / medium / low\"\n}\n],\n\"moving_objects\": [\"...\"],\n\"static_objects\": [\"...\"]\n}\nUser Message:\nText prompt: \"<TEXT_PROMPT>\"\nUsing the first-frame image provided, analyze how the scene should be\nmodified or animated to satisfy the text description.\nPlease determine:\n- Which required objects are currently missing, based on the prompt.\n- Which objects must move or animate to create the described action.\n- Which objects remain static as part of the background.\n- Realistic object placement and timing relative to the prompt\u2019s intent.\nProduce the full JSON output exactly as specified in the system instructions.\nFigure 10. Prompt used for object proposal\n17\nSub-Instruction Trajectory Planning Prompt (GPT-4.1)\nSystem Message:\nYou are a video motion planning expert generating trajectories for frames\n{CHUNK_START} to {CHUNK_END}.\nCurrent phase: {PHASE_NAME}\nPhase description: {PHASE_DESCRIPTION}\nTotal frames in video: {TOTAL_FRAMES_NUM}\n**COORDINATE SYSTEM: **\n{COORDS_GUIDE}\n**DIRECTIONAL MAPPINGS: **\n- RIGHT: x1 += delta; x2 += delta\n- LEFT: x1 -= delta; x2 -= delta\n- UP: y1 -= delta; y2 -= delta\n- DOWN: y1 += delta; y2 += delta\nFocus ONLY on moving objects: {MOVING_OBJECTS}\nIgnore static objects in outputs: {STATIC_OBJECTS}\nUser Message:\nText prompt: \"{TEXT_PROMPT}\"\n{HISTORY_TEXT}\nGenerate a smooth trajectory from frame {CHUNK_START} to frame {CHUNK_END}\nfor this {PHASE_NAME}.\nIMPORTANT: Since multiple trajectories will be generated, explore DIFFERENT\nvalid motion paths. Consider variations in:\n- Path shape (straight, curved, arc)\n- Speed profile (constant, accelerating, decelerating)\n- Intermediate waypoints (different approaches to the goal)\nFor each object, you should maintain its size (box dimensions) and only\nchange its position unless you are specifically instructed to resize.\nFor EACH frame from {CHUNK_START} to {CHUNK_END}, output:\nFrame_N: [[\"object_name\", [x1, y1, x2, y2]], ...], caption: <description>\nRequirements:\n- Smooth motion (delta 0.03{0.08 per frame)\n- Consistent with phase objectives\n- Maintain object sizes\n- Only include moving objects: {MOVING_OBJECTS}\nFigure 11. Prompt used for sub-instruction trajectory planning\n18\nPlan-Alignment Verifier Prompt (GPT-4.1)\nSystem Message:\nYou are an expert at evaluating video motion trajectories.\nYour task is to verify if the motion from the first frame to the last frame\naligns with the expected phase goal.\nRate the alignment on a scale of 0.0 to 1.0 where:\n- 1.0 = Perfect alignment, the last frame clearly achieves the phase goal\n- 0.7-0.9 = Good alignment with minor deviations\n- 0.4-0.6 = Partial alignment, some aspects correct\n- 0.0-0.3 = Poor alignment, does not achieve the goal\nReturn ONLY a JSON object with:\n{\n\"score\": <float between 0 and 1>,\n\"explanation\": \"<brief explanation of why this score was given>\"\n}\nUser Message (paired with first/last-frame images):\nPhase: {PHASE_NAME}\nPhase Description: {PHASE_DESCRIPTION}\nExpected End Goal: {END_GOAL}\nPlease compare the FIRST frame (starting state) with the LAST frame (ending state).\nDoes the last frame show that the phase goal has been achieved?\nConsider:\n- Object positions relative to the goal\n- Whether objects moved in the expected direction\n- Whether the motion is consistent with the phase description\nFigure 12. Prompt used for plan-alignment verification\n19\nPhysical Plausibility Verifier Prompt (Gemini 2.5-Flash)\nText Prompt (sent together with the sketch video file):\nAnalyze this video sequence and evaluate whether the motion obeys physical laws.\nIMPORTANT NOTE: This video is generated by copy & paste composition - each frame is\ncreated by pasting objects onto a background. Therefore, please focus on evaluating\nthe movement trajectories and positions of individual objects across frames, not\nvisual quality, shadows, or composition artifacts.\nConsider for each moving object (one of the laws):\nNewtonian Consistency: acceleration / deceleration should be physically plausible;\nPenetration Violation: objects must not pass through static elements;\nGravitational Coherence: objects should not be floating in the air without\nanything holding it\nDeformation Consistency: object size should remain stable unless specified.\nRate the physical plausibility on a scale of 0.0 to 1.0 where:\n- 1.0 = Perfectly realistic, obeys this physical laws\n- 0.7-0.9 = Mostly realistic with minor issues\n- 0.4-0.6 = Some unrealistic aspects but acceptable\n- 0.0-0.3 = Highly unrealistic, violates physics (teleportation, impossible speeds,\netc.)\nReturn ONLY a JSON object:\n{\n\"score\": <float between 0 and 1>,\n\"explanation\": \"<brief explanation focusing on object movement quality,\nhighlight any physics violations>\"\n}\nExample:\nA example for the specific physical Law\nFigure 13. Prompt used for physical plausibility verification\n20\n",
    "title": "Planning with Sketch-Guided Verification for Physics-Aware Video Generation",
    "arxiv_id": "2511.17450v1",
    "category": "recent",
    "authors": [
      "Yidong Huang",
      "Zun Wang",
      "Han Lin",
      "Dong-Ki Kim",
      "Shayegan Omidshafiei",
      "Jaehong Yoon",
      "Yue Zhang",
      "Mohit Bansal"
    ],
    "published": "2025-11-21",
    "num_pages": 20,
    "num_chars": 65768
  },
  {
    "text": "Unmasking Airborne Threats: Guided-Transformers for Portable Aerosol Mass\nSpectrometry\u22c6\nKyle M. Regana,\u2217, Michael McLoughlinc, Wayne A. Brydenc, Gonzalo R. Arceb\naCenter for Bioinformatics and Computational Biology, University of Delaware, Newark, 19713, Delaware, United States\nbDepartment of Electrical and Computer Engineering, University of Delaware, Newark, 19716, Delaware, United States\ncZeteo Tech Inc., Sykesville, 21784, Maryland, United States\nAbstract\nMatrix Assisted Laser Desorption/Ionization Mass Spectrometry (MALDI-MS) is essential for biomolecular analysis\nin human health protection, offering precise identification of pathogens through unique mass spectral signatures to\nsupport environmental monitoring and disease prevention. However, traditional MALDI-MS relies on labor-intensive\nsample preparation and multi-shot spectral averaging, confining it to laboratory settings and hindering its application\nin dynamic, real-world settings critical for public health surveillance. These limitations are amplified in emerging\nportable aerosol MALDI-MS systems, where autonomous sampling produces noisy, single-shot spectra from a mix-\nture of aerosol analytes, demanding novel computational detection methods to safeguard against infectious diseases.\nTo address this, we introduce the Mass Spectral Dictionary-Guided Transformer (MS-DGFormer): a computational\nframework that processes raw, minimally prepared mass spectral data for accurate multi-label classification, directly\nenhancing real-time pathogen monitoring. Utilizing a transformer architecture to model long-range dependencies\nin spectral time-series, MS-DGFormer incorporates a novel dictionary encoder with Singular Value Decomposition\n(SVD)-derived denoised side information, empowering the model to extract vital biomolecular patterns from noisy\nsingle-shot spectra with high reliability. This approach achieves robust spectral identification in aerosol samples,\nsupporting autonomous, real-time analysis in field-deployable systems. By reducing preprocessing requirements,\nMS-DGFormer facilitates portable MALDI-MS deployment in high-risk areas like public spaces, transforming\nhuman health strategies through proactive environmental monitoring, early detection of biological threats, and rapid\nresponse to mitigate disease spread.\nPreprint Notice.This manuscript is a preprint and has been submitted to *Computer in Biology and Medicine*.\nKeywords:Pathogen Detection, Mass Spectrometry, Machine Learning, Bioaerosols, Public Health,\n1. Introduction\nMatrix Assisted Laser Desorption/Ionization Mass\nSpectrometry (MALDI-MS) has evolved to be a power-\nful tool to characterize and identify large biomolecules.\nMALDI-MS is a \u201csoft ionization\u201d method and typically\nutilizes a light absorbing matrix that, when mixed with\nan analytical sample and illuminated with a pulsed laser\n\u22c6This preprint has been submitted toComputer in Biology and\nMedicine.\n\u2217Corresponding author\nEmail addresses:regank@udel.edu(Kyle M. Regan ),\nmike.mcloughlin@zeteotech.com(Michael McLoughlin),\nwayne.bryden@zeteotech.com(Wayne A. Bryden),\narce@udel.edu(Gonzalo R. Arce)light, will create ions from biomolecules to include pro-\nteins, peptides, and lipids [1]. For biodefense applica-\ntions, a major advantage of MALDI is that it can be\ncombined with Time-of-Flight analyzers [2], which do\nnot require a complex fluidic system and can be minia-\nturized for field applications [3]. To achieve high mass\nresolution and accuracy, multiple methods such as de-\nlayed extraction and ion reflectors have been developed\nto compensate for energy spread during the ionization\nprocess [4]. Because MALDI-MS uses a sample de-\nposited onto a substrate, deconvolution of a mixed sam-\nple can be quite complex and has motivated the devel-\nopment of single particle methods [5], [6]. A portable\nprototype MALDI-MS, introduced in \u201cdigitalMALDI:arXiv:2511.17446v1  [cs.LG]  21 Nov 2025\nA Single-Particle\u2013Based Mass Spectrometric Detection\nSystem for Biomolecules\u201d [7] demonstrated the ability\nto produce spectra from environmental aerosol parti-\ncles. The spectrometer autonomously samples aerosol\nparticles, irradiates each by an ultraviolet laser creating\nions from individual particles, then analyzes the ions\nby time-of-flight. The portability of this spectrometer\nenables in-field measurements for rapid identification\nof possible biological threats such as airborne bacteria,\nfungi, viruses, toxins, or nonvolatile chemicals.\nThis research is driven by the urgent need to pro-\ntect communities from natural and engineered biolog-\nical threats, such as infectious outbreaks or bioterror-\nism. By enabling rapid detection of airborne pathogens,\nour work could curb disease spread and enhance pub-\nlic safety. Envision deploying compact devices in high-\ntraffic areas like airports, transit systems, or stadiums\nfor continuous, real-time scanning, to transform how we\nsafeguard against environmental biological risks.\nAt the heart of this approach lies the near autonomous\nsampling of atmospheric *aerosols, a technique that en-\nables continuous monitoring without relying on exten-\nsive human intervention. Yet, creating a threat detection\nsystem that is robust, accurate, and precise is not easy.\nFalse positives risk sparking unwarranted alarm, while\nfalse negatives could allow a silent spread of infec-\ntion. Unlike controlled laboratory conditions with care-\nfully prepared samples, environmental sampling cap-\ntures a complex mixture of aerosol particles\u2014primarily\nharmless background particles\u2014making it challenging\nto identify the unique mass spectra of pathogens. This\nwork tackles these challenges, laying the groundwork\nfor a reliable system capable of safeguarding society\nfrom biological threats.\nMALDI-MS is already FDA-cleared for identifying\nbacterial and fungal isolates in clinical labs, capable\nof distinguishing over 10,000 strains. While extensive\nresearch applies MALDI-MS to biological specimens,\nmost focus on meticulously prepared lab samples re-\nquiring days of culturing or separation. Even then,\nsingle-laser-shot spectra are noisy, so multi-shot aver-\naging is standard to boost signal-to-noise ratio before\ndatabase matching.\nThis works well in clinics where shots target the same\nanalyte, but environmental sampling involves diverse\nparticles per shot. Averaging can blur features, as shown\nin Fig. 1, which depicts a batch of spectra from 80 dust\naerosols mixed with five spectra each fromBacillus glo-\nbigii,E. coli, Insulin, and Ubiquitin. Averaging as-\nsumes uniform analytes [8][9], but mixtures yield mud-\ndled spectra (Fig. 1A). A single-shot method is essential\nto isolate individual analyte spectra (Fig. 1B-E).To enhance single-shot analysis, we leverage the low-\nrank structure of spectra using Singular Value Decom-\nposition (SVD), a mathematical technique for denois-\ning data by identifying dominant patterns. For a noise-\nless spectrumz\u2208Rland noisy versions=z+\u03f5, we\nform a matrixS\u2208Rn\u00d7lfromnspectra of the same\nclass. SVD decomposesS=U\u03a3VT, whereU\u2208Rn\u00d7n\nandVT\u2208Rl\u00d7lare orthogonal matrices and\u03a3\u2208Rn\u00d7l\nis a diagonal matrix containing the singular values in\ndescending order. The matrixShas a low-rank struc-\nture, withr\u226amin (n,l), indicating thatzresides within\nanr-dimensional subspace ofRlspanned by the firstr\ncolumns ofV[10]. By retaining the toprsingular val-\nues (withr\u226amin(n,l)) approximates the signal sub-\nspace, filtering out noise [11].\nThis assumes uniform analytes, so for mixtures, we\nbuild a dictionary of denoised sub-dictionaries per an-\nalyte class via SVD, creating a union of subspaces as\nside information for feature extraction. We then employ\na transformer encoder, a machine learning model adept\nat processing sequences like spectral peaks with posi-\ntional context, to generate embeddings for input spectra\nand dictionaries separately. This separation allows spec-\ntral embeddings to extract biologically relevant features\nfrom the dictionary during training, and dictionary re-\nmoval during inference, halving parameters for faster,\ndeployable predictions crucial for field use in biological\nmonitoring.\n2. Methods\n2.1. Aerosol MALDI-MS Data Acquisition\nIn this study, aerosol particles were ionized us-\ning ultraviolet (349 nm) laser pulses in a portable\nMALDI-Time-of-Flight (ToF) mass spectrometer, as\ndetailed in our prototype system [7]. Each mass\nspectrum is represented as an intensity vectors=\n[s1,s2,...,s l]T\u2208Rlpaired with a correspondingm\nzvec-\ntorm=[m 1,m2,...,m l]T\u2208Rl. For batches ofnparti-\ncles, the spectra form a matrix:\nS=\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0s11s12...s 1l\ns21s22...s 2l\n............\nsn1sn2...s nl\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\u2208Rn\u00d7l\n.\nSincemis identical across measurements (unless ma-\nchine parameters change), a two-dimensionalmmatrix\nis unnecessary.\nDue to the prototype phase, our data collection is lim-\nited to a handful of targets. To simulate field-expected\n2\nFigure 1: (A) Top: An example batch of particles containing 80% dust particulate, with the remaining 20% evenly divided among the four biological\nmarkers. Each row represents a mass spectrum, and each column corresponds to a mass-to-charge ratio value. Bottom: The column-wise average.\n(B) Top: The heatmap of rows from (A) corresponding toB. globigiispectra. Bottom: the average spectrum. (C), (D), (E) same format as (A) but\nwithE. coli, Insulin, and Ubiquitin, respectively.\nspectral profiles for environmental pathogen monitor-\ning, we selected bacterial (multi-peak, noisy), protein\n(few-peak), and non-biological (peakless) types. Data\nincluded two bacteria (Bacillus globigii,Escherichia\ncoli), two proteins (insulin, ubiquitin) as positives, and\nArizona Road Dust as negative background. For safety,\nsamples were aerosolized and collected in a lab set-\nting, mimicking real-world conditions. Data acquisi-\ntions were performed in a class-specific manner, with\nm\nzrestricted to the range [500,10,000] Da. Each class\nproduces a matrix of raw spectra,S, which is split into\n80% for training and 20% for testing. Consequently,\nour model is trained on individual raw spectra, simulat-\ning single-shot detection, to ensure robust performance\nin field scenarios where a matrix of spectra may con-\ntain multiple, co-occurring classes. Table 1 summarizes\nspectra counts and the training/testing split.Table 1: Number of Spectra per Class\nClass # of Samples Training Samples Test Samples\nA.R.Dust 630 504 126\nB.globigii 1500 1200 300\nE.coli 1500 1200 300\nInsulin 1400 1120 280\nUbiquitin 1500 1200 300\nTotal 6530 5224 1306\n2.2. Sparse Signal Processing\nA mass spectrumscan be modeled by a linear com-\nbination of columns, referred to as atoms, from a dictio-\nnary matrixD\u2208Rl\u00d7\u03b1, such thats=Dx, wherex\u2208R\u03b1\nis a sparse coefficient vector (||x|| 0\u226a\u03b1). This synthe-\nsis model leverages sparsity to denoise and extract rel-\nevant features from noisy spectra, crucial for real-time\n3\npathogen detection in complex aerosol signatures.\nCommon dictionaries include orthogonal bases like\nCosines, Fourier, or Wavelets (wherel=\u03b1), but\nover-complete dictionaries ((l\u226a\u03b1)) [12] allow\nmore flexible representations, advancing applications\nin compressive sensing[13], dictionary learning[14],\nmedical imaging[15], classification[16], and object\ndetection[17].\nThese approaches often solve the relaxed convex op-\ntimization problem known as Basis Pursuit (BP)[18]:\nmin\nx||x||1s.t.s=Dx(1)\nwhere the sparse vector,x, encodes rich task-specific\ninformation. This motivated data-driven dictionaries, as\ndemonstrated in the Face Recognition imaging problem\n[19], where training samples form columns based on\nthe principle that high-dimensional data from the same\nclass lie in a low-dimensional subspace. A test sam-\nple is thus represented as a sparse linear combination of\ntraining samples via Sparse Representation Classifica-\ntion (SRC)[19]. SRC has been applied to many applica-\ntions such as image classification [20], denoising [21],\nand deep learning [22].\n2.3. Proposed Dictionary Construction\nInspired by SRC, we constructed the dictionaryD\u2208\nRl\u00d7\u03b1using\u03b1training spectra evenly distributed across\ncclasses (\u03b1/cper class), arranged column-wise with\nsame-class spectra grouped into sub-dictionariesD i\n(i=1,...,c). Thus,D i=[d i,1,...,d i,\u03b1/c], andD=\n[D1,...,D c].\nThis class-specific clustering exploits the low-rank\nstructure inherent in spectra from the same biomolecu-\nlar class (rankr\u226a\u03b1/c). To denoise and capture essen-\ntial features, we applied Singular Value Decomposition\n(SVD) to each sub-dictionary:\n\u02dcDi=U r\u03a3rVT\nr,r\u226a\u03b1\nc,\nwhereU r\u2208Rl\u00d7randV r\u2208R\u03b1/c\u00d7rare orthogonal matri-\nces of left and right singular vectors, and\u03a3 r\u2208Rr\u00d7rcon-\ntains the toprsingular values. The denoised dictionary\nwas then formed by stacking these low-rank approxima-\ntions:\n\u02dcD=[ \u02dcD1,\u02dcD2,..., \u02dcDc]\u2208R\u03b1\u00d7l.\nThis creates a union of rank-rsubspaces that efficiently\nrepresent key biomolecular patterns in noisy spectra, en-\nhancing multi-label classification for airborne pathogen\n/uni00000013/uni00000011/uni00000018 /uni00000014/uni00000011/uni00000019 /uni00000016/uni00000011/uni00000015 /uni00000018/uni00000011/uni00000018 /uni0000001b/uni00000011/uni00000016/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni0000002c/uni00000052/uni00000051/uni00000003/uni0000002c/uni00000051/uni00000057/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000057/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000039/uni0000000c/uni00000025/uni00000011/uni0000004a/uni0000004f/uni00000052/uni00000045/uni0000004c/uni0000004a/uni0000004c/uni0000004c\n/uni00000016/uni00000011/uni00000015 /uni00000016/uni00000011/uni00000019 /uni00000017/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000017/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni0000003d/uni00000052/uni00000052/uni00000050/uni00000048/uni00000047/uni00000003/uni0000002c/uni00000051\n/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013\n/uni0000002c/uni00000051/uni00000047/uni00000048/uni0000005b/uni00000015/uni000000ee/uni00000014/uni00000013/uni00000016/uni00000016/uni000000ee/uni00000014/uni00000013/uni00000016/uni00000017/uni000000ee/uni00000014/uni00000013/uni00000016/uni00000036/uni0000004c/uni00000051/uni0000004a/uni00000058/uni0000004f/uni00000044/uni00000055/uni00000003/uni00000039/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000003/uni0000000b/uni0000004f/uni00000052/uni0000004a/uni0000000c\n/uni00000036/uni0000004c/uni00000051/uni0000004a/uni00000058/uni0000004f/uni00000044/uni00000055/uni00000003/uni00000039/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000056\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000018 /uni00000014/uni00000011/uni00000019 /uni00000016/uni00000011/uni00000015 /uni00000018/uni00000011/uni00000018 /uni0000001b/uni00000011/uni00000016\n/uni00000050/uni00000012/uni0000005d/uni00000003/uni0000000b/uni0000004e/uni00000027/uni00000044/uni0000000c/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni0000002c/uni00000052/uni00000051/uni00000003/uni0000002c/uni00000051/uni00000057/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000057/uni0000005c/uni00000003/uni0000000b/uni00000050/uni00000039/uni0000000c/uni00000035/uni00000044/uni00000051/uni0000004e/uni00000003/uni00000015/uni00000003/uni00000024/uni00000053/uni00000053/uni00000055/uni00000052/uni0000005b/uni00000011\n/uni00000016/uni00000011/uni00000015 /uni00000016/uni00000011/uni00000019 /uni00000017/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000017\n/uni00000050/uni00000012/uni0000005d/uni00000003/uni0000000b/uni0000004e/uni00000027/uni00000044/uni0000000c/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni0000003d/uni00000052/uni00000052/uni00000050/uni00000048/uni00000047/uni00000003/uni0000002c/uni00000051\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013Figure 2: Top: Heatmap of 200 mass spectra fromBacillus globigii.\nMiddle: A low-rank approximation via the Singular Value Decompo-\nsition (SVD) with rankr=2. Bottom: The first 50 singular values\nfrom the SVD plotted on a y-axis log-scale.\ndetection in public health scenarios. The approach\u2019s ef-\nficacy is shown in Fig. 2, where SVD onBacillus glo-\nbigiidata reveals the first two singular values captur-\ning primary features, with subsequent values as noise; a\nrank-2 approximation sharpens peaks and reduces noise\nfor clearer identification.\n2.4. Transformers for Time-Series and Mass Spectrom-\netry\nTransformers, first introduced for natural language\nprocessing [23], have been extended to time-series anal-\nysis, including forecasting [24, 25, 26] and classifica-\ntion [27, 28]. In mass spectrometry (MS), they support\npeptide/protein identification and protein structure pre-\ndiction [29, 30], capitalizing on long-range dependen-\ncies among spectral peaks. Typically, these models use\ndenoised, high-resolution spectra from lab instruments,\nwithm\nzvalues as positional inputs. Recent MS advance-\nments include PowerNovo [31], an ensemble of trans-\nformer and BERT models for tandem MS peptide se-\nquencing, and a semi-autoregressive transformer frame-\nwork for rapid sequencing [32]. These methods acceler-\nate proteomics but often rely on preprocessed data, con-\ntrasting our focus on raw, noisy single-shot spectra from\n4\nFigure 3: The input spectral embedding layer creates a sequence of\nsmall overlapping patches from the mass spectrumsthrough one-\ndimensional convolution filters, transforming the 1D spectrum to 2D\nsequence.\nportable aerosol systems for real-time pathogen detec-\ntion in environmental health monitoring.\n2.5. Proposed MS-DGFormer Architecture\nThe Mass Spectral Dictionary-Guided Transformer\n(MS-DGFormer) processes raw input spectra and de-\nnoised dictionary spectra through separate embedding\nand encoding pathways, enabling robust multi-label\nclassification of biomolecular patterns in noisy aerosol\ndata for real-time public health monitoring (see Fig. 4\nfor model overview).\nInput Embedding.To handle the intensity vectors\u2208Rl\nand correspondingm\nzvaluesm\u2208Rl, we adapt the\n\"patchification\" from Vision Transformers [33] for 1D\nspectra. Overlapping patches are extracted via 1D con-\nvolution with kernel size\u03c1, stride\u03b3, andhoutput chan-\nnels (hidden dimension), yieldingN=l\u2212\u03c1\n\u03b3+1 embed-\ndings:\npi,j=\u03c1\u22121X\nk=0wj,ks\u03b3i+k+b j,\nforming matrixP\u2208RN\u00d7h. This convolutional method\ncaptures local peaks amid noise, outperforming linear\nprojections by reducing edge artifacts and enhancing ro-\nbustness (Fig. 3). Transformer attention is permutation-\ninvariant, focusing on pairwise token relationships with-\nout inherent order. Positional embeddings address this\nby encoding sequence positions, using methods like\nfixed sinusoids [23], rotary embeddings (RoPE) [34], or\nlearnable parameters. In mass spectrometry, them\nzvec-\ntormprovides intrinsic positional data from time-of-\nflight. For overlapping patches projected to dimension\nh, we patchmsimilarly:M i=m [\u03b3(i\u22121)+1:\u03b3(i\u22121)+\u03c1]\u2208R\u03c1,formingM\u2208RN\u00d7\u03c1. A linear projection maps these to\nh:\nMpe=MWT+b\u2208RN\u00d7h,\nwithW\u2208Rh\u00d7\u03c1andb\u2208Rh. AddingMpeto spectral\nembeddingsPintegrates intensity andm\nzpositions, al-\nlowing learned extrapolation tohfor better biomolecu-\nlar pattern recognition in noisy spectra.\nDictionary Embeddings.Similarly, each spectrum in\nthe denoised dictionary \u02dcD\u2208R\u03b1\u00d7l(Section 2.3) un-\ndergoes convolutional embedding, producing patch se-\nquences \u02dcdp\ni\u2208RN\u00d7hfori=1,...,\u03b1, stacked into\n\u02dcDp\u2208R\u03b1\u00d7N\u00d7h. The same positional embeddingsMpe\nare added to consistently encodem\nzpositions. Sepa-\nrate learnable weights for input and dictionary spectra\ndistinguish noisy from denoised features, allowing tar-\ngeted extraction of clean biomolecular signatures for\nimproved detection accuracy.\nEncoder Blocks.Both input and dictionary pathways\nemploy transformer encoder blocks based on Vaswani\net al. [23], featuring multi-head self-attention, an MLP,\nlayer normalization, and residuals for gradient stability.\nFor the input, embeddingsP(with added positional em-\nbeddings) are projected to queries (Q), keys (K), and\nvalues (V):\nQ=PWQ,K=PWK,V=PWV,\nwith scaled dot-product attention per head and concate-\nnation viaWO. This captures long-range spectral de-\npendencies essential for noisy data. For the dictionary,\nembeddings \u02dcDp(with added positional embeddings)\nare processed per sub-dictionary \u02dcDp\ni\u2208R\u03b1\nc\u00d7N\u00d7h. For\neach sub-dictionary, a learnable sequence \u02dcdL\ni\u2208R1\u00d7N\u00d7h\nis concatenated, the resulting tensor is permuted to\nRN\u00d7(\u03b1\nc+1)\u00d7h, and attention is applied slice-wise across se-\nquences at each patch position (Fig. 5). Thecsequence\ntokens gather information globally throughout the sub-\ndictionary, providing aggregated side information for\nthe input encoder\u2019s output. Keeping the input and dic-\ntionary encoders separate ensures denoised priors guide\nclassification without contaminating raw signals, facili-\ntating precise pathogen identification for biodefense ap-\nplications.\nSelection Attention.Following encoding, the input se-\nquence (shapeN\u00d71\u00d7h) selectively extracts features\nfrom thecaggregated sub-dictionary sequences (shape\nN\u00d7c\u00d7h) via a multi-head cross-attention layer (Fig. 6).\nThe input acts as queries, with sub-dictionaries as keys\nand values, enabling class-specific feature integration\n5\nFigure 4: The Mass Spectral Dictionary-Guided Transformer (MS-DGFormer) architecture. (A) Input embedding module. (B) Dictionary Embed-\nding Module. (C) Selection Attention Mechanism. (D) Final peak prediction layer.\n(e.g., prioritizing the relevant low-rank subspace for a\ngiven pathogen). A residual connection preserves orig-\ninal input information, enhancing model stability and\naccuracy in noisy environmental samples.\nPeak Prediction.Known peak locations in training\nclasses form ground-truthy i\u2208RN, wherey i,j=1 if\nclassihas a peak at patchj. An MLP processes the\nmodel outputPout\u2208RN\u00d7hfor binary predictions:\n\u02c6y=sigmoid\u0010\nReLU(PoutW(1)+b(1))W(2)+b(2)\u0011\n,\nwithW(1)\u2208Rh\u00d7\u03d5,b(1)\u2208R\u03d5,W(2)\u2208R\u03d5\u00d71, andb(2)\u2208R.\nThis outputs probabilities per patch, supporting multi-\nlabel classification for rapid biomolecular threat detec-\ntion in aerosols. The final predicted class \u02c6cis the one\nwith maximum cosine similarity to ground truth vectors\nycforc\u2208{1,...,5}:\n\u02c6c=arg max\nc\u02c6y\u00b7y c\n\u2225\u02c6y\u2225\u2225y c\u2225.\nTraining optimizes binary cross-entropy between \u02c6yand\ny, while class predictions inform evaluation metrics: ac-\ncuracy, precision, recall, and F1 score.\n3. Results\n3.1. Competing Models\nTo evaluate our model\u2019s performance, we bench-\nmarked it against recurrent baselines and a dictionary-\nablated variant, focusing on sequence modeling for\nnoisy mass spectra in pathogen detection. To ensure\na fair comparison, we strive to maintain consistency inmodel parameters where possible; however, due to ar-\nchitectural differences, exact parameter matching is not\nalways feasible.\nRecurrent Neural Network (RNN)[35], Long Short-\nTerm Memory (LSTM)[36], and Bidirectional LSTM\n(biLSTM)[37] models retained our input embedding\nand peak prediction layers for consistent processing and\noutput. Positional embeddings were omitted, as RNNs\nand LSTMs inherently capture sequential order. The\ndictionary, input encoder, dictionary encoder, and se-\nlection attention were replaced with RNN, LSTM, or\nbiLSTM blocks. This evaluation directly compares re-\ncurrent vs. transformer-based sequence handling in\nbiomolecular classification.\nTo assess the dictionary\u2019s impact in our model, we\ntrained a model without the dictionary embedding,\nencoder, and selection attention (MS-Former), which\nhalved the total parameter count. Essentially this\nmodel is a standard transformer. For fair comparison,\nwe trained variants with 3 input encoder layers (MS-\nFormer-3; 4.13M parameters) to match our core archi-\ntecture and 7 layers (MS-Former-7) to approximate to-\ntal parameters (8-9M across models), isolating the dic-\ntionary\u2019s role in enhancing accuracy for environmental\nhealth applications.\n3.2. Model and Dictionary Hyperparameters\nExperiments used convolutional embedding window\nsize\u03c1=100 and overlap\u03b3=50 (50%), yielding\nN=1765 patches for spectra of lengthl=88300. Cor-\nrespondingm\nzvalues were patched identically. Patches\nmapped to hidden dimensionh=256. Multi-head at-\ntention (n heads =8,d k=32) was applied in the in-\n6\nFigure 5: The processing of a sub-dictionary is illustrated by exemplifying the first low-rank approximated sub-dictionary \u02dcD1\u2208R\u03b1\nc\u00d7l. The\nspectra [ \u02dcd1,1,..., \u02dcd1,\u03b1\nc]Tare transformed into token sequences via convolution with overlapping kernels andhoutput channels, yielding \u02dcDp\n1\u2208\nR\u03b1\nc\u00d7N\u00d7h, where each kernel encodes temporal peak information. A learnable token sequence \u02dcdL\n1\u2208R1\u00d7N\u00d7his concatenated with \u02dcDp\n1, forming\n\u02dcDp\n1\u2208R(\u03b1\nc+1)\u00d7N\u00d7h. This tensor is permuted toRN\u00d7(\u03b1\nc+1)\u00d7h, for attention to be computed independently across theNtemporal positions. The\nattention mechanism aggregates information across the\u03b1\nc+1 sequences at each temporal location. Finally, the learnable tokens \u02dcdL\n1, now enriched\nwith contextual information, are extracted to represent the aggregated temporal information.\nFigure 6: An input spectral sequenceP\u2208RN\u00d7his first encoded by\nthe input encoder. Each sub-dictionary\u2019s sequences are permuted to\nRN\u00d7(\u03b1\nc+1)\u00d7h, processed by the dictionary encoder, and the respective\nlearnable token sequences ( \u02dcdL\ni) are extracted. Multi-head attention\nselects dictionary features for each temporal position.\nput encoder\u2019s self-attention, dictionary encoder\u2019s slice\nattention, and selection attention. Attention MLP inter-\nmediate dimension was 2048; peak prediction MLP was\n\u03d5=512. Both encoders hadL=3 layers. Dictionary\nDcomprised\u03b1=32 sequences from 4 positive classes\n(B. globigii,E. coli, insulin, ubiquitin; 8 per class), ex-\ncluding dust. Limited by single 4070 GPU memory, it\nwas denoised via sub-dictionary SVD with rankr=2,\nproviding efficient side information for robust pathogen\ndetection in portable systems.\nOur model and competing models were trained for\n300 epochs with batch size of 8, a learning rate of\n10\u22124with 10% warm-up, and a cosine annealing de-\ncay. There were two types of regularization imple-\nmented: neuron dropouts placed similarly to [23] with\n0.1 dropout probability, and binary cross-entropy label\nsmoothing. This type of label smoothing treats each 1\nas 0.9 and each 0 as 0.1, so the model does not becomeoverconfident in its peak predictions.\n3.3. MS-DGFormer Evaluation\nWe begin by examining the results obtained from\nMS-DGFormer before proceeding to a comparison with\ncompeting models. To understand the features captured\nin each sub-dictionary, we visualize the attention maps\nderived from each learnable sequence, \u02dcdL\ni. Figure 7 dis-\nplays the average attention scores, revealing that patches\ncontaining spectral peaks receive higher attention scores\ncompared to those without. Thus, \u02dcdL\nieffectively focuses\non the peaks found within theithsub-dictionary.\nNext, to gain insight into how the model processes\nthe raw overlappingm\nzvalues and maps them to its em-\nbedding space, we extract and visualize the positional\nembeddings. For visualization purposes, we flatten both\nthe rawm\nzvalues and the corresponding positional em-\nbeddings. Specifically, the rawm\nzmatrixM\u2208R1765\u00d7100\nis flattened to a vector inR176500, and the positional\nembeddingsP\u2208R1765\u00d7256are flattened to a vector in\nR451840. We then plot these flattened vectors, focusing\non the segments corresponding to the first 10 patches,\nas illustrated in Fig. 8. From this figure, it is evident\nthat the model preserves the structural characteristics of\nthe rawm\nzvalues, such as their overlapping nature (with\n50% overlap between adjacent patches), while mapping\nthem to a higher-dimensional embedding space (from\n100 to 256 dimensions per patch). Additionally, the\namplitude of the positional embeddings is adjusted to\na range of approximately\u00b15, aligning with the intensity\nscales observed in the mass spectra. To demonstrate the\nalignment between the positional embeddings and the\n7\n/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019dL\n1\n/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019dL\n2\n/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019dL\n3\n/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019dL\n4\n/uni00000013/uni00000011/uni00000013/uni00000015/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000018/uni00000013/uni00000011/uni00000014/uni00000013/uni00000013/uni00000013/uni00000011/uni00000014/uni00000015/uni00000018/uni00000013/uni00000011/uni00000014/uni00000018/uni00000013/uni00000013/uni00000011/uni00000014/uni0000001a/uni00000018/uni00000013/uni00000011/uni00000015/uni00000013/uni00000013/uni0000002f/uni00000048/uni00000044/uni00000055/uni00000051/uni00000048/uni00000047/uni00000003/uni00000027/uni0000004c/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000044/uni00000055/uni0000005c/uni00000003/uni00000036/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni00000048/uni00000056/uni0000000a/uni00000003/uni00000024/uni00000057/uni00000057/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000030/uni00000044/uni00000053/uni00000056/uni00000036/uni00000058/uni00000045/uni00000010/uni00000027/uni0000004c/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000044/uni00000055/uni0000005c/uni00000003/uni00000036/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055\n/uni00000033/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000003/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055Figure 7: Each class\u2019s sub-dictionary attention maps averaged across\nheads are shown. The larger attention scores are located at each class\u2019s\ntrue peak locations showing the dictionary\u2019s efficacy for feature ex-\ntraction.\noriginalm\nzvalues, we normalize both to a common am-\nplitude range and plot them on a uniform linear space.\nFig. 9 illustrates this alignment for each patch, high-\nlighting the model\u2019s ability to encode positional infor-\nmation effectively.\nFinally, we visualize the attention maps within the\nselection attention head to understand how the model\nselects features from the sub-dictionaries based on the\ninput spectrum class. To achieve this, we input a repre-\nsentative bacteria, protein, and noise spectrum into the\nmodel and extract the attention map from the selection\nattention head for visualization. Fig. 9 shows the at-\ntention scores across the sub-dictionaries for each input\nspectrum. From this figure, it is evident that for each\ninput spectrum, the attention scores are significantly\nhigher for the sub-dictionary corresponding to the same\nclass. However, for the Arizona Road Dust class, the\nattention scores are more dispersed and lower in mag-\nnitude. This behavior is expected because the Arizona\nRoad Dust spectra primarily contain noise peaks, which\ndo not align well with the denoised spectra represented\nin the sub-dictionaries.\n3.4. Competing Model Comparisons\nHaving demonstrated the spectral features captured\nby MS-DGFormer, we now proceed to evaluate its\nperformance against competing models using standard\nclassification metrics. Each model is assessed on the\nM1M500M1000M1500/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003m\nz/uni00000029/uni0000004f/uni00000044/uni00000057/uni00000057/uni00000048/uni00000051/uni00000048/uni00000047/uni00000003M/uni00000003/uni0000000b/uni00000052/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000053/uni0000004c/uni00000051/uni0000004a/uni00000003m\nz/uni0000000c\nMpe\n1Mpe\n500Mpe\n1000Mpe\n1500/uni00000018\n/uni00000013/uni00000018/uni00000028/uni00000050/uni00000045/uni00000048/uni00000047/uni00000047/uni00000048/uni00000047/uni00000003m\nz/uni00000029/uni0000004f/uni00000044/uni00000057/uni00000057/uni00000048/uni00000051/uni00000048/uni00000047/uni00000003Mpe/uni00000003/uni0000000b/uni00000033/uni00000052/uni00000056/uni00000011/uni00000003/uni00000028/uni00000050/uni00000045/uni00000048/uni00000047/uni00000011/uni0000000c\nM1M3M5M7M9/uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000013/uni00000015/uni00000029/uni0000004c/uni00000055/uni00000056/uni00000057/uni00000003/uni00000014/uni00000013/uni00000003/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000048/uni00000056\nMpe\n1Mpe\n3Mpe\n5Mpe\n7Mpe\n9/uni00000013/uni00000011/uni00000013/uni00000014\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000014/uni00000029/uni0000004c/uni00000055/uni00000056/uni00000057/uni00000003/uni00000014/uni00000013/uni00000003/uni0000004f/uni00000048/uni00000044/uni00000055/uni00000051/uni00000048/uni00000047/uni00000003/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000048/uni00000056\n/uni00000014 /uni00000016 /uni00000018 /uni0000001a /uni0000001c\nm\nz/uni00000003/uni00000033/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000048/uni00000056/uni00000003/uni00000049/uni00000052/uni00000055/uni00000003M/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003Mpe/uni00000013/uni00000011/uni00000013/uni00000013/uni00000015\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000015/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003/uni00000024/uni0000004f/uni0000004c/uni0000004a/uni00000051/uni00000048/uni00000047/uni00000003/uni00000052/uni00000051/uni00000003/uni00000038/uni00000051/uni0000004c/uni00000049/uni00000052/uni00000055/uni00000050/uni00000003/uni0000002f/uni0000004c/uni00000051/uni00000056/uni00000053/uni00000044/uni00000046/uni00000048\nMpe\n1:10\nM1:10Figure 8: Top: The overlapping patches ofm\nzvalues are flattened to\nRN\u03c1(left).Mis embedded via a learnable linear layer producingMpe\n(right). Middle: The first 10 patches ofMandMpe. Bottom: The\npatches are normalized and plotted on the same linspace.\ntest set, with performance measured using micro and\nmacro accuracy, precision, recall, and F1-score. Table\nIII presents the macro metrics, where MS-DGFormer\nachieves the highest scores across all metrics, despite\nhaving fewer parameters than most other models, with\nthe exception of MS-Former-3. Notably, the biLSTM-\nbased model outperforms MS-Former-7 but still falls\nshort of MS-DGFormer\u2019s performance.\nFor brevity, we focus on the micro F1-score in our\nanalysis, as it effectively balances the trade-offbetween\nfalse positives and false negatives, which is critical for\nclassification tasks. Table IV displays the micro F1-\nscores for each class, with MS-DGFormer consistently\nachieving the highest F1-score across all classes. In\ncontrast, the other models exhibit significant perfor-\nmance degradation on the Arizona Road Dust class.\nThis is particularly concerning because dust-like parti-\ncles are commonly encountered in environmental con-\nditions, and misclassifying them as biological agents\ncould result in a high rate of false alarms.\n3.5. Computational Efficiency\nReal-time field analysis demands rapid processing\nof continuous spectral streams, where both parameter\ncount and hardware requirements are critical. Our de-\nsign improves efficiency by separating the dictionary\nembedding/encoder from the input embedding/encoder.\n8\nFigure 9: Top:E.coliinput spectrum and the attention map from the\nselection attention mechanism. The attention scores are larger for\nE.coli, capturing the learning of the selection mechanism. Middle:\nUbiquitin input spectrum and its corresponding selection attention\nmap. Bottom: Arizona Road Dust and its corresponding selection\nattention map showing no dominant features selected from a single\nclass.\nThe dictionary sequences remain constant during train-\ning and are encoded independently of the input spec-\ntrum. Only the learned sequence per sub-dictionary is\nused:\u03b1sequences enter the dictionary encoder during\ntraining, but onlycsequences are passed to the selection\nattention head.\nBecause the dictionary input is spectrum-\nindependent, we precompute and store theclearned\nsub-dictionary sequences, removing the dictionary\nembedding layer, dictionary encoder, and dictionary\nitself from the inference model. This yields an efficient\nvariant, MS-DGFormer-E, where only the pre-trained\nweights of the remaining components are loaded. The\ncsequences are stored in memory and fed directly into\nthe selection attention head, cutting parameters from\n8.36\u00d7106to 4.39\u00d7106without performance loss.\nThis architecture also scales efficiently. Adding more\nsequences to sub-dictionaries during training does notTable 2: Macro Metrics Across All Classes.\nModelMacro Metrics\nAccuracy Precision Recall F1 Params\nRNN-6 0.560 0.832 0.560 0.491 9.52M\nLSTM-4 0.679 0.821 0.679 0.641 9.50M\nBiLSTM-6 0.939 0.916 0.939 0.915 9.48M\nMS-Former-3 0.709 0.845 0.709 0.664 4.13M\nMS-Former-7 0.862 0.876 0.862 0.824 9.39M\nMS-DGFormer 0.983 0.982 0.983 0.982 8.36M\nTable 3: Micro F1 Scores For Each Class\nModel A.R.Dust B.globigii E.coli Insulin Ubiq.\nRNN-6 0.278 0.045 0.408 0.795 0.926\nLSTM-4 0.331 0.623 0.374 0.906 0.972\nBiLSTM-6 0.736 0.926 0.954 0.976 0.983\nMS-Former-3 0.369 0.328 0.880 0.787 0.952\nMS-Former-7 0.553 0.666 0.984 0.921 0.994\nMS-DGFormer 0.949 0.991 0.979 0.987 0.994\naffect inference time, as the number of sequences used\nat inference,c, remains fixed. If\u03b8new classes are\nadded, the training dictionary expands to\u03b1\nc\u03b8+\u03b1se-\nquences, yet inference still requires onlyc+\u03b8sequences,\nwithc\u226a\u03b1\nc.\nWe evaluated inference speed and spectral throughput\non batches of size 1, 4, and 8, averaging 100 runs after\n10 warm-up runs. As shown in Table V , MS-DGFormer-\nE achieves nearly a 2\u00d7increase in mean inference speed\nand more than a 2\u00d7increase in throughput over the full\nMS-DGFormer. The only model with comparable infer-\nence time is MS-Former-3, due to its similar parameter\ncount, but its classification performance is significantly\nlower. While results are hardware-specific, the relative\nefficiency gains should generalize across platforms.\n4. Discussion\nWe have introduced an approach capable of classify-\ning individual noisy mass spectra without the need to\ncollect several spectra of the same class to create an\naveraged spectrum. The mass spectra are turned into\nsequences of small overlapping patches enabling atten-\ntion mechanisms to capture peak locations. Although,\nthis alone is not enough to accurately classify the spec-\ntral input due to intense noise. We notice that if we\ndo have a batch of spectra all from the same class, the\nSVD is able to significantly reduce noise and reveal true\nspectral peaks. However, it is unlikely this will nat-\nurally occur in the environment. We construct a dic-\ntionary composed of training samples from each class,\n9\nTable 4: Inference Performance Metrics for Different Models and Batch Sizes\nModel Batch Size 1 Batch Size 4 Batch Size 8\nMean (ms)\u2193Std (ms)\u2193Spectra/s\u2191 Mean (ms)\u2193Std (ms)\u2193Spectra/s\u2191 Mean (ms)\u2193Std (ms)\u2193Spectra/s\u2191\nRNN-6 125.32 3.36 7.98 156.30 4.03 25.59 221.37 5.94 36.14\nLSTM-4 108.62 3.31 9.21 116.619 4.79 34.30 194.51 6.79 41.13\nBiLSTM-6 156.38 2.56 6.39 263.821.4615.16 223.67 4.18 35.77\nMS-Former-3 11.882.5584.15 34.703.31115.27 62.874.91127.25\nMS-Former-7 23.46 2.32 42.63 74.31 4.18 53.83 141.19 3.08 56.66\nMS-DGFormer 72.27 3.76 13.84 95.66 2.85 41.81 127.172.5862.91\nMS-DGFormer-E 12.31 1.67 81.23 35.983.70111.16 66.333.92120.60\nclustered together to resemble sub-dictionaries, and per-\nform the SVD on the sub-dictionaries to be used as side-\ninformation. One learnable (randomly initialized) se-\nquence is concatenated to each sub-dictionary to capture\nfeatures for their respective sub-dictionary for both ef-\nfectiveness and efficiency. Further, our model processes\nthe input spectrum and dictionary through separate en-\ncoders allowing the features to be learned indepen-\ndently, with another attention head capable of selecting\nspecific information from the dictionary. In this man-\nner, only the learned dictionary sequences are required\nfor inference, and the dictionary components can be re-\nmoved from the model. This significantly improves in-\nference speed and spectral throughput since nearly half\nof the model parameters reside in the dictionary compo-\nnents. Finally, our MS-DGFormer is compared against\ncompeting sequential models and achieves the highest\nmicro and macro classification metrics with drastically\nfaster inference metrics.\nOverall, our proposed architecture, MS-DGFormer,\naddresses several challenges that arise when transition-\ning MALDI-MS systems from laboratory to environ-\nmental settings. Traditional MALDI-MS workflows re-\nquire extensive sample preparation and pre-processing;\nour approach eliminates much of this burden through\nautonomous aerosol sampling, shifting the workload\nto computational post-processing and machine learning\ninference. Rather than relying on multi-shot spectral\naveraging to obtain clean spectra, we leverage SVD-\ndenoised sub-dictionaries to provide rich side informa-\ntion for accurate classification of raw, minimally pro-\ncessed spectra.\nFalse positive classification is a particular challenge\nin environmental sampling, where negative classes such\nas dust are abundant. MS-DGFormer demonstrates ro-\nbust performance in these scenarios, effectively mitigat-\ning misclassification.\nOur results highlight the potential of real-time\npathogen detection using portable aerosol MALDI-MSsystems, paving the way for field-deployable solutions\nthat can transform environmental monitoring, biological\nthreat detection, and efforts to reduce disease spread.\nDeclaration of competing interest\nW.B., and M.M. have competing interests. W.B. is\nthe President and CEO of Zeteo Tech, Inc. M.M. is the\nVice President of Research and CTO at Zeteo Tech, Inc.\nAcknowledgments\nThis work was supported in part by the National In-\nstitute of Health under award number T32GM142603\nand in part by Zeteo Tech Inc. Due to privacy or ethi-\ncal restrictions, the data from the study is only available\nupon request from the corresponding author. Code is\navailable upon request from the corresponding author.\nAuthor Contributions\nConceptualization, K.R., G.A., M.M. and W.B;\nmethodology, K.R., G.A., M.M. and W.B; software,\nK.R; validation, G.A., M.M., W.B.; formal analysis,\nK.R., G.A.; investigation, K.R., G.A., M.M. and W.B;\nresources, K.R., G.A., M.M. and W.B; data curation,\nK.R, M.M, W.B; writing\u2014original draft preparation,\nK.R., G.A; writing\u2014review and editing, G.A., M.M.\nand W.B.; visualization, K.R.; supervision, G.A., M.M.\nand W.B.; project administration, G.A., M.M. and W.B.;\nfunding acquisition, G.A., M.M. and W.B. All authors\nhave read and agreed to the published version of the\nmanuscript.\nDeclaration of generative AI and AI-assisted tech-\nnologies in the manuscript preparation process\nDuring the preparation of this work the author(s) used\nGrok xAI in order for grammar correction and readabil-\nity. After using this tool/service, the author(s) reviewed\n10\nand edited the content as needed and take(s) full respon-\nsibility for the content of the published article.\nReferences\n[1] A. El-Aneed, A. Cohen, J. Banoub, Mass spec-\ntrometry, review of the basics: Electrospray,\nmaldi, and commonly used mass analyzers, Ap-\nplied Spectroscopy Reviews 44 (3) (2009) 210\u2013\n230. doi:10.1080/05704920902717872.\n[2] W. C. Wiley, I. H. McLaren, Time-of-flight mass\nspectrometer with improved resolution, Review of\nScientific Instruments 26 (12) (2004) 1150\u20131157.\ndoi:10.1063/1.1715212.\n[3] W. A. Bryden, R. C. Benson, H. W. Ko, C. Fense-\nlau, R. J. Cotter, Tiny-tof mass spectrome-\nter for biodetection, in: P. J. Stopa, M. A.\nBartoszcze (Eds.), Rapid Methods for Analy-\nsis of Biological Materials in the Environment,\nSpringer Netherlands, Dordrecht, 2000, pp. 101\u2013\n110. doi:10.1007/978-94-015-9534-6_10.\n[4] R. J. Cotter, Time-of-flight mass spectrometry,\nin: Electrospray and MALDI Mass Spectrome-\ntry, John Wiley & Sons, Ltd, 2010, pp. 345\u2013364.\ndoi:10.1002/9780470588901.ch10.\n[5] A. L. van Wuijckhuijse, M. A. Stowers,\nW. A. Kleefsman, B. L. M. van Baar, C. E.\nKientz, J. C. M. Marijnissen, Matrix-assisted\nlaser desorption/ionisation aerosol time-of-\nflight mass spectrometry for the analysis of\nbioaerosols: development of a fast detector\nfor airborne biological pathogens, Journal of\nAerosol Science 36 (5-6) (2005) 677\u2013687.\ndoi:10.1016/j.jaerosci.2004.11.003.\n[6] C. Papagiannopoulou, R. Parchen, P. Rubbens,\nW. Waegeman, Fast pathogen identifica-\ntion using single-cell matrix-assisted laser\ndesorption/ionization-aerosol time-of-flight mass\nspectrometry data and deep learning methods,\nAnalytical Chemistry 92 (11) (2020) 7523\u20137531.\ndoi:10.1021/acs.analchem.9b05806.\n[7] D. Chen, W. A. Bryden, M. McLoughlin, S. A.\nEcelberger, T. J. Cornish, L. P. Moore, K. M. Re-\ngan, digitalmaldi: A single-particle-based mass\nspectrometric detection system for biomolecules,\nJournal of Mass Spectrometry 60 (2) (2025)\ne5110. doi:10.1002/jms.5110.[8] M. McLoughlin, G. Arce, Deterministic prop-\nerties of the recursive separable median fil-\nter, IEEE Transactions on Acoustics, Speech,\nand Signal Processing 35 (1) (1987) 98\u2013106.\ndoi:10.1109/TASSP.1987.1165026.\n[9] G. R. Arce, N. C. Gallagher, T. A. Nodes, Median\nfilters: Theory for one- and two-dimensional fil-\nters, in: T. S. Huang (Ed.), Advances in Computer\nVision and Image Processing, V ol. 2, JAI Press,\nGreenwich, CT, 1986, pp. 89\u2013166.\n[10] J. A. Fessler, R. R. Nadakuditi, Linear Algebra for\nData Science, Machine Learning, and Signal Pro-\ncessing, Cambridge University Press, 2024.\n[11] C. Eckart, G. Young, The approximation of one\nmatrix by another of lower rank, Psychometrika\n1 (3) (1936) 211\u2013218.\n[12] S. Mallat, Z. H. Zhang, Matching pursuits with\ntime-frequency dictionaries, IEEE Transactions\non Signal Processing 41 (12) (1993) 3391\u20133401.\ndoi:10.1109/78.258082.\n[13] D. L. Donoho, Compressed sensing, IEEE Trans-\nactions on Information Theory 52 (4) (2006)\n1289\u20131306.\n[14] M. Aharon, M. Elad, A. B. Malach, K-svd: An\nalgorithm for designing overcomplete dictionar-\nies for sparse representation, IEEE Transactions\non Signal Processing 54 (11) (2006) 4311\u20134322.\ndoi:10.1109/TSP.2006.881199.\n[15] S. Ravishankar, Y . Bresler, Mr image recon-\nstruction from highly undersampled k-space data\nby dictionary learning, IEEE Transactions on\nMedical Imaging 30 (5) (2011) 1028\u20131041.\ndoi:10.1109/TMI.2010.2090538.\n[16] M. Yamac, M. Ahishali, A. Degerli, S. Ki-\nranyaz, M. E. H. Chowdhury, M. Gabbouj,\nConvolutional sparse support estimator-based\ncovid-19 recognition from x-ray images,\nIEEE Transactions on Neural Networks and\nLearning Systems 32 (5) (2021) 1810\u20131820.\ndoi:10.1109/TNNLS.2021.3070467.\n[17] M. Ahishali, M. Yamac, S. Kiranyaz, M. Gab-\nbouj, Representation based regression for object\ndistance estimation, Neural Networks 158 (2023)\n15\u201329. doi:10.1016/j.neunet.2022.11.011.\nURLhttps://www.sciencedirect.com/science/article/pii/S089360802200452X\n11\n[18] S. Chen, D. L. Donoho, M. A. Saunders, Atomic\ndecomposition by basis pursuit, SIAM Journal\non Scientific Computing 20 (1) (2001) 33\u201361.\ndoi:10.1137/S1064827500394074.\n[19] J. Wright, A. Y . Yang, A. Ganesh, S. Sas-\ntry, Y . Ma, Sparse representation for com-\nputer vision and pattern recognition, Proceed-\nings of the IEEE 98 (6) (2009) 1031\u20131044.\ndoi:10.1109/JPROC.2009.2015716.\n[20] J. Yang, K. Yu, Y . Gong, T. Huang, Linear spatial\npyramid matching using sparse coding for image\nclassification, in: 2009 IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2009, pp.\n1794\u20131801. doi:10.1109/CVPR.2009.5206757.\n[21] J.-L. Starck, E. Candes, D. Donoho, The curvelet\ntransform for image denoising, IEEE Transac-\ntions on Image Processing 11 (6) (2002) 670\u2013684.\ndoi:10.1109/TIP.2002.1014998.\n[22] W. Wen, C. Wu, Y . Wang, Y . Chen, H. Li, Learning\nstructured sparsity in deep neural networks (2016).\narXiv:1608.03665.\nURLhttps://arxiv.org/abs/1608.03665\n[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin,\nAttention is all you need, in: Advances in Neu-\nral Information Processing Systems (NeurIPS),\nV ol. 30, Curran Associates, Inc., 2017, pp. 5998\u2013\n6008. arXiv:1706.03762.\nURLhttps://arxiv.org/abs/1706.03762\n[24] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li,\nH. Xiong, W. Zhang, Informer: Beyond efficient\ntransformer for long sequence time-series fore-\ncasting (2021). arXiv:2012.07436.\nURLhttps://arxiv.org/abs/2012.07436\n[25] T. Zhou, Z. Ma, Q. Wen, X. Wang, L. Sun,\nR. Jin, Fedformer: Frequency enhanced decom-\nposed transformer for long-term series forecasting\n(2022). arXiv:2201.12740.\nURLhttps://arxiv.org/abs/2201.12740\n[26] A. Garza, C. Challu, M. Mergenthaler-Canseco,\nTimegpt-1 (2024). arXiv:2310.03589.\nURLhttps://arxiv.org/abs/2310.03589\n[27] G. Zerveas, S. Jayaraman, D. Patel, A. Bhamidi-\npaty, C. Eickhoff, A transformer-based framework\nfor multivariate time series representation learning\n(2020). arXiv:2010.02803.\nURLhttps://arxiv.org/abs/2010.02803[28] J. Xu, H. Wu, J. Wang, M. Long, Anomaly trans-\nformer: Time series anomaly detection with asso-\nciation discrepancy (2022). arXiv:2110.02642.\nURLhttps://arxiv.org/abs/2110.02642\n[29] M. Ekvall, P. Truong, W. Gabriel, M. Wil-\nhelm, L. K\"all, Prosit transformer: A trans-\nformer for prediction of ms2 spectrum in-\ntensities, Journal of Proteome Research\n21 (5) (2022) 1359\u20131364, pMID: 35413196.\ndoi:10.1021/acs.jproteome.1c00870.\nURLhttps://doi.org/10.1021/acs.jproteome.1c00870\n[30] M. Yilmaz, W. E. Fondrie, W. Bittremieux, W. S.\nNoble, Sequence-to-sequence translation from\nmass spectra to peptides with a transformer\nmodel, Nature Communications 15 (2024) 6427.\ndoi:10.1038/s41467-024-49731-x.\nURLhttps://www.nature.com/articles/s41467-024-49731-x\n[31] D. V . Petrovskiy, et al., Powernovo: de novo pep-\ntide sequencing via tandem mass spectrometry us-\ning an ensemble of transformer and bert models,\nScientific Reports 14 (2024) 15000.\n[32] Y . Zhao, S. Wang, J. Huang, B. Meng, D. An,\nX. Fang, Y . Wei, X. Dai, A transformer-based\nsemi-autoregressive framework for high-speed\nand accurate de novo peptide sequencing,\nCommunications Biology 8 (1) (2025) 234.\ndoi:10.1038/s42003-025-07584-0.\nURLhttps://doi.org/10.1038/s42003-025-07584-0\n[33] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,\nN. Houlsby, An image is worth 16x16 words:\nTransformers for image recognition at scale, arXiv\npreprint arXiv:2010.11929 (2021).\nURLhttps://doi.org/10.48550/arXiv.2010.11929\n[34] J. Su, Y . Lu, S. Pan, A. Murtadha, B. Wen, Y . Liu,\nRoformer: Enhanced transformer with rotary po-\nsition embedding (2023). arXiv:2104.09864.\nURLhttps://arxiv.org/abs/2104.09864\n[35] J. L. Elman, Finding structure in time,\nCognitive Science 14 (2) (1990) 179\u2013211.\ndoi:10.1207/s15516709cog1402_1.\n[36] S. Hochreiter, J. Schmidhuber, Long short-term\nmemory, Neural Computation 9 (8) (1997) 1735\u2013\n1780. doi:10.1162/neco.1997.9.8.1735.\n12\n[37] M. Schuster, K. Paliwal, Bidirectional recur-\nrent neural networks, IEEE Transactions on\nSignal Processing 45 (11) (1997) 2673\u20132681.\ndoi:10.1109/78.650093.\n13\n",
    "title": "Unmasking Airborne Threats: Guided-Transformers for Portable Aerosol Mass Spectrometry",
    "arxiv_id": "2511.17446v1",
    "category": "recent",
    "authors": [
      "Kyle M. Regan",
      "Michael McLoughlin",
      "Wayne A. Bryden",
      "Gonzalo R. Arce"
    ],
    "published": "2025-11-21",
    "num_pages": 13,
    "num_chars": 52175
  },
  {
    "text": "                 GRAPHIC \u2013 Guidelines for Reviewing Algorithmic Practices in Human-centred Design and Interaction for Creativity  Joana Rovira Martins* University of Coimbra, CISUC/LASI \u2013 Centre for Informatics and Systems of the University of Coimbra, Department of Informatics Engineering; University of Coimbra, Interdisciplinary Research Institute, Computational Media Design, jmmartins@dei.uc.pt Pedro Martins University of Coimbra, CISUC/LASI \u2013 Centre for Informatics and Systems of the University of Coimbra, Department of Informatics Engineering, pjmm@dei.uc.pt Ana Boavida University of Coimbra, CISUC/LASI \u2013 Centre for Informatics and Systems of the University of Coimbra, Department of Informatics Engineering, aboavida@dei.uc.pt  Artificial Intelligence (AI) has been increasingly applied to creative domains, leading to the development of systems that collaborate with humans in design processes. In Graphic Design, integrating computational systems into co-creative workflows presents specific challenges, as it requires balancing scientific rigour with the subjective and visual nature of design practice. Following the PRISMA methodology, we identified 872 articles, resulting in a final corpus of 71 publications describing 68 unique systems. Based on this review, we introduce GRAPHIC (Guidelines for Reviewing Algorithmic Practices in Human-centred Design and Interaction for Creativity), a framework for analysing AI-based systems applied to Graphic Design. Its goal is to understand how current systems support human-AI collaboration in the Graphic Design discipline. The framework comprises main dimensions, which our analysis revealed to be essential across diverse system types: (1) Collaborative Panorama, (2) Processes and Modalities, and (3) Graphic Design Principles. Its application revealed research gaps, including the need to balance initiative and control between agents, improve communication through explainable interaction models, and promote systems that support transformational creativity grounded in core design principles.   CCS CONCEPTS \u2022 Applied computing \u2192 Media arts \u2022 Applied computing \u2192 Computer-aided design \u2022 General and reference \u2192 Design \u2022 Human-centered computing \u2192 Collaborative interaction Additional Keywords and Phrases: Human-Computer Co-creation, Graphic Design, Creative Systems, Design Principles  * Corresponding author. \n2 1 INTRODUCTION Computational approaches can be explored to amplify human creative potential, as seen in human-computer co-creative systems, defined by their ability to collaborate with humans to generate new ideas [35, 38]. Despite advances in the field, these systems still present challenges due to the unpredictability of human creativity and the diverse strategies that can be employed during the creative process [68]. Human behaviour can influence the creative process to the extent that the need to control it can vary both from use to use and within the various phases of the same process [68]. Therefore, beyond technical skills, the development of such tools should consider the subjective nature of creative cognition and the interaction models that effectively support co-creation [44, 63]. In design, this challenge is complex since it requires balancing scientific rigour with design nuances, two notions often considered epistemologically opposed [44]. This issue is particularly pronounced in Graphic Design, which relies on visual language to communicate effectively. One way to address this is by considering a basic visual vocabulary of communication, i.e., the fundamental principles of Graphic Design [45]. These (such as balance, emphasis, or contrast) serve as a foundation for developing visually effective and comprehensible solutions [67]. As Leborg (2013) argues, reflecting on our creations alters the creative process, since we think differently when we have a language to describe things. Accordingly, co-creative systems in this context should facilitate experimentation and support clear reflection on design principles used.   We conducted a systematic literature review on AI-based systems in Graphic Design. Following the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) methodology [59], we analysed 872 articles, of which 25 were included. We also added 46 articles to our final corpus based on citations, totalling 71 articles analysed, describing 68 unique systems\u2014three systems were discussed in six separate publications. This overview aims to enhance researchers' understanding of the current state of co-creative systems in Graphic Design. We propose the Guidelines for Reviewing Algorithmic Practices in Human-centred Design and Interaction for Creativity (GRAPHIC) framework. This is based on three dimensions: (1) Collaborative panorama, which analyses human-computer partnership; (2) Explored modalities and processes, encompassing both technical and creative aspects; and (3) Graphic Design principles, which ground the analysis in the discipline\u2019s visual language. These dimensions have allowed us to identify potential research opportunities that go beyond this framework and can be adapted by adjacent design areas. 2 BACKGROUND Artificial Intelligence (AI) skills are not the only decisive factor in the success of a human-AI collaboration [54]. Various researchers have proposed guidelines and frameworks to understand and enhance interaction models [1, 2, 51, 64, 70].  Regarding interaction, Shi et al. [76] developed a taxonomy of Human-GenAI interaction divided into five dimensions: Purposes of Using GenAI, Feedback from Models to Humans, Control from Humans to Models, Levels of Engagement, and Application Domains. More recently, Rezwana and Ford [69] developed FAICO, a framework that identifies key aspects of effective human-AI communication, including modalities, Response Mode, Timing, Types, Explanation Details, and Tone.  Within the scope of shared control between humans and AI, Cimolino and Graham [14] proposed an analytical framework that confronts six domains across four axes: Role of AI, Supervision, Influence and Mediation. Moruzzi and Margarido [58] introduced the UCCC framework, defining nine dimensions organised into three categories: Interaction Guidance & Response, Interaction Configuration, and Interaction Dynamics.  These methods are characterised by their broad orientation. By focusing on specific areas of design, different challenges are identified, such as the ability to adapt to diverse user needs [68]. Windl et al. [89] identified four strategies \n3 that describe how designers include AI models into their design process: a priori, post-hoc, model-centric, and competence-centric. Another example that helps characterise the designer-AI partnership is the approach proposed by Shi et al. [77], which defines five dimensions: Scope, Access, Agency, Flexibility, and Visibility. Hwang [25] also analysed various AI-based design support tools and identified four categories: Editors, Transformers, Blenders, and Generators. Despite these contributions, significant gaps remain, particularly in fields like Graphic Design. To address this, we propose a framework based on a systematic review and analysis of co-creative systems in Graphic Design, evaluating them according to the fundamentals of Design, such as its principles and how they are applied. 3 METHOD We provide a detailed description of the methodology employed in data collection and the dimensions defined to analyse the surveyed literature. Details of the corpus are available here: <Link> 3.1 Data Collection The approach used was based on the PRISMA methodology [59], which is structured into six phases (see Figure 1).  \n Figure 1: The process of data collection following the PRISMA framework. \n\n4 3.1.1 Identification  This literature review aims to map the landscape of human-computer co-creative systems in Graphic Design. We focused on systems developed by researchers rather than commercially available tools. This decision was made considering that research contributions provide detailed tool descriptions, enabling more effective assessment [20]. The search query was defined following an initial study to identify relevant terms in the areas involved. To validate their suitability for the research focus, we identified some papers aligned with our objective and searched for these terms in the authors' titles, abstracts or keywords.  After testing various search queries to determine which combinations got the most results, we opted for the search query: 'graphic design' + creative system + co-creative system + creativity support tools. We utilised the Google Scholar database, as it offers a comprehensive overview of the current state of the art. The last search was conducted in October 2024. Our search produced 872 results. Articles with duplicate titles (40) and inaccessible articles (32) were removed, resulting in 800 articles for screening. Additional articles were identified through direct citations from the previously obtained items. We collected 60 articles, of which six were excluded due to duplication or inaccessibility. 3.1.2 Screening, Eligibility and Inclusion  Once the results were identified, we defined the following exclusion criteria to guide the selection and eligibility phases:  EC1: The article discussed design in a broad scope. EC2: The article did not describe Graphic Design tasks. EC3: The primary contribution of the paper was not related to creative systems.  EC4: The creative system for Graphic Design is not the main topic; instead, the article discusses designing creative systems. The selection phase resulted in 160 articles for analysis. After the eligibility phase, 25 articles were included (see Table 1). In addition, 46 articles were included through the citation search, bringing the total to 71. During analysis, we found three articles referring to systems already represented in other articles; therefore, 68 unique systems were included in the final analysis. For all subsequent evaluation phases, the criteria remained the same. Table 1: Classification of article references, distinguishing between the systematic review corpus and additional supporting citations. Type of Source References Systematic Review [7, 10, 12, 15, 17, 21, 23, 24, 28, 29, 43, 47, 48, 52, 53, 61, 65, 66, 68, 71, 86, 87, 91, 93, 101] Additional [4\u20136, 8, 9, 11, 13, 16, 19, 22, 26, 27, 31\u201334, 37, 39\u201342, 46, 50, 56, 57, 60, 62, 72, 74, 78\u201385, 92, 94\u2013100, 102]  3.2 Data Analysis Faced with the corpus collected, we defined a set of evaluation dimensions to guide our analysis. To ensure their relevance, we first reviewed a small, random sample of ten papers to identify structural tendencies. Considering the main objective and after several rounds of discussion and refinement, we defined three dimensions: (i) Collaborative Panorama\u2014the nature of collaboration between human and system; (ii) Processes and Modalities\u2014the processes and modalities explored during interaction; (iii) Graphic Design Principles\u2014the integration of Graphic Design principles. These dimensions form the basis of our framework, which is presented in detail in the following section. \n5 4 THE GRAPHIC FRAMEWORK Under defined dimensions, we developed a framework (see Figure 2) that defines relevant parameters within the context of co-creative systems in Graphic Design, grounded in prior literature [30, 70, 77, 88]. Given the complexity of balancing scientific rigour with the subjective nuances inherent in the design field [44], this model serves as a conceptual guide for creators in developing effective and comprehensive co-creative systems. \n Figure 2: Overview of the GRAPHIC framework's three dimensions and key parameters. 4.1.1 Collaboration Panorama  This first dimension refers to human-computer collaborative panorama, focusing on the collaborative scenario and interaction model. The first category aims to understand the level of collaboration within the design process, the system's role, and who controls the process [77]. Concerning the level of collaboration and control, we defined options that reflect varying degrees of both aspects throughout the process. Regarding the system's role, we identified three distinct roles: Aid\u2014the system acts as a helper; Collaborator\u2014the system acts as a partner; and Evaluator\u2014the system only evaluates the work done. We also analysed the collaborative dynamics by examining the computer's objective when making its contributions, the division of tasks between the parties and who makes the final decision\u2014human or machine. Concerning the objective, we defined two possibilities: the computer follows the same objective as the human or acts provocatively [30, 36, 70, 75]. Concerning task division, we distinguished two scenarios: either agents work on the same task or work on different tasks [30, 36, 70]. The second category explores human-computer interactive dynamics. Based on the literature [30, 70], we analysed how contributions are organised, whether through turn-taking or in parallel, and the nature of involvement, whether both \n\n6 parties contribute to a shared environment or maintain separate environments. Additionally, we analysed the interactive dynamics in terms of system initiative and the nature of both initial and ongoing interaction. Regarding the first point, we analysed whether the system needs to be prompted (the user proceeds independently and requests input from the system when needed) or whether their input is automatically part of the process.   The type of initial interaction was analysed considering five categories: The system initiates\u2014the process begins with a contribution from the system; Do and Ask\u2014users contribute, notifying the computer that they have finished and asking for a response; Do and have automatic contributions\u2014users make a first contribution and the computer automatically suggests recommendations; Define and Ask\u2014users define all or most of the parameters and ask for a contribution; Define context/parameters and Ask\u2014users give the computer clues or context and ask for a contribution.  As for the type of continuous interaction, we have defined four scenarios: Ask and have to accept\u2014after requesting the computer's contribution, users must accept it without alternatives; Ask and can accept/reject\u2014users can reject or accept the computer\u2019s contribution; Ask and can change outside the process\u2014users are given access to an editable version of the output, allowing modifications, but outside of the collaboration environment; Ask and can change within the process\u2014users can modify the computer's contribution during the process, which then continues based on those changes.   4.1.2 Processes and Modalities  This dimension aims to understand how design processes are structured and how their variants are addressed. The first two parameters, User Expertise and Activity Type, prove to be determining factors in shaping the process. People with different levels of expertise tend to have different needs [49], as do the types of activities.  To understand which phases these systems commonly support, based on existing literature [20, 77], we defined five phases that constitute the process, regardless of the task. In terms of methodology, we assessed the clarity of it, whether there is an explicit division between process phases, and whether it supports divergent and/or convergent thinking [77]. We also analysed the creative types explored, adopting Boden\u2019s [3] classification: combinatory, exploratory and transformational.  The third category addresses interactive modalities, including the system's inputs and outputs, communication channels, and exploration types. For inputs, we identified four modalities: text, visual (e.g., drawings), gesture and colour. Regarding communication channels and exploration types, we identified two categories for each. Communication channels can be visual or textual, while the exploration can be visual and conceptual (systems that operate on the semantic relationships between concepts). 4.1.3 Graphic Design Principles One way to mitigate the challenges in Graphic Design is to utilise its principles as a communication platform. This dimension aims to understand the current systems in this context. For the design principles parameter, we first analysed how different designers propose and categorise theoretical principles. The selection of principles was conducted systematically by identifying and comparing those advocated by various authors [45, 55, 67, 73, 88, 90]. After a comparative analysis, we chose to base our framework on White\u2019s [88] categorisation (Image, Form + Space, Layout, Typography and Colour), due to its structured organisation, which facilitates consistent evaluation across diverse systems. \n7 In addition to identifying which principles are addressed, the framework also aims to understand how they are addressed during the process. This may occur either explicitly, the system presents the design principles clearly and identifiably as part of its operation or interface; or implicitly, where the system incorporates the principles during the creative process without naming or visibly presenting them to the user. Another aspect to consider is when these principles are addressed: during the creative process, by the system or the user, or only in the final result, only through the result can we see that the principles have been worked on during the process.   Given the central theme of this survey, it was also relevant to assess the degree of user control over the principles and the flexibility of that control. For the first parameter, we defined three levels: Yes\u2014the user has direct control over the principles; Not exactly\u2014the user can influence the principles, but not entirely directly; No\u2014the user has no control over the principles. Regarding when this manipulation occurs, we identified two possible phases: during configuration or during the creative process. 5 RESULTS In this section, we present the results obtained from the evaluation, both in terms of general information and human-AI collaboration, considering the three defined dimensions. Regarding general information, Figure 2 shows the distribution of articles over the years. A significant peak occurs in 2020, with a progressive increase since 2015. Despite a slight decline after 2021, the number of articles remains relatively constant until 2024. This trend reflects ongoing interest in creative systems applied to Graphic Design.  \n Figure 3: Frequency of papers per year. 5.1 Understanding Collaboration Panorama  To understand collaboration, we identified relevant evaluation parameters in human-computer interaction, including the collaboration scenario and interaction modalities.  \n\n8 5.1.1 Collaboration scenario The corpus analysis reveals that human-computer collaboration scenarios are often limited and unbalanced. As shown in Figure 4, many of the analysed articles fall into the \u201cPartial\u201d collaboration category [5, 39, 40, 102], indicating that many systems still assign computational systems a predominantly supportive role. This is the case with the Evotype [57], a generative tool that creates glyphs exploring the boundaries between legibility and expressiveness. The system receives as input an SVG file containing a set of shapes used to develop the glyphs. The tool acts as an aid system with a partial level of collaboration. Although a substantial number of articles are categorised as \u201cMid\u201d [7, 34, 42], or \u201cSignificant\u201d [22, 43, 87] collaboration, these remain fewer than those in the \u2018Partial\u2019 category. This suggests that many state-of-the-art systems do not yet promote a truly balanced human-computer partnership. This imbalance is further reflected when considering control during the creative process. Only 18 (26.47%) cases demonstrate equal levels of control between the user and the system, while in most cases (50, 73.53%), the user retains final decision-making authority [12, 16, 61].  \n Figure 4: Frequency of papers per collaboration amount.  Another key factor is explainability. Although a few projects (8, 11.76%) try to implement strategies to mitigate explainability barriers [7, 29, 41], the vast majority (60, 88.23%) do not [9, 62, 95]. The Creative PenPal [31], a co-creative system that presents sketches to inspire users during design tasks, attempts to explain which object informed its suggestions to improve collaboration. Similarly, MagicBrush [91], a symbol-based Chinese painting system designed for beginners, provides explanations of the meanings of the selected elements. These systems only provide brief explanations of the contribution, not the process that led to it.  Most systems feature divided tasks with turn-taking [13, 15, 91] (see Figure 5), meaning that the human and the computer contribute at different times. Collaborative work on the same task is poorly represented [16, 28, 60]. For instance, VLT [86], a system for transferring layouts using vector graphics, enables both parties to share the same task and level of control, allowing mutual modification of contributions.  \n\n9  Figure 5: Frequency of papers by task division, confronted with participation style. 5.1.2 Interaction model As mentioned before, the dominant participation style is turn-taking (61, 89.71%), with fewer examples of parallel interaction (8, 11.76%), and one case allowing for both styles of participation [61]. The robotic arm developed by Lim et al. [47] is an example of a system that allows parallel processes. The robotic arm enables both parties to draw independently yet toward a shared goal. The analysis of initial interaction types (see Figure 6) shows that users most often lead initial interactions with the \u201cDefine parameters/context and ask\u201d type [34, 74, 85], followed by \u201cDefine and Ask\u201d [13, 95, 97]. In PatternPursuit [71], the user defines a theme, and the system selects images that will be worked on within that scope. In VisiFit [10], the user defines all the components, specifically the images they want to use, and the system generates a solution.   \n Figure 6: Frequency of papers per initial interaction type.  For ongoing interaction (see Figure 7), the most frequent type is \u201cAsk and can accept/refuse\u201d [15, 17, 21], in which users maintain control over the computer's contribution but do not necessarily influence the process. Cheng et al. [9] proposed a system that enables users to edit images interactively through a sequence of interactions, allowing them to explain their goals to the system. Cases where the user can negotiate or modify the system's contributions within the process are the second least frequent scenario [7, 22, 91], indicating that there is still room for the development of more collaborative and adaptive systems. ICONATE [100] addressed this by allowing users to directly modify the system's contribution, thereby generating more solutions in response to that change.  \n\n10  Figure 7: Frequency of papers per continued interaction type. 5.2 Understanding Exploration This analysis reveals how creative systems are used to support activities within the graphic design discipline, particularly in terms of input/output modalities and coverage in supporting the design process. 5.2.1 Target audience and activities Most systems were developed for Graphic Design specialists [12, 33, 34] (42, 61.76%), with 16 (23.53%) articles not specifying the target audience [32, 37, 43] and 10 (14.71%) developed for novices [6, 48, 91]. This suggests that many existing tools require prior knowledge of design practice, possibly because they include features that presuppose technical expertise. Warner [86] states that designers often draw on existing compositions, adapting them to their design. In this sense, the system he developed enables layout transfer by extracting the rules defined in the input layout and applying them to a given output design. This method also allows iterative refinement and negotiation of the rules defined in a multimodal way, from automatic rule standardisation to manual manipulation of elements. This approach allows the designer to directly apply various rules within the layout design principle, but requires some knowledge of their application.  As Figure 8 illustrates, most systems focus on exploring Graphic Elements/Artefacts [24, 27, 37]. This may indicate that the research landscape focuses more on generating individual graphic components that can be used in various contexts rather than on the application of these elements in specific contexts, such as advertising [22, 83] or branding [28, 101].   \n Figure 8: Frequency of papers per Graphic Design activity. 5.2.2 Interaction Modalities In terms of interaction modalities explored in the corpus (see Figure 9), the visual input alone (27, 39.71%) [13, 19, 48] and the combination of visual + textual (22, 32.35%) [46, 60, 79] are the most common. Textual input (14, 20.59%) [11, \n\n11 15, 41] also appears frequently. Likewise, visual channels are the most used for communication (52, 76.47%). This reflects the inherently visual nature of the Graphic Design discipline.   \n Figure 9: Frequency of papers per input modality.  All systems analysed employed visual exploration (68, 100%), and a smaller subset also supported conceptual exploration (29, 42.65%) [29, 43, 78]. This suggests interest in integrating semantic associations into AI-supported creativity, mirroring humans' creative reasoning.  Another relevant aspect concerns the type of results created by these systems. The data show a clear emphasis on traditional graphic artefacts, such as posters (16, 23.53%) [52, 87, 101], followed by images (12, 23.53%) [9, 24, 92] and sketches (12, 23.53%) [31, 47, 48]. Other outputs, such as typefaces (7, 10.29%) [84, 96, 102], mood boards (5, 7.35%) [12, 23, 41], and pictograms (4, 5.88%) [15, 37, 79], among others, also appear, but less frequently. There is also a small group of systems that focus on generating evaluation metrics (4, 5.88%) [81, 94, 97] or visual heat maps (3, 4.41%) [4, 19, 99], which tend to be focused on evaluating Graphic Design artefacts. 5.2.3 Design process and creative exploration To understand the general panorama of the systems regarding the design process, we analysed the methodology applied to the collaborative processes conducted. Given this, we observed that only nine (13.24%) [12, 34, 71] systems have an explicit design process, while 59 (86.76%) [16, 26, 82] work with an implicit methodology. This indicates that most systems do not clearly separate the convergent and divergent phases of the process, making it difficult for the user to realise which phase they are in. Nonetheless, 39 (57.35%) systems support both convergent and divergent thinking [22, 61, 100], 16 (23.53%) support only convergent thinking [5, 62, 93], and three (4.41%) support only divergent thinking [16, 29, 41]. As Figure 10 shows, the corpus analysed focuses more on the development of systems that support the Creation phase [9, 28, 86], followed by the Ideation phase [8, 16, 60]. These stages often explore all three types of creativity defined by Boden [3], whereas the Background Research [29, 42, 71] and Evaluation [19, 81, 94] phases mainly support combinatorial and exploratory creativity. We can conclude that Transformational creativity is little explored, indicating that systems do not promote changes in the conceptual space during the creative process, acting as provocative partners.  \n\n12  Figure 10: Frequency of papers confronting the design process phase and the creative type explored. 5.3 Understanding Graphic Design Principles  This dimension analyses how design principles are applied in systems and the user\u2019s capacity to engage with them. One of the aspects assessed was which of the selected principles are most used and how these systems allow the user to explore them directly.  5.3.1 Design Principles The Graphic Design systems should function based on their fundamental principles to facilitate a graphic communication channel between humans and computers. As Figure 11 shows, the most worked on principle is Layout (37, 54.41%) [22, 52, 61], followed by Colour (34, 50%) [21, 40, 93] and Form+Space (32, 47.06%) [31, 37, 74]. These principles are mostly worked on implicitly (51, 75%) [8, 57, 99] and less explicitly (17, 25%) [22, 28, 86], and are worked on during the creative process (43, 63.24%) [16, 80, 91], rather than just in the final result (26, 38.24%) [37, 39, 99]. CreativeConnect [12] is an example of a system that operates on principles implicitly during the creative process. This system supports graphic designers in the ideation process by extracting elements from reference images. This way, the user can prioritise what the system should consider in the selected images, whether Layout or Form. With this information, the system generates sketches that follow the references placed on the mood board by the user and the respective requirements.  \n Figure 11: Frequency of papers per Graphic Principle worked on. 5.3.2 User Manipulation  Figure 12 reveals that in systems which apply design principles implicitly, users typically lack direct control and can only indirectly influence the rules applied [5, 19, 74]. Figure 13 shows when such influence occurs. In the case of direct manipulation, this occurs above all during the creative process [21, 43, 86]. When users can only influence indirectly, they can do so both during setup [39, 56, 82] and during the process [7, 29, 61]. There is a system that allows indirect \n\n13 manipulation during configuration and explicit manipulation during the process [22]. Vinci [22] is a collaborative system for generating advertising posters. In this sense, users can provide images of a product and its slogans and only choose the working elements, but cannot manipulate them. The system generates results based on existing posters, and users can then edit the poster and update the generated results to reflect their design preferences, thereby manipulating the design principles explicitly.  \n Figure 12: Frequency of papers by clarity of Design Principles, confronted with direct user manipulation.  \n Figure 13: Frequency of papers by direct manipulation, confronted with the phase it occurs.  More systems do not consider (28, 41.18%) [11, 47, 93] or hardly consider (11, 16.18%) [7, 37, 91] how users want to work with each principle than those that do so implicitly (24, 35.29%) [15, 34, 74] or clearly (5, 7.35%) [22, 86, 87]. Despite being based on design principles, they tend not to learn from the user's choices. When they do, it is implicitly, with no clear cause-and-effect relationship or negotiation on how principles are applied. MetaMap [34] is an example of a system that implicitly aligns its contributions with user preferences regarding the principles. The convergence is not transparent, and the user lacks direct control, making it difficult to express their desired manipulation of the principles clearly. By cross-referencing these values with the collaborative amount (see Figure 14), we see that systems with higher collaboration levels enable more balanced and diverse application of design principles [22, 28, 101]. This indicates that more robust systems in terms of collaborative scenarios not only increase the complexity of the interaction model but also manage to work more extensively with the different design principles. For instance, OptiMuse [101], a system for developing slides, allows users to input commands and engage in rule-based conversations to generate alternatives until their goals are met. This system works on all five principles selected. \n\n14  \n Figure 14: Design principles addressed according to the amount of collaboration.  Despite addressing design principles, users are often unable to manipulate them directly. Systems with high collaboration levels more frequently allow users to manipulate the design principles consciously (see Figure 15). Reframer [44], is a human-IA design system that supports an iterative and real-time process, and users can manipulate all the principles worked out by the system. At the \u201cPartial\u201d and \u201cMid\u201d levels, most interactions fall under \u201cNo\u201d or \u201cNot exactly\u201d categories, revealing limited users\u2019 autonomy. At the \u2018None\u2019 level, manipulation does not occur, indicating an almost total absence of practical influence on the fundamentals of Graphic Design. Figure 16 complements this conclusion by analysing when the user can influence the principles. It shows that greater human-computer collaboration tends to shift the user\u2019s influence from the setup phase to the creative process itself.  \n Figure 15: User's direct manipulation of design principles according to the amount of collaboration.  \n\n15  Figure 16: User's direct manipulation of design principles phases according to the amount of collaboration.  This data suggests a correlation between collaboration level and balanced manipulation throughout the design process. This reinforces the importance of designing systems that enable active, ongoing, and meaningful collaboration to enhance both computational and user creative potential in the design context. 6 DISCUSSION The works presented in Section 5 represent a fair sample of co-creative system approaches applied to Graphic Design. Based on the analysis, several research opportunities were identified:  \u2022 Unbalanced collaboration: Systems often lack a balanced collaboration process, where human and computer share equal control. There is a lack of flexibility in task division, as tasks are predefined and cannot be altered. This lack of negotiation can influence the success of the process, as different users may have different needs in different aspects of the design process.  \u2022 Unbalanced initiative: Initiatives tend to be unbalanced, with the computer often acting when prompted. In contrast to human-human collaboration, we realise that what would be expected is for the two to intervene either equally or at least without needing to be called upon [18].  \u2022 Limited explainability: Few systems employ techniques that promote explainability. Although some systems include strategies, these remain insufficient, given the general panorama of the area. Design is highly subjective, making explainability fundamental in collaboration, as understanding our colleagues' reasoning helps us agree with it or improve decisions. In design, the concept gives strength to design decisions and, without an explanation, these can go unnoticed. \u2022 Partial support for the design process: Most systems focus on specific phases, mainly ideation and creation, neglecting the design process as a whole, which extends from initial research to perfecting the proposed solution. There are a few systems that promote this collaboration throughout the process, reflecting a fragmented view of the creative process. This view limits the potential for collaboration and the development of coherent solutions. \u2022 Narrow creativity types: Systems tend to support combinatorial and exploratory creativity, rarely promoting transformational creativity. Most operate within defined conceptual spaces with limited capacity for reframing them. This contradicts the potential for collaboration in a scenario where the user could take \n\n16 advantage of conceptual changes and not work with a partner who tends to limit themselves to exploring a previously defined space. \u2022 Unbalanced control over design principles: In many systems, users cannot directly manipulate Graphic Design principles, which are instead controlled by the system. Furthermore, when the user can manipulate, few systems clearly consider their options when developing the artefact. This unbalanced control jeopardises both the user's expressiveness and the positive use of collaboration. \u2022 Lack of relation among principles: While some systems address one or two principles, considering the classification we have chosen, few integrate three or more in an interrelated manner. This limits their ability to produce cohesive visual compositions where principles influence each other. Systems that integrate and relate all principles could better mirror real-world design processes and foster effective human-computer collaboration. We recommend that developers of co-creative Graphic Design tools consider these shortcomings. The GRAPHIC can become a valuable evaluative tool for assessing such systems and promoting the advancement of the state-of-the-art. This is not closed, and dimensions other than those discussed here can be added, for instance, the usability of systems. An interface that supports the interaction models mentioned in a user-friendly way is essential for successful human-computer collaboration.  6.1 Limitations One limitation of this research is that the systematic review is based solely on the Google Scholar database. However, given the interdisciplinary nature of Graphic Design and co-creative systems, Google Scholar was selected for its broad indexing capabilities, which helped to capture relevant publications from diverse academic communities that might have been missed in more domain-specific databases. 7 CONCLUSION This article presents a systematic review of 68 systems published between 2007 and 2024 on human-computer collaboration in the field of Graphic Design. This review fills a significant gap in understanding how computational systems should effectively support Graphic Design processes. The analysis identifies research opportunities, including the need to balance collaboration and initiative on both sides, improve communication between designers and AI through explainable approaches, and develop systems that focus more on transformational creativity and are based on established design principles. Another opportunity identified was to develop systems that support the design process continuously, from the exploration phase to the finalisation of the artefact.  This literature review provides a framework to promote the development of more effective computational systems in the design field, fostering creative, flexible, and realistic collaboration between humans and computers. The framework can serve as a valuable evaluation tool for advancing the state of the art and can be expanded with complementary dimensions, such as usability, which also plays a crucial role in the success of human-computer interaction. ACKNOWLEDGMENTS This work is funded by national funds through FCT \u2013 Foundation for Science and Technology, I.P., within the scope of the research unit UID/00326\u2014Centre for Informatics and Systems of the University of Coimbra. It is also funded by European Regional Development Fund (FEDER), through the Central Regional Program (Centro2030), Portugal 2030 and the European Union | Operation No 17372, Funds Branch Operation Code CENTRO2030-FEDER-01186800.  \n17 REFERENCES [1] Ashraf Abdul, Jo Vermeulen, Danding Wang, Brian Y. Lim, and Mohan Kankanhalli. 2018. Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI \u201918), April 21, 2018. Association for Computing Machinery, New York, NY, USA, 1\u201318. https://doi.org/10.1145/3173574.3174156 [2] Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N. Bennett, Kori Inkpen, Jaime Teevan, Ruth Kikin-Gil, and Eric Horvitz. 2019. Guidelines for Human-AI Interaction. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, May 02, 2019. ACM, Glasgow Scotland Uk, 1\u201313. https://doi.org/10.1145/3290605.3300233 [3] Margaret A Boden. 2004. The Creative Mind: Myths and Mechanisms. Routledge. [4] Zoya Bylinskii, Nam Wook Kim, Peter O\u2019Donovan, Sami Alsheikh, Spandan Madan, Hanspeter Pfister, Fredo Durand, Bryan Russell, and Aaron Hertzmann. 2017. Learning Visual Importance for Graphic Designs and Data Visualizations. In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology (UIST \u201917), October 20, 2017. Association for Computing Machinery, New York, NY, USA, 57\u201369. https://doi.org/10.1145/3126594.3126653 [5] Alice Cai, Steven R Rick, Jennifer L Heyman, Yanxia Zhang, Alexandre Filipowicz, Matthew Hong, Matt Klenk, and Thomas Malone. 2023. DesignAID: Using Generative AI and Semantic Diversity for Design Inspiration. In Proceedings of The ACM Collective Intelligence Conference, November 06, 2023. ACM, Delft Netherlands, 1\u201311. https://doi.org/10.1145/3582269.3615596 [6] Neill D. F. Campbell and Jan Kautz. 2014. Learning a manifold of fonts. ACM Trans. Graph. 33, 4 (July 2014), 91:1-91:11. https://doi.org/10.1145/2601097.2601212 [7] Chen Chen, Cuong Nguyen, Thibault Groueix, Vladimir G. Kim, and Nadir Weibel. 2024. MemoVis: A GenAI-Powered Tool for Creating Companion Reference Images for 3D Design Feedback. ACM Trans. Comput.-Hum. Interact. 31, 5 (November 2024), 67:1-67:41. https://doi.org/10.1145/3694681 [8] Liuqing Chen, Pan Wang, Hao Dong, Feng Shi, Ji Han, Yike Guo, Peter R.N. Childs, Jun Xiao, and Chao Wu. 2019. An artificial intelligence based data-driven approach for design ideation. J. Vis. Commun. Image Represent. 61, (May 2019), 10\u201322. https://doi.org/10.1016/j.jvcir.2019.02.009 [9] Yu Cheng, Zhe Gan, Yitong Li, Jingjing Liu, and Jianfeng Gao. 2020. Sequential Attention GAN for Interactive Image Editing. https://doi.org/10.48550/arXiv.1812.08352 [10] Lydia B Chilton, Ecenaz Jen Ozmen, Sam H Ross, and Vivian Liu. 2021. VisiFit: Structuring Iterative Improvement for Novice Designers. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI \u201921), May 07, 2021. Association for Computing Machinery, New York, NY, USA, 1\u201314. https://doi.org/10.1145/3411764.3445089 [11] Lydia B. Chilton, Savvas Petridis, and Maneesh Agrawala. 2019. VisiBlends: A Flexible Workflow for Visual Blends. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI \u201919), May 02, 2019. Association for Computing Machinery, New York, NY, USA, 1\u201314. https://doi.org/10.1145/3290605.3300402 [12] DaEun Choi, Sumin Hong, Jeongeon Park, John Joon Young Chung, and Juho Kim. 2024. CreativeConnect: Supporting Reference Recombination for Graphic Design Ideation with Generative AI. In Proceedings of the CHI Conference on Human Factors in Computing Systems, May 11, 2024. ACM, Honolulu HI USA, 1\u201325. https://doi.org/10.1145/3613904.3642794 [13] David Chuan-En Li and Nikolas Martelaro. 2021. Learning Personal Style from Few Examples. In Proceedings of the 2021 ACM Designing Interactive Systems Conference, 2021. . Retrieved February 26, 2025 from https://dl.acm.org/doi/abs/10.1145/3461778.3462115 [14] Gabriele Cimolino and T.C. Nicholas Graham. 2022. Two Heads Are Better Than One: A Dimension Space for Unifying Human and Artificial Intelligence in Shared Control. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (CHI \u201922), April 28, 2022. Association for Computing Machinery, New York, NY, USA, 1\u201321. https://doi.org/10.1145/3491102.3517610 [15] Jo\u00e3o M. Cunha, Nuno Louren\u00e7o, Pedro Martins, and Penousal Machado. 2020. Visual Blending for Concept Representation: A Case Study on Emoji Generation. New Gener. Comput. 38, 4 (November 2020), 739\u2013771. https://doi.org/10.1007/s00354-020-00107-x [16] Nicholas Davis, Chih Pin Hsiao, Kunwar Yashraj Singh, Lisa Li, Sanat Moningi, and Brian Magerko. 2015. Drawing apprentice: An enactive co-creative agent for artistic collaboration. In Proceedings of the 2015 ACM SIGCHI Conference on Creativity and Cognition, June 22, 2015. Association for Computing Machinery, Inc, 185\u2013186. https://doi.org/10.1145/2757226.2764555 [17] Iram Fatima. 2023. Designing with AI\u2009: A User Study to Explore the Future Role of AI as a Collaborative Tool in Graphics Design. Retrieved October 18, 2024 from https://urn.kb.se/resolve?urn=urn:nbn:se:sh:diva-52673 [18] Luke Feast. 2012. Professional perspectives on collaborative design work. CoDesign 8, (December 2012). https://doi.org/10.1080/15710882.2012.734828 [19] Camilo Fosco, Vincent Casser, Amish Kumar Bedi, Peter O\u2019Donovan, Aaron Hertzmann, and Zoya Bylinskii. 2020. Predicting Visual Importance Across Graphic Design Types. In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology, October 20, 2020. ACM, Virtual Event USA, 249\u2013260. https://doi.org/10.1145/3379337.3415825 [20] Jonas Frich, Lindsay Macdonald Vermeulen, Christian Remy, Michael Mose Biskjaer, and Peter Dalsgaard. 2019. Mapping the landscape of creativity support tools in HCI. dl.acm.org 18, (May 2019). https://doi.org/10.1145/3290605.3300619 [21] Amrita Ganguly, Chuan Yan, John Joon Young Chung, Tong Steven Sun, Yoon Kiheon, Yotam Gingold, and Sungsoo Ray Hong. 2024. ShadowMagic: Designing Human-AI Collaborative Support for Comic Professionals\u2019 Shadowing. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology (UIST \u201924), October 11, 2024. Association for Computing Machinery, New York, NY, USA, 1\u201315. https://doi.org/10.1145/3654777.3676332 [22] Shunan Guo, Zhuochen Jin, Fuling Sun, Jingwen Li, Zhaorui Li, Yang Shi, and Nan Cao. 2021. Vinci: An Intelligent Graphic Design System for Generating Advertising Posters. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, May 06, 2021. ACM, Yokohama Japan, 1\u201317. https://doi.org/10.1145/3411764.3445117 [23] Lena Hegemann. 2020. Reciprocal Explanations: An Explanation Technique for Human-AI Partnership in Design Ideation. (2020). Retrieved July 26, 2023 from https://www.diva-portal.org/smash/record.jsf?pid=diva2:1468469 [24] Yenkai Huang. 2024. Latent Auto-recursive Composition Engine. Dartm. Coll. Master\u2019s Theses (May 2024). Retrieved from https://digitalcommons.dartmouth.edu/masters_theses/131 [25] Angel Hsing-Chi Hwang. 2022. Too Late to be Creative? AI-Empowered Tools in Creative Processes. In CHI Conference on Human Factors in Computing Systems Extended Abstracts, April 27, 2022. ACM, New Orleans LA USA, 1\u20139. https://doi.org/10.1145/3491101.3503549 [26] Ali Jahanian, Shaiyan Keshvari, S. V. N. Vishwanathan, and Jan P. Allebach. 2017. Colors -- Messengers of Concepts: Visual Design Mining for Learning Color Semantics. ACM Trans. Comput.-Hum. Interact. 24, 1 (January 2017), 2:1-2:39. https://doi.org/10.1145/3009924 \n18 [27] Ali Jahanian, Jerry Liu, Qian Lin, Daniel Tretter, Eamonn O\u2019Brien-Strain, Seungyon Claire Lee, Nic Lyons, and Jan Allebach. 2013. Recommendation System for Automatic Design of Magazine Covers. In Proceedings of the 2013 international conference on Intelligent user interfaces, March 19, 2013. ACM, Santa Monica California USA, 95\u2013106. https://doi.org/10.1145/2449396.2449411 [28] Seoyeon Jang, Yunwoo Jeong, Heekyoung Jung, and Tek-Jin Nam. 2023. Coherent visual design through attribute-specific feedback: a hybrid approach to intelligent design agents. Digit. Creat. 34, 4 (October 2023), 311\u2013330. https://doi.org/10.1080/14626268.2023.2301355 [29] Janin Koch, Andr\u00e9s Lucero, Lena Hegemann, and Antti Oulasvirta. 2019. May AI? Design ideation with cooperative contextual bandits. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, May 02, 2019. ACM, 12\u20132019. https://doi.org/10.1145/3290605.3300863 [30] Jeba Rezwana and Mary Lou Maher. 2021. COFI: A Framework for Modeling Interaction in Human-AI Co-Creative Systems. In Proceedings of the 12th International Conference on Computational Creativity, 2021. Association for Computational Creativity (ACC), 276\u2013283. [31] Jeba Rezwana, Mary Lou Maher, and Nicholas Davis. 2021. Creative PenPal: A Virtual Embodied Conversational AI Agent to Improve User Engagement and Collaborative Experience in Human-AI Co-Creative Design. Jt. Proc. ACM IUI 2021 Workshop Coll. Stn. USA (2021). [32] Shiqi Jiang, Chenhui Li, and Changbo Wang. 2021. CoPaint: Guiding Sketch Painting with Consistent Color and Coherent Generative Adversarial Networks. In Advances in Computer Graphics, 2021. Springer International Publishing, Cham, 229\u2013241. https://doi.org/10.1007/978-3-030-89029-2_18 [33] Florian Kadner, Yannik Keller, and Constantin Rothkopf. 2021. AdaptiFont: Increasing Individuals\u2019 Reading Speed with a Generative Font Model and Bayesian Optimization. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI \u201921), May 07, 2021. Association for Computing Machinery, New York, NY, USA, 1\u201311. https://doi.org/10.1145/3411764.3445140 [34] Youwen Kang, Zhida Sun, Sitong Wang, Zeyu Huang, Ziming Wu, and Xiaojuan Ma. 2021. MetaMap: Supporting Visual Metaphor Ideation through Multi-dimensional Example-based Exploration. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI \u201921), May 07, 2021. Association for Computing Machinery, New York, NY, USA, 1\u201315. https://doi.org/10.1145/3411764.3445325 [35] Anna Kantosalo. 2019. Human-Computer Co-Creativity \u2014 Designing, Evaluating and Modelling Computational Collaborators for Poetry Writing. Retrieved from http://cs.helsinki.fi/ [36] Anna Kantosalo and Hannu Toivonen. 2016. Modes for creative human-computer collaboration: Alternating and task-divided co-creativity. In Proceedings of the Seventh International Conference on Computational Creativity, 2016. 77\u201384. [37] Takuro Karamatsu, Gibran Benitez-Garcia, Keiji Yanai, and Seiichi Uchida. 2020. Iconify: Converting Photographs into Icons. In Proceedings of the 2020 Joint Workshop on Multimedia Artworks Analysis and Attractiveness Computing in Multimedia, June 08, 2020. 7\u201312. https://doi.org/10.1145/3379173.3393708 [38] Pegah Karimi, Kazjon Grace, Mary Lou Maher, and Nicholas Davis. 2018. Evaluating Creativity in Computational Co-Creative Systems. (July 2018). Retrieved from http://arxiv.org/abs/1807.09886 [39] Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, and Kota Yamaguchi. 2021. Constrained Graphic Layout Generation via Latent Optimization. In Proceedings of the 29th ACM International Conference on Multimedia (MM \u201921), October 17, 2021. Association for Computing Machinery, New York, NY, USA, 88\u201396. https://doi.org/10.1145/3474085.3475497 [40] EunJin Kim and Hyeon-Jeong Suk. 2016. Key Color Generation for Affective Multimedia Production: An Initial Method and Its Application. In Proceedings of the 24th ACM international conference on Multimedia, 2016. . Retrieved February 26, 2025 from https://dl.acm.org/doi/abs/10.1145/2964284.2964323 [41] Janin Koch, Nicolas Taffin, Michel Beaudouin-Lafon, Markku Laine, Andr\u00e9s Lucero, and Wendy E. MacKay. 2020. ImageSense: An Intelligent Collaborative Ideation Tool to Support Diverse Human-Computer Partnerships. Proc. ACM Hum.-Comput. Interact. 4, CSCW1 (May 2020). https://doi.org/10.1145/3392850 [42] Janin Koch, Nicolas Taffin, Andr\u00e9s Lucero, and Wendy E. Mackay. 2020. SemanticCollage: Enriching Digital Mood Board Design with Semantic Labels. In Proceedings of the 2020 ACM Designing Interactive Systems Conference (DIS \u201920), July 03, 2020. Association for Computing Machinery, New York, NY, USA, 407\u2013418. https://doi.org/10.1145/3357236.3395494 [43] Tomas Lawton, Kazjon Grace, and Francisco J Ibarrola. 2023. When is a Tool a Tool? User Perceptions of System Agency in Human\u2013AI Co-Creative Drawing. In Proceedings of the 2023 ACM Designing Interactive Systems Conference, July 10, 2023. ACM, Pittsburgh PA USA, 1978\u20131996. https://doi.org/10.1145/3563657.3595977 [44] Tomas Lawton, Francisco J Ibarrola, Dan Ventura, and Kazjon Grace. 2023. Drawing with Reframer: Emergence and Control in Co-Creative AI. In Proceedings of the 28th International Conference on Intelligent User Interfaces, 2023. 264\u2013277. [45] Christian Leborg. 2013. Gramatica Visual. Editorial GG. [46] Jianan Li, Jimei Yang, Jianming Zhang, Chang Liu, Christina Wang, and Tingfa Xu. 2021. Attribute-Conditioned Layout GAN for Automatic Graphic Design. IEEE Trans. Vis. Comput. Graph. 27, 10 (October 2021), 4039\u20134048. https://doi.org/10.1109/TVCG.2020.2999335 [47] Jeongki Lim, Teemu Leinonen, Lasse Lipponen, Henry Lee, Julienne DeVita, and Dakota Murray. 2023. Artificial intelligence as relational artifacts in creative learning. Digit. Creat. 34, 3 (July 2023), 192\u2013210. https://doi.org/10.1080/14626268.2023.2236595 [48] Yuyu Lin, Jiahao Guo, Yang Chen, Cheng Yao, and Fangtian Ying. 2020. It Is Your Turn: Collaborative Ideation With a Co-Creative Robot through Sketch. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, April 21, 2020. ACM, Honolulu HI USA, 1\u201314. https://doi.org/10.1145/3313831.3376258 [49] Xuanhui Liu, Alexander Maedche, and Simin He. 2019. Designing an AI-based Advisory Platform for Design Techniques.  [50] Zhuohua Liu, Bin Yang, Jingrui An, and Caijuan Huang. 2023. Similarity evaluation of graphic design based on deep visual saliency features. J. Supercomput. 79, 18 (December 2023), 21346\u201321367. https://doi.org/10.1007/s11227-023-05468-w [51] Maria Teresa Llano, Mark d\u2019Inverno, Matthew Yee-King, Jon McCormack, Alon Ilsar, Alison Pease, and Simon Colton. 2020. Explainable Computational Creativity. In Proceedings of the 11th International Conference on Computational Creativity, 2020. Association for Computational Creativity (ACC), 334\u2013341. [52] Daniel Lopes, Jo\u00e3o Correia, and Penousal Machado. 2022. EvoDesigner: Towards Aiding Creativity in Graphic Design. In Artificial Intelligence in Music, Sound, Art and Design, 2022. Springer International Publishing, Cham, 162\u2013178. https://doi.org/10.1007/978-3-031-03789-4_11 [53] Daniel Lopes, Jo\u00e3o Correia, and Penousal Machado. 2022. EvoDesigner: Evolving Poster Layouts. Entropy 24, 12 (December 2022), 1751. https://doi.org/10.3390/e24121751 [54] Ryan Louie, Andy Coenen, Cheng Zhi Huang, Michael Terry, and Carrie J Cai. 2020. Novice-AI music co-creation via AI-steering tools for deep generative models. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI \u201920), April 21, 2020. ACM, Honolulu, HI, USA, 1\u201313. https://doi.org/10.1145/3313831.3376739 [55] Ellen Lupton and Jennifer Cole Phillips. 2008. Graphic Design: The New Basics. Princeton Architectural Press. \n19 [56] Paridhi Maheshwari, Nitish Bansal, Surya Dwivedi, Rohan Kumar, Pranav Manerikar, and Balaji Vasan Srinivasan. 2019. Exemplar based experience transfer. In Proceedings of the 24th International Conference on Intelligent User Interfaces (IUI \u201919), March 17, 2019. Association for Computing Machinery, New York, NY, USA, 673\u2013680. https://doi.org/10.1145/3301275.3302300 [57] Tiago Martins, Jo\u00e3o Correia, Ernesto Costa, and Penousal Machado. 2016. Evotype: From Shapes to Glyphs. In Proceedings of the Genetic and Evolutionary Computation Conference 2016 (GECCO \u201916), July 20, 2016. Association for Computing Machinery, New York, NY, USA, 261\u2013268. https://doi.org/10.1145/2908812.2908907 [58] Caterina Moruzzi and Solange Margarido. 2024. A User-centered Framework for Human-AI Co-creativity. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, May 11, 2024. ACM, Honolulu HI USA, 1\u20139. https://doi.org/10.1145/3613905.3650929 [59] Matthew J. Page, Joanne E. McKenzie, Patrick M. Bossuyt, Isabelle Boutron, Tammy C. Hoffmann, Cynthia D. Mulrow, Larissa Shamseer, Jennifer M. Tetzlaff, Elie A. Akl, Sue E. Brennan, Roger Chou, Julie Glanville, Jeremy M. Grimshaw, Asbj\u00f8rn Hr\u00f3bjartsson, Manoj M. Lalu, Tianjing Li, Elizabeth W. Loder, Evan Mayo-Wilson, Steve McDonald, Luke A. McGuinness, Lesley A. Stewart, James Thomas, Andrea C. Tricco, Vivian A. Welch, Penny Whiting, and David Moher. 2021. The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. BMJ 372, (March 2021), n71. https://doi.org/10.1136/bmj.n71 [60] Pegah Karimi, Jeba Rezwana, Safat Siddiqui, Mary Lou Maher, and Nasrin Dehbozorgi. 2020. Creative sketching partner: an analysis of human-AI co-creativity. In Proceedings of the 25th International Conference on Intelligent User Interfaces, March 17, 2020. ACM, 221\u2013230. https://doi.org/10.1145/3377325.3377522 [61] Xiaohan Peng, Janin Koch, and Wendy E. Mackay. 2024. DesignPrompt: Using Multimodal Interaction for Design Exploration with Generative AI. In Proceedings of the 2024 ACM Designing Interactive Systems Conference (DIS \u201924), July 01, 2024. Association for Computing Machinery, New York, NY, USA, 804\u2013818. https://doi.org/10.1145/3643834.3661588 [62] H Q Phan, J Lu, Paul Asente, Antoni Bert Chan, and Hongbo Fu. 2016. Patternista: Learning Element Style Compatibility and Spatial Composition for Ring-based Layout Decoration. In In Proceedings of the Joint Symposium on Computational Aesthetics and Sketch Based Interfaces and Modeling and Non-Photorealistic Animation and Rendering (Expresive \u201916), 2016. Eurographics Association, Goslar, DEU, 79\u201388. [63] J Poon and Mary Lou Maher. 1997. Co-evolution and emergence in design. Artif. Intell. Eng. 11, 3 (July 1997), 319\u2013327. https://doi.org/10.1016/S0954-1810(96)00047-7 [64] Muhammad Raees, Inge Meijerink, Ioanna Lykourentzou, Vassilis-Javed Khan, and Konstantinos Papangelis. 2024. From explainable to interactive AI: A literature review on current trends in human-AI interaction. International Journal Human-Computer Studies. 189, (2024), 103301. https://doi.org/10.1016/j.ijhcs.2024.103301 [65] S\u00e9rgio M. Rebelo, Tiago Martins, Jo\u00e3o Bicker, and Penousal Machado. 2021. Exploring Automatic Fitness Evaluation for Evolutionary Typesetting. In Proceedings of the 13th Conference on Creativity and Cognition (C&C \u201921), June 22, 2021. Association for Computing Machinery, New York, NY, USA, 1\u20139. https://doi.org/10.1145/3450741.3465247 [66] S\u00e9rgio M. Rebelo, Mariana Sei\u00e7a, Pedro Martins, Jo\u00e3o Bicker, and Penousal Machado. 2022. ESSYS* Sharing #UC: An Emotion-driven Audiovisual Installation. In 2022 IEEE VIS Arts Program (VISAP), October 2022. 70\u201379. https://doi.org/10.1109/VISAP57411.2022.00017 [67] Elizabeth Resnick. 2003. Design for Communication: Conceptual Graphic Design Basics (1st Edition ed.). Wiley. [68] Jeba Rezwana. 2023. Towards designing engaging and ethical human-centered AI partners for human-AI co-creativity. PhD Thesis. The University of North Carolina at Charlotte. Retrieved October 18, 2024 from https://search.proquest.com/openview/c77ad36ed43f8639fcdd17ef22d382b3/1?pq-origsite=gscholar&cbl=18750&diss=y [69] Jeba Rezwana and Corey Ford. 2025. Improving User Experience with FAICO: Towards a Framework for AI Communication in Human-AI Co-Creativity. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA \u201925), April 03, 2025. ACM, Yokohama Japan, 1\u20139. https://doi.org/10.48550/arXiv.2504.02526 [70] Jeba Rezwana and Mary Lou Maher. 2023. Designing Creative AI Partners with COFI: A Framework for Modeling Interaction in Human-AI Co-Creative Systems. ACM Trans. Comput.-Hum. Interact. 30, 5 (September 2023), 67:1-67:28. https://doi.org/10.1145/3519026 [71] Joana Rovira Martins, Jo\u223c ao Miguel Cunha, Pedro Martins, and Ana Boavida. 2024. PatternPursuit: Pattern Generation using Libraries Built on Graphic Decomposition. In Proceedings of the 15th International Conference on Computational Creativity, ICCC 2024, J\u00f6nk\u00f6ping, June 17-21, 2024, 2024. Association for Computational Creativity (ACC). [72] Alexander Sage, Eirikur Agustsson, Radu Timofte, and Luc Van Gool. 2018. Logo Synthesis and Manipulation With Clustered Generative Adversarial Networks. 2018. 5879\u20135888. Retrieved March 25, 2025 from https://openaccess.thecvf.com/content_cvpr_2018/html/Sage_Logo_Synthesis_and_CVPR_2018_paper.html [73] Timothy Samara. 2014. Design Elements: A Graphic Style Manual: Under- standing the Rules and Knowing When to Break Them (2nd Edition ed.). Rockport Publishers. [74] Sara (Salevati) Feldman. 2017. Co-Creation: Human and AI Collaboration in Creative Expression. July 01, 2017. BCS Learning & Development. https://doi.org/10.14236/ewic/EVA2017.84 [75] Emma Schleiger, Claire Mason, Claire Naughtin, Andrew Reeson, and Cecile Paris. 2024. Collaborative Intelligence: A Scoping Review Of Current Applications. Appl. Artif. Intell. 38, 1 (December 2024), 2327890. https://doi.org/10.1080/08839514.2024.2327890 [76] Jingyu Shi, Rahul Jain, Hyungjun Doh, Ryo Suzuki, and Karthik Ramani. 2024. An HCI-Centric Survey and Taxonomy of Human-Generative-AI Interactions. https://doi.org/10.48550/arXiv.2310.07127 [77] Yang Shi, Tian Gao, Xiaohan Jiao, and Nan Cao. 2023. Understanding Design Collaboration Between Designers and Artificial Intelligence: A Systematic Literature Review. Proc. ACM Hum.-Comput. Interact. 7, CSCW2 (September 2023), 1\u201335. https://doi.org/10.1145/3610217 [78] Kihoon Son, DaEun Choi, Tae Soo Kim, Young-Ho Kim, and Juho Kim. 2024. GenQuery: Supporting Expressive Visual Search with Generative Models. In Proceedings of the CHI Conference on Human Factors in Computing Systems, May 11, 2024. ACM, Honolulu HI USA, 1\u201319. https://doi.org/10.1145/3613904.3642847 [79] Tsai-Ho Sun, Chien-Hsun Lai, Sai-Keung Wong, and Yu-Shuen Wang. 2019. Adversarial Colorization of Icons Based on Contour and Color Conditions. In Proceedings of the 27th ACM International Conference on Multimedia (MM \u201919), October 15, 2019. Association for Computing Machinery, New York, NY, USA, 683\u2013691. https://doi.org/10.1145/3343031.3351041 [80] Michihiko Ueno and Shin\u2019ichi Satoh. 2021. Continuous and Gradual Style Changes of Graphic Designs with Generative Model. In Proceedings of the 26th International Conference on Intelligent User Interfaces (IUI \u201921), April 14, 2021. Association for Computing Machinery, New York, NY, USA, 280\u2013289. https://doi.org/10.1145/3397481.3450666 [81] Johannes Wachs, B\u00e1lint Dar\u00f3czy, Aniko Hannak, Katinka P\u00e1ll, and Christoph Riedl. 2018. And Now for Something Completely Different: Visual Novelty in an Online Network of Designers. In Proceedings of the 10th ACM Conference on Web Science (WebSci \u201918), May 15, 2018. Association for Computing Machinery, New York, NY, USA, 163\u2013172. https://doi.org/10.1145/3201064.3201088 \n20 [82] Baoyuan Wang, Yizhou Yu, Tien-Tsin Wong, Chun Chen, and Ying-Qing Xu. 2010. Data-driven image color theme enhancement. ACM Trans. Graph. 29, 6 (December 2010), 146:1-146:10. https://doi.org/10.1145/1882261.1866172 [83] Guolong Wang, Zheng Qin, Junchi Yan, and Liu Jiang. 2020. Learning to Select Elements for Graphic Design. In Proceedings of the 2020 International Conference on Multimedia Retrieval, June 08, 2020. ACM, Dublin Ireland, 91\u201399. https://doi.org/10.1145/3372278.3390678 [84] Yizhi Wang, Yue Gao, and Zhouhui Lian. 2020. Attribute2Font: creating fonts you want from attributes. ACM Trans. Graph. 39, 4 (August 2020), 69:69:1-69:69:15. https://doi.org/10.1145/3386569.3392456 [85] Zhangyang Wang, Jianchao Yang, Hailin Jin, Eli Shechtman, Aseem Agarwala, Jonathan Brandt, and Thomas S. Huang. 2015. DeepFont: Identify Your Font from An Image. In Proceedings of the 23rd ACM international conference on Multimedia (MM \u201915), October 13, 2015. Association for Computing Machinery, New York, NY, USA, 451\u2013459. https://doi.org/10.1145/2733373.2806219 [86] Jeremy Warner. 2023. Enhancing Visual Media Through Reflection and Recomposition. Doctor of Philosophy. University of California, Berkeley. Retrieved October 21, 2024 from https://www.proquest.com/openview/48927720acfa5e39fae6021d57809819/1?pq-origsite=gscholar&cbl=18750&diss=y [87] Jeremy Warner, Kyu Won Kim, and Bjoern Hartmann. 2023. Interactive Flexible Style Transfer for Vector Graphics. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (UIST \u201923), October 29, 2023. Association for Computing Machinery, New York, NY, USA, 1\u201314. https://doi.org/10.1145/3586183.3606751 [88] Alexander White. 2002. The Elements of Graphic Design (1st Edition ed.). Allworth Press. [89] Maximiliane Windl, Sebastian S. Feger, Lara Zijlstra, Albrecht Schmidt, and Pawel W. Wozniak. 2022. \u2018It Is Not Always Discovery Time\u2019: Four Pragmatic Approaches in Designing AI Systems. In CHI Conference on Human Factors in Computing Systems, April 27, 2022. ACM, New Orleans LA USA, 1\u201312. https://doi.org/10.1145/3491102.3501943 [90] Wucius Wong. 1993. Principles of Form and Design (1st Edition ed.). Wiley. [91] Haoran Xu, Shuyao Chen, and Ying Zhang. 2023. Magical Brush: A Symbol-Based Modern Chinese Painting System for Novices. In Conference on Human Factors in Computing Systems - Proceedings, April 19, 2023. Association for Computing Machinery. https://doi.org/10.1145/3544548.3581429 [92] Qinying Xu, Daryl D\u2019Souza, and Vic Ciesielski. 2007. Evolving Images for Entertainment. (2007). [93] Chuan Yan, John Joon Young Chung, Yoon Kiheon, Yotam Gingold, Eytan Adar, and Sungsoo Ray Hong. 2022. FlatMagic: Improving Flat Colorization through AI-driven Design for Digital Comic Professionals. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (CHI \u201922), April 29, 2022. Association for Computing Machinery, New York, NY, USA, 1\u201317. https://doi.org/10.1145/3491102.3502075 [94] Bin Yang. 2021. Perceptual similarity measurement based on generative adversarial neural networks in graphics design. Appl. Soft Comput. 110, (October 2021), 107548. https://doi.org/10.1016/j.asoc.2021.107548 [95] Shuai Yang. 2020. Artistic Text Stylization for Visual-Textual Presentation Synthesis. In Proceedings of the 1st ACM International Conference on Multimedia in Asia (MMAsia \u201919), January 10, 2020. Association for Computing Machinery, New York, NY, USA, 1\u20135. https://doi.org/10.1145/3338533.3372211 [96] Zhen Zeng, Xiaohua Sun, and Xiang Liao. 2019. Artificial Intelligence Augments Design Creativity: A Typeface Family Design Experiment. In Design, User Experience, and Usability. User Experience in Advanced Technological Environments, 2019. Springer International Publishing, Cham, 400\u2013411. https://doi.org/10.1007/978-3-030-23541-3_29 [97] Jiajing Zhang, Jinhui Yu, Kang Zhang, Xianjun Sam Zheng, and Junsong Zhang. 2017. Computational Aesthetic Evaluation of Logos. ACM Trans. Appl. Percept. 14, 3 (June 2017), 20:1-20:21. https://doi.org/10.1145/3058982 [98] Yunke Zhang, Kangkang Hu, Peiran Ren, Changyuan Yang, Weiwei Xu, and Xian-Sheng Hua. 2017. Layout Style Modeling for Automating Banner Design. In Proceedings of the on Thematic Workshops of ACM Multimedia 2017 (Thematic Workshops \u201917), October 23, 2017. Association for Computing Machinery, New York, NY, USA, 451\u2013459. https://doi.org/10.1145/3126686.3126718 [99] Nanxuan Zhao, Ying Cao, and Rynson W. H. Lau. 2018. What characterizes personalities of graphic designs? ACM Trans. Graph. 37, 4 (August 2018), 1\u201315. https://doi.org/10.1145/3197517.3201355 [100] Nanxuan Zhao, Nam Wook Kim, Laura Mariah Herman, Hanspeter Pfister, Rynson W.H. Lau, Jose Echevarria, and Zoya Bylinskii. 2020. ICONATE: Automatic Compound Icon Generation and Ideation. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, April 21, 2020. ACM, Honolulu HI USA, 1\u201313. https://doi.org/10.1145/3313831.3376618 [101] Jiayi Zhou, Renzhong Li, Junxiu Tang, Tan Tang, Haotian Li, Weiwei Cui, and Yingcai Wu. 2024. Understanding Nonlinear Collaboration between Human and AI Agents: A Co-design Framework for Creative Design. In Proceedings of the CHI Conference on Human Factors in Computing Systems, May 11, 2024. ACM, Honolulu HI USA, 1\u201316. https://doi.org/10.1145/3613904.3642812 [102] Lian Zhouhui, Bo Zhao, Xudong Chen, and Jianguo Xiao. 2018. EasyFont: A Style Learning-Based System to Easily Build Your Large-Scale Handwriting Fonts. ACM Trans. Graph. Vol 38, No 1 (2018). Retrieved February 26, 2025 from https://dl.acm.org/doi/abs/10.1145/3213767  Prior Publication Statement This manuscript is original work and has not been published previously. It is not currently under consideration for publication elsewhere. This submission has no significant overlap with any other paper by the authors, either published or concurrently submitted.  \n",
    "title": "GRAPHIC--Guidelines for Reviewing Algorithmic Practices in Human-centred Design and Interaction for Creativity",
    "arxiv_id": "2511.17443v1",
    "category": "recent",
    "authors": [
      "Joana Rovira Martins",
      "Pedro Martins",
      "Ana Boavida"
    ],
    "published": "2025-11-21",
    "num_pages": 20,
    "num_chars": 68040
  },
  {
    "text": "REMSA: ANLLM AGENT FORFOUNDATIONMODELSE-\nLECTION INREMOTESENSING\nBinger Chen, Tacettin Emre B \u00a8ok, Behnood Rasti, Volker Markl & Beg \u00a8um Demir\nTechnische Universit \u00a8at Berlin & BIFOLD\nBerlin, Germany\nchen@tu-berlin.de\nABSTRACT\nFoundation Models (FMs) are increasingly integrated into remote sensing (RS) pipelines\nfor applications such as environmental monitoring, disaster assessment, and land-use map-\nping. These models include unimodal vision encoders trained in a single data modality and\nmultimodal architectures trained in multiple sensor modalities, such as synthetic aperture\nradar (SAR), multispectral, and hyperspectral imagery, or jointly in image-text pairs in\nvision-language settings. FMs are adapted to diverse tasks, such as semantic segmenta-\ntion, image classification, change detection, and visual question answering, depending on\ntheir pretraining objectives and architectural design. However, selecting the most suitable\nremote sensing foundation model (RSFM) for a specific task remains challenging due to\nscattered documentation, heterogeneous formats, and complex deployment constraints. To\naddress this, we first introduce the RSFM Database (RS-FMD), the first structured and\nschema-guided resource covering over 150 RSFMs trained using various data modalities,\nassociated with different spatial, spectral, and temporal resolutions, considering different\nlearning paradigms. Built on top ofRS-FMD, we further presentREMSA(Re mote-\nsensingM odelS electionA gent), the first LLM agent for automated RSFM selection from\nnatural language queries.REMSAcombines structured FM metadata retrieval with a task-\ndriven agentic workflow. In detail, it interprets user input, clarifies missing constraints,\nranks models via in-context learning, and provides transparent justifications. Our system\nsupports various RS tasks and data modalities, enabling personalized, reproducible, and ef-\nficient FM selection. To evaluateREMSA, we introduce a benchmark of 75 expert-verified\nRS query scenarios, resulting in900task-system-model configurations under a novel\nexpert-centered evaluation protocol.REMSAoutperforms multiple baselines, including\nnaive agent, dense retrieval, and unstructured retrieval augmented generation based LLMs,\nshowing its utility in real decision-making applications.REMSAoperates entirely on pub-\nlicly available metadata of open source RSFMs, without accessing private or sensitive data.\nOur code and data can be found here:https://github.com/be-chen/REMSA.\n1 INTRODUCTION\nWith the growing availability of remote sensing (RS) missions and their onboard sensors (e.g., Sentinel-\n2 (Drusch et al., 2012), Sentinel-1 (Torres et al., 2012), EnMAP (Guanter et al., 2015)), RS plays an increas-\ningly important role in many applications such as agriculture, disaster response, urban development, and\nbiodiversity monitoring. These applications increasingly rely on foundation models (FMs) that can gener-\nalize across various RS data modalities with different spatial, spectral and temporal resolutions, geospatial\nextents and applications, while being transferable and effective even with limited labeled data. Recently,\nnumerous FMs have emerged in the RS domain, offering powerful capabilities for interpreting complex RS\n1arXiv:2511.17442v1  [cs.CV]  21 Nov 2025\nFigure 1: Architecture ofREMSA\ndata. These models include vision-only encoders trained on single or multiple RS data modalities (e.g.,\nSatMAE (Cong et al., 2022), CROMA (Fuller et al., 2023)) and vision\u2013language models (VLMs) trained\njointly on RS data modalities and text (e.g., GRAFT (Mall et al., 2024), TEOChat (Irvin et al., 2025), Earth-\nDial (Soni et al., 2025)). These models are pretrained on large-scale RS datasets encompassing a diverse\nrange of sensor modalities, including RGB, multispectral, hyperspectral, synthetic aperture radar (SAR), and\nlight detection and ranging (LiDAR), across multiple spatial and temporal resolutions. Each FM exhibits its\nstrengths in distinct applications, such as classification, object detection, change detection, captioning, and\nvisual question answering (VQA). For instance, in practice, change detection typically relies on multitem-\nporal SAR or optical data inputs, while fine-grained land cover mapping often benefits from high-resolution\noptical imagery. This diversity brings new possibilities for multi-modal RS applications, but it also raises the\nchallenge of selecting the most suitable FM for a given task with data modality and operational constraints.\nDespite these advances, selecting an FM that is suitable for a specific RS task remains a challenge. Users\nmust balance diverse constraints such as the available data modalities and volume, geographic coverage,\ncomputational resources, and task-specific evaluation priorities. These constraints have been shown to sig-\nnificantly influence RSFM generalization and robustness (Purohit et al., 2025; Plekhanova et al., 2025). With\nhundreds of remote sensing foundation models (RSFMs) now publicly available (Guo et al., 2024; Li et al.,\n2025) and no unified structured schema to organize their properties (such as model architectures, training\ndata, or reported performance), the selection process is often manual, time-consuming, and error-prone. Ex-\nisting approaches rely on searching across scattered repositories and publications, manually parsing papers\nand model cards, and running exhaustive experiments (Ramachandran et al., 2025; Adorni et al., 2025),\nall without guaranteed reproducibility or transparency. Even public RS benchmarks (Lacoste et al., 2023;\nSimumba & et al.; Li et al., 2024) mainly compare model accuracy on fixed applications, offering little\nsupport for matching models to user-specific constraints or deployment trade-offs. This makes a unified,\nmachine-readable database (DB) of RSFMs a necessary basis for any systematic selection and automation.\nRecent advances in large language model (LLM) agent have shown the feasibility of combining language\nunderstanding, tool invocation, multi-turn interaction, and automatic structured reasoning to assist decision-\nmaking processes (Singh et al., 2024; Xiong et al., 2025; Agashe et al., 2025; Liu et al., 2025a). However,\nmost LLM agents target general-purpose question answering. To our knowledge, no prior work has devel-\noped a domain-specific agent for FM selection in operational, constraint-heavy RS scenarios. In particular,\nRS tasks involve complex trade-offs across sensors, spatial, spectral, and temporal resolutions, as well as\n2\ndata availability. Existing LLMs lack the domain knowledge and structured access to model documentation\nto address these constraints. Hence, such an agent must provide more robust and interpretable solutions.\nIn this work, we first introduce the Remote Sensing Foundation Model Database (RS-FMD), the first\nschema-guided catalog of more than 150 RSFMs, covering various data modalities, pretraining strategies,\nand benchmark results. On top ofRS-FMD, we proposeREMSA, the first LLM-based agent for automated\nFM selection in RS. As shown in Figure 1,REMSAis a modular agent that automates FM selection through\nstructured query interpretation and dynamic tool use. It extracts user intent from free text input and converts\nit into constraints. And based on the task state, the agent selectively calls tools to retrieve relevant FMs\nfromRS-FMD, rank FMs using LLM-based reasoning, interact with the user in clarification loops, and\nprovide transparent explanations. A memory mechanism further enhances accuracy and personalization. To\nevaluateREMSA, we build the first benchmark dataset of real user queries and establish an expert-driven\nevaluation protocol. We also implement a set of carefully constructed baselines, ensuring fair and mean-\ningful comparisons withREMSA.REMSAis designed to support a broad range of end-users, including RS\nscientists, machine learning researchers, and industry practitioners who need to identify suitable RSFMs for\ntheir tasks. BecauseREMSAaccepts free-text queries and incorporates structured interpretation together\nwith multi-turn clarification, it can guide even non-experts who may not be familiar with RS modalities or\nFM architectures. This makesREMSAsuitable for both exploratory use by practitioners and rigorous FM\nselection in research settings. AlthoughREMSAuses a modular agent design, our contribution is method-\nological. We treat RSFM selection as a research problem of how FMs should be compared, selected, and\ndeployed under real constraints. In summary, we make the followingcontributions:\n\u2022 We introduceRS-FMD, the first structured and schema-guided DB of over 150 RSFMs. We will release\nit as a community resource with continuous maintenance and updates.\n\u2022 We proposeREMSA, a modular LLM agent that combines structured metadata grounding, dense re-\ntrieval, in-context ranking, clarification, explanation, memory augmentation, and a task-aware orches-\ntration mechanism to support complex FM selection in real RS settings.\n\u2022 We construct thefirstbenchmark dataset and design an evaluation protocol for FM selection, encompass-\ning 75 realistic queries across various RS tasks and provide 900 evaluation results.\n2 RELATEDWORK\nFoundation Models and Model Selection.Due to the rapid emergence of RSFMs, there has been extensive\nresearch into their capabilities and benchmarks (Liu et al., 2025b; Wu et al., 2024; Pathak et al., 2025). In\nRS, recent surveys and benchmarks (Xiao et al., 2024; Ramachandran et al., 2025; Li et al., 2024) have sys-\ntematically cataloged FMs and evaluated their performance on applications such as land cover classification,\nwildfire scar segmentation, urban change detection, visual question answering, etc. However, these works\nprimarily focus on descriptive analysis or standardized evaluation, offering limited support for automated FM\nselection. Large-scale evaluations such as GEO-Bench-2 (Simumba & et al.) further highlight that RSFM\nperformance varies strongly across capability dimensions, but still do not address automatic FM selection.\nRecent work also shows that pre-training data coverage (geographic and sensor diversity) strongly affects\nRSFM generalization (Purohit et al., 2025; Plekhanova et al., 2025). While current benchmarks document\nthese properties, they do not use them to guide model choice, further motivating automated FM selection.\nAdditionally, there is a new capabilities encoding approach that estimates a model\u2019s performance on unseen\ndownstream tasks, reducing the need for exhaustive fine-tuning (Adorni et al., 2025). Although this provides\nvaluable tools for comparative evaluation, it is still a benchmarking tool that does not address end-to-end\nautomatic FM selection workflows. Moreover, previous surveys and benchmarks are static and task-specific,\nlacking a unified schema or machine-readable representation of RSFMs. In contrast, ourRS-FMDcon-\nsolidates the available FMs into a structured, extensible resource that directly supports automated retrieval,\ncomparison, and selection. Another relevant line of work is AutoML, which includes frameworks such as\n3\nAuto-WEKA (Thornton et al., 2013), Auto-sklearn (Feurer et al., 2015), and CAML (Neutatz et al., 2024).\nThey automate the selection of parameters, algorithms, or pipelines through meta-learning and optimization\ntechniques. Although these approaches show the feasibility of automating model choice in classical machine\nlearning settings, they have not been extended to the selection of FMs, particularly in the RS domain. To\nour knowledge, there is no existing autonomous method or agent that assists scientists in selecting the most\nsuitable FM for their specific constraints and applications. Our work fills this gap by combiningRS-FMD\nandREMSA, presenting the first domain-specialized agentic workflow for FM selection that automates the\nmatching of user constraints to appropriate models.\nTool-Augmented Agents in Remote Sensing.Recent developments in retrieval-augmented language mod-\nels and tool-augmented agents such as ReAct (Yao et al., 2023), HuggingGPT (Shen et al., 2023), and\nToRA (Gou et al., 2024) show the feasibility of combining LLMs with structured retrieval and external\ntool invocation for complex reasoning and planning. In RS, several works have explored modular agentic\nworkflows. GeoLLM-Squad (Lee et al., 2025) introduces a multi-agent orchestration framework that decom-\nposes geospatial tasks into specialized sub-agents, improving scalability and correctness over single-agent\nbaselines. RS-Agent (Xu et al., 2024) integrates retrieval pipelines and tool scheduling to process spatial\nquestion answering tasks, while ThinkGeo (Shabbir et al., 2025) introduces a benchmark for evaluating\nmulti-step tool-augmented agents on RS workflows. Recently, TEOChat (Irvin et al., 2025) extended large\nvision-language assistants to temporal RS data by training on instruction following datasets, supporting con-\nversational analysis of time-series data. These agents highlight the benefits of agent-based modularity and\nretrieval-augmented reasoning. However, they primarily target geospatial information extraction, change de-\ntection, or VQA applications rather than FM selection workflows. Our agent explicitly integrates a curated\nFM database with structured retrieval, agentic ranking, interactive constraint resolution, and transparent\nmodel reasoning, making it the first tool-augmented agent tailored for FM selection in RS.\n3 REMOTESENSINGFOUNDATIONMODELDATABASE(RS-FMD)\nRS-FMDis a curated DB of all RSFMs we could find (\u02dc150 RSFMs), serving as the structured knowledge\nbase behindREMSA. It enables interpretable and constraint-aware FM selection by consolidating heteroge-\nneous knowledge resources into a unified, machine-readable format. To buildRS-FMD, we conducted a\nsystematic search for RSFMs using multiple sources. We reviewed survey papers and popular FM lists, sur-\nveyed recent RS and ML venues, ran keyword searches on arXiv, and inspected linked GitHub repositories.\nSchema Design.Each record follows a schema covering properties such as identifiers, architecture, modal-\nities, and pretrained model weights, along with structured fields for pretraining datasets and benchmark\nevaluations. This schema ensures traceability, comparability, and extensibility across FMs. The full schema\nand an example record are in Appendix A.. This comprehensive schema enables our FM selection agent to\nground its reasoning in model capabilities and match models to user-defined applications and constraints. It\nalso ensures that critical properties, such as input data modalities, spatial, spectral, and temporal character-\nistics, and training configurations, can be queried and filtered in a principled and automated manner.\nAutomated database population.Populating this database requires extracting structured information from\ndiverse sources, such as papers, model cards, and repositories. Due to the scale and heterogeneity of available\nmodel documentation, fully manual curation is impractical. Therefore, we adopt an automated knowledge\nextraction approach coupled with confidence-guided human verification. Our approach is a schema-guided\nLLM extraction pipeline inspired by a general knowledge extraction approach OneKE (Luo et al., 2025), but\nsignificantly adapted to our domain and use case. Specifically, we extend their approach by introducing our\nown schema definitions, adding a dedicated confidence scoring step, and optimizing prompt design for RS\nmodel descriptions. The process is entirely automated and iterative: for each FM, we collect and input a set\nof unstructured sources, then issue multiple LLM calls to generate independent structured outputs in each\n4\niteration. Each output is validated against the schema, parsed, and aggregated. This iterative strategy allows\nus to exploit both the probabilistic uncertainty of each iteration and the self-consistency across iterations.\nFields for which the model produces divergent outputs or low log-probabilities are marked as uncertain and\npassed to the human verification stage. The resulting pipeline effectively converts complex heterogeneous\ntext sources into machine-readable JSON records with minimal manual intervention.\nConfidence Score for Human Verification.Ensuring the reliability of the extracted metadata is critical\nfor FM selection. To this end, we define a confidence score for each field in each record, enabling targeted\nhuman verification only where the uncertainty is high. Our confidence score combines two complementary\ncriteria: the model\u2019s generation probability and the consistency of outputs across multiple LLM sampling\nrounds. For each field, we compute the confidence score as follows:\nConfidence=w logp\u00b7NormalizedLogProb+w cons\u00b7SelfConsistency (1)\nwhereNormalizedLogProbquantifies the LLM\u2019s internal certainty by mapping the raw log-probability of\nthe generated field value to a bounded range, andSelfConsistencymeasures the fraction of LLM generations\nthat agreed on the same value among multiple independent sampling iterations.\nTo ensure interpretability and stable scaling, we normalize raw log-probabilities using a temperature-\ncontrolled sigmoid function. We set the temperature\u03c4= 0.5to avoid saturation and preserve sensitivity\nin the moderate-confidence regime. We setw logp = 0.7andw cons = 0.3to prioritize the log-probability\nsignal while still leveraging the stabilizing effect of self-consistency. These weights were empirically de-\ntermined via a grid search on a validation set of 10 FM records with manually verified ground truth. We\noptimized for maximum agreement between the confidence score and human verification decisions, using the\narea under the precision-recall curve (AUC) as the selection criterion. We observed that prioritizing the log-\nprobability signal improved precision, while incorporating self-consistency helped identify low-confidence\noutliers. However, these weights are not necessarily fixed and can be adjusted by users depending on the\nproperties of their LLMs, model domains, or confidence calibration needs. Any field with a final confidence\nbelow a threshold\u03b8= 0.75is automatically flagged for human review. Importantly, annotators inspect only\nthe flagged fields rather than full model records. Reviewing all fields for all FMs would require substantial\nannotation effort, as each record contains many heterogeneous metadata elements. To assess the risk of\nconfidently incorrect extractions, we manually inspected all fields for 10 records and found high-confidence\noutputs to be consistently accurate, supporting the reliability of our scoring mechanism. In practice, oc-\ncasional field-level errors have limited impact on FM selection aas the most decisive properties (modality,\narchitecture, compute requirements, and performance) are usually clearly stated and rarely mis-extracted.\nDiversity of Coverage.The current release ofRS-FMDspans a broad range of RSFMs pretrained on\nvarious data modalities (multispectral, hyperspectral, SAR, LiDAR, and text) and employing diverse model\narchitectures (transformer-based encoders, CNN\u2013transformer hybrids, vision\u2013language models). Pretrain-\ning data sources range from small curated datasets to million-scale image collections, and spatial resolutions\nspan from sub-meter imagery to coarse multispectral composites. By consolidating these heterogeneous in-\nformation into a schema-guided resource,RS-FMDsupports reproducible comparison, systematic bench-\nmarking, and agent-compatible retrieval. We will maintainRS-FMDby hosting on a public repository\nunder a permissive license. We periodically monitors new RSFM releases and inserts verified entries. To\nsupport broader scalability, we are developing a user interface where model authors can upload documenta-\ntion of new models. The system will automatically extract metadata and present it to authors for correction\nbefore submission. We will review community-submitted updates to ensure consistency and reliability.\n4REMSAAGENTARCHITECTURE\nThe goal ofREMSAis to automate the selection of FMs for RS tasks through a reasoning-centered, modular\nagentic workflow.REMSAintegrates structured knowledge grounding, LLM-based ranking with in-context\n5\nlearning, and iterative clarification to produce transparent and reproducible selections. Selecting an ap-\npropriate RSFM is challenging, as the models differ in data modalities, pretraining strategies, benchmark\nperformance, and resource requirements. In addition, users often provide incomplete or ambiguous task\ndescriptions, requiring the agent to interpret intent and reconcile trade-offs among candidate models. To ad-\ndress these challenges,REMSAprovides an integrated pipeline combining different agent components and\nexternal tools. This pipeline can achieve different targets such as structured retrieval, ranking, clarification,\nand memory archiving under a customized orchestration mechanism. This section will describe the agent\nworkflow and the details of each component and tool.\n4.1 AGENTWORKFLOW\nFigure 1 illustrates the architecture ofREMSA. The system is composed of two main layers: theLLM\nagent coreand a set ofexternal tools. The LLM agent core consists of two key components: theIn-\nterpreter, which parses user inputs into structured constraints and extracts user intent, and theTask Or-\nchestratordynamically decides which external tool to invoke at each step based on the current task state.\nWhen a user submits an free-text query, the query parser transforms it into a structured representation of\nconstraints. We prompt the LLM with a carefully designed schema that covers both mandatory and op-\ntional fields relevant to RSFM selection (See Appendix B. for complete schema.). Specifically, the parser\nextracts the targetapplication(e.g., land cover classification, surface water segmentation) and the re-\nquiredmodality(e.g., multispectral, SAR) as mandatory fields to narrow the FM search space. Then\nREMSAintegrates broader practical constraints through optional fields and clarification steps, including\ndata availability, compute budget, fine-tuning requirements, and output quality priorities. Once constraints\nare available, theTask Orchestratorinitiates a control loop that manages the entire selection process. At\neach step, it first evaluates the current task state, i.e., which constraints are available, how many candidates\nremain, and how confident the system is. Then it invokes the appropriate tool accordingly. If no mandatory\nconstraints are missing, the orchestrator calls theRetrieval Toolto generate an initial candidate set. If the\ncandidate set is small and all constraints are satisfied, theRanking Toolis applied directly. If there are too\nmany candidates or if ranking results yield low confidence scores, the orchestrator calls theClarification\nGenerator Toolto ask the user for additional input. The updated query is then passed back through the\nsame loop. Once the top-kresult is obtained, theExplanation Generator Toolis invoked to produce the final\nreport. This decision-making process is executed by empirical thresholds for ranking confidence, constraint\ncoverage, and clarification rounds. The orchestration ensures that tool invocation is adaptive, goal-oriented,\nand transparent. To support personalization and self-improvement,REMSAalso integratesTask Memory,\nwhich stores past user interactions in a vector database. Relevant memory entries are retrieved via cosine\nsimilarity to improve future interactions. More details on the implemented workflow are in Appendix C..\nTo enhanceREMSA\u2019s reliability, we have several built-in mechanisms to mitigate failures. The orchestrator\nmonitors confidence signals and triggers clarification rounds when ranking is uncertain. Rule-based con-\nstraint eliminates candidates that violate hard requirements. A fallback \u201dclosest-match\u201d mode returns the\nsafest alternative when no candidate fully satisfies the constraints. Our modular design also allows for in-\ntegrating explicit feedback mechanisms (e.g., an LLM-as-a-Judge component that re-evaluates low-quality\nselections), makingREMSAextensible to more robust self-correction strategies.\n4.2 AGENTTOOLS\nThe following tools operate as callable interfaces outside of the agent core. Each tool is invoked indepen-\ndently by the orchestrator, depending on the state of the task, supporting retrieval, ranking, clarification, and\nexplanation within the RSFM selection workflow. Our design supports extensibility for tool integration.\nRetrieval Tool.To generate an initial set of candidates, the retrieval tool encodes both the structured user\nconstraints and the FM entries in theRS-FMDusing Sentence-BERT embeddings (Reimers & Gurevych,\n6\n2019). To preserve the structure of the metadata in the embedding, each metadata field is prefixed with a to-\nken of the type-indicator (e.g., [APPLICATION], [MODALITY]) before encoding.REMSAuses Facebook\nAI Similarity Search (FAISS) (Meta, 2025) for an efficient approximate search based on cosine similarity.\nThe tool returns a list of the most relevant FMs determined by a configurable similarity threshold. User can\nadjust it based on their domain requirements. In our experiments, we set this threshold empirically to ensure\nbroad coverage while minimizing irrelevant matches. This tool is optimized for high recall: it includes soft\nmatches and does not enforce strict constraints, allowing the downstream pipeline to handle finer filtering.\nRanking Tool.While the retrieval tool provides a broad list of relevant FMs, it cannot fully capture user-\nspecific needs and deployment trade-offs. This task can be handled by a ranking tool. The ranking tool\nrefines the candidate FM list using a hybrid strategy to balances efficiency, flexibility, and interpretability:\n\u2022Rule-Based Filtering:Candidates that violate hard constraints, such as required modality, sensor support,\nor minimum performance, are eliminated using deterministic logic. These hard constraints are defined\nbased on fields extracted by the interpreter.\n\u2022In-Context LLM Ranking:The remaining candidates are re-ranked by an LLM prompted with the struc-\ntured query and FM metadata, using expert-crafted few-shot examples to illustrate selection. The LLM\nreturns an ordered list with brief justifications, leveraging in-context reasoning without any model training\n(details in Appendix D.). We also compute a confidence score for each selection following Section 3.\nClarification Generator Tool.If the orchestrator detects insufficient constraints or a low overall confidence\nscore of selected FMs, it invokes the clarification tool. This tool inspects the parsed schema to determine\nmissing or underspecified fields (e.g., modality, region, or performance bounds) and formulates clarification\nquestions. The tool generates each question based on the interpreter schema. We limit the clarification to\nthree rounds to avoid user fatigue. The agent will integrate the responses with initial user input, parse and\nmerge them into the evolving task specification, in order to iteratively refine the selection process.\nExplanation Generator Tool.Once a ranking is available, this tool generates structured, human-readable\nexplanations. It uses a prompt-driven LLM to synthesize the rationale for each selected FM, including\nkey reasons considering suitability and trade-offs. Each output includes the model name, bullet points for\nexplanation, and links to the corresponding paper and code repository. This tool enhances transparency and\nuser trust by exposing the decision process (prompt is in Appendix E.). The output is in JSON format.\n5 EVALUATIONPROTOCOL ANDBENCHMARK\nEvaluating FM selection in RS is challenging due to the lack of dedicated benchmarks. Previous works\nmainly focus on evaluating model performance on fixed applications or datasets, rather than assessing the\nability to recommend the most suitable FM under diverse real-world deployment constraints. In this work,\nwe leverageRS-FMDto construct the first agent-oriented benchmark for FM selection, systematically\ncovering diverse models, modalities, and deployment constraints.\nBenchmark Construction.Our evaluation protocol relies on structured expert review, ensuring method-\nological rigor without imposing excessive annotation overhead. We curate a benchmark of 75 natural lan-\nguage queries to keep evaluation feasible while still ensuring diversity. We will publish these queries in our\nrepository. All model-query pairs were evaluated by two experts from a computer science background with\nexpertise in RS. We used a structured rubric to ensure consistency. Full details of the expert procedure are\nprovided in Appendix G.. The evaluation resulted in 900 expert ratings as we compare the top-3 FMs from\nREMSAand 3 baselines. Each instance must be carefully rated across seven criteria. Thus, the evaluation\nworkload is substantial despite the modest query count. To maximize representativeness, we create the query\nusing structured templates of various scenarios and instantiate them (full templates is in Appendix H..) The\nqueries diverse over data availability, computational resources, application complexity, and evaluation prior-\n7\nTable 1: Expert evaluation criteria.\nCriterion Description\nApplication Compatibility Whether the model fits the user requested application\nModality Match Whether the model supports the required input data modality\nReported Performance Performance reported on similar datasets or applications\nEfficiency Suitability for the user\u2019s computational resources\nPopularity Based on GitHub repository stars and citations\nGeneralizability Diversity and scale of pretraining data\nRecency Whether the model reflects recent developments\nities. The resulting queries cover a wide range of tasks, including flood mapping with SAR data, crop type\nclassification using multispectral or hyperspectral imagery, urban expansion monitoring with optical time\nseries, and disaster response, such as sea ice and wildfire detection. These tasks cover both single-date and\nmulti-temporal analysis, single- and multi-modal inputs, and varying resource environments. All queries\nwere reviewed by a domain expert for factual accuracy and corrected for consistency.\nBaselines.There is no prior work on automated FM selection for RS deployment, and existing AutoML\nor agent systems cannot directly perform this task. We therefore design baselines that serve as both mean-\ningful comparisons and implicit ablations ofREMSA, with each baseline removing or modifying specific\ncomponents to assess their contributions:\n1.REMSA-NAIVE : Same toolset and DB asREMSA, but employs only basic sequential orchestration with-\noutREMSA\u2019s adaptive, task-aware control logic. It relies on LangChain\u2019s default single-step execu-\ntion, where the LLM independently chooses tools without structured workflow or multi-turn coordina-\ntion (LangChain, 2025). This baseline tests the effectiveness of our orchestration mechanism.\n2. DB-RETRIEVAL : Returns the top-kmodels from the FAISS-based dense retrieval overRS-FMD, with\nranking, clarification, memory, and orchestration removed. This serves as a retrieval-only baseline and\nisolates the contribution of LLM-based ranking and constraint reasoning.\n3. UNSTRUCTURED-RAG : A generic RAG setup where the LLM receives the query and raw, unstructured\nFM descriptions and outputs top-kFMs with brief justifications (prompt in Appendix F.). This baseline\ntests whether LLM can perform FM selection without our structured, modular agent.\nEvaluation Protocol and Criteria.For each query,REMSAand all baselines output their top-3 FM selec-\ntions. These model-query pairs were then evaluated independently and blindly by the two experts using the\ncriteria in Table 1. After individual scoring, disagreements were resolved through rubric-guided discussion.\nThe evaluation was performed once during the scoring, and no adjustments were made to any FMs thereafter\nto avoid introducing bias. Each FM is rated on a 1-5 scale (0.5 precision) on 7 criteria in Table 1, covering\ntask relevance and deployability under real-world constraints. Several criteria use explicit rules. For exam-\nple, generalizability combines geographic, modality, and dataset-scale factors, popularity relies on citations\nor GitHub activity, and recency is based on publication year. They are designed to be transparent, repro-\nducible, and grounded in practical needs rather than ad-hoc user preferences. More details on the evaluation\nprocedure are in Appendix G.. The final score is a weighted sum of all criteria ratings (Our weight setting is\nin Appendix I.). The score is linearly mapped to 1-100 scale to better show the differences.\nAlthough exhaustive empirical benchmarking of all candidate models is infeasible, our protocol offers a re-\nproducible and practical proxy for evaluating agent performance in real-world FM selection workflows. To\nsupport transparency and broader community adoption, we publicly release the full set of evaluation queries,\nexpert guidelines, scoring criteria, and model metadata used in the evaluation. This enables reproducibility\nand provides a standardized foundation for future research on FM selection in RS and beyond. Our evalua-\ntion does not assume a single ground-truth \u201dbest\u201d FM. Experts compare the top-ranked candidates from all\n8\nTable 2: Comparison to the baselines.\nSystem Avg Top-1 Avg Set Top-1 Hit HQ Hit MRR\nREMSA(Ours)75.76 75.0322.67%40.00% 0.38\nREMSA-NAIVE72.67 72.0025.33%37.33% 0.36\nDB-RETRIEVAL67.37 68.78 13.33% 17.33% 0.25\nUNSTR.-RAG 71.23 68.39 13.33% 30.67% 0.24Table 3: Sensitivity analysis on evaluation criteria.\nCriteria Setting Avg Set Top-1 Hit MRR Note\nFull Scoring (All Criteria) 75.03 22.67% 0.38\nGreen:\nIncrease\nRed:\nDropw/o Application Compatibility 73.32 21.33% 0.36\nw/o Modality Match 70.88 22.67% 0.36\nw/o Reported Performance 75.05 22.67% 0.38\nw/o Efficiency 80.23 25.33% 0.38\nw/o Popularity+Recency 75.13 25.33% 0.39\nw/o Generalizability 75.10 22.67% 0.38\nsystems, and a system is preferred when its top model is judged more suitable than other systems.REMSA\nreturns top-kFMs with explanations, enabling users to choose based on their own preferences.\n6 RESULTS ANDANALYSIS\nWe conduct experiments to comprehensively evaluate the effectiveness ofREMSAin RSFM selection. Since\nno prior work directly targets real FM selection under diverse deployment constraints, we develop our own\nbaselines. This section presents our experiment setup, quantitative results, and sensitivity analysis, followed\nby a discussion of limitations and representative examples.\nExperimental Setup.We useGPT-4.1(OpenAI, 2025) forREMSAcore and all baselines to\nensure fairness. However, we designREMSAto be LLM-agnostic and support any LLM (e.g.,\nDeepSeek-R1(DeepSeek-AI et al., 2025),LLaMA-3(Dubey et al., 2024)). Our benchmark consists\nof 75 diverse natural language user input queries. For each input,REMSAand all baselines (all described\nin Section 5) select the top-3candidate FMs for comparison. Domain experts rate each candidate using\nthe criteria in Table 1, and we report both single-model and set-level scores to evaluate selection accuracy\nand reasoning quality across multiple agent decision points. During evaluation, all clarification rounds in\nREMSAwere executed automatically, with the system interacting with an independent LLM simulating user\nresponses. No human was involved in these interactions, ensuring consistency and preventing evaluator bias.\nEvaluation Metrics.We use complementary metrics to evaluate both the best model and the overall set\nquality: (1)Average Top-1 Score(expert score of the top-ranked model), (2)Average Set Score(average\nscore of the top-3 models), (3)Top-1 Hit Rate(fraction where the system\u2019s top model is the expert\u2019s highest-\nscored), (4)High-Quality Hit Rate(fraction where the top model scores\u226580), and (5)Mean Reciprocal\nRank - MMR(rank of the expert-preferred model within the top-3).\n6.1 COMPARISON TO BASELINES\nAs shown in Table 2,REMSAconsistently outperforms all baselines in all evaluation metrics, demonstrat-\ning its effectiveness in selecting FMs under various real constraints. Illustrative examples of expert-scored\nmodel-query pairs are provided in Appendix J..REMSAachieves the highest Average Top-1 Score (75.76)\nand Average Set Score (75.03), indicating not only that the top-ranked models are aligned with expert prefer-\nences, but also that the top-3 selections offer strong and diverse alternatives. Compared to DB-RETRIEVAL,\nwhich relies on similarity-based retrieval over structured metadata,REMSAimproves Top-1 Hit Rate from\n13.33% to 22.67%, and MRR from 0.25 to 0.38. This underscores the value of reasoning beyond retrieval,\nespecially when user queries involve constraints (e.g., modality, resolution, compute budget) not explicitly\nstored in the metadata. Although UNSTRUCTURED-RAG has access to full model descriptions, its perfor-\nmance remains lower due to the lack of structured guidance and modular reasoning. This result shows that\nREMSA\u2019s ability to combine structured schema grounding with dynamic tool orchestration enables precise\nalignment with user needs. BothREMSAandREMSA-NAIVEperform notably better than retrieval-only or\nunstructured RAG baselines, showing effectiveness of our agent architecture: grounding the selection pro-\n9\ncess in a structured schema and enabling tool-based reasoning provides a substantial advantage. However,\nREMSAimproves further in all major evaluation metrics. AlthoughREMSAhas a slightly lower Top-1 Hit\nRate (22.67% vs. 25.33%), the higher Average Top-1 Score (75.76) and MRR (0.38) suggest thatREMSA\nselects high-quality models more consistently at the top of its ranking. This indicates that our orchestration\nlogic, including multi-turn clarification and reasoning heuristics, contributes meaningfully to performance,\nespecially when model choices are ambiguous or task formulations are complex.\nLatency Trade-off.To assess the latency-performance trade-off, we measure the average end-to-end run-\ntime per query. As expected, single-step methods are faster: DB-Retrieval takes 0.77s, Unstr.-RAG 11.9s,\nandREMSA-Naive 22.7s, whereasREMSArequires31.7sdue to multi-stage reasoning and optional clarifi-\ncation. Despite this moderate overhead,REMSAdelivers the highest expert-validated accuracy across major\nmetrics, indicating that its additional reasoning steps yield meaningful and consistent gains.\n6.2 SENSITIVITYANALYSIS ONEVALUATIONCRITERIA\nTo understand how wellREMSAaligns with expert-defined evaluation principles, we perform a sensitivity\nanalysis by removing each scoring criterion individually from the expert evaluation protocol. As shown in\nTable 3, the performance is generally robust in most dimensions, but some removals reveal important insights\ninto which criteria contribute the most to the effective selection of the model. Both the removal of Applica-\ntion Compatibility and Modality Match lead to notable performance drops, confirming thatREMSAactively\nprioritizes functionally appropriate models aligned with the user\u2019s objective. Notably, removing Reported\nPerformance and Generalizability yields minimal change in overall results, implying that these dimensions\nare either captured implicitly through other criteria or are less decisive in the current benchmark setup. In\ncontrast, removing Efficiency or Popularity+Recency actually leads to a modest performance gain. This\nsuggests that while these criteria add practical relevance for deployment, they may occasionally favor well-\nknown or resource-efficient models over technically optimal ones. The sensitivity results further validate\nthatREMSAdoes not overfit to superficial indicators such as citations or recency, but instead emphasizes\ncore compatibility and reasoning in its final decisions.\n7 CONCLUSION ANDDISCUSSION\nWe proposedREMSA, the first LLM Agent combine a FM database for real RSFM selection problems. By\norchestrating modular tools for metadata retrieval, in-context ranking, multi-turn clarification, and memory-\naugmented reasoning,REMSAdelivers high-quality and consistent selections. A key foundation is our\nRS-FMD- the first database for RSFM. It consolidates heterogeneous descriptions into a structured form\nfor transparent retrieval and comparison. On an expert-driven benchmark,REMSAoutperforms retrieval-\nonly, unstructured RAG, and naive agent baselines. In future work, we plan to expand the benchmark to\nrarer and more complex scenarios, explore lightweight supervised enhancements, and improve explanation\nspecificity and trustworthiness. We also aim to reduce expert burden by introducing semi-automated scoring\nand community-assisted annotation, which will makeRS-FMDand the benchmark easier to extend to new\nFMs. In addition, we plan to adopt a mixed expert- and benchmark-based evaluation mechanism to further\nstrengthen robustness. We further envision extendingREMSAtoward adaptive decision-making, where the\nagent not only selects but also recommends model adaptation strategies, such as fine-tuning or domain-\nspecific adjustment, and identifies opportunities for incorporating additional modalities when beneficial.\nLimitations.AlthoughREMSAperforms successfully in the selection of RSFMs, some limitations remain:\nFor example, our benchmark is based on 75 expert-annotated queries, which may miss rare or emerging\nuse cases. However, the overall evaluation effort is substantial, totaling 900 expert ratings. In addition,\nthe ranking relies on in-context learning rather than supervised training, which may limit performance on\n10\ncomplex or uncommon queries. Despite these limitations,REMSAdemonstrates the feasibility of constraint-\naware agentic RSFM selection, setting the basis for future extensions to other scientific domains.\nACKNOWLEDGMENTS\nThis work is supported by the German Ministry for Economic Affairs and Climate Action through the\nEOekoLand Project under Grant 50RP2230B.\nREFERENCES\nPierre Adorni, Minh-Tan Pham, and et al. Towards efficient benchmarking of foundation models in remote\nsensing: A capabilities encoding approach.CoRR, abs/2505.03299, 2025.\nSaaket Agashe, Jiuzhou Han, and et al. Agent S: an open agentic framework that uses computers like a\nhuman. InThe Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore,\nApril 24-28, 2025. OpenReview.net, 2025.\nYezhen Cong, Samar Khanna, and et al. Satmae: Pre-training transformers for temporal and multi-spectral\nsatellite imagery. InAdvances in Neural Information Processing Systems 35: Annual Conference on\nNeural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 -\nDecember 9, 2022, 2022.\nDeepSeek-AI, Daya Guo, and et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforce-\nment learning.CoRR, abs/2501.12948, 2025.\nMatthias Drusch, Umberto Del Bello, and et al. Sentinel-2: Esa\u2019s optical high-resolution mission for gmes\noperational services.Remote sensing of Environment, 120:25\u201336, 2012.\nAbhimanyu Dubey, Abhinav Jauhri, and et al. The llama 3 herd of models.CoRR, abs/2407.21783, 2024.\nMatthias Feurer, Aaron Klein, and et al. Efficient and robust automated machine learning. InAdvances\nin Neural Information Processing Systems 28: Annual Conference on Neural Information Processing\nSystems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 2962\u20132970, 2015.\nAnthony Fuller, Koreen Millard, and James R. Green. CROMA: remote sensing representations with con-\ntrastive radar-optical masked autoencoders. InAdvances in Neural Information Processing Systems 36:\nAnnual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,\nUSA, December 10 - 16, 2023, 2023.\nZhibin Gou, Zhihong Shao, and et al. Tora: A tool-integrated reasoning agent for mathematical problem\nsolving. InThe Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Aus-\ntria, May 7-11, 2024. OpenReview.net, 2024.\nLuis Guanter, Hermann Kaufmann, and et al. The enmap spaceborne imaging spectroscopy mission for\nearth observation.Remote. Sens., 7(7):8830\u20138857, 2015.\nXin Guo, Jiangwei Lao, Bo Dang, Yingying Zhang, Lei Yu, Lixiang Ru, Liheng Zhong, Ziyuan Huang,\nKang Wu, Dingxiang Hu, et al. Skysense: A multi-modal remote sensing foundation model towards\nuniversal interpretation for earth observation imagery. InProceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 27672\u201327683, 2024.\nJeremy Andrew Irvin, Emily Ruoyu Liu, and et al. Teochat: A large vision-language assistant for temporal\nearth observation data. InThe Thirteenth International Conference on Learning Representations, ICLR\n2025, Singapore, April 24-28, 2025. OpenReview.net, 2025.\n11\nAlexandre Lacoste, Nils Lehmann, and et al. Geo-bench: Toward foundation models for earth monitoring.\nInAdvances in Neural Information Processing Systems 36: Annual Conference on Neural Information\nProcessing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.\nLangChain. Langchain.https://python.langchain.com/docs/introduction/, 2025. On-\nline; accessed 20-August-2025.\nChaehong Lee, Varatheepan Paramanayakam, and et al. Multi-agent geospatial copilots for remote sensing\nworkflows.CoRR, abs/2501.16254, 2025.\nXiang Li, Jian Ding, and Mohamed Elhoseiny. Vrsbench: A versatile vision-language benchmark dataset for\nremote sensing image understanding. InAdvances in Neural Information Processing Systems 38: Annual\nConference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada,\nDecember 10 - 15, 2024, 2024.\nYansheng Li, Jieyi Tan, Bo Dang, Mang Ye, Sergey A Bartalev, Stanislav Shinkarenko, Linlin Wang, Yingy-\ning Zhang, Lixiang Ru, Xin Guo, et al. Unleashing the potential of remote sensing foundation models via\nbridging data and computility islands.The Innovation, 2025.\nXiao Liu, Tianjie Zhang, and et al. Visualagentbench: Towards large multimodal models as visual foundation\nagents. InThe Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore,\nApril 24-28, 2025. OpenReview.net, 2025a.\nXiao Liu, Tianjie Zhang, and et al. Visualagentbench: Towards large multimodal models as visual foundation\nagents. InThe Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore,\nApril 24-28, 2025. OpenReview.net, 2025b.\nYujie Luo, Xiangyuan Ru, and et al. Oneke: A dockerized schema-guided LLM agent-based knowledge\nextraction system. InCompanion Proceedings of the ACM on Web Conference 2025, WWW 2025, Sydney,\nNSW, Australia, 28 April 2025 - 2 May 2025, pp. 2871\u20132874. ACM, 2025.\nUtkarsh Mall, Cheng Perng Phoo, and et al. Remote sensing vision-language foundation models without\nannotations via ground remote alignment. InThe Twelfth International Conference on Learning Repre-\nsentations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024.\nMeta. Faiss.https://ai.meta.com/tools/faiss/, 2025. Online; accessed 20-August-2025.\nFelix Neutatz, Marius Lindauer, and Ziawasch Abedjan. Automl in heavily constrained applications.VLDB\nJ., 33(4):957\u2013979, 2024.\nOpenAI. Gpt-4.1.https://openai.com/index/gpt-4-1/, 2025. Online; accessed 20-August-\n2025.\nPriyank Pathak, Shyam Marjit, and et al. Lr0.fm: low-resolution zero-shot classification benchmark for\nfoundation models. InThe Thirteenth International Conference on Learning Representations, ICLR 2025,\nSingapore, April 24-28, 2025. OpenReview.net, 2025.\nElena Plekhanova, Damien Robert, and et al. Ssl4eco: A global seasonal dataset for geospatial foundation\nmodels in ecology. InIEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,\nCVPR Workshops 2025, Nashville, TN, USA, June 11-15, 2025, pp. 2403\u20132414. Computer Vision Foun-\ndation / IEEE, 2025.\nMirali Purohit, Gedeon Muhawenayo, and et al. How does the spatial distribution of pre-training data affect\ngeospatial foundation models?CoRR, abs/2501.12535, 2025.\n12\nRahul Ramachandran, Sujit Roy, Manil Maskey, Daniela Szwarcman, and Paolo Fraccaro. A primer for\nassessing foundation models for earth observation.Cornell University, 2025.\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong,\nChina, November 3-7, 2019, pp. 3980\u20133990. Association for Computational Linguistics, 2019.\nAkashah Shabbir, Muhammad Akhtar Munir, and et al. Thinkgeo: Evaluating tool-augmented agents for\nremote sensing tasks.CoRR, abs/2505.23752, 2025.\nYongliang Shen, Kaitao Song, and et al. Hugginggpt: Solving AI tasks with chatgpt and its friends in\nhugging face. InAdvances in Neural Information Processing Systems 36: Annual Conference on Neural\nInformation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023,\n2023.\nNaomi Simumba and Nils Lehmann et al. Geo-bench-2: From performance to capability, rethinking evalu-\nation in geospatial ai.CoRR.\nSimranjit Singh, Michael Fore, and Dimitrios Stamoulis. Evaluating tool-augmented agents in remote sens-\ning platforms. InThe Twelfth International Conference on Learning Representations, ICLR 2024, Ma-\nchine Learning for Remote Sensing Workshop. OpenReview.net, 2024.\nSagar Soni, Akshay Dudhane, and et al. Earthdial: Turning multi-sensory earth observations to interactive\ndialogues. InIEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville,\nTN, USA, June 11-15, 2025, pp. 14303\u201314313. Computer Vision Foundation / IEEE, 2025.\nChris Thornton, Frank Hutter, and et al. Auto-weka: combined selection and hyperparameter optimization of\nclassification algorithms. InThe 19th ACM SIGKDD International Conference on Knowledge Discovery\nand Data Mining, KDD 2013, Chicago, IL, USA, August 11-14, 2013, pp. 847\u2013855. ACM, 2013.\nRamon Torres, Paul Snoeij, and et al. Gmes sentinel-1 mission.Remote sensing of Environment, 120:9\u201324,\n2012.\nHaoning Wu, Zicheng Zhang, and et al. Q-bench: A benchmark for general-purpose foundation models\non low-level vision. InThe Twelfth International Conference on Learning Representations, ICLR 2024,\nVienna, Austria, May 7-11, 2024. OpenReview.net, 2024.\nAoran Xiao, Weihao Xuan, Junjue Wang, Jiaxing Huang, Dacheng Tao, Shijian Lu, and Naoto Yokoya.\nFoundation models for remote sensing and earth observation: A survey.CoRR, abs/2410.16602, 2024.\nWei Xiong, Chengshuai Shi, and et al. Building math agents with multi-turn iterative preference learning.\nInThe Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April\n24-28, 2025. OpenReview.net, 2025.\nWenjia Xu, Zijian Yu, and et al. Rs-agent: Automating remote sensing tasks through intelligent agents.\nCoRR, abs/2406.07089, 2024.\nShunyu Yao, Jeffrey Zhao, and et al. React: Synergizing reasoning and acting in language models. InThe\nEleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5,\n2023. OpenReview.net, 2023.\n13\nAPPENDIX\nA. COMPLETERS-FMDSCHEMASPECIFICATION\nTo properly represent the properties of each FM, we designed a comprehensive data schema forRS-FMD.\nThe schema includes the essential characteristics of model architectures, pretraining strategies, supported\nmodalities, and benchmark performance.\nEach model record includes fields such as unique identifiers, names, versions, release and update timestamps,\nand links to associated publications, code repositories, and pretrained weights. These metadata elements\nensure traceability and reproducibility of the database entries.\nBeyond these core descriptors, the schema incorporates detailed fields that capture architectural\nspecifics (e.g., backbone type, number of layers, number of parameters), pretraining approaches (e.g., pre-\ntext training type, masking strategy), and modality integration. The design anticipates the diversity of RS\nmodels and supports future extensions.\nTo capture information about pretraining and evaluation comprehensively, the schema defines two nested\nstructures:\n\u2022PretrainingPhase: This substructure records the datasets used for pretraining, geographical coverage,\ntime range, image resolutions, token sizes, augmentation strategies, sampling methods, and masking ratios.\n\u2022Benchmark: This substructure captures evaluation metrics, including the applications, dataset, perfor-\nmance scores, and training hyperparameters used during evaluation.\nMany fields are annotated withfree textmetadata. This annotation signals that the field may contain\nnatural language summarization that requires specialized treatment in confidence scoring and downstream\nverification.\nTable 4 provides a comprehensive description of the fields of our data schema inRS-FMD, including nested\nstructures for pretraining phases and benchmarks.\nTable 4: Complete schema specification ofRS-FMD, including nested pretraining phases and benchmarks.\nField Type Description\nMain Model Fields\nmodel id string Unique identifier of the model (free text)\nmodel name string Only the name of the model without extra descrip-\ntions (free text)\nversion string Version identifier (free text)\nrelease date date Release date of the model\nlastupdated date Last updated date\nshort description string Short summary describing the model (free text)\npaper link URL URL to the associated publication\ncitations integer Number of citations\nrepository URL URL to the code repository\nweights URL URL to pretrained model weights\nbackbone string Specific backbone used (free text)\nnum layers integer Number of layers\nnum parameters float Model size in millions of parameters\npretext training type string Type of pretext training strategy (free text)\n14\nField Type Description\nmasking strategy string Masking strategy applied during training (free\ntext)\npretraining string Description of pretraining approach (free text)\ndomain knowledge list[string] Domain-specific knowledge or methods incorpo-\nrated\nbackbone modifications list[string] Modifications made to the backbone\nsupported sensors list[string] Supported satellite sensors\nmodality integration type string Integration type (free text)\nmodalities list[string] Input data modalities (free text)\nspectral alignment{full, partial, none}Whether the model models spectral continuity\ntemporal alignment{full, partial, none}Whether the model models temporal sequences\nspatial resolution string Spatial resolution of data (free text)\ntemporal resolution string Temporal resolution of data (free text)\nbands list[string] Spectral bands used\nNested: PretrainingPhase\ndataset string Dataset used for pretraining (free text)\nregions coverage list[string] Geographical regions covered\ntime range string Time range of pretraining data (free text)\nnum images integer Number of images used\ntoken size string Token size (free text)\nimage resolution string Input image resolution (free text)\nepochs integer Number of epochs\nbatch size integer Batch size\nlearning rate string Learning rate (free text)\naugmentations list[string] Augmentations applied\nprocessing list[string] Additional preprocessing steps\nsampling string Sampling strategy (free text)\nprocessing level string Processing level (free text)\ncloud cover string Cloud cover filtering (free text)\nmissing data string Handling of missing data (free text)\nmasking ratio float Masking ratio\nNested: Benchmark\napplication type string Type of application evaluated (free text)\napplication string Specific application domain (free text)\ndataset string Benchmark dataset name (free text)\nmetrics list[string] List of evaluation metrics\nmetrics value list[float] Numeric values for each metric\nsensor list[string] Sensors used\nregions list[string] Regions evaluated\noriginal samples integer Total number of samples before sampling\nnum samples integer Actual number of samples used\nsampling percentage float Fraction of dataset retained (0\u2013100)\nnum classes integer Number of classes\nclasses list[string] Names of each class\nimage resolution string Input image resolution (free text)\nspatial resolution string Spatial resolution (free text)\nbands used list[string] Bands used during evaluation\n15\nField Type Description\naugmentations list[string] Data augmentations applied\noptimizer string Optimizer used (free text)\nbatch size integer Batch size\nlearning rate float Learning rate\nepochs integer Number of epochs\nloss function string Loss function (free text)\nsplit ratio string Train/val/test split ratio (free text)\nBelow we include a complete example of anRS-FMDrecord for the RSFMA2-MAE. This illustrates how\nthe schema is instantiated with real metadata.\n1{\n2\"model_id\": \"A2-MAE\",\n3\"model_name\": \"A2-MAE\",\n4\"version\": \"v1\",\n5\"release_date\": \"2024-06-16\",\n6\"last_updated\": \"2024-06-16\",\n7\"short_description\": \"A2-MAE is a spatial-temporal-spectral unified remote\nsensing pre-training method based on an anchor-aware masked autoencoder. It\nleverages a global-scale, multi-source dataset (STSSD) and introduces an\nanchor-aware masking strategy and a geographic encoding module to\nefficiently integrate spatial, temporal, and spectral information from\ndiverse remote sensing imagery.\",\n8\"paper_link\": \"https://arxiv.org/abs/2406.08079\",\n9\"citations\": 7,\n10\"backbone\": \"ViT-Large\",\n11\"pretext_training_type\": \"Masked Autoencoder (MAE) with anchor-aware masking\nand geographic encoding\",\n12\"masking_strategy\": \"Anchor-aware masking (AAM): dynamically adapts masking\n...\",\n13\"pretraining\": \"Self-supervised pre-training on the STSSD dataset...\",\n14\"domain_knowledge\": [\n15\"Geographic encoding (latitude, longitude, GSD)\",\n16\"Spatial-temporal-spectral relationships\",\n17\"Clustering-based data pruning\"\n18],\n19\"supported_sensors\": [\n20\"Sentinel-2\", \"Landsat-8\", \"Gaofen-1\", \"Gaofen-2\"\n21],\n22\"modality_integration_type\": \"Homogeneous Multimodal\",\n23\"modalities\": [\"Multispectral\", \"Multi-temporal\"],\n24\"spectral_alignment\": \"partial\",\n25\"temporal_alignment\": \"partial\",\n26\"spatial_resolution\": \"0.8-30m\",\n27\"temporal_resolution\": \"2020-2023, periodic seasonal revisits\",\n28\"bands\": [\n29\"Sentinel-2: B1-B12\",\n30\"Landsat-8: B1-B7\",\n31\"Gaofen-1: B1-B4\",\n32\"Gaofen-2: B1-B4\"\n33],\n34\"pretraining_phases\": [\n16\n35{\n36\"dataset\": \"STSSD\",\n37\"regions_coverage\": [\"Global (12k urban centers, 10k nature reserves)\"],\n38\"time_range\": \"2020-2023\",\n39\"num_images\": 2500000,\n40\"token_size\": \"16x16\",\n41\"image_resolution\": \"0.8-30m (cropped 256x256 to 3200x3200)\",\n42\"epochs\": 130,\n43\"batch_size\": 1024,\n44\"learning_rate\": \"1e-4 (cosine decay)\",\n45\"processing\": [\n46\"Atmospheric/radiation correction\",\n47\"Pan-sharpening (Gaofen)\",\n48\"Cropping/resizing alignment\"\n49],\n50\"sampling\": \"Clustering-based pruning (keep hardest 10%)\",\n51\"cloud_cover\": \">=10%\",\n52\"masking_ratio\": 0.75\n53}\n54],\n55\"benchmarks\": [\n56{\n57\"task\": \"Classification\",\n58\"application\": \"Land cover classification\",\n59\"dataset\": \"EuroSAT\",\n60\"metrics\": [\"Accuracy\"],\n61\"metrics_value\": [99.09],\n62\"sensor\": [\"Sentinel-2\"],\n63\"regions\": [\"34 European countries\"]\n64},\n65{\n66\"task\": \"Classification\",\n67\"application\": \"Multi-label classification\",\n68\"dataset\": \"BigEarthNet\",\n69\"metrics\": [\"mAP\"],\n70\"metrics_value\": [83.0]\n71},\n72{\n73\"task\": \"Segmentation\",\n74\"application\": \"Surface water segmentation\",\n75\"dataset\": \"Sen1Floods11\",\n76\"metrics\": [\"mIoU\"],\n77\"metrics_value\": [88.87]\n78},\n79{\n80\"task\": \"Segmentation\",\n81\"application\": \"Cropland segmentation\",\n82\"dataset\": \"CropSeg\",\n83\"metrics\": [\"mIoU\"],\n84\"metrics_value\": [44.81]\n85},\n86{\n87\"task\": \"Change Detection\",\n88\"application\": \"LEVIR-CD\",\n89\"dataset\": \"LEVIR-CD\",\n17\n90\"metrics\": [\"mIoU\"],\n91\"metrics_value\": [84.32]\n92},\n93{\n94\"task\": \"Change Detection\",\n95\"application\": \"Urban change detection\",\n96\"dataset\": \"OSCD\",\n97\"metrics\": [\"F1\"],\n98\"metrics_value\": [53.97]\n99},\n100{\n101\"task\": \"Change Detection\",\n102\"application\": \"Semantic change segmentation\",\n103\"dataset\": \"DynamicEarthNet\",\n104\"metrics\": [\"mIoU\"],\n105\"metrics_value\": [46.0]\n106}\n107]\n108}\nB. STRUCTUREDQUERYSCHEMA\nBelow we show the complete JSON schema template used by the query interpreter:\n{\n\"application\": \"string\", // Mandatory\n\"modality\": \"string\", // Mandatory\n\"sensor\": \"string or list of strings\", // Optional\n\"spatial_resolution\": \"string or numeric\", // Optional\n\"temporal_resolution\": \"string or numeric\",// Optional\n\"bands\": \"list of strings\", // Optional\n\"avaliable_data\": \"string\", // Optional\n\"deployment_device\": \"string\", // Optional\n\"priority_metrics\": \"list of string\", // Optional\n\"min_performance\": { // Optional\n\"metric\": \"list of string\",\n\"value\": \"list of number\"\n},\n\"region\": \"string or list of strings\", // Optional\n\"domain_keywords\": \"list of strings\" // Optional\n}\n18\nC. IMPLEMENTATIONDETAILS\nAlgorithm 1: REMSAWorkflow for RSFM Selection\nInput:User Queryq, desired number of recommendationsk\nOutput:Top-kselected models with explanations\n1InitializeClarifyCounter\u21900\n2InitializeMaxClarify\u21903\n3repeat\n4Constraints\u2190ParseQuery(q);// LLM parses constraints\n5ifmandatory constraints missingthen\n6ifClarifyCounter < MaxClarifythen\n7q\u2190ClarifyUser(q, Constraints)IncrementClarifyCounter\n8else\n9break;// Stop clarifying to avoid user fatigue\n10untilAll mandatory constraints are present;\n11Candidates\u2190RetrieveModels(q);// Embedding retrieval (Top K)\n12Filtered\u2190FilterCandidates(Candidates, Constraints)\n13if|Filtered|= 0then\n14BestMatch\u2190SelectClosestModel(Candidates, Constraints)\n15Explanation\u2190GenerateExplanation(q, BestMatch)\n16return{Recommendation: BestMatch, Explanation}\n17if|Filtered|> MaxCandidatesthen\n18ifClarifyCounter < MaxClarifythen\n19q\u2190ClarifyUser(q, Constraints)\n20IncrementClarifyCounter\n21Go to line 3;// Restart process with clarified query\n22Scores\u2190RankCandidates(q, Filtered)OverallConfidence\u2190ComputeConfidence(Scores)\n23ifOverallConfidence < ConfidenceThresholdthen\n24ifClarifyCounter < MaxClarifythen\n25q\u2190ClarifyUser(q, Constraints)\n26IncrementClarifyCounter\n27Go to line 3\n28TopK\u2190Top-kcandidates inFilteredranked byScores\n29Explanation\u2190GenerateExplanation(q, TopK)\n30return{Recommendations: TopK, Explanation}\nThe workflow ofREMSAis shown in Algorithm 1. The pipeline is implemented in Python usingpydantic\nfor schema validation, and the OpenAI GPT-based models for extraction. Each input document is processed\nin multiple iterations to collect diverse generations. TheRS-FMDis stored in JSONL records and versioned\nvia DVC to ensure reproducibility.\nD. LLM-BASEDIN-CONTEXTRANKINGPROMPT\nTo re-rank candidate foundation models without training a dedicated learning-to-rank model, we leverage in-\ncontext learning (ICL) with a LLM. The prompt explicitly instructs the LLM to prioritize user requirements,\ncompare candidate models, and produce a ranked list with explanations. We provide few-shot examples\n19\ncreated by an expert in the prompt to guide the model toward consistent ranking behavior. The prompt is\nconnected toRS-FMDto provide the metadata of the candidate models. Below is the prompt template we\nare using in the ranking module:\nPrompt Template:\nYou are an expert in remote sensing foundation model selection.\nYou will be given:\n1. A structured user query specifying task requirements and constraints.\n2. A list of candidate models retrieved from a database, each with metadata\nfields.\nYour goal:\n- Rank the candidate models from most to least suitable for the user\u2019s query.\n- For each model, provide a brief explanation in several bullet points\ndescribing why it is placed at that rank.\n- Prioritize hard constraints (application, modality, required sensor, and\nmin_performance if provided), then consider secondary preferences (spatial/\ntemporal resolution, application type, domain keywords, etc.).\n- When two models equally satisfy the constraints and preferences, prefer the\nmodel that is more efficient, better validated on diverse benchmarks, or\nmore versatile(multimodal, multi-temporal).\n[Example]\nStructured Query:\n{\n\"application\": \"land cover classification\",\n\"modality\": \"multispectral\",\n\"sensor\": [\"Sentinel-2\"],\n\"min_performance\": {\n\"metric\": [\"accuracy\"],\n\"value\": [85]\n}\n}\nCandidate Models:\n1. S2MAE\n2. Prithvi\n3. CACo\nRanking Output:\n1. S2MAE\n- Directly supports Sentinel-2 multispectral data\n- Achieves 99.1\\% accuracy on EuroSAT, exceeding 85\\% requirement\n- Purpose-built for land cover classification\n2. Prithvi\n- Supports multi-temporal multispectral data, including Sentinel-2\n- Accuracy slightly below requirement on similar tasks\n- More generalist FM\n3. CACo\n- Only supports RGB modality\n- Accuracy below the 85\\% requirement\n- Designed mainly for change detection and event retrieval\nYour Task:\n20\nGiven the following new query and candidates, produce a ranked list with\nexplanations.\nStructured Query:\n{query}\nCandidate Models:\n{candidates}\nPlease output the ranked list as JSON in the following format:\n[\n{\n\"model\": <model_name>,\n\"rank\": <integer>,\n\"reason\": [<short bullet points>]\n},\n...\n]\nE. EXPLANATIONGENERATORPROMPT\nThe explanation generator uses an LLM to produce concise, interpretable justifications for the final ranked\nFM list. The prompt template in our explanation generator is given as follows:\nYou are an expert in remote sensing foundation model selection.\nThe structured user query is:\n{query}\nThe final ranked candidate models with their metadata are:\n{ranked_models}\nYour task:\n1. For each model, output a JSON object with:\n- \"model_name\"\n- \"explanation\" (several bullet points on why it is recommended)\n- \"paper_link\"\n- \"repository\"\n2. Highlight how the model satisfies or partially satisfies the query.\n3. Mention key trade-offs if relevant (accuracy vs. efficiency, modality\ncoverage, etc.).\nF. PROMPT FORRAG-LLM BASELINE\nFor the LLM-RAG baseline, we prompt an LLM with the original user input and the retrieved model\ndocumentation as a context. The LLM is instructed to select and rank the top three remote sensing foundation\nmodels and provide concise explanations for each recommendation.\nYou are an expert in remote sensing foundation models.\nThe user has provided the following task description:\n21\n{user_input}\nBelow is a set of candidate models with their documentation:\n{context_str}\nYour task:\n1. Select and rank the top 3 remote sensing foundation models most suitable for\nthe task.\n2. For each selected model, provide:\n-- A short explanation of why it fits the task requirements.\n-- The reason for its ranking position compared to others.\n-- Any other relevant information from the context.\n3. Follow this exact output format:\n1. model: <model_name>\nexplanation:\n- <reason 1>\n- <reason 2>\n- <reason 3>\n2. model: <model_name>\nexplanation:\n- <reason 1>\n- <reason 2>\n- <reason 3>\n3. model: <model_name>\nexplanation:\n- <reason 1>\n- <reason 2>\n- <reason 3>\nG. EXPERTEVALUATIONPROCEDURE\nExpert Background.All annotations were performed by two experts with a computer science background\nand specialization in RS. Both have prior experience working with RSFMs, have published in the relevant\ndomains, and are familiar with model architectures, pretraining datasets, and evaluation practices.\nAnnotation Protocol.To ensure consistency and reproducibility, we followed a structured, multi-stage\nscoring protocol:\n\u2022Rubric Design.We created a detailed rubric for all seven criteria in Table 1, including definitions, exam-\nples, and decision rules.\n\u2022Calibration Phase.Both experts annotated an initial subset of model-query pairs. Disagreements were\nused to refine the rubric until interpretations aligned.\n\u2022Independent and Blind Scoring.Experts then rated all remaining model-query pairs independently and\nwithout access to system identities or each other\u2019s scores.\n\u2022Disagreement Resolution.Any pair with substantial disagreement was re-examined in a controlled dis-\ncussion, with decisions resolved strictly according to the rubric.\nObjective Scoring Rules.Where possible, we used explicit rules to reduce subjectivity:\n22\n\u2022 Reported Performance. Reported performance was determined by checking for benchmarks that matched\nthe queried task. If none existed, we evaluated performance on broader but related tasks. For example, if\nthe query specifies the task as scene classification, and there is no benchmark for this, we look for gen-\neral classification benchmarks. Depending on its performance, this model gets a moderate/high reported\nperformance score. Models with no relevant benchmarks received a low score.\n\u2022 Efficiency. Model parameter counts were normalized to a 0-5 scale as a proxy for complexity, and com-\nbined with reported performance to obtain a final efficiency score. Specifically, we divide this complexity\nmeasure by the reported performance to produce a final efficiency score, also on a 0-5 scale. Popularity.\nPopularity was used as a practical usability indicator rather than a measure of inherent model quality. We\nused normalized GitHub star counts (when code exists) and Google Scholar citation counts (when paper\nis unavailable). This reflects maturity, community adoption, and available ecosystem support.\n\u2022 Generalizability. We quantified pretraining diversity using three measurable components extracted from\nofficial FM documentation:\n1. Geographic diversity: global (score 5), multi-regional (3\u20134), or single-region coverage (1\u20132).\n2. Sensor-modality diversity: number of distinct modalities used in pretraining e.g., optical, SAR, multi-\nspectral, hyperspectral).\n3. Dataset scale: reported total area, number of scenes, or total images.\nThese components were combined into a composite 1-5 score. Inter-annotator agreement confirmed that\nthe rule-based definitions reduced subjectivity.\n\u2022 Recency. Recency was defined by the publication year or the latest model-card update:\n2025\u20132026= 5,2024 = 4,2023 = 3,2022 = 2, . . .\nGiven the rapid evolution in RSFMs, this criterion serves as a soft heuristic rather than a primary determi-\nnant.\nReference Sources.All judgments were grounded in publicly available references for each foundation\nmodel. Experts used: (1) published papers and preprints; (2) official GitHub repositories and model docu-\nmentation; (3) public benchmark results; (4) citation databases; and (5) described pretraining datasets from\nofficial sources. These references provided the necessary information on modality support, reported perfor-\nmance, efficiency, generalizability, popularity, and recency.\nH. QUERYTEMPLATE FORCREATINGBENCHMARKDATASET\nTo construct a representative and diverse benchmark dataset for evaluation, we define 16 structured query\ntemplates. Each template corresponds to a specific category of user constraints:\n\u2022Data Availability (A1\u2013A5):\n\u2013A1:No Training Data\u2014 User wants to use pre-trained models directly.\n\u2013A2:Sufficient Labeled Data\u2014 User has enough labels to fine-tune or train from scratch.\n\u2013A3:Few-shot Labels\u2014 User has a small set of labeled data only and requires models that\ngeneralize in low-data regimes.\n\u2013A4:Unlabeled Data Only\u2014 User has input data but no labels and seeks models suited for\nunsupervised or self-supervised settings.\n\u2013A5:Data Adaptation Needed\u2014 User\u2019s data differs from typical inputs, requiring domain\nadaptation or compatibility adjustments.\n\u2022Computational Resources (B1\u2013B3):\n\u2013B1:Limited Resources\u2014 e.g., CPU-only laptop.\n\u2013B2:Moderate Resources\u2014 e.g., desktop with GPU.\n\u2013B3:High Resources\u2014 e.g., cluster-scale GPU compute.\n\u2022Application Complexity (C1\u2013C3):\n23\nTable 5: Structured query templates used for benchmark dataset generation. Each template maps to one\nconstraint category. Slot values ({application},{sensor},{region}) are drawn from a predefined\nvocabulary and paraphrased by an LLM.\nTemplate (Natural Language) Categories\nI\u2019m looking for a model I can use out-of-the-box for{application}using{modality}data. I don\u2019t\nhave any labeled training data.A1\nI have a well-labeled dataset for{application}with{modality}in{region}. Which model would\nbe best to fully fine-tune from scratch?A2\nI only have a few labeled samples for{application}using{sensor}. I want a model that can adapt\nwell in a few-shot setting.A3\nI have a lot of unlabeled{modality}imagery from{region}. I need a model that works well with\nself-supervised or unsupervised learning for{application}.A4\nMy data uses{sensor}with{spatial resolution}resolution, but most models I\u2019ve seen don\u2019t\nsupport it. Can you recommend one that can be adapted?A5\nI\u2019m working on{application}but only have access to a laptop with no GPU. Which model would be\nsmall enough to run locally?B1\nI\u2019m using a desktop with a single GPU and doing{application}on{modality}imagery. Which\nmodels balance performance and efficiency?B2\nFor{application}, I have access to cloud GPUs and can afford large models. What\u2019s the most powerful\nfoundation model I can try?B3\nI\u2019m doing basic{application}(e.g., 3\u20134 land classes). What lightweight model would you suggest for\nfast experimentation?C1\nI\u2019m working on multi-class classification{application}with{modality}images. The task isn\u2019t\ntrivial, but I don\u2019t need pixel-level precision.C2\nI need a model for high-resolution segmentation or fine-grained{application}. Accuracy and spatial\ndetail are important.C3\nFor{application}using{sensor}data, I mainly care about achieving the highest overall accuracy,\neven if the model is large.D1\nFor{application}using{sensor}imagery, I want clean and accurate outputs with minimal false\ndetections; clear boundaries and reliable predictions are most important.D2\nFor{application}using{sensor}imagery, I need to ensure all target instances are captured, even if\nsome false alarms occur; completeness is critical.D3\nI need fast inference for{application}in near real-time on{device}. What\u2019s a good lightweight\nmodel?D4\nI\u2019m doing{application}on{modality}in{region}, but I only have few-shot labels and limited\ncompute. Which model fits this setup best?Composite\n\u2013C1:Simple Application\u2014 Applications with low label granularity or few classes (e.g., binary\nclassification, basic change detection).\n\u2013C2:Moderate Application\u2014 Applications with moderate difficulty, such as multi-class clas-\nsification or coarse semantic segmentation.\n\u2013C3:Complex Application\u2014 Applications requiring fine-grained spatial precision, multi-class\nsegmentation, multi-modal fusion, or high-resolution outputs.\n\u2022Evaluation Priorities (D1\u2013D4):\n\u2013D1:Accuracy-Focused\u2014 Maximize correctness of classification or segmentation outcomes.\n\u2013D2:Output Quality-Critical\u2014 Prioritize clean, well-bounded, and visually reliable out-\nputs (e.g., high mIoU, sharp edges, no artifacts).\n\u2013D3:Coverage-Critical\u2014 Ensure all relevant regions or objects are detected, even at the cost\nof some false positives (e.g., disaster mapping, change detection).\n24\n\u2013D4:Speed-Critical\u2014 Require lightweight or low-latency models for fast inference on edge\ndevices.\nAccordingly, Table 5 shows the full list of templates used to generate the benchmark queries. Slot val-\nues (e.g.,{application},{sensor},{region}) are drawn from a predefined vocabulary and instan-\ntiated using sampling and LLM-based paraphrasing.\nI. EXPERTSCORINGWEIGHTCONFIGURATION\nTo aggregate model evaluation scores during expert labeling, we apply a weighted linear combination of the\nseven criteria from Table 1. The weights are as follows:\nCriterion Weight (%)\nApplication Compatibility 25\nModality Match 20\nReported Performance 20\nEfficiency 15\nGeneralizability 10\nPopularity 5\nRecency 5\nThese weights were empirically determined on the basis of expert interviews. We normalize raw scores\nbefore aggregation.\nJ. ILLUSTRATIVEEXAMPLES OFEXPERTSCORING\nTo improve transparency, we provide several examples demonstrating how experts applied the scoring rubric\nto real model-query pairs. Each example includes: (1) the natural-language query, (2) the top-3 FM selec-\ntions from all systems, and (3) the expert ratings across the seven criteria defined in Table 1. These examples\nshow how rubric-guided, independent scoring yields consistent and interpretable evaluations.\nExample 1:\nQuery:I need a model for fine-grained land cover classification using high-resolution multispectral im-\nagery. Accuracy and spatial detail are important.\nSelected FMs (Top-3 from Each System):See Table 6.\nExample 2:\nQuery:I only have a few labeled samples for urban expansion detection using Sentinel-1 and Sentinel-2\ntime series data from 2016-2023. I want a model that can adapt well in a few-shot setting.\nSelected FMs (Top-3 from Each System):See Table 6.\nThese examples illustrate how the rubric was applied in practice and how expert judgments reflect both task\nrequirements and model capabilities. They also demonstrate how rubric-guided scoring minimizes subjective\nvariation across annotators.\n25\nTable 6: Evaluation results for queries 1 and 2.Criteria:CR1 - Application Compatibility; CR2 - Modality\nMatch; CR3 - Reported Performance; CR4 - Efficiency; CR5 - Generalizability; CR6 - Popularity; CR7 -\nRecency.\nSystem Rank FM CR1 CR2 CR3 CR4 CR5 CR6 CR7 Final Score\nQuery 1\nREMSA1 OmniSat 5 5 5 5 4 3 4 94\n2 FlexiMo 4 4.5 4 2.5 1.5 3.5 5 75\n3 CtxMIM 5 5 4.5 3 1.5 3.5 3 83.5\nREMSA-Naive1 OmniSat 5 5 5 5 4 3 4 94\n2 FlexiMo 4 4.5 4 2.5 1.5 3.5 5 75\n3 CtxMIM 5 5 4.5 3 1.5 3.5 3 83.5\nDB-Retrieval1 SpectralEarth 3 3 3.5 1.5 3 3 5 59.5\n2 OmniSat 5 5 5 5 4 3 4 94\n3 MATTER 4 4.5 4 4.5 3.5 1 2 75\nUnstr.-RAG1 FoMo 5 5 3.5 1.5 2 1.5 5 79.5\n2 DynamicVis 4 4 4 3.5 3.5 2 5 75\n3 SatVision-TOA 2.5 4 2.5 0 2.5 5 4 55\nQuery 2\nREMSA1 SSL4EO-S12 5 5 4 4 4.5 4.5 3 89.5\n2 Ial-SimCLR 3.5 5 3.5 5 2 3 3 77.5\n3 SeCo 3 3 3.5 5 5 2.5 1 67\nREMSA-Naive1 SoftCon 5 5 4.5 3 3 4 4 87\n2 SkySense 5 5 5 1 3.5 5 4 85.5\n3 SSL4EO-S12 5 5 4 4 4.5 4.5 3 89.5\nDB-Retrieval1 CACo 3 3 4 4 4 4 3 70\n2 SeCo 3 3.5 5 5 5 2.5 1 67\n3 SSL4EO-S12 5 5 4 4 4.5 4.5 3 89.5\nUnstr.-RAG1 CACo 3 3 4 4 4 4 3 70\n2 Copernicus-FM 3 3.5 3 1 3.5 5 5 62.5\n3 AnySat 3.5 5 3.5 1.5 4 4.5 5 74\n26\n",
    "title": "REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing",
    "arxiv_id": "2511.17442v1",
    "category": "recent",
    "authors": [
      "Binger Chen",
      "Tacettin Emre B\u00f6k",
      "Behnood Rasti",
      "Volker Markl",
      "Beg\u00fcm Demir"
    ],
    "published": "2025-11-21",
    "num_pages": 26,
    "num_chars": 77686
  },
  {
    "text": "InTAct: Interval-based Task Activation Consolidation for Continual Learning\nPatryk Krukowski\nJagiellonian University\nIDEAS NCBR\npatryk.krukowski@doctoral.uj.edu.plJan Miksa\nJagiellonian UniversityPiotr Helm\nJagiellonian University\nJacek Tabor\nJagiellonian UniversityPawe\u0142 Wawrzy \u00b4nski\nIDEAS Research InstitutePrzemys\u0142aw Spurek\nJagiellonian University\nIDEAS Research Institute\nAbstract\nContinual learning aims to enable neural networks to acqu-\nire new knowledge without forgetting previously learned\ninformation. While recent prompt-based methods perform\nstrongly in class-incremental settings, they remain vulnera-\nble under domain shifts, where the input distribution chan-\nges but the label space remains fixed. This exposes a per-\nsistent problem known as representation drift. Shared re-\npresentations evolve in ways that overwrite previously use-\nful features and cause forgetting even when prompts iso-\nlate task-specific parameters. To address this issue, we in-\ntroduce InTAct, a method that preserves functional beha-\nvior in shared layers without freezing parameters or storing\npast data. InTAct captures the characteristic activation ran-\nges associated with previously learned tasks and constra-\nins updates to ensure the network remains consistent within\nthese regions, while still allowing for flexible adaptation el-\nsewhere. In doing so, InTAct stabilizes the functional role of\nimportant neurons rather than directly restricting parame-\nter values. The approach is architecture-agnostic and inte-\ngrates seamlessly into existing prompt-based continual le-\narning frameworks. By regulating representation changes\nwhere past knowledge is encoded, InTAct achieves a prin-\ncipled balance between stability and plasticity. Across di-\nverse domain-incremental benchmarks, including Domain-\nNet and ImageNet-R, InTAct consistently reduces represen-\ntation drift and improves performance, increasing Average\nAccuracy by up to 8 percentage points over state-of-the-art\nbaselines.\n1. Introduction\nContinual learning enables neural networks to acquire new\nknowledge over time while retaining previously learned in-\nformation. Human beings achieve this naturally by balan-cingstability, the preservation of past knowledge, andpla-\nsticity, the ability to learn new tasks. Deep neural networks,\nhowever, struggle to maintain this balance [21]. When ad-\napting to new data, their internal representations oftendrift,\naltering the learned features that were important for earlier\ntasks and ultimately leading tocatastrophic forgetting[34].\nMany existing approaches mitigate forgetting by constra-\nining parameter updates, yet few explicitly control how the\nnetwork\u2019s function itself evolves across tasks. Uncontrolled\nchanges in these representations remain a key source of in-\nstability.\nThis challenge of internal representation drift is espe-\ncially evident inprompt-based continual learning[11,\n47, 53, 54]. These methods currently achieve state-of-the-\nart performance in bothclass-incrementalanddomain-\nincrementalsettings, but their reliance on shared compo-\nnents makes them vulnerable to representational drift. In\nclass-incremental learning (CIL), where tasks introduce\nnew classes, prompt-based models approach optimal per-\nformance. Contrary, in domain-incremental learning (DIL),\nwhere the label space is fixed but input distributions shift,\nthese models often degrade as internal features drift. Altho-\nugh prompting reduces interference at the embedding level,\nit does not explicitly regulate how shared representations\nevolve within trainable modules, leaving the model\u2019s inter-\nnal function unconstrained.\nRecent work such as the Kolmogorov\u2013Arnold Classi-\nfier (KAC) [17] introduces the notion offunctional locality,\nconstraining how output heads change across tasks. Yet, this\nprinciple has not been extended to deeper representations,\nleaving open the fundamental question:how can we con-\ntrol where and how much a network\u2019s internal function\nshould adapt during continual learning?\nWe address this challenge withInterval-based Task Ac-\ntivation Consolidation(InTAct), a framework that enfor-\nces functional stability across all layers of the model (seearXiv:2511.17439v1  [cs.LG]  21 Nov 2025\nTasks\nTaskN\nTaskN+1\nTaskN+2class: goldfish\nclass: bird\nclass: roosterModel\nLayersIn-Layer Transformation\nRysunek 1. During training on task(N+1), InTAct preserves the functional region established by taskN(pink) while allowing the model\nto learn new knowledge in a distinct region (orange). After training, these regions merge into an expanded protected region (yellow),\ndefining where the layer\u2019s transformation should remain stable in future updates. The dashed curves illustrate transformations from earlier\ntasks, highlighting that their behaviors remain preserved within their respective regions, while adaptation is freely allowed outside them.\nFig. 1). Rather than typical approaches, which operate in\nthe parameters\u2019 space, InTAct regularizes activations direc-\ntly, which preserves network behaviour. After learning each\ntask, it summarizes the stable activation patterns as boun-\nded regions,activation intervals, and aggregates them into\nmultidimensionalactivation hypercubes. An interval here\nis a one-dimensional range capturing the lower and upper\nbounds of a neuron\u2019s typical activation values, representing\nthe region within which its response remains functionally\nconsistent. A hypercube generalizes this concept to multiple\nneurons within a layer, jointly describing where the layer\u2019s\ntransformation is expected to be stable.\nWhen learning new tasks, these hypercubes act as soft\nfunctional constraints: the model is encouraged to keep acti-\nvations within established stable regions when appropriate,\nyet retains flexibility to explore new regions for adaptation.\nThis mechanism enables continual learning that is both ro-\nbust and adaptive, preserving prior functionality while sup-\nporting new knowledge acquisition. By operating at the re-\npresentation level, InTAct directly controls how the model\u2019s\nfunction evolves, instead of only constraining parameters.\nWhen integrated into state-of-the-art prompt-based fra-\nmeworks, InTAct improves Average Accuracy (AA) by\nup to 8 percentage points (p.p.) on challenging domain-\nincremental benchmarks such as DomainNet [37] and\nImageNet-R [15]. Moreover, InTAct remains effective even\nin settings without pretrained backbones, demonstrating its\nversatility as a general continual learning regularization\nstrategy. These results show that activation-level consolida-\ntion provides a general and effective approach for robust\nand adaptive continual learning systems.The main contributions of this work are:\n1. We propose InTAct1, a continual learning method that\npreserves stable functional behavior through a novel re-\ngularization guided by activation hypercubes.\n2. InTAct achieves a principled balance between stability\nand plasticity, maintaining the model\u2019s functionality on\npast tasks while enabling flexible adaptation to new ones.\n3. We demonstrate that InTAct consistently improves state-\nof-the-art prompt-based methods by up to 8 p.p. in AA\non challenging continual learning benchmarks.\n2. Related Works\nFor an extended literature overview, see Supplementary Ma-\nterials (SM) 6.\nContinual LearningContinual learning methods [8,\n33, 36] fall into replay-based, regularization-based, and\nparameter-isolation-based categories. Replay-based appro-\naches store exemplars [2\u20134, 27, 29, 39, 50] or generate\nsynthetic data [46, 57] to revisit past tasks. Regularization-\nbased methods add loss terms to constrain parameter chan-\nges, leveraging current [20, 26] or prior data [1, 5, 21,\n60], or null-space projections [22, 48, 51, 52]. Parameter-\nisolation techniques allocate task-specific subnetworks,\ne.g., Progressive Neural Networks [42, 43], Piggyback [32],\nPackNet [31], HAT [44], and SupSup [56], but require task\nIDs at inference.\n1Code available at:https://github.com/pkrukowski1/\nInTAct\nclass: goldfish class: bird class: roosterTask N Task N+1 Task N+2\nPrompt PoolPretrained\nFeature\nExtractorQuery\nFunction\nClassifier\nInTAct\nSoftmaxEmbedding Layer\nRysunek 2. Integration of InTAct with prompt-based methods. In-\nTAct stabilizes features within the pretrained extractor and con-\nstrains activation changes in the classifier, mitigating representa-\ntion drift across tasks.\nPrompt-Based Continual LearningPrompt-based me-\nthods leverage pretrained models for rehearsal-free conti-\nnual learning. L2P [11] and DualPrompt [47] optimize ad-\naptive prompts to guide frozen backbones. CoDA-Prompt\n[53] decomposes attention for modularity, and C-Prompt\n[54] ensures prompt stability across tasks. While prompt-\nbased approaches reduce forgetting, they still suffer from\ndrift in shared parameters, such as classifiers or learna-\nble prompts, especially in DIL scenarios. InTAct addres-\nses this by regularizing these shared parameters to prevent\ndrift of important features, maintaining stable representa-\ntions across tasks. Figure 2 illustrates how InTAct integra-\ntes with these prompt-based methods, stabilizing shared pa-\nrameters and reducing representation drift across multiple\ntasks in continual learning.\nInterval Arithmetic in Continual LearningInterval ari-\nthmetic [6, 35] has been applied in continual learning, for\nexample, in InterContiNet [55], which constrains weights\nusing intervals to ensure multi-task performance, and HINT\n[24], which maps intervals in the embedding space to the\nnetwork via hypernetworks. In contrast, we use interval ari-\nthmetic to define activation hypercubes that capture prior\ntask data, without propagating intervals and avoiding the\nwrapping effect. Each layer is then regularized to keep its\ntransformations within these hypercubes, ensuring stable\nrepresentations across tasks.\n3. Method\nAt its core, continual learning should allow a network to\nevolve gradually, adjusting its learned representations lo-cally to accommodate new knowledge while leaving its\nfunctional structure intact. In practice, however, most ap-\nproaches either constrain parameters globally or rely on re-\nplaying past data, both of which limit scalability and effi-\nciency. What is missing is a mechanism that can selectively\nstabilize regions of the network\u2019s function that matter, wi-\nthout halting its capacity to adapt elsewhere.\nGoal.Our method addresses this gap by preserving pre-\nviously learned internal representations without freezing\nparameters or using data replay. We summarize the activa-\ntion distribution of each layerlfrom past tasks and define a\nprotected region, a hypercubeH lcapturing the centralp%\nof neuron activations. This region represents the critical ac-\ntivation manifold to preserve. During new task learning, all\nparameters remain fully trainable, but the transformation of\nlayerl+1is regularized to stay stable for any input within\nHl, while regions outside it remain free to adapt. This tar-\ngeted constraint maintains prior functionality and enables\nnew knowledge acquisition, achieving a principled balance\nbetween stability and plasticity.\nIdea Visualization.To illustrate the concept, Fig. 3\nshows how InTAct incrementally learns three segments of a\nGaussian function. In the first task, the network learns to ap-\nproximate the first segment and identifies the corresponding\nactivation hypercubes. During the second task, the model le-\narns the next segment while InTAct actively protects the ac-\ntivations established in the first task (Fig. 3a) from drifting.\nThis exact process continues for subsequent tasks, demon-\nstrating how InTAct successfully balances stability (prese-\nrving old segments) and plasticity (learning new ones).\nThe following sections describe how these activation hy-\npercubes are computed and updated throughout the conti-\nnual learning process.\n3.1. Activation Interval Representation\nWe aim to summarize activation distributions from past ta-\nsks without storing all raw activations. LetS Hbe the set\nof layer indices for which a hypercube from the preceding\nlayer is available. For each tasktand each layerl\u2208 S H, we\nbuild a hypercubeH l,tthat captures the stable activation re-\ngion at that layer. Importantly, InTAct need not be applied\nto every layer.\nLetx l,tdenote the activation vector of thel-th layer in-\ndex inS Hafter learning taskt. To define the hypercube\nboundaries, we capture the centralp%of the activation di-\nstribution by excluding the extreme\u03b1=100\u2212p\n2%tails, for\nexample,\u03b1= 5forp= 90.\nFor each neuronjin the selected layer, we determine the\nper-task activation range by computing its lower and upper\n(a) Task 1: learning the first segment of the Gaus-\nsian.\n(b) Task 2: adapting to the second segment while\npreserving the first.\n(c) Task 3: completing the Gaussian while reta-\nining prior mappings.\nRysunek 3. The model learns a Gaussian function in three sequential tasks. At each stage, InTAct constrains activations within previously\nestablished hypercubes, allowing new segments to be learned without overwriting prior knowledge.\nbounds, which together define the characteristic hypercube:\nHl,t=\u0002\nxl,t,xl,t\u0003\n,(1)\nwhere\nxl,t[j] =percentile\u03b1%(xl,t[j]),(2)\nxl,t[j] =percentile(100\u2212\u03b1)% (xl,t[j]).(3)\nImportantly, these hypercubes are computed onlyaftera\ntask has been fully learned, ensuring they reflect stable ac-\ntivation patterns rather than transient learning dynamics.\nCumulative Hypercube Update.As the model learns se-\nquentially, we must preserve the functional regions from\nallprevious tasks. We define a cumulative hypercube,H(t)\nl,\nwhich represents the total protected region for layer index\nl\u2208 S Hafter learningttasks.\nAfter the new tasktis learned and its hypercubeH l,t\nis computed, we merge it with the previous cumulative hy-\npercubeH(t\u22121)\nl. The update rule is a simple elementwise\nexpansion:\nH(t)\nl=h\nmin\u0000\nx(t\u22121)\nl, xl,t\u0001\n,max\u0000\nx(t\u22121)\nl,xl,t\u0001i\n,(4)\nwherex(t\u22121)\nlandx(t\u22121)\nlare the bounds of the cumula-\ntive hypercube from the previous step, and themin(\u00b7)and\nmax(\u00b7)operations are applied elementwise.\nAs new tasks arrive, these cumulative hypercubes gra-\ndually expand to cover the full range of activations obse-\nrved across all tasks. This compact representation ensures\nthat each layer retains the functional regions essential for\nsolving past tasks while leaving room for adaptation.\n3.2. Functional Preservation via Regularization\nIn this subsection, we describe our regularization method.\nThe central idea is to permit parameter updates (\u2206\u03b8)only\nif they do not alter the network\u2019s output function for inputs\ncorresponding to previous tasks. Our approach targets this\ngoal directly in the activation space.Internal Representation Drift Loss.When training on\na new taskt, parameter updates\u2206\u03b8can alter the output\nof each layer. This representation drift changes the overall\nfunction and degrades performance on past tasks.\nOur method constrains the transformationf(\u00b7;\u03b8 l)of\neach layer indexl\u2208 S H. We require that for any inputx\ndrawn from the protected cumulative hypercube of the pre-\nvious layer index,H(t\u22121)\nl\u22121, the layer\u2019s output remains inva-\nriant to the parameter update\u2206\u03b8 l. This condition is forma-\nlized as:\nf(x;\u03b8 l+ \u2206\u03b8 l) =f(x;\u03b8 l),\u2200x\u2208 H(t\u22121)\nl\u22121.(5)\nConsider an affine layer followed by an activation:\nf(x;\u03b8 l) =\u03c3(W lx+b l), where\u03b8 l={W l, bl}, andW l\nandb lare learnable parameters. A direct and robust way to\nsatisfy Eq. (5) is to enforce invariance at the pre-activation\nlevel. If the pre-activation remains constant, the output of\nstandard activation functions (e.g., ReLU) will also be inva-\nriant. This yields a simpler, stricter constraint:\n(Wl+ \u2206W l)x+ (b l+ \u2206b l) =W lx+b l,\u2200x\u2208 H(t\u22121)\nl\u22121,\nwhich simplifies to:\n\u2206W lx+ \u2206b l= 0,\u2200x\u2208 H(t\u22121)\nl\u22121.(6)\nWhile formulated for affine layers, this principle is readily\nextended to other types, such as convolutional layers (see\nSM 13).\nEnforcing this pointwise constraint over allx\u2208 H(t\u22121)\nl\u22121\nis intractable. Our key insight is to leverage interval ari-\nthmetic to enforce the constraint over theentire hyper-\ncubeH(t\u22121)\nl\u22121simultaneously. We represent the hypercube\nas\u0002\nx(t\u22121)\nl\u22121,x(t\u22121)\nl\u22121\u0003\nand reformulate the constraint (Eq. (6))\nas:\n\u2206W l\u0002\nx(t\u22121)\nl\u22121,x(t\u22121)\nl\u22121\u0003\n+ \u2206b l= [0,0].(7)\nTo translate this constraint into a differentiable loss, we\ncompute the bounds of the resulting hypercube. Using stan-\ndard interval arithmetic (detailed in SM 12), the output in-\nterval for thei-th neuron is\u0002\n\u02dcxi+ \u2206b l,i,\u02dcxi+ \u2206b l,i\u0003\n, where:\n\u02dcxi=\u0000\n\u2206w\u22a4\nl,i\u0001+x(t\u22121)\nl\u22121\u2212\u0000\n\u2206w\u22a4\nl,i\u0001\u2212x(t\u22121)\nl\u22121,(8)\n\u02dcxi=\u0000\n\u2206w\u22a4\nl,i\u0001+x(t\u22121)\nl\u22121\u2212\u0000\n\u2206w\u22a4\nl,i\u0001\u2212x(t\u22121)\nl\u22121.(9)\nHere,(\u2206w\u22a4\nl,i)+and(\u2206w\u22a4\nl,i)\u2212denote the element-wise po-\nsitive and negative parts of thei-th row of\u2206W l. The loss\nis applied only to layers for which an input hypercube is\navailable. Letl 1denote the first such layer. Earlier layers\ncannot be regularized because no input hypercube exists for\nthem. For eachl\u2208 S H\\l1, we use the following penalty:\nLIntDrift =\u03bb IntDriftX\nl\u2208SH\\l1nlX\ni=1\u0002\n(\u02dcxi+\u2206b l,i)2+(\u02dcxi+\u2206b l,i)2\u0003\n.\n(10)\nThis constrains the network within preserved activation re-\ngions while allowing updates outside them.\nRemaining Challenges.The proposedL IntDrift loss effec-\ntively stabilizes internal representations operating within\nthe protected activation hypercubes of preceding layers. Ho-\nwever, two key challenges remain. First, early feature lay-\ners, those before the first defined hypercubeH l1, are not\nconstrained by this mechanism. To ensure full feature stabi-\nlity, a complementary regularization strategy is required, as\nintroduced in the next section. Second, as tasks accumulate,\ncumulative hypercubes (Eq. (4)) may expand into empty re-\ngions between disjoint activation spaces, potentially over-\nconstraining the model and reducing plasticity.\nHow InTAct Differs from Existing Regularizers.\nUnlike parameter-based regularizers (e.g., EWC, SI) that\npenalize weight changes,L IntDrift directly constrains func-\ntional change in activation space, ensuring that the ne-\ntwork\u2019s behavior, not just its parameters, remains consi-\nstent. In contrast to output-level methods like LwF or those\nrelying on discrete point sampling [7], our interval-based\nformulation enforces stability across continuous activation\nregions, offering a principled and tractable guarantee of\nfunctional preservation. Hypercubes act as compact, data-\nfree summaries of past tasks, avoiding replay or architec-\ntural overhead and making InTAct lightweight, privacy-\npreserving, and easily integrable into existing continual le-\narning frameworks.\nActivation Compactness Regularization.To prevent the\nexcessive growth of activation hypercubes and improve re-\npresentational efficiency, we introduce a compactness regu-\nlarization term. This loss,L Var, encourages activations from\nthe current task to remain concentrated in a smaller, denser\nregion of the feature space.For a layer indexl\u2208 S Hand theN tsamples of the\ncurrent taskt, letx i,l\u22121 be the input to the layer with index\nlfor samplei(i.e., the activation from the layer with index\nl\u22121). We compute the empirical mean activation \u00aff(t)\nlusing\nthe current modelf(\u00b7;\u03b8(t)\nl):\n\u00aff(t)\nl=1\nNtNtX\ni=1f(xi,l\u22121;\u03b8(t)\nl).(11)\nThe activation dispersionV(t)\nlis the mean squared deviation\nfrom this mean:\nV(t)\nl=1\nNtNtX\ni=1\r\r\rf(xi,l\u22121;\u03b8(t)\nl)\u2212\u00aff(t)\nl\r\r\r2\n2.(12)\nThe compactness regularization is the sum of these disper-\nsions over the relevant layer indices:\nLVar=\u03bb VarX\nl\u2208SHV(t)\nl,(13)\nwhere\u03bb Var>0controls the strength of the penalty. Minimi-\nzingL Varforces tighter clustering of activations, resulting in\nmore compact hypercubes. This ensures our functional re-\ngularization (L IntDrift ) operates over smaller, more meaning-\nful regions, mitigating hypercube over-expansion and pre-\nserving model plasticity for future tasks.\nInter-Task Alignment Regularization.The cumulative\nhypercube update rule (Eq. (4)) is prone to over-regularizing\ninactive space, particularly when activation regions from se-\nquential tasks are disjoint. If a new task\u2019s hypercube is far\nfrom the previous cumulative region, the rule expands to co-\nver the wide gap between them, unnecessarily constraining\na \"free\"region that contains no meaningful activations. This\nseverely restricts plasticity, especially in deeper layers.\nTo prevent this and promote smoother transitions, we in-\ntroduce an inter-task alignment regularization term,L Align.\nThis loss encourages the centers of the newly learned acti-\nvation hypercubes (H l,t) to remain close to the centers of\nthe hypercubes calculated from preceding tasks (H l,t\u22121).\nWe define the center of a hypercubeH lascl= (xl+\nxl)/2, and the radius asr l= (xl\u2212xl)/2. The alignment\nloss penalizes the squared distance between the new center\ncl,tand the previous centerc l,t\u22121:\nLAlign=\u03bb AlignX\nl\u2208SH\u2225cl,t\u2212cl,t\u22121\u22252\n2\nrmean\nl,t\u22121+\u03b5.(14)\nHere,\u03b5is a small constant for numerical stability, and the\ndenominator provides an adaptive scaling mechanism. We\nuse the mean radius of the previous task\u2019s hypercube across\nitsdldimensions,rmean\nl,t\u22121=1\ndlPdl\nj=1r(j)\nl,t\u22121.\nIntuitively, the radiusrmean\nl,t\u22121 reflects the spread of past\nrepresentations. A small radius indicates that past tasks oc-\ncupied a narrow, critical region, thus scaling the penalty up\nand enforcing stricter alignment. Conversely, a large radius\nimplies widely spread representations, weakening the con-\nstraint and granting the model more freedom to adapt. This\nadaptive scaling efficiently stabilizes the model\u2019s represen-\ntational space while preserving flexibility.\nFeature Distillation Loss.To stabilize the early feature\nlayers (those with indicesl < l 1), we introduce a feature\ndistillation loss,L Feat. This loss is applied at the layer with\nindexl start, where the first hypercubeH lstart is defined.\nFor each input samplex i, we compare its feature represen-\ntation from the current modelf(t)\nFeat(xi)against the frozen re-\npresentation from the previous modelf(t\u22121)\nFeat(xi). Crucially,\nwe do this only on a subset of feature dimensions indicated\nby a binary maskM:\nLFeat=\u03bbFeat\nNtNtX\ni=1\r\r\r\u0000\nf(t)\nFeat(xi)\u2212f(t\u22121)\nFeat(xi)\u0001\n\u2299M\r\r\r2\n2\ns+\u03b5,\n(15)\nwheresis the number of active entries (1s) inM, ands+\u03b5\nnormalizes the loss magnitude.\nUnlike standard knowledge distillation, which often\nover-restricts the model by constraining the entire feature\nvector,L Featimplements aselective stabilization. It prese-\nrves only a controlled subset of features deemed important\nfrom past tasks, leaving the remaining dimensions free to\nadapt to the new task. This targeted approach preserves le-\narned structure while maintaining the flexibility required to\nacquire new information.\nFinal Regularization Objective.Our full training objec-\ntive combines the base continual learning lossL Taskwith the\nproposed regularization terms:\nLTotal=L Task+L IntDrift +L Var+L Align+L Feat.(16)\nEach component targets a different aspect of the stabi-\nlity\u2013plasticity trade-off:L IntDrift constrains changes in the\nmodel\u2019s internal function across tasks,L Varkeeps activa-\ntion distributions compact to prevent representational over-\nexpansion,L Alignregulates the growth of protected hypercu-\nbes over time, andL Featstabilizes early feature layers while\nallowing adaptation in later ones. Their effects are illustra-\nted in Fig. 4.\nBecause these regularizers act directly on activations, In-\nTAct can be seamlessly integrated into diverse continual le-\narning methods. An algorithm of the training procedure ap-\npears in SM 14, and ablations isolating each term are repor-\nted in Section 4.4. Experiments\nWe evaluate the effectiveness of InTAct across multi-\nple benchmarks under both CIL and DIL scenarios. Our\nexperiments compare InTAct with regularization-based and\nprompt-based continual learning methods, demonstrating\nconsistent performance gains across different architectures\nand learning paradigms. Additional results showing the ef-\nfectiveness of InTAct without pretrained backbones are pro-\nvided in SM 15. Moreover, SM 16 presents a comparison\nwith a broader range of methods for the CIL scenario.\n4.1. Experimental Setup\nDatasets.We evaluate the effectiveness of InTAct on\nseveral datasets. (1) For regularization-based approaches:\nSplit MNIST, Split FMNIST, and Split CIFAR-10; (2) For\nprompt-based approaches: Split CIFAR-100, DomainNet,\nand ImageNet-R. Except for Split CIFAR-100, we evalu-\nate performance under both CIL and DIL scenarios. Full\ndataset and task details are provided in SM 8.\nArchitectures.For the Split MNIST and Split FMNIST\ndatasets, we use a multilayer perceptron (MLP) with three\nhidden layers of 400 neurons each. For Split CIFAR-10, we\nemploy a pretrained ResNet-18 [14] as the feature extrac-\ntor, where the last residual block is unfrozen and adapted\nduring continual learning. For Split CIFAR-100, ImageNet-\nR, and DomainNet, we follow the CODA-Prompt, using a\nViT-B/16 encoder [9] pretrained on ImageNet-1K [41]. We\napply our method to activations of the last layer before Soft-\nmax as presented on Figure 2. Additional architectural and\nimplementation details are provided in SM 8.\nMetrics.We evaluate the performance of our method\nusing two standard continual learning metrics: Average Ac-\ncuracy (AA) and Average Forgetting (AF). Formal defini-\ntions of these metrics are provided in SM 11.\n4.2. Results\nThis subsection discusses evaluation results of InTAct\nacross diverse continual learning benchmarks. Overall, the\nresults highlight InTAct\u2019s ability to enhance stability while\npreserving plasticity across varied scenarios.\nResults for Prompt-based Methods (DIL).Overall, In-\nTAct consistently delivers the strongest performance across\nall prompt-based methods in the DIL setting, substantially\nimproving both AA and reducing AF. The detailed results\nare reported in Tab. 1 for the 6-task DomainNet benchmark,\nwhere tasks share labels but differ in visual domains. Inte-\ngrating InTAct yields gains of 4\u20136 percentage points in AA\nand decreases AF by up to 60%, with the largest impro-\nvements observed when combined with DualPrompt (AA:\nInTAct InTAct InTAct\nTask N\nTask N+1\nRysunek 4. Illustration of how the InTAct loss components (Eq. 23) regulate representation updates between consecutive tasks.L IntDrift and\nLFeatpreserve previously learned in-layer transformations within protected activation hypercubes (pink), maintaining consistency of past\nfeature space while allowing adaptation in new regions.L Varconstrains the expansion of activation hypercubes so that new regions (orange)\nremain compact and balanced across tasks, preventing uncontrolled growth.L Align enforces smooth transitions by aligning consecutive\nhypercubes and avoiding fragmentation of functional regions. Purple outlines denote the InTAct-regularized case, where the representation\nremains stable and well-aligned across tasks.\n50.87%\u219256.83%, AF:12.13\u21924.66). Similar trends\nfor L2P and CODA-Prompt confirm that InTAct stabilizes\nshared prompt representations effectively across different\nprompting strategies without additional replay mechanisms\nor architectural overhead.\nTabela 1. Results on the 6-task (345 classes each) DomainNet\nbenchmark (DIL setting). We report the mean and standard de-\nviation of AA and AF averaged over 3 random seeds.\nMethod AA (\u2191) AF (\u2193)\nUpper-Bound79.65\u2013\nL2P48.44\u00b10.09 14.56\u00b10.05\nw/ InTAct53.44\u00b10.02 6.95\u00b10.99\nDualPrompt50.87\u00b10.31 12.13\u00b10.2\nw/ InTAct56.83\u00b10.28 4.66\u00b10.11\nCODA-P52.52\u00b10.22 11.54\u00b10.16\nw/ InTAct57.36\u00b10.25 6.49\u00b10.11\nTab. 2 further shows consistent benefits on the 15-task\nImageNet-R benchmark, a more challenging scenario with\ngreater domain diversity and more incremental steps. Note\nthat integrating InTAct not only preserves prior knowledge\nbut can also enhance performance on earlier tasks (e.g., AF\ndrops from1.01to\u22120.36with DualPrompt), highlighting\nthe method\u2019s ability to stabilize representations in prompt-\nbased architectures. Across both benchmarks, InTAct im-\nproves stability while maintaining plasticity, all without re-\nplay buffers, replay, or architectural changes.\nAcross both DomainNet and ImageNet-R in the DIL set-\nting, InTAct consistently enhances stability without hinde-\nring plasticity. These gains are achieved without additional\nreplay mechanisms or architectural modifications.Tabela 2. Results on the 15-task (200 classes each) ImageNet-R\nbenchmark (DIL setting). We report the mean and standard devia-\ntion of AA and AF averaged over 5 random seeds.\nMethod AA (\u2191) AF (\u2193)\nUpper-Bound77.13\u2212\nL2P50.34\u00b10.26 1.46\u00b10.19\nw/ InTAct58.34\u00b10.24 0.02\u00b10.1\nDualPrompt53.31\u00b10.36 1.01\u00b10.17\nw/ InTAct61.85\u00b10.45\u22120.36\u00b10.17\nCODA-P61.21\u00b10.49 0.68\u00b10.11\nw/ InTAct66.11\u00b10.24 0.35\u00b10.07\nResults for Prompt-based Methods (CIL).Tab. 3 re-\nports AA and AF on ImageNet-R for 10- and 20-step CIL\nscenarios. Across all baselines, integrating InTAct consi-\nstently improves or matches performance, confirming its\nrobustness and plug-and-play compatibility with diverse\nprompt-based continual learning frameworks. For L2P, In-\nTAct delivers clear AA gains and lower standard devia-\ntions, indicating that our regularization stabilizes optimiza-\ntion and promotes smoother learning. DualPrompt also be-\nnefits, maintaining high accuracy with reduced forgetting\nand improved consistency. The largest improvements ap-\npear for CODA-Prompt, where InTAct boosts AA by nearly\none point in both settings, showing that functional preserva-\ntion effectively complements prompt-based methods.\nThis improvement is most pronounced in CODA-Prompt\ndue to its shared, trainable prompt pool, composed of\nprompt tokens (P m), keys (K m), and attention vectors\n(Am), which are jointly updated across tasks. During con-\ntinual learning, this shared structure can easily drift, cau-\nsing instability in both the generated prompts and the clas-\nTabela 3. Results on the ImageNet-R dataset (CIL setting). We\nreport the mean and standard deviation of AA and AF averaged\nover 5 random seeds.\nMethod10 steps 20 steps\nAA (\u2191) AF (\u2193) AA (\u2191) AF (\u2193)\nUpper-Bound 77.13 \u2013 77.13 \u2013\nL2P69.29\u00b10.732.03\u00b10.1965.89\u00b11.301.24\u00b10.14\nw/ InTAct69.44\u00b10.452.89\u00b10.4666.18\u00b10.361.30\u00b10.13\nDualPrompt71.32\u00b10.621.71\u00b10.24 67.87\u00b11.391.07\u00b10.14\nw/ InTAct70.98\u00b10.601.68\u00b10.07 67.89\u00b10.661.19\u00b10.18\nCODA-P75.45\u00b10.561.60\u00b10.2072.37\u00b11.191.00\u00b10.15\nw/ InTAct76.40\u00b10.162.07\u00b10.1673.30\u00b10.411.56\u00b10.26\nsifier head. InTAct counteracts this by jointly stabilizing the\nprompt-generation process and the classifier\u2019s decision bo-\nundaries through its regularization losses (LIntDrift,LFeat,\nLAlign). This coordinated stabilization reduces internal re-\npresentation drift and prevents interference between evo-\nlving prompts and classification layers.\nOverall, InTAct enhances the stability and consistency of\nCODA-Prompt, demonstrating that even advanced prompt-\nbased approaches remain vulnerable to internal functional\ndrift. By precisely regulating how and where a model\u2019s re-\npresentations evolve during continual learning, InTAct pro-\nvides a principled and broadly applicable solution. Consi-\nstent findings are observed on the CIFAR-100 dataset, with\nadditional results reported in SM 15.\n4.3. Ablations\nIn Tab. 4, we report an ablation study analyzing the con-\ntribution of each component of InTAct. The experiment is\nconducted on the Split CIFAR-10 benchmark under the DIL\nsetting. We report the AA averaged over five seeds for the\nfull model and three seeds for each ablated variant, where\na single component is removed at a time. The results show\nthat excluding any term consistently degrades performance,\ndemonstrating that all components of InTAct contribute in\na complementary manner. The most significant drop oc-\ncurs whenL Varis removed, highlighting its importance in\ncontrolling the compactness of activation intervals. Without\nthis term, the network\u2019s representations are regularized over\nexcessively large hypercubes, as described in Eq. (5), which\nweakens the effectiveness of our regularization. Moreover,\nwe also investigate how different values of\u03bb Varaffect AA,\nand present these results in SM 15.\nWe also observe that removingL Align slightly decreases\nAA. This component prevents distant activation regions\nfrom merging into excessively large hypercubes when tasks\nare related (see Eq. (4)). Without it, two small but separate\nregions can combine into a much larger one, causing the\nmodel to regularize a substantial amount of irrelevant spaceTabela 4. Ablation study of different components in InTAct. The\nresults are averaged over 3 random seeds for ablations.\nMethod AA\nInTAct75.84\u00b10.63\nAblateL IntDrift 71.63\u00b11.34\nAblateL Var 67.43\u00b11.32\nAblateL Align 74.37\u00b10.46\nAblateL Feat 71.04\u00b10.54\nand limiting its ability to adapt effectively to new tasks.\n4.4. Additional Experiments\nImpact of\u03bb IntDrift on AA in DIL setting.Fig. 5 il-\nlustrates how different values of\u03bb IntDrift affect the sta-\nbility\u2013plasticity trade-off when InTAct is integrated into\nCODA-Prompt under the DIL scenario on the Domain-\nNet dataset. While accuracy naturally decreases as new ta-\nsks are introduced, the strength of the regularization plays\na key role in controlling this decline. A significant value\n(\u03bbIntDrift = 0.1) overconstrains the model, hindering adap-\ntation to new tasks, whereas a very small value (\u03bb IntDrift =\n0.0001) provides insufficient functional stability. The best\nperformance is obtained for\u03bb IntDrift = 0.001, which yields\nthe highest AA across tasks by achieving a balanced trade-\noff.\nRysunek 5. Impact of the\u03bb IntDrift hyperparameter on AA for the\nCODA-Prompt method. Results are averaged over 2 random seeds.\nThese results confirm that our regularization does not\noverconstrain the model and effectively mediates the sta-\nbility\u2013plasticity balance. All other hyperparameters are fi-\nxed to the optimal configuration previously determined for\nCODA-Prompt on DomainNet under the DIL setting to iso-\nlate the effect of\u03bb IntDrift . For clarity, the reported values cor-\nrespond to the scaled coefficient\u03bb\u2032\nIntDrift =\u03bbIntDrift\nCwhere\nCis the total number of classes in the benchmark (here,\nC= 345).\n5. Conclusions\nWe introduced InTAct, a continual learning method that\npreserves knowledge at the activation level. Rather than\nconstraining parameters or replaying data, InTAct protects\nkey activation ranges that encode past knowledge, keeping\ntransformations consistent within these regions while al-\nlowing flexibility elsewhere. This prevents representation\ndrift without reducing adaptability. InTAct is lightweight,\nmodel-agnostic, and integrates easily with existing architec-\ntures, improving stability and accuracy across benchmarks\nsuch as DomainNet and ImageNet-R, and narrowing the gap\nto the multi-task upper bound.\nLimitations.Current activation regions are represented as\nhypercubes, which may not capture the full complexity of\nactivation geometries. Future work will explore more fle-\nxible region representations and improved mechanisms for\nconsolidating knowledge across tasks.\nLiteratura\n[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny,\nMarcus Rohrbach, and Tinne Tuytelaars. Memory aware sy-\nnapses: Learning what (not) to forget, 2018. 2, 1\n[2] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide\nAbati, and Simone Calderara. Dark experience for general\ncontinual learning: a strong, simple baseline. InAdvances in\nneural information processing systems, pages 15920\u201315930,\n2020. 2, 1\n[3] Arslan Chaudhry, Marc\u2019Aurelio Ranzato, Marcus Rohrbach,\nand Mohamed Elhoseiny. Efficient lifelong learning with a-\ngem. InInternational Conference on Learning Representa-\ntions, 2018. 7\n[4] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhose-\niny, Thalaiyasingam Ajanthan, Puneet K. Dokania, Philip\nH. S. Torr, and Marc\u2019Aurelio Ranzato. On tiny episo-\ndic memories in continual learning, 2019. arXiv preprint\narXiv:1902.10486. 2, 1\n[5] Zhikang Chen, Abudukelimu Wuerkaixi, Sen Cui, Haoxuan\nLi, Ding Li, Jingfeng Zhang, Bo Han, Gang Niu, Houfang\nLiu, Yi Yang, Sifan Yang, Changshui Zhang, and Tianling\nRen. Learning without isolation: Pathway protection for con-\ntinual learning. InInternational Conference on Machine Le-\narning, 2025. 2, 1\n[6] Germund Dahlquist and \u00c5ke Bj\u00f6rck.Numerical methods in\nscientific computing, volume I. SIAM, 2008. 3, 2\n[7] MohammadReza Davari, Nader Asadi, Sudhir Mudur, Rahaf\nAljundi, and Eugene Belilovsky. Probing representation for-\ngetting in supervised and unsupervised continual learning,\n2022. 5\n[8] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Pa-\nrisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne\nTuytelaars. A continual learning survey: Defying forgetting\nin classification tasks.IEEE transactions on pattern analysis\nand machine intelligence, 44(7):3366\u20133385, 2021. 2, 1[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mo-\nstafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain\nGelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth\n16x16 words: Transformers for image recognition at scale,\n2021. 6, 2, 4\n[10] Arthur Douillard and Timoth\u00e9e Lesort. Continuum: Simple\nmanagement of complex continual learning scenarios, 2021.\n4\n[11] Zhanxin Gao, Jun Cen, and Xiaobin Chang. Consistent\nprompting for rehearsal-free continual learning. InProce-\nedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 28463\u201328473, 2024. 1, 3\n[12] Dipam Goswami, Yuyang Liu, Bart\u0142omiej Twardowski, and\nJoost van de Weijer. Fecam: Exploiting the heterogeneity of\nclass distributions in exemplar-free continual learning, 2024.\n1\n[13] Dipam Goswami, Albin Soutif-Cormerais, Yuyang Liu, San-\ndesh Kamath, Bart Twardowski, and Joost van de Weijer. Re-\nsurrecting old classes with new data for exemplar-free conti-\nnual learning. InProceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 28525\u2013\n28534, 2024. 2\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition, 2015. 6, 4\n[15] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-\nvath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,\nSamyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt,\nand Justin Gilmer. The many faces of robustness: A critical\nanalysis of out-of-distribution generalization.ICCV, 2021.\n2, 4\n[16] Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and\nZsolt Kira. Re-evaluating continual learning scenarios: A\ncategorization and case for strong baselines, 2019. 4\n[17] Yusong Hu, Zichen Liang, Fei Yang, Qibin Hou, Xialei Liu,\nand Ming-Ming Cheng. Kac: Kolmogorov-arnold classifier\nfor continual learning, 2025. 1\n[18] Paul Janson, Wenxuan Zhang, Rahaf Aljundi, and Moha-\nmed Elhoseiny. A simple baseline that questions the use of\npretrained-models in continual learning. InNeurIPS 2022\nWorkshop on Distribution Shifts: Connecting Methods and\nApplications, 2022. 1\n[19] Luc Jaulin, Michel Kieffer, Olivier Didrit, and Eric Walter.\nApplied Interval Analysis: With Examples in Parameter and\nState Estimation, Robust and Control. Springer, London,\n2001. 6, 7\n[20] Sanghwan Kim, Lorenzo Noci, Antonio Orvieto, and Tho-\nmas Hofmann. Achieving a better stability-plasticity trade-\noff via auxiliary networks in continual learning. InProce-\nedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 11930\u201311939, 2023. 2, 1\n[21] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel\nVeness, Guillaume Desjardins, Andrei A. Rusu, Kieran\nMilan, John Quan, Tiago Ramalho, Agnieszka Grabska-\nBarwinska, Demis Hassabis, Claudia Clopath, Dharshan Ku-\nmaran, and Raia Hadsell. Overcoming catastrophic forget-\nting in neural networks.Proceedings of the National Aca-\ndemy of Sciences, 114(13):3521\u20133526, 2017. 1, 2\n[22] Yajing Kong, Liu Liu, Zhen Wang, and Dacheng Tao. Ba-\nlancing stability and plasticity through advanced null space\nin continual learning. InEuropean Conference on Computer\nVision, pages 219\u2013236, 2022. 2, 1\n[23] Alex Krizhevsky. Learning multiple layers of features from\ntiny images.University of Toronto, 2012. 4\n[24] Patryk Krukowski, Anna Bielawska, Kamil Ksi \u02db a \u02d9zek, Pawe\u0142\nWawrzy \u00b4nski, Pawe\u0142 Batorski, and Przemys\u0142aw Spurek. Hint:\nHypernetwork approach to training weight interval regions in\ncontinual learning.Information Sciences, 717:122261, 2025.\n3, 2\n[25] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing\ncontinuous prompts for generation, 2021. 2\n[26] Zhizhong Li and Derek Hoiem. Learning without forgetting.\nIEEE transactions on pattern analysis and machine intelli-\ngence, 40(12):2935\u20132947, 2017. 2, 1, 11\n[27] Yan-Shuo Liang and Wu-Jun Li. Inflora: Interference-free\nlow-rank adaptation for continual learning. InProceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 23638\u201323647, 2024. 2, 1\n[28] Xuan Liu and Xiaobin Chang. Lora subtraction for drift-\nresistant space in exemplar-free continual learning. InPro-\nceedings of the IEEE/CVF conference on Computer Vision\nand Pattern Recognition, pages 15308\u201315318, 2025. 2\n[29] David Lopez-Paz and Marc\u2019Aurelio Ranzato. Gradient epi-\nsodic memory for continual learning, 2022. 2, 1\n[30] Tamasha Malepathirana, Damith Senanayake, and Saman\nHalgamuge. Napa-vq: Neighborhood-aware prototype au-\ngmentation with vector quantization for continual learning.\nInProceedings of the IEEE/CVF International Conference\non Computer Vision, pages 11674\u201311684, 2023. 1\n[31] Arun Mallya and Svetlana Lazebnik. Packnet: Adding mul-\ntiple tasks to a single network by iterative pruning. InPro-\nceedings of the IEEE conference on Computer Vision and\nPattern Recognition, pages 7765\u20137773, 2018. 2, 1\n[32] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggy-\nback: Adapting a single network to multiple tasks by learning\nto mask weights. InProceedings of the European Conference\non Computer Vision (ECCV), pages 67\u201382, 2018. 2, 1\n[33] Marc Masana, Xialei Liu, Bartlomiej Twardowski, Mikel\nMenta, Andrew D. Bagdanov, and Joost van de Weijer.\nClass-incremental learning: survey and performance evalu-\nation on image classification.IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 45(5):5513\u20135533, 2022.\n2, 1\n[34] Michael McCloskey and Neal J. Cohen. Catastrophic inter-\nference in connectionist networks: The sequential learning\nproblem. pages 109\u2013165. Academic Press, 1989. 1\n[35] Ramon E. Moore, R. Baker Kearfott, and Michael J. Cloud.\nIntroduction to interval analysis. SIAM, 2009. 3, 2, 5, 7, 8\n[36] German I. Parisi, Ronald Kemker, Jose L. Part, Christopher\nKanan, and Stefan Wermter. Continual lifelong learning with\nneural networks: A review.Neural Networks, 113:475\u2013480,\n2019. 2, 1\n[37] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate\nSaenko, and Bo Wang. Moment matching for multi-source\ndomain adaptation. InProceedings of the IEEE InternationalConference on Computer Vision, pages 1406\u20131415, 2019. 2,\n4\n[38] Gr\u00e9goire Petit, Adrian Popescu, Hugo Schindler, David Pi-\ncard, and Bertrand Delezoide. Fetril: Feature translation for\nexemplar-free class-incremental learning. InProceedings of\nthe IEEE/CVF winter conference on applications of compu-\nter vision, pages 3911\u20133920, 2023. 2\n[39] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu,\nIrina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn\nwithout forgetting by maximizing transfer and minimizing\ninterference. InInternational Conference on Learning Re-\npresentations, 2018. 2, 1\n[40] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy P.\nLillicrap, and Greg Wayne. Experience replay for continual\nlearning, 2019. 11\n[41] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\nLi Fei-Fei. Imagenet large scale visual recognition challenge,\n2015. 6\n[42] Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins,\nHubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Ra-\nzvan Pascanu, and Raia Hadsell. Progressive neural ne-\ntworks, 2016. arXiv preprint arXiv:1606.04671. 2, 1\n[43] Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins,\nHubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Ra-\nzvan Pascanu, and Raia Hadsell. Progressive neural ne-\ntworks, 2022. 2, 1\n[44] Joan Serr\u00e0, D\u00eddac Sur\u00eds, Marius Miron, and Alexandros Ka-\nratzoglou. Overcoming catastrophic forgetting with hard at-\ntention to the task. InInternational Conference on Machine\nLearning, pages 4548\u20134557, 2018. 2, 1\n[45] Wuxuan Shi and Mang Ye. Prototype reminiscence and\naugmented asymmetric knowledge aggregation for non-\nexemplar class-incremental learning. InProceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 1772\u20131781, 2023. 1\n[46] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim.\nContinual learning with deep generative replay. InAdvances\nin neural information processing systems, 2017. 2, 1\n[47] James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola\nCascante-Bonilla, Donghyun Kim, Assaf Arbelle, Rameswar\nPanda, Rogerio Feris, and Zsolt Kira. Coda-prompt: Con-\ntinual decomposed attention-based prompting for rehearsal-\nfree continual learning, 2023. 1, 3, 7, 10, 11, 12\n[48] Shixiang Tang, Dapeng Chen, Jinguo Zhu, Shijie Yu, and\nWanli Ouyang. Layerwise optimization by gradient de-\ncomposition for continual learning. InProceedings of the\nIEEE/CVF conference on Computer Vision and Pattern Re-\ncognition, pages 9634\u20139643, 2021. 2, 1\n[49] Marco Toldo and Mete Ozay. Bring evanescent representa-\ntions to life in lifelong class incremental learning. InPro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 16732\u201316741, 2022. 1\n[50] Edoardo Urettini and Antonio Carta. Online curvature-aware\nreplay: Leveraging 2nd order information for online conti-\nnual learning. InInternational Conference on Machine Le-\narning, 2025. 2, 1\n[51] Shipeng Wang, Xiaorong Li, Jian Sun, and Zongben Xu. Tra-\nining networks in null space of feature covariance for conti-\nnual learning. InProceedings of the IEEE/CVF conference\non Computer Vision and Pattern Recognition, pages 184\u2013\n193, 2021. 2, 1\n[52] Zhen Wang, Liu Liu, Yiqun Duan, Yajing Kong, and Da-\ncheng Tao. Continual learning with lifelong vision transfor-\nmer. InProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 171\u2013181, 2022.\n2, 1\n[53] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun,\nHan Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent\nPerot, Jennifer Dy, and Tomas Pfister. Dualprompt: Com-\nplementary prompting for rehearsal-free continual learning,\n2022. 1, 3\n[54] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ru-\noxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer\nDy, and Tomas Pfister. Learning to prompt for continual le-\narning, 2022. 1, 3\n[55] Maciej Wo\u0142czyk, Karol Piczak, Bartosz W\u00f3jcik, Lukasz Pu-\nstelnik, Pawe\u0142 Morawiecki, Jacek Tabor, Tomasz Trzcinski,\nand Przemys\u0142aw Spurek. Continual learning with guarantees\nvia weight interval constraints. InInternational Conference\non Machine Learning, pages 23897\u201323911. PMLR, 2022. 3,\n2\n[56] Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Ani-\nruddha Kembhavi, Mohammad Rastegari, Jason Yosinski,\nand Ali Farhadi. Supermasks in superposition. InAdvan-\nces in Neural Information Processing Systems, pages 15173\u2013\n15184, 2020. 2, 1\n[57] Chenshen Wu, Luis Herranz, Xialei Liu, Yaxing Wang, Joost\nvan de Weijer, and Bogdan Raducanu. Memory replay gans:\nLearning to generate new categories without forgetting. In\nAdvances in Neural Information Processing Systems, 2018.\n2, 1\n[58] Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-\nmnist: a novel image dataset for benchmarking machine le-\narning algorithms, 2017. 4\n[59] Lu Yu, Bartlomiej Twardowski, Xialei Liu, Luis Herranz,\nKai Wang, Yongmei Cheng, Shangling Jui, and Joost van de\nWeijer. Semantic drift compensation for class-incremental\nlearning. InProceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 6982\u20136991,\n2020. 1\n[60] Friedemann Zenke, Ben Poole, and Surya Ganguli. Conti-\nnual learning through synaptic intelligence, 2017. 2, 1\n[61] Da-Wei Zhou, Hai-Long Sun, Han-Jia Ye, and De-Chuan\nZhan. Expandable subspace ensemble for pre-trained model-\nbased class-incremental learning. InProceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Re-\ncognition, pages 23554\u201323564, 2024. 2\nInTAct: Interval-based Task Activation Consolidation for Continual Learning\nSupplementary Material\n6. Extended Overview of Related Works\nThis section provides an extended literature review of\ncontinual learning methods. It first categorizes classical\napproaches into replay-based, regularization-based, and\nparameter-isolation techniques. Subsequently, the focus shi-\nfts to modern parameter-efficient methods like prompt-\nbased continual learning (e.g., L2P, DualPrompt, CODA-\nPrompt) and contemporary strategies for managing feature\ndrift in the representation space. Finally, we review recent\napplications of interval arithmetic in continual learning,\ncontrasting these ideas with our proposed approach of re-\ngularizing functional transformations across model layers.\nContinual Learning.Continual learning methods [8, 33,\n36] can generally be divided into three categories: replay-\nbased, regularization-based, and parameter-isolation-based.\nReplay-basedmethods employ memory or rehearsal me-\nchanisms to recall past tasks during training, thereby ma-\nintaining low loss on those tasks. Two primary strategies\nare exemplar replay, which stores selected training samples\n[2\u20134, 27, 29, 39, 50], and generative replay, where models\nsynthesize previous data using generative models [46, 57].\nRegularization-basedmethods typically introduce a regu-\nlarization term into the loss function to constrain parame-\nter changes for previously learned tasks. This regulariza-\ntion may be defined by the current task data [20, 26] or\nprevious tasks data [1, 5, 21, 60] variants. Recent methods\nlimit weight updates to null space of previous data feature\ncovariance [22, 48, 51, 52]. Liang and Li [27] leverage gra-\ndient information from old tasks to construct a subspace\nfor LoRA\u2019s dimensionality reduction matrix, thereby redu-\ncing interference between the current task and the previous\nones.Parameter-isolationmethods learn task-specific sub-\nnetworks within the model. Techniques such as Progressive\nNeural Networks [42, 43], Piggyback [32], PackNet [31],\nHAT [44] and SupSup [56] allocate and combine parame-\nters for individual tasks. While effective in task-aware set-\ntings, these methods are based on assigning test samples to\ntasks, which may be problematic in some continual learning\nscenarios.\nPrompt-Based Continual Learning.These methods are\nphilosophically rooted in parameter-efficient learning,\nwhere the vast majority of the model (the backbone) rema-\nins unchanged to prevent catastrophic forgetting. The core\nmechanism involves introducing a small set of new, learna-\nble parameters calledpromptsfor each task. These prompts\nare typically vectors that are prepended to the input sequ-ence embeddings or inserted into intermediate layers. This\neffectively steers the frozen model\u2019s behavior towards the\nnew task\u2019s objective without overwriting knowledge from\nprevious tasks.\nA key innovation in this area is the concept of aprompt\npool. Rather than learning a single, monolithic prompt for\neach task, methods like L2P [11] learn a collection of shared\nprompt vectors. During inference, the model uses a query-\nbased mechanism (e.g., matching input features to prompt\nkeys) to select a sparse combination of prompts from this\npool that are most relevant to the current input instance. This\napproach not only adapts to the current task but also allows\nthe model to potentially recognize and handle inputs from\nprevious tasks by selecting the appropriate \"old\"prompts,\nenabling a form of rehearsal-free task routing.\nFurther refinements focus on thestructureandfunction\nof the prompts themselves. For instance, DualPrompt [47]\nintroduced the idea of learning two distinct sets of prompts:\nageneralprompt shared across all tasks to capture com-\nmon knowledge, and a set oftask-specificprompts to cap-\nture task-unique features. CoDA-Prompt [53] builds on this\nby using a decomposed attention mechanism, allowing the\nmodel to explicitly query general versus task-specific know-\nledge encoded in the prompts, thereby improving modula-\nrity and reducing interference between task-specific instruc-\ntions.\nDespite their success in parameter-efficient, rehearsal-\nfree learning, these methods face a fundamental limita-\ntion, particularly in domain-incremental learning (DIL):\nthey struggle to adapt the classifier to distributional shifts\nbetween tasks. While the backbone features remain largely\nfixed, the learned prompts or adapters often fail to capture\nsemantic drift in task labels or decision boundaries. Con-\nsequently, when a new task comes from a domain substan-\ntially different from previous ones, the classifier may mi-\nsrepresent the task despite informative backbone features.\nThis highlights the need for future approaches that strategi-\ncally update classifier parameters or the backbone itself, or\ndevelop more flexible prompt-tuning strategies capable of\nhandling evolving task distributions.\nAddressing Feature Drift in Continual Learning.A\ncritical challenge in continual learning is the feature drift\nof old classes when new tasks are learned without access to\nprevious samples. Methods often define prototypes of clas-\nses [12, 18] in the feature space and prevent their drift. SDC\n[59] and [49] estimate and compensate for feature drift with\ncurrent-task data following each training phase. NAPA-VQ\n[30] and Prototype Reminiscence [45] reshape old prototy-\npes through topological information with current-data sam-\nples. FeTrIL [38] introduces a feature translation strategy\nthat aligns new and old class feature distributions. ADC [13]\ngenerates adversarial pseudo-exemplars of new classes to\nadjust prototypes of earlier classes. EASE [61] introduces a\nsemantic-guided prototype complement strategy that resha-\npes prototypes of old classes in the feature space adjusted by\nthe current task\u2019s data. Liu and Chang [28] defines a drift-\nresistant space in which the model weights can be adjusted\nto the new task without interfering with old data features. In\nthis work, we do not refer to the concept of prototype. In-\nstead, we mitigate the feature drift by enforcing each layer\nto preserve its performed transformation on its subdomain\ndefined by the previous tasks\u2019 data.\nInterval Arithmetic in Continual Learning.Interval\narithmetic [6, 35] has been applied to continual learning in\nInterContiNet [55], where the key idea is to employ inte-\nrval constraints on the weights associated with successive\ntasks. The intersection of these intervals defines the subset\nof weights that yield satisfactory performance across all ta-\nsks. HINT [24] employs intervals in the embedding space\nand leverages a hypernetwork to map them to the weight\nspace of the target network. In contrast, we propose a novel\napproach, also grounded in interval arithmetic: each layer\nof the neural network is regularized to preserve its transfor-\nmation within a subdomain delineated by a hyper-interval,\nwhich encapsulates data from previously learned tasks.\nParameter-Isolation Approaches.Another strategy is to\nprevent interference across tasks by isolating parameters.\nProgressive Neural Networks [43] expand the architecture\nwith task-specific columns and lateral connections, reusing\nprior knowledge without overwriting it. While this elimi-\nnates forgetting by construction, network growth scales li-\nnearly with the number of tasks, making these approaches\nimpractical for long task sequences or large-scale settings.\nRegularization-Based Methods.Regularization-based\napproaches constrain weight updates to preserve infor-\nmation from earlier tasks. Elastic Weight Consolidation\n(EWC) [60] introduces a Fisher information\u2013based penalty\nto protect important parameters, while Synaptic Intel-\nligence (SI) [26] and Memory Aware Synapses (MAS)\n[21] estimate weight importance through gradient flow.\nLearning Without Forgetting (LwF) [1] further anchors\npredictions of past tasks using knowledge distillation.\nDespite their success, these methods focus primarily on the\nparameter space, leaving internal representations vulnera-\nble to drift. As a result, preserving weight importance does\nnot necessarily maintain the functional behavior of learned\nfeatures.Representation-Level Stability.A complementary di-\nrection emphasizes stabilizing learned representations ra-\nther than weights. Approaches inspired by interval analysis\n[35] and activation regularization aim to limit representa-\ntion drift by constraining activations to remain within task-\nspecific bounds. However, these ideas have seen limited ad-\noption in continual learning due to the difficulty of summa-\nrizing activation distributions efficiently.\n7. Preliminaries\nThis section presents the key technical concepts underlying\nprompt-based continual learning. We begin by explaining\nhow parameter-efficient prefix tuning allows a frozen pre-\ntrained feature extractor to be adapted without modifying\nits weights. We then provide an overview of three represen-\ntative prompt-management frameworks, L2P, DualPrompt,\nand CODA-Prompt, which illustrate how modern methods\norganize and select prompts to support rehearsal-free lear-\nning while minimizing interference across tasks.\nPrompt-Based Continual Learning.Recent work\nexplores continual learning through prompts, leveraging\nthe representational capacity of pretrained vision models.\nThe core idea is to introduce a small set of learnable prompt\ntokens, which guide a frozen network backbone to adapt\nto new tasks, significantly reducing catastrophic forgetting\nwithout requiring rehearsal of old data. We now detail the\nmechanics and principal architectures of these methods.\nLetf \u03b8denote a pretrained encoder (e.g., a Vision Trans-\nformer [9]) with frozen parameters\u03b8. We denote the input\ntoken embeddings to thel-th layer ash(l)\u2208RL\u00d7D, where\nLis the sequence length (tokens) andDis the embed-\nding dimension. In prompt-based continual learning, each\ntask is associated with a set of learnable prompt vectors,\np\u2208RLp\u00d7D, whereL pis the prompt length. These prompts\nserve as additional tokens that condition the model\u2019s self-\nattention, allowing it to adapt to new tasks without modify-\ning the shared backbone parameters\u03b8.\nA common and widely used implementation of prompt\ninjection is prefix-tuning [25]. In a standard multi-head self-\nattention (MSA) layer, the inputh(l)is linearly projected\ninto Query (Q), Key (K), and Value (V) matrices using fro-\nzen projection matricesW Q, WK, WV. Prefix-tuning intro-\nduces a layer-specific promptp(l)composed of two sets of\nvectors,p(l)\nK, p(l)\nV\u2208RLp\u00d7D. These vectors are prepended\n(concatenated) to theKandVmatrices, respectively, re-\nplacing the standard MSA operation with a prompted one:\nQ=h(l)WQ,\nK\u2032= [p(l)\nK;h(l)WK],\nV\u2032= [p(l)\nV;h(l)WV],\nh(l+1)= MSA(Q, K\u2032, V\u2032),(17)\nWhereMSA(\u00b7)denotes the standard multi-head self-\nattention operation,[; ]is concatenation along the sequence\ndimension, andW Q, WK, WVare the frozen projection ma-\ntrices.K\u2032andV\u2032are the augmented Key and Value ma-\ntrices incorporating the prompt componentsp(l)\nKandp(l)\nV.\nwhere[; ]denotes concatenation along the sequence length\ndimension. This design enables task-specific adaptation pu-\nrely through the small set of learnable promptsp(l)while\nkeeping the entire backbonef \u03b8frozen. Additionally, the\nmodel includes a trainable classifier head (a linear layer)\nwhose weights are optimized alongside the prompts to map\nthe prompt-adapted feature representations to the specific\nclass logits.\nPrompt Selection and Memory Structure.A central\nchallenge is managing and selecting the correct prompts\nwhen the task identity is unknown at test time (task-agnostic\ncontinual learning). Learning to Prompt (L2P) [54] addres-\nses this by storing prompts in a shared prompt pool (or me-\nmory)P={(k i, pi)}M\ni=1, whereMis the pool size. Each\npromptp iis associated with a corresponding learnable key\nki\u2208RD.\nDuring training on each task, a subset of the key-prompt\npairs is optimized via backpropagation to capture task-\nrelevant knowledge, whilef \u03b8remains frozen. At inference\ntime, L2P uses a two-stage mechanism to process an input\nx:\n1.Query Pass (Prompt Selection):The inputxis first\npassed through the frozen, unprompted encoderf \u03b8. The\nresulting output embedding of the[CLS]token,q(x) =\nf\u03b8(x)[CLS] , is used as a query. This query is compared\nto all keysk i\u2208 Pusing a similarity function (e.g., co-\nsine similarity) to select a subset containing theNmost\nrelevant prompts.\n2.Execution Pass (Task Processing):TheNselected\nprompts{p j}j\u2208S, withS\u2282 {1, . . . , M}, are first aggre-\ngated into a single final prompt, typically via averaging:\npfinal=1\n|S|P\nj\u2208Spj, where|S|denotes cardinality of\nS. The inputxis then processed by the encoderf \u03b8a se-\ncond time, wherep finalis injected into the self-attention\nlayers (e.g., using Eq. 17).\nThis mechanism allows the model to retrieve the most rele-\nvant knowledge from memory based on the input features.\nDuring training, only theNselected key\u2013prompt pairs are\nupdated, while the rest of the pool remains unchanged.\nOptimization Objective.Since the prompt retrieval me-\nchanism (e.g., Top-K selection) is typically discrete and\nnon-differentiable, gradients from the primary classifica-\ntion objective cannot flow back to update the keys{k i}.\nTo address this, prompt-based methods employ a compo-\nsite loss function. The learnable prompt vectorspand the\nclassifier head are optimized via the standard task lossL task(e.g., Cross-Entropy). Simultaneously, the keyskare opti-\nmized via an auxiliary surrogate lossL aux(e.g., a mat-\nching loss) that minimizes the distance between the query\nand the selected keys. The total objective is formulated as\nLTotal=L task+\u03bbL aux, ensuring that the keys learn to cap-\nture the input distribution while the prompts learn to mini-\nmize the prediction error.\nComplementary and Decomposed Prompting.Dual-\nPrompt [53] extends L2P by dividing the prompt space into\ntwo functional parts: a sharedG-Prompt(g), which captu-\nres task-invariant knowledge, and a task-specificE-Prompt\n(et), which captures task-unique features. DualPrompt in-\nserts E-Promptse t={e(l)\nt}into a set of shallow layers\n(e.g., layers[1, . . . , L e]) and the G-Promptg={g(l)}into\ndeeper layers (e.g., layers[L e+ 1, . . . , L g]). The forward\npass is a sequential application of prompted blocks:\nh(l+1)=(\nBlock(h(l), e(l)\nt)forl\u2208[1, L e]\nBlock(h(l), g(l))forl\u2208[L e+ 1, L g](18)\nwhereBlock(h, p)denotes a Transformer block using\npromptpin its MSA (via Eq. 17). Unlike L2P\u2019s shared\npool, DualPrompt learns a discrete set of key-prompt pa-\nirs{(k t, et)}(one for each taskt). At inference, the query\nq(x)is used to find the best-matching keyk tvia cosine si-\nmilarity, and its corresponding E-Prompte tis selected.\nCODA-Prompt [47] generalizes this mechanism by com-\nposing the final prompt from a weighted sum ofMlearna-\nble prompt componentsP={P m}M\nm=1. The queryq(x)\ncomputes a set of attention weights\u03b1(x) ={\u03b1 m(x)}M\nm=1\nover the components, representing a continuous composi-\ntion over the memory:\n\u03b1m(x) =exp\u0000\n\u27e8q(x)\u2299A m, Km\u27e9/\u03c4\u0001\nPM\nj=1exp\u0000\n\u27e8q(x)\u2299A j, Kj\u27e9/\u03c4\u0001,(19)\nwhereK m, Am\u2208RDare learnable keys and query-\nmodulation vectors,\u2299is the Hadamard product, and\u03c4is a\ntemperature parameter. The final promptp(x)is then com-\nposed as:\np(x) =MX\nm=1\u03b1m(x)P m.(20)\nThis continuous, differentiable composition increases flexi-\nbility, allows capacity to scale by adding new components\nwhen needed, and uses orthogonality regularization to re-\nduce interference.\n8. Implementation Details\nThis section provides all necessary implementation and\ntechnical details. We discuss the architectures utilized, the\nconstruction of the dataset tasks, and the GPU resources\nused for the computations.\nArchitectures.We perform experiments using three wi-\ndely used neural architectures:\n\u2022MLP: A three-layer fully connected network with 400\nhidden units per layer, used for low-dimensional datasets\n(Split MNIST and Split FMNIST). This serves as a ligh-\ntweight baseline.\n\u2022ResNet-18[14]: A convolutional network with residual\nconnections, used for medium-scale datasets such as\nSplit CIFAR-10. We use it as a feature extractor and\nunfreeze only the final residual block to allow limited ad-\naptation. As a head, we employ a lightweight MLP clas-\nsifier consisting of a single hidden layer with 400 units.\n\u2022ViT[9]: A Vision Transformer that operates on patch to-\nkens with global self-attention. It is used for larger and\nmore diverse benchmarks (DomainNet and ImageNet-R),\nand also for Split CIFAR-100. In this setup, ViT acts\nas both the feature extractor and the query function in\nprompt-based frameworks, and is kept fully frozen during\ntraining.\nThe classifier head is configured according to the continual\nlearning setting. In the CIL scenario, where each task in-\ntroduces new, non-overlapping classes, the classifier head is\nexpanded by adding output neurons for the new labels. In\nthe DIL scenario, the label set is shared across tasks, so the\nclassifier head remains fixed and only the input distribution\nchanges.\nFor prompt-based methods under CIL, the CODA-\nPrompt formulation applies an output masking mechanism\nthat restricts predictions to classes observed so far. To en-\nsure fair comparison, we adopt the same masking for all\nprompt-based baselines and for InTAct in CIL experiments,\nincluding Split CIFAR-10. We do not apply masking in the\nMLP experiments, as the model is intentionally capacity-\nlimited and we aim to evaluate each method\u2019s ability to mi-\ntigate forgetting without additional constraints.\nDatasets.We evaluate InTAct on six widely used conti-\nnual learning benchmarks that vary in scale, visual comple-\nxity, and type of distribution shift. All datasets are normali-\nzed to the[0,1]range. Unless stated otherwise, we consider\nboth the CIL and DIL scenarios.\n\u2022Split MNIST[16]: The MNIST dataset is partitioned into\nfive sequential tasks, each forming a binary classification\nproblem from two non-overlapping digit classes.\n\u2022Split FMNIST[58]: Constructed analogously to\nSplit MNIST using Fashion-MNIST, providing higher\nvisual diversity and texture across tasks.\n\u2022Split CIFAR-10[23]: CIFAR-10 is divided into five ta-\nsks, each containing two distinct object categories. This\nbenchmark introduces natural image variability, incre-\nasing the difficulty of representation stability.\n\u2022Split CIFAR-100[23]: CIFAR-100 is split into ten tasks,\neach containing ten classes. This benchmark is evaluatedonly under the CIL setting.\n\u2022DomainNet[37]: A large-scale multi-domain benchmark\nwith six visually distinct domains (clipart, painting, real,\nsketch, quickdraw, infograph). In the DIL setting, each\ndomain forms one task (6 tasks total). In the CIL setting,\nwe split classes into 5 tasks, with all domains present in\neach task. This results in domain-mixed CIL, where se-\nmantic novelty, not visual style, drives task progression.\n\u2022ImageNet-R[15]: A distribution shift benchmark of 200\nImageNet classes represented as renditions (e.g., dra-\nwings, sculptures, cartoons). In the DIL setting, we con-\nstruct sequential tasks by randomly partitioning the 200\nclasses into 15 or 5 partitions, while keeping the label\nspace constant. In the CIL setting, we evaluate two confi-\ngurations with 10 and 20 tasks, where each task contains a\ndisjoint subset of classes, but all domains remain present\nin every task.\nAll datasets are normalized to the[0,1]pixel range. For\nImageNet-R, images are resized such that the shorter side\nis 256px, then during training, randomly cropped to224\u00d7\n224with random horizontal flip, and normalized using the\nImageNet mean and standard deviation. At evaluation, the\nrandom crop is replaced with a center crop of the same size.\nCode and Reproducibility.For experiments involving\nprompt-based architectures, we build directly on the offi-\ncial implementation of CODA-Prompt, and we adapt the\nDomainNet DIL task handlers from the KAC GitHub re-\npository. To ensure strict reproducibility and a fair compa-\nrison with reported baselines, we match the software envi-\nronment used in CODA-Prompt, including the sametimm\nversion (0.4.12), which is essential to ensure identical pre-\ntrained ViT weights. Our method InTAct is integrated into\nthis codebase without modifying the underlying prompt se-\nlection or training routines.\nFor all remaining experiments (i.e., non-prompt-\nbased settings), we implement InTAct along with other\nreferenced methods within a custom continual lear-\nning framework, using the following package versions:\npyrootutils1.0.1,hydra-core1.3.2,omegaconf\n2.3.0,torch2.5.1,torchvision0.20.1,lightning\n2.5.0,lightning-fabric2.5.0,wandb0.19.1, and\ncontinuum[10].\nAll experiments were conducted on NVIDIA A100\n(80GB) and V100 (32GB) GPUs within a DGX cluster, as\nwell as on standalone RTX 4090 machines. Full training pi-\npelines, configuration files, and scripts for reproducing all\nreported results are available in our GitHub repository.\n9. Selected Hyperparameters of InTAct\nThis section summarizes the key hyperparameters and tra-\nining procedures used for InTAct across all benchmarks. For\ntransparency, we report both the selected values and the cor-\nresponding search ranges.\nOptimization Procedure.For Split MNIST,\nSplit FMNIST, and Split CIFAR-10 (under both CIL\nand DIL), we select hyperparameters using a Bayesian\noptimization framework. The following search ranges are\nused consistently:\n\u2022 Batch size (grid): {512, 256, 128}.\n\u2022 Number of epochs (grid): {15, 10, 5}.\n\u2022 Learning rate: log-uniform in[10\u22124,10\u22122].\n\u2022\u03bb Feat: uniform in[0,1000].\n\u2022\u03bb IntDrift : uniform in[0,1000].\n\u2022\u03bb Var: uniform in[0,10].\nWe emphasize that the baseline results reported in Ta-\nble 9 and Table 10 are also obtained using Bayesian search.\nFor readability, the hyperparameter values reported in the\ntables are rounded; the exact values used in experiments are\nprovided in the code. All training scripts and the final selec-\nted hyperparameters are included in the codebase.\nFor Split CIFAR-100, DomainNet, and ImageNet-R\n(prompt-based experiments), we adopt the hyperparameter\nconfigurations reported in CODA-Prompt wherever availa-\nble. CODA-Prompt does not provide results for Domain-\nNet under DIL, nor for ImageNet-R under DIL with 5- or\n15-task settings. In these cases, we transfer the best con-\nfiguration identified in the corresponding CIL setting for\nDomainNet, and for ImageNet-R we reuse the configura-\ntion tuned for the 5-task CIL scenario. Training schedules\nand learning-rate schedulers remain unchanged. To accom-\nmodate GPUs with lower memory capacity, we reduce the\nbatch size from 128 to 64 in these experiments.\nFor InTAct integrated into prompt-based architectures\nunder the CIL setting, we additionally perform a grid se-\narch over:\n\u2022\u03bb Var: {0.001, 0.01, 0.1, 1.0},\n\u2022\u03bb IntDrift : {0.0001, 0.001, 0.01, 0.1},\n\u2022\u03bb Feat: {0.0001, 0.001, 0.01, 0.1}.\nIn the DIL setting, we use the same grid search, but addi-\ntionally scale\u03bb IntDrift by the total number of classes encoun-\ntered throughout training. We found that the classifier head\nto which this regularization is applied is sensitive to the ab-\nsolute magnitude of\u03bb IntDrift , making this scaling beneficial\nfor stable optimization.\nThe configuration achieving the highest validation AA is\nused for all final results reported in the main paper.\nBest Hyperparameters.The selected hyperparame-\nters for InTAct on Split MNIST, Split FMNIST, and\nSplit CIFAR-10 are shown in Table 5. Table 6 reports\nthe hyperparameters used when integrating InTAct with\nprompt-based methods. Note that in the CIL setting, we\nset\u03bb IntDrift to zero because the outputs corresponding topreviously seen classes are masked in the classifier at the\ntop of the ViT architecture. In this case, the loss component\ndefined in Eq. (10) does not apply.\n10. Interval Arithmetic\nInterval arithmetic, introduced in its modern form by Ra-\nmon E. Moore [35], provides a method for performing com-\nputations on ranges of real numbers rather than single po-\nint values. An intervalxis a closed, bounded set of real\nnumbers defined by its endpoints,x= [x l, xu], such that\nxl\u2264xu. This framework is essential for validated nume-\nrics, as it allows for the rigorous containment of numerical\nerrors, including rounding errors and uncertainties in initial\ndata.\n10.1. Fundamental Operations\nThe fundamental principle of interval arithmetic is theinc-\nlusion property: the resulting interval of an operation must\ncontain all possible results from applying the same opera-\ntion to any real numbers within the operand intervals. Let\nx= [x l, xu]andy= [y l, yu]be two intervals. The elemen-\ntary arithmetic operations are defined as follows:\nAddition.The sum is obtained by adding the respective\nendpoints:\nx+y= [x l+yl, xu+yu](21)\nSubtraction.The difference is computed by cross-adding\nthe endpoints:\nx\u2212y= [x l\u2212yu, xu\u2212yl](22)\nMultiplication.The product is the interval spanning the\nminimum and maximum of the four products of the\nendpoints.\nx\u00b7y= [z l, zu],(23)\nwhere\nzl= min(x lyl, xlyu, xuyl, xuyu),(24)\nzu= max(x lyl, xlyu, xuyl, xuyu).(25)\nDivision.Division is defined as multiplication by the reci-\nprocal ofy, provided that0/\u2208y.\nx\ny=x\u00b7\u00141\nyu,1\nyl\u0015\n,if0/\u2208[y l, yu]\nSuppose the divisor intervalycontains zero. In that\ncase, the operation is typically undefined or results in\nextended intervals (e.g., a union of two intervals), de-\npending on the specific framework being used [35].\nTabela 5. Selected hyperparameters used for InTAct across the Split MNIST, Split FMNIST, and Split CIFAR-10 benchmarks under both\nCIL and DIL settings.\nDataset Scenario Batch size Epochs Learning rate\u03bb Feat \u03bbIntDrift \u03bbVar\nSplit MNISTCIL512 5 2\u00d710\u22124779 6 4.2\nDIL512 5 1\u00d710\u22124602 634 4.0\nSplit FMNISTCIL128 10 4\u00d710\u22124227 731 0.8\nDIL512 5 1\u00d710\u22124927 196 1.6\nSplit CIFAR-10CIL32 15 1\u00d79\u22123391 792 0.2\nDIL64 5 6\u00d710\u22124239 96 0.7\nTabela 6. Selected regularization hyperparameters for InTAct integrated into prompt-based methods. Rows correspond to datasets and\nlearning scenarios; columns list the regularization coefficients used for each base method.\nL2P DualPrompt CODA-Prompt\nDataset Scenario\u03bb Feat \u03bbIntDrift \u03bbVar \u03bbFeat \u03bbIntDrift \u03bbVar \u03bbFeat \u03bbIntDrift \u03bbVar\nSplit CIFAR-100 CIL0.0001 0.0 0.001 0.1 0.0 0.001 0.01 0.0 0.001\nDomainNet CIL0.1 0.0 0.001 0.001 0.0 0.01 0.0001 0.0 1.0\nDomainNet DIL0.10.0001\n3450.001 0.0010.001\n3450.01 0.00010.001\n3451.0\nImageNet-R CIL (10 tasks)0.0001 0.0 0.01 0.001 0.0 0.001 0.1 0.0 0.1\nImageNet-R CIL (20 tasks)0.001 0.0 0.01 0.0001 0.0 0.001 0.1 0.0 0.1\nImageNet-R DIL (5 tasks)0.010.1\n2000.01 0.010.0001\n2000.1 0.0010.1\n2000.1\nImageNet-R DIL (15 tasks)0.10.01\n2000.01 0.00010.01\n2000.1 0.10.001\n2000.1\n10.2. Interval Matrix Multiplication\nThe operations of interval arithmetic extend naturally to li-\nnear algebra, enabling computations with interval matrices.\nAn interval matrixAis a matrix whose elements are in-\ntervals. Given two compatible interval matrices,AandB,\ntheir productC=ABis an interval matrix where each ele-\nmentc ijthe interval extension of the standard dot product\ndefines:\ncij=nX\nk=1aikbkj.(26)\nHere, each multiplicationa ikbkjis an interval multiplica-\ntion, and the summationPrepresents a sequence of inte-\nrval additions. This operation is fundamental for solving in-\nterval linear systems, which are crucial for analyzing the\neffects of bounded uncertainties in models. However, this\ndefinition is highly susceptible to the dependency problem;\nif the same interval variable appears in multiple entries of\nAorB, the resulting bounds onCcan be much wider than\nthe true range.\n10.3. Interval Convolutions\nInterval arithmetic can be extended to operations like co-\nnvolutions, which are fundamental in areas such as signal\nprocessing and image analysis. By representing an input si-\ngnalXand a filter kernelWas intervals, one can analyze\nsystems where inputs or parameters are subject to boundeduncertainties.\nA key operation is the interval convolution. For a 2D in-\nputX, an interval kernel (filter)W, and an optional interval\nbiasb, the resulting output feature mapYis computed as:\nYij=\uf8eb\n\uf8edX\nk,lXi+k,j+l \u00b7Wk,l\uf8f6\n\uf8f8+b.(27)\nThis computation propagates the interval bounds through\nthe filtering operation, producing a rigorous enclosure for\nthe true output. This is particularly useful in robust state\nestimation and control, where system parameters or measu-\nrements are known to lie within certain bounds [19].\n10.4. Key Properties and Considerations\nWhile interval arithmetic provides rigorous bounds, it has\ntwo important characteristics.\nFirst, to be implemented correctly on a computer, in-\nterval arithmetic must usedirected rounding(oroutward\nrounding). For any operation, the computed lower bound\nmust be rounded down (towards\u2212\u221e) and the computed\nupper bound must be rounded up (towards+\u221e). This prac-\ntice, supported by standards like IEEE 754, ensures that\nthe resulting floating-point interval strictly encloses the true\n(and often unknowable) real-number interval. It is critical to\nnote, however, that standard computing environments and\nprogramming languages, such as Python or MATLAB, do\nnot satisfy this requirement by default for general floating-\npoint operations. Specialized interval arithmetic libraries\n(e.g.,python-intervals,JuliaIntervals) or to-\nols that explicitly manage floating-point control registers\nare necessary to ensure correct outward rounding and, thus,\nthe validity of the computed bounds.\nSecond, interval arithmetic is subject to thedependency\nproblem. When a variable appears multiple times in an\nexpression, interval arithmetic treats each occurrence inde-\npendently, which can lead to a significant overestimation of\nthe true range. A classic example is computingx\u2212xwith\nx= [0,1]. The result is:\n[0,1]\u2212[0,1] = [0\u22121,1\u22120] = [\u22121,1]\nThis is a valid enclosure, but the true range for the function\nf(x) =x\u2212xover the interval[0,1]is simply{0}. This ove-\nrestimation, also known as thewrapping effect, is a central\nchallenge in interval-based methods, as seen in both matrix\nmultiplication and complex dynamic systems [19, 35].\n11. Metrics\nWe evaluate continual learning performance using two stan-\ndard metrics: the Average Accuracy (AA) and the Average\nForgetting (AF). These metrics assess how well the mo-\ndel retains knowledge across tasks and how much it forgets\nafter learning new ones.\nLet there beNtasks in sequence. LetR i,jdenote the test\naccuracy on taskiafter the model has been trained on task\nj. In particular,R i,Nrepresents the final accuracy on taski\nafter training on allNtasks.\nAverage Accuracy (AA).The AA measures the mean\nperformance over all tasks at the end of training:\nAA=1\nNNX\ni=1Ri,N.(28)\nA higherA Nindicates better overall continual learning per-\nformance and knowledge retention across all tasks.\nAverage Forgetting (AF).The standard definition of AF,\nas used for example in [3], measures for each task the gap\nbetween its best performance and its final performance:\n\u02dcAF=1\nN\u22121N\u22121X\ni=1\u0000\nmax\nj\u2208{i,...,N}Ri,j\u2212Ri,N\u0001\n,(29)\nwhereR i,jdenotes the accuracy on taskiafter training up\nto taskj.\nHowever, following the evaluation protocol used in the\nCODA-Prompt paper [47], and in order to ensure compa-\nrability with prompt-based baselines, we adopt the forget-\nting formulation implemented in their official codebase. Foreach tasktand each earlier taski < t, forgetting is accu-\nmulated as the incremental reduction in performance from\nstept\u22121tot, averaged over all earlier tasks:\nAF=1\nN\u22121NX\nt=2 \n1\nt\u22121t\u22121X\ni=1\u0000\nRi,t\u22121\u2212Ri,t\u0001!\n.(30)\nHere,R i,tdenotes the accuracy on taskiafter completing\ntraining on taskt. A lower value ofF Nindicates better re-\ntention of previously learned knowledge.\nThis corresponds exactly to the computation used in the\nCODA-Prompt evaluation script, ensuring direct compara-\nbility across all reported results.\n12. Detailed Derivation of Internal Represen-\ntation Drift Loss\nHere, we provide the complete derivation for theL IntDrift\nregularization term introduced in the main paper. Our ob-\njective is to ensure that thepreactivationsof thel-th layer,\nl\u2208 S H, remain unchanged for all inputsxbelonging to the\nhypercubeH(t\u22121)\nl\u22121. Formally, consider the affine layer\nf(x;\u03b8 l) =W lx+b l.(31)\nAfter applying the parameter update(\u2206W l,\u2206bl), the new\npreactivation becomes\nf(x;\u03b8 l+ \u2206\u03b8 l) = (W l+ \u2206W l)x+ (b l+ \u2206b l).(32)\nTo ensure that the preactivations of the layer remain unchan-\nged for everyx\u2208 H(t\u22121)\nl\u22121, we require that\nf(x;\u03b8 l+ \u2206\u03b8 l)\u2212f(x;\u03b8 l) = 0,\u2200x\u2208 H(t\u22121)\nl\u22121.(33)\nSubstituting the expressions above yields the condition\n\u2206W lx+ \u2206b l= 0,\u2200x\u2208 H(t\u22121)\nl\u22121.(34)\nUsing interval arithmetic, we extend this constraint over\nthe full hypercubeH(t\u22121)\nl\u22121= [x(t\u22121)\nl\u22121,x(t\u22121)\nl\u22121]. The interval\nversion of the preactivation-stability condition becomes\n\u2206W l\u0002\nx(t\u22121)\nl\u22121,x(t\u22121)\nl\u22121\u0003\n+ \u2206b l= [0,0].(35)\nThis condition must hold for each output coordinate. Let\nw\u22a4\nl,idenote thei-th row of the weight update matrix\u2206W l.\nWe therefore compute the interval bounds of the linear map-\npingw\u22a4\nl,iH(t\u22121)\nl\u22121. For each coordinatei, interval arithmetic\nyields:\nw\u22a4\nl,iH(t\u22121)\nl\u22121=hX\njmin{w l,ijx(t\u22121)\nl\u22121,j, wl,ijx(t\u22121)\nl\u22121,j},(36)\nX\njmax{w l,ijx(t\u22121)\nl\u22121,j, wl,ijx(t\u22121)\nl\u22121,j}i\n.\n(37)\nTo obtain a differentiable form of the bounds, we decom-\nposew\u22a4\nl,iinto its positive and negative parts:\n\u0000\nw\u22a4\nl,i\u0001+= max(0, w\u22a4\nl,i),(38)\n\u0000\nw\u22a4\nl,i\u0001\u2212= max(0,\u2212w\u22a4\nl,i),(39)\nso thatw\u22a4\nl,i=\u0000\nw\u22a4\nl,i\u0001+\u2212\u0000\nw\u22a4\nl,i\u0001\u2212. The interval product\nw\u22a4\nl,iH(t\u22121)\nl\u22121is then computed as [35]:\nw\u22a4\nl,iH(t\u22121)\nl\u22121=\u0010\u0000\nw\u22a4\nl,i\u0001+\u2212\u0000\nw\u22a4\nl,i\u0001\u2212\u0011\u0002\nx(t\u22121)\nl\u22121,x(t\u22121)\nl\u22121\u0003\n(40)\n=\u0000\nw\u22a4\nl,i\u0001+\u0002\nx(t\u22121)\nl\u22121,x(t\u22121)\nl\u22121\u0003\n(41)\n\u2212\u0000\nw\u22a4\nl,i\u0001\u2212\u0002\nx(t\u22121)\nl\u22121,x(t\u22121)\nl\u22121\u0003\n(42)\n=\u0002\u0000\nw\u22a4\nl,i\u0001+x(t\u22121)\nl\u22121,\u0000\nw\u22a4\nl,i\u0001+x(t\u22121)\nl\u22121\u0003\n(43)\n\u2212\u0002\u0000\nw\u22a4\nl,i\u0001\u2212x(t\u22121)\nl\u22121,\u0000\nw\u22a4\nl,i\u0001\u2212x(t\u22121)\nl\u22121\u0003\n.(44)\nUsing interval subtraction\u0002\na, b\u0003\n\u2212\u0002\nc, d\u0003\n=\u0002\na\u2212d, b\u2212c\u0003\n,\nwe obtain the final hypercube\u0002\n\u02dcxl,i,\u02dcxl,i\u0003\n:\n\u02dcxl,i=\u0000\nw\u22a4\nl,i\u0001+x(t\u22121)\nl\u22121\u2212\u0000\nw\u22a4\nl,i\u0001\u2212x(t\u22121)\nl\u22121,(45)\n\u02dcxl,i=\u0000\nw\u22a4\nl,i\u0001+x(t\u22121)\nl\u22121\u2212\u0000\nw\u22a4\nl,i\u0001\u2212x(t\u22121)\nl\u22121.(46)\nAdding the bias update\u2206b l,i, the constraint from\nEq. (35) becomes:\n\u0002\n\u02dcxl,i+ \u2206b l,i,\u02dcxl,i+ \u2206b l,i\u0003\n= [0,0].(47)\nTo enforce this condition during training, our regularizer\nLIntDrift penalizes the squaredL 2norm of these hypercube\nendpoint bounds, driving both toward zero, which yields\nEquation (10).\n13. Internal Representation Drift Loss for\nOther Layers\nThis section presents the full derivation of the interval re-\npresentation drift loss for convolutional and batch normali-\nzation layers.\nConvolutional Layer.Here, we adapt theL IntDrift deriva-\ntion from Appendix 12 for a 2D convolutional layer. All\nnotation (such asl,t, andH) follows the definitions in the\nprevious section.\nA convolutional layerf(x;\u03b8 l) = Conv2d(x, W l) +b l\nis a linear operator that applies a shared filterW lacross all\nspatial locations of the inputx. A key insight is that this\noperation is a form of affine transformation (like a fully-\nconnected layer) applied to local patches of the input.\nLet\u2206W land\u2206b lbe the updates to the layer\u2019s parame-\nters. Our stability objective is to ensure that the preactiva-\ntion change is zero for any inputxwithin the prior task\u2019shypercubeH(t\u22121)\nl\u22121:\nConv2d(x,\u2206W l) + \u2206b l= 0,\u2200x\u2208 H(t\u22121)\nl\u22121.(48)\nWe can analyze this by focusing on a single preactivation\nyiin thei-th output channel. This value is computed by\na dot product between thei-th filter of the kernel update,\n\u2206wl,i, and the corresponding input patchx p:\nyi= \u2206w\u22a4\nl,ixp+ \u2206b l,i.(49)\nHere,\u2206w l,iandx pare the flattened vector representa-\ntions of thei-th filter and the input patch, respectively.\nThis formulation is mathematically identical to the affine\nlayer case presented in Appendix 12. The inputx pis drawn\nfrom a hypercubeH patch defined by the interval bounds\n[x(t\u22121)\nl\u22121,x(t\u22121)\nl\u22121]of the corresponding input elements.\nTherefore, we can directly apply the same interval arith-\nmetic derivation. The interval bounds on the output drift for\nthei-th channel,\u0002\n\u02dcxl,i,\u02dcxl,i\u0003\n, are computed analogously. Let\n\u2206wl,ibe the flattenedi-th filter update, and letxpandxpbe\nthe corresponding flattened lower and upper bounds of the\ninput patch. The bounds are:\n\u02dcxl,i=\u0000\n\u2206w\u22a4\nl,i\u0001+xp\u2212\u0000\n\u2206w\u22a4\nl,i\u0001\u2212xp,(50)\n\u02dcxl,i=\u0000\n\u2206w\u22a4\nl,i\u0001+xp\u2212\u0000\n\u2206w\u22a4\nl,i\u0001\u2212xp.(51)\nNote that\u0000\n\u2206w\u22a4\nl,i\u0001+and\u0000\n\u2206w\u22a4\nl,i\u0001\u2212are the positive and\nnegative parts of the filter update, and the products are\nelement-wise dot products. The constraint from Eq. (35) be-\ncomes:\n\u0002\n\u02dcxl,i+ \u2206b l,i,\u02dcxl,i+ \u2206b l,i\u0003\n= [0,0].(52)\nOur regularizerL IntDrift penalizes the squaredL 2norm of\nthese hypercube endpoints, just as in the affine case, driving\nthe representation drift within the defined input domain to\nzero.\nBatchNorm Layer.The same principle applies to Batch-\nNorm (BN) layers. In inference, a BN layer computes an\naffine transformation:\nf(x;\u03b8 l) =\u03b3 l \nx\u2212\u00b5 lp\n\u03c32\nl+\u03f5!\n+\u03b2l,(53)\nwhere the learnable parameters are\u03b8 l= (\u03b3 l, \u03b2l), and the\nfixed (non-learnable) parameters are the running statistics\n\u00b5land\u03c32\nl. We can rewrite this as a standard affine layer\nf(x) =Weff\nlx+beff\nl, where:\nWeff\nl=\u03b3lp\n\u03c32\nl+\u03f5(54)\nbeff\nl=\u03b2l\u2212\u03b3l\u00b5lp\n\u03c32\nl+\u03f5.(55)\nWhen the parameters are updated by(\u2206\u03b3 l,\u2206\u03b2 l), thedrift\nin the output,\u2206f(x), is:\n\u2206f(x) =f(x;\u03b8 l+ \u2206\u03b8 l)\u2212f(x;\u03b8 l)(56)\n= \n\u2206\u03b3lp\n\u03c32\nl+\u03f5!\nx+ \n\u2206\u03b2l\u2212\u2206\u03b3l\u00b5lp\n\u03c32\nl+\u03f5!\n(57)\n= \u2206Weff\nlx+ \u2206beff\nl.(58)\nThis drift\u2206f(x)is itself an affine transformation ofx. We\nwant to enforce\u2206f(x) = 0for allx\u2208 H(t\u22121)\nl\u22121. This is the\nexact same problem as in Appendix 12.\nTherefore, we can computeL IntDrift for a BN layer by ap-\nplying the derivation from Eq. (45) and Eq. (46), simply by\nreplacing the weight update\u2206W lwith the effective weight\nupdate\u2206Weff\nland the bias update\u2206b lwith the effective\nbias update\u2206beff\nl.\n14. Training Algorithm\nThe complete training procedure for InTAct is provided in\nAlgorithm 1. To make this procedure applicable to a wide\nrange of continual learning setups, whether using standard\narchitectures or prompt-based extensions, we adopt a uni-\nfied formulation for both input handling and parameter opti-\nmization.\nUnified Notation.Let\u03a8denote the set ofall learnable\nparametersin the current training configuration. This set\nmay include model parameters, prompt parameters, or any\nadditional task-specific components that are updated during\ntraining. Before describing the two possible settings, we in-\ntroduce the procedure QUERYPROMPTS: this procedure is\nresponsible for processing an input samplexand returning\n(i) a possibly modified inputx p, where prompt or context\ntokens may be injected, and (ii) an auxiliary loss termL aux\nthat arises when prompt mechanisms impose additional le-\narning objectives. Using this unified interface for input pro-\ncessing allows Algorithm 1 to operate consistently across\ndifferent architectures and training strategies.\n\u2022When prompt-based methods are not applied:In this\ncase, no prompt parameters are present (\u03d5=\u2205), and the\nonly learnable parameters are those inherent to the mo-\ndel, denoted by\u03b8. Thus, the optimization target reduces\nto\u03a8 =\u03b8. Since no prompt mechanism is used, QUERY-\nPROMPTSbecomes an identity operation: it returns the\noriginal input (x p=x) and contributes no auxiliary loss\n(Laux= 0).\n\u2022When prompt-based methods are applied:Here, the\nsystem includes a set of learnable prompt parameters\u03d5,\nand may also include other components that remain tra-\ninable, depending on the chosen configuration. To de-\nscribe this generally, we write\u03a8\u2286 {\u03b8, \u03d5}. In this set-\nting, QUERYPROMPTSuses the inputxto retrieve promptor context tokens, producing a modified inputx p\u0338=x\nand, when applicable, generating a prompt-related auxi-\nliary lossL aux.\nThis unified formulation ensures that Algorithm 1 han-\ndles input transformation, gradient propagation, and all as-\nsociated loss terms in a consistent manner across diverse\ncontinual learning configurations.\nAlgorithm 1InTAct Training Algorithm\nRequire:Number of tasksT; Sequential tasks{D t}T\nt=1; Neural network\nfwith parameters\u03b8; Optional prompt module parameters\u03d5.\nRequire:Set of layer indicesS Hfor which activation hypercubes are\ncomputed.\nRequire:Hyperparameters:\u03bb Var,\u03bb IntDrift ,\u03bb Feat,\u03bb Prompt ,\u03b1; Training\nepochsE; Learning rate\u03b7.\nEnsure:Optimized parameters\u03a8.\n1:Initialize:Parameters\u03b8, \u03d5; HypercubesH(0)\nl\u2190 \u2205for alll\u2208 S H.\n2:Define Active Parameters:\u03a8\u2286 {\u03b8, \u03d5}.\n3:fortaskt= 1toTdo\n4:forepoche= 1toEdo\n5:forbatch(x, y)\u223c D tdo\n6: {Step 1: Prompt Query & Pre-processing}\n7:x p,Laux\u2190QUERYPROMPTS(x)\n8: {Step 2: Forward Pass}\n9:\u02c6y\u2190f(x p;\u03b8)\n10: {Step 3: Loss Calculation}\n11:L Total\u2190 L CE+\u03bb PromptLaux\n12: Calculate variance lossL Var(Eq. (13))\n13:L Total\u2190 L Total+\u03bb VarLVar\n14:ift\u22652then\n15: Calculate feature distillation lossL Feat(Eq. (15))\n16: Calculate internal representation drift loss (Eq. (10))\n17: Calculate inter-task alignment lossL Align (Eq. 14)\n18:L Total\u2190 L Total+\u03bb FeatLFeat+\u03bb IntDriftLIntDrift +L Align\n19:end if\n20: {Step 4: Optimization}\n21: Update\u03a8\u2190\u03a8\u2212\u03b7\u2207 \u03a8LTotal\n22:end for\n23:end for\n24: {Step 5: Update Activation Hypercubes}\n25: Collect activationsA l={h l(xp;\u03b8)|x\u2208 D t}for alll\u2208 S H\n26:foreach layer indexl\u2208 S Hdo\n27:v \u2190percentile\u03b1(Al);v\u2190percentile100\u2212\u03b1 (Al)\n28:ift= 1then\n29:H(t)\nl\u2190[v,v]\n30:else\n31: LetH(t\u22121)\nl= [h,h]\n32:H(t)\nl\u2190h\nmin(h , v),max( h,v)i\n33:end if\n34:end for\n35:end for\n15. Additional Results\nIn this section, we report additional experiments and analy-\nsis that further demonstrate the effectiveness of InTAct.\nResults on CIFAR-100 under the CIL scenario.Tab. 7\nshows results on the 10-task CIFAR-100. Across all me-\nthods, InTAct maintains or improves accuracy while sub-\nstantially reducing variance, confirming its stabilizing ef-\nfect. For L2P and DualPrompt, InTAct matches baseline ac-\ncuracy but halves standard deviation, preserving functional\nstability without limiting adaptability. The largest gain oc-\ncurs for CODA-Prompt, where AA rises from86.25%to\n87.31%and variance drops sharply, approaching the upper\nbound.\nTabela 7. Results on the 10-task (10 classes each) CIFAR-100\nbenchmark (CIL setting). We report the mean and standard de-\nviation of AA and AF averaged over 5 random seeds.\nMethod AA (\u2191) AF (\u2193)\nUpper-Bound89.30\u2013\nL2P82.50\u00b11.101.75\u00b10.42\nw/ InTAct82.40\u00b10.301.75\u00b10.18\nDualPrompt83.05\u00b11.161.72\u00b10.40\nw/ InTAct82.71\u00b10.511.66\u00b10.33\nCODA-P86.25\u00b10.741.67\u00b10.26\nw/ InTAct87.31\u00b10.171.77\u00b10.2\nResults on DomainNet under the CIL scenario.Table 8\nsummarizes performance on the 5-task DomainNet bench-\nmark (CIL). Adding InTAct to L2P and DualPrompt impro-\nves AA while maintaining competitive AF. Notably, InTAct\nreduces variance across random seeds, indicating increased\ntraining stability on this challenging multi-domain setting.\nTabela 8. Results on the 5-task (69 classes each) DomainNet\nbenchmark (CIL setting). We report the mean and standard devia-\ntion of AA and AF calculated over 3 random seeds. Results from\nbaselines are obtained from the paper [47].\nMethod AA (\u2191) AF (\u2193)\nUpper-Bound79.65\u2013\nL2P69.58\u00b10.392.25\u00b10.08\nw/ InTAct70.02\u00b10.173.13\u00b10.34\nDualPrompt70.73\u00b10.492.03\u00b10.22\nw/ InTAct71.33\u00b10.133.87\u00b10.59\nCODA-P73.24\u00b10.59 3.46\u00b10.09\nw/ InTAct72.12\u00b10.10 3.59\u00b10.13\nResults for Regularization-based Methods (CIL).\nInTAct achieves the best performance among all\nregularization-based methods on the CIL benchmarks,\nconsistently improving AA across tasks. The detailed re-\nsults are shown in Table 9 for Split MNIST, Split FMNIST,\nand Split CIFAR-10. In particular, InTAct outperformsthe strongest baseline (LwF) by more than six percentage\npoints on Split MNIST and Split FMNIST, demonstrating\nits ability to mitigate catastrophic forgetting and stabilize\ninternal representations across sequential tasks. Note that\nthese gains are achieved without any task-specific archi-\ntectural modifications or rehearsal buffers, highlighting the\nrobustness and generality of the proposed interval-based\nregularization.\nTabela 9. Comparison of regularization-based continual learning\nbaselines and InTAct across multiple datasets (CIL setting). Re-\nsults report the AA metric values and are averaged over 5 random\nseeds.\nMethod Split MNIST Split FMNIST Split CIFAR-10\nEWC20.57\u00b10.78 19.92\u00b10.02 30.43\u00b11.69\nLwF28.63\u00b10.61 25.92\u00b10.84 24.26\u00b10.75\nMAS17.91\u00b12.93 18.74\u00b13.40 21.75\u00b10.67\nSI18.40\u00b10.33 17.30\u00b10.52 20.18\u00b10.27\nInTAct35.27\u00b12.27 34.25\u00b12.91 31.28\u00b10.77\nResults for Regularization-Based Methods (DIL).In-\nTAct achieves also the strongest performance among\nregularization-based methods on the DIL benchmarks. The\ndetailed results are reported in Table 10 for Split MNIST,\nSplit FMNIST, and Split CIFAR-10. On Split MNIST, In-\nTAct clearly outperforms EWC, MAS, and SI, achieving the\nhighest AA. For Split FMNIST, all methods perform com-\npetitively, with InTAct and EWC obtaining similar results.\nOn Split CIFAR-10, InTAct matches the best-performing\nbaseline (LwF), confirming that the proposed regularization\nis effective even in more complex visual tasks.\nTabela 10. Comparison of regularization-based continual learning\nbaselines and InTAct across multiple datasets (DIL setting). Re-\nsults report the AA metric values and are averaged over 5 random\nseeds.\nMethod Split MNIST Split FMNIST Split CIFAR-10\nEWC78.90\u00b12.28 92.30\u00b10.80 68.93\u00b11.13\nLwF75.54\u00b10.37 84.74\u00b10.7676.45\u00b10.63\nMAS77.59\u00b12.84 91.16\u00b12.56 69.35\u00b13.40\nSI69.84\u00b110.15 85.03\u00b13.98 67.66\u00b10.38\nInTAct82.51\u00b10.83 92.57\u00b10.6375.84\u00b10.63\nThese gains are achieved without task-specific know-\nledge distillation or memory buffers, highlighting the ge-\nneral applicability of InTAct in DIL scenarios.\nImpact of\u03bb Varon AA.In Fig. 6, we illustrate the effect\nof varying\u03bb Varon AA for the Split MNIST dataset under\nthe CIL setting.\nRysunek 6. Impact of the\u03bb Varhyperparameter on AA for the\nSplit MNIST dataset under the CIL scenario. Results are avera-\nged over 5 seeds.\nWe observe that increasing the value of\u03bb Vargenerally\nleads to improved AA. This is because stronger representa-\ntion variance regularization (Eq. (13)) encourages the hid-\nden representations to become more compact and structu-\nred. In turn, this results in activation hypercubes of smaller\nvolume. Since our regularization mechanism constrains in-\nlayer transformations within these hypercubes, operating on\na smaller and more coherent region of the activation space\nyields a more effective and stable control signal. All other\nhyperparameters are fixed to their respective optimal values\nto isolate the effect of\u03bb Var.\n16. Extended Table Results in CIL Scenario\nIn this section, we present extended results on the CIFAR-\n100, DomainNet, and ImageNet-R benchmarks (Tables 11,\n12, and 13). Overall, InTAct consistently enhances the per-\nformance of existing prompt-based methods (L2P, Dual-\nPrompt, CODA-P), improving AA while achieving compe-\ntitive or lower AF. Beyond prompt-based approaches, In-\nTAct also surpasses non-prompt continual learning baseli-\nnes such as ER [40], sequential fine-tuning (FT), its impro-\nved variant FT++, which uses the same classifier implemen-\ntation as prompt-based methods in the CIL scenario, and\nLwF [26] on most benchmarks. The notable exception is\nDomainNet, where LwF surprisingly achieves the highest\nAA. These results demonstrate that InTAct effectively bo-\nosts prompt-based methods and remains competitive aga-\ninst traditional continual learning strategies. In all tables,\nthe best results in each column are highlighted in bold.\n17. Additional Visualization\nHere, we present additional visualizations of representation\ndrift in InTAct and LwF, and provide analysis based on the\nSplit MNIST dataset in the DIL scenario.Tabela 11. Extended results on the 10-task (10 classes each)\nCIFAR-100 benchmark (CIL setting). We report the mean and\nstandard deviation of AA and AF averaged over 5 random seeds.\nBest results in each column are highlighted in bold. Results from\nbaselines are obtained from [47].\nMethod AA (\u2191) AF (\u2193)\nUpper-Bound89.30\u2212\nER (5000)76.20\u00b11.04 8.50\u00b10.37\nFT9.92\u00b10.27 29.21\u00b10.18\nFT++49.91\u00b10.42 12.30\u00b10.23\nLwF.MC64.83\u00b11.03 5.27\u00b10.39\nL2P82.50\u00b11.10 1.75\u00b10.42\nw/ InTAct82.40\u00b10.30 1.75\u00b10.18\nDualPrompt83.05\u00b11.16 1.72\u00b10.40\nw/ InTAct82.71\u00b10.511.66\u00b10.33\nCODA-P86.25\u00b10.74 1.67\u00b10.26\nw/ InTAct87.31\u00b10.171.77\u00b10.20\nTabela 12. Extended results on the 5-task (69 classes each) Doma-\ninNet benchmark (CIL setting). We report the mean and standard\ndeviation of AA and AF calculated over 3 random seeds. Best re-\nsults in each column are highlighted in bold. Results from baseli-\nnes are obtained from [47].\nMethod AA (\u2191) AF (\u2193)\nUpper-Bound79.65\u2212\nER (5000)58.32\u00b10.47 26.25\u00b10.24\nFT18.00\u00b10.26 43.55\u00b10.27\nFT++39.28\u00b10.21 44.39\u00b10.31\nLwF.MC74.78\u00b10.43 5.01\u00b10.14\nL2P69.58\u00b10.39 2.25\u00b10.08\nw/ InTAct70.02\u00b10.17 3.13\u00b10.34\nDualPrompt70.73\u00b10.492.03\u00b10.22\nw/ InTAct71.33\u00b10.13 3.87\u00b10.59\nCODA-P73.24\u00b10.593.46\u00b10.09\nw/ InTAct72.12\u00b10.10 3.59\u00b10.13\nAnalysis of Internal Representation Drift.We provide\na qualitative analysis of internal representation stability in\nFigure 7. This experiment, conducted on SplitMNIST, visu-\nalizes theinternal representation driftexperienced by data\nfrom past tasks as new tasks are learned. The plot tracks\ntheL 1norm of the activation difference for specific refe-\nrence images (e.g., the image \"0\"from Task 1, \"2\"from Task\n2, etc.) compared to their original activations recorded after\ntheir respective tasks were first learned.\nTabela 13. Extended results on the ImageNet-R dataset (CIL setting). We report the mean and standard deviation of AA and AF averaged\nover 5 random seeds. Results include 10- and 20-tasks baselines. Best results in each column are highlighted in bold. Results from baselines\nare obtained from [47].\nMethod10 steps 20 steps\nAA (\u2191) AF (\u2193) AA (\u2191) AF (\u2193)\nUpper-Bound77.13\u221277.13-\nER (5000)64.43\u00b11.16 10.30\u00b10.05 52.43\u00b10.87 7.70\u00b10.13\nFT10.12\u00b10.51 25.69\u00b10.23 4.75\u00b10.40 16.34\u00b10.19\nFT++48.93\u00b11.15 9.81\u00b10.31 35.98\u00b11.38 6.63\u00b10.11\nLwF.MC66.73\u00b11.25 3.52\u00b10.39 54.05\u00b12.66 2.86\u00b10.26\nL2P69.29\u00b10.73 2.03\u00b10.19 65.89\u00b11.30 1.24\u00b10.14\nw/ InTAct69.44\u00b10.45 2.89\u00b10.46 66.18\u00b10.36 1.30\u00b10.13\nDualPrompt71.32\u00b10.62 1.71\u00b10.24 67.87\u00b11.39 1.07\u00b10.14\nw/ InTAct70.98\u00b10.60 1.68\u00b10.07 67.89\u00b10.66 1.19\u00b10.18\nCODA-P75.45\u00b10.561.64\u00b10.1072.37\u00b11.190.96\u00b10.15\nw/ InTAct76.40\u00b10.162.07\u00b10.1673.30\u00b10.41 1.56\u00b10.26\nRysunek 7. Activation drift across tasks on SplitMNIST (DIL sce-\nnario). For each task, we record hidden activations of a reference\nimage and then track the normalized absolute change of these ac-\ntivations as new tasks are learned. LwF shows steadily increasing\ndrift, whereas InTAct keeps activation changes bounded, indica-\nting substantially more stable internal representations.\nConclusions from the Visualization.The results show a\nstark contrast between our method (InTAct) and the LwF\nbaseline. For LwF (dashed lines), the activation drift is both\nsignificant and cumulative. As new tasks are introduced, the\ninternal representations for data from all previous tasks drift\nsubstantially. For instance, the reference sample from Task\n1 (blue dashed line) experiences a large initial drift that con-\ntinues to increase as more tasks are added. In contrast, In-\nTAct (solid lines) demonstrates superior stability. While a\nsmall, initial drift is observed when the first new task is in-\ntroduced (e.g., the blue solid line at Task 2), the drift re-\nmains bounded and stabilizes for all subsequent tasks. This\nvisualization provides strong evidence that InTAct success-fully preserves the functional behavior of the network on\ndata from previous tasks, directly mitigating a key cause of\ncatastrophic forgetting.\n",
    "title": "InTAct: Interval-based Task Activation Consolidation for Continual Learning",
    "arxiv_id": "2511.17439v1",
    "category": "recent",
    "authors": [
      "Patryk Krukowski",
      "Jan Miksa",
      "Piotr Helm",
      "Jacek Tabor",
      "Pawe\u0142 Wawrzy\u0144ski",
      "Przemys\u0142aw Spurek"
    ],
    "published": "2025-11-21",
    "num_pages": 23,
    "num_chars": 99077
  },
  {
    "text": "A Framework for Adaptive Stabilisation of Nonlinear Stochastic\nSystems\u22c6\nSeth Siriyaa, Jingge Zhub, Dragan Ne\u0161i\u0107b, Ye Pub\naLeibniz University Hannover, Institute of Automatic Control, 30167 Hannover, Germany\nbUniversity of Melbourne, Department of Electrical and Electronic Engineering, Parkville VIC 3010, Australia\nAbstract\nWe consider the adaptive control problem for discrete-time, nonlinear stochastic systems with linearly parameterised uncer-\ntainty. Assuming access to a parameterised family of controllers that can stabilise the system in a bounded set within an\ninformative region of the state space when the parameter is well-chosen, we propose a certainty equivalence learning-based\nadaptive control strategy, and subsequently derive stability bounds on the closed-loop system that hold for some probabilities.\nWe then show that if the entire state space is informative, and the family of controllers is globally stabilising with appropri-\nately chosen parameters, high probability stability guarantees can be derived.\nKey words:Adaptive control; discrete-time systems; least squares; nonlinear systems; stochastic systems.\n1 Introduction\nAdaptive control (AC) is concerned with the control of\ndynamical systems with parameters which are uncer-\ntain, online, in a single trajectory. It is useful in situ-\nations where one is not afforded the luxury of running\nmany experiments to collect data offline for the purpose\nof system identification. Rigorously characterising the\nstochastic stability properties of these methods is im-\nportantforunderstandingthesituationsinwhichanAC\nstrategy can be successfully deployed on a system. Of\ngreatinterestaremethodsthatcanprovablyhandlenon-\nlinear systems subject to stochastic process noise, since\nmany real-world systems are nonlinear and influenced\nby factors with random characteristics.\nThe stability problem in stochastic AC has a rich his-\ntory, with the first stability results for AC in unstable\nlinearplantsobtainedbackinthe1980sbyGoodwinand\nCaines [7]. Although results for linear systems are well\nexplored, the nonlinear setting continues to attract at-\ntention.Inaseriesofworks[8,22,13,14],thefundamental\n\u22c6This paper was not presented at any IFAC meeting. Cor-\nresponding author S. Siriya.\nEmail addresses:siriya@irt.uni-hannover.de(Seth\nSiriya),jingge.zhu@unimelb.edu.au(Jingge Zhu),\ndnesic@unimelb.edu.au(Dragan Ne\u0161i\u0107),\nye.pu@unimelb.edu.au(Ye Pu).limits of the stabilisation problem for a class of discrete-\ntime systems withlinearly parameterised uncertainties\n\u2014 that is, systems where the model uncertainties are\nformulated as a linear combination of known nonlinear\nbasis functions of the states and controls \u2014 is investi-\ngated.Theseworksrequirethatitispossibleforcontrols\ntocompletelycancelallsystemdynamicsinasinglestep,\nandtheyshowthatacertaintyequivalenceleastsquares-\nbased AC strategy provably stabilises the system un-\nder various assumptions, often involving restrictions on\nthe growth of basis functions. In [6], stability-based re-\ngret bounds were derived for an online gradient-based\ndiscrete-timeACmethodforlinearlyparameterisedsys-\ntems withmatched uncertainties\u2014 that is, problems\nwhere the controls may not necessarily cancel the entire\nsystem dynamics, but just the model uncertainties. The\nAC problem for discrete-time input-constrained linear\nsystems subject to unbounded stochastic disturbances\nwas also recently addressed in [19] and [20] under vari-\nous assumptions, which can be viewed as a linearly pa-\nrameterised nonlinear system. Moreover, the adaptive\nstabilisation problem for stochastic control-affine SISO\nsystems and stochastic discrete-time parametric strict-\nfeedbacksystemshasalsobeenaddressedin[22]and[21]\nrespectively. Although all these results are interesting,\nit is desirable to obtain results which are applicable to\nother systems with linearly parameterised uncertainty\nwithout such structural restrictions.arXiv:2511.17436v1  [eess.SY]  21 Nov 2025\nSome works in the continuous-time, deterministic set-\nting do not impose such structural restrictions. In [5],\na framework forlearning-basedAC is proposed which\ncombinesanintegralinput-to-statestabilisingcontroller\nwithanextremumseeking-basedparameterupdatelaw.\nBylearning,wemeanthattheparameterestimatescon-\nverge towards the true parameters [4], and by learning-\nbased we mean that the stability guarantees are reliant\non learning. Moreover, in [11], an AC framework is pro-\nposed for a class of linearly parameterised systems that\ncombines a dead-beat parameter estimator with acer-\ntaintyequivalencecontrollaw,meaningthecontrolleras\nchosen as if the estimated parameter is the true param-\neter. These results provide aframeworkfor AC, such\nthat the systems they can be applied to depend on the\nspecificallychosencontroller.However,stabilityguaran-\ntees require that the chosen controller can globally sta-\nbilise the system when given the true parameter, which\nis restrictive in many real-world situations.\nThere are some AC works applicable to linearly pa-\nrameterised systems without requiring the aforemen-\ntioned structural restrictions and global stabilisability,\nmainly from the robust adaptive model predictive con-\ntrol (MPC) community (see [1], [2], [12], [17]). However,\nthey typically require the knowledge of a bounded un-\ncertainty set which the true parameter belongs to, and\nthat a feasible solution exists to the robust MPC prob-\nlem that accounts for all possible parameters in this set.\nHowever, one may not always know enough about the\nsystem to determine such a set.\nMotivatedbytheseaforementionedgaps,weaimtopro-\nvide a modular framework for the AC of a broad class of\ndiscrete-time stochastic nonlinear systems with linearly\nparameterised uncertainties, which does not require a\nprioriknowledgeofasetofparameterswhichrendersthe\nsystem bounded. Moreover, we aim to derive stochastic\nstabilityguaranteesfortheclosed-loopsystemwhichare\napplicable even when the system is not globally stabilis-\nable. This guarantee need to be derived under assump-\ntions that can be easily verified based on the system\nmodel offline. Our contributions are as follows.\nFirstly,weprovideaframeworkforcertaintyequivalence\nAC in linearly parameterised systems, that combines a\nparameterised stabilising policy with regularised least\nsquares(RLS)forparameterestimation.Theframework\nis intuitive since the parameterised controller is selected\nbased on the CE principle.\nSecondly, we derive a probabilistic guarantee that the\nstates of the system will remain positively invariant on\na subset of the state space for all time, and that a non-\nasymptotic bounds on the parameter estimation error\nwillholdsforsufficientlylargetime,undersomeassump-\ntions. These bounds asymptotically converge towards\nzero, and thus characterise a bound on how fast learn-\ning is occurring. The assumptions are related to the in-stability of the system and the growth rate of the basis\nfunctions. They also rely on the assumption that when\nthe parameter estimation error is sufficiently small, the\nchosen controller parameterised policy will render the\nstates of the system positively invariant in a subset of\nthestatespacesatisfyingaregionalexcitationcondition.\nThis latter condition ensures the regressor data is infor-\nmative for learning, so the parameter estimation error\nwill eventually be small enough for the control policy to\nenforce invariance.\nThirdly, we derive probabilistic stability bounds under\nthepreviousassumptions,andbyadditionallyassuming\nthe existence of stochastic Lyapunov function over the\npositively invariant set for the closed-loop system when\nthe parameter estimate used for control is close to the\ntrueparameter.Thisguaranteeisenabledviaalearning-\nbased analysis, where the convergence of the parameter\nestimatesensuresthateventuallythecontrollerperforms\nsimilar to the case when the true parameter is known,\nand thus is able to stabilise the system.\nFourthly, by strengthening the assumptions to require\nthatthesystemproducesinformativeregressordataover\nthe entire state space such thatglobal excitationholds,\nand the existence of a global stochastic Lyapunov func-\ntion when the parameter estimation error is sufficiently\nsmall, we derivehigh probability stability bounds\u2014 that\nis, stability bounds which hold with arbitrary probabil-\nity less than 1.\nFinally, we demonstrate our proposed framework on the\nadaptive control problem for a stochastic PWA system,\nandaninput-constrainedlinearsystemsubjecttoGaus-\nsian disturbances. In particular, we show that with an\nappropriately selected policy, the probabilistic guaran-\nteefortheestimationerrorandpositiveinvarianceholds\non both examples. Moreover, the probabilistic stability\nboundscanbeestablishedforthePWAsystemexample,\nand the stronger high probability stability bound result\ncan be established for the input-constrained linear sys-\ntem example.\nThe work is organised as follows. In Section 2, we intro-\nducetheclassofdiscrete-timestochasticsystemsandthe\nACframeworkconsideredinthispaper,aswellasstand-\ning assumptions. Section 3 contains our main results,\nwhere we recall the definition of regional and global ex-\ncitation, introduce our notion of a stochastic Lyapunov\nfunction,andderivetheestimationerrorboundandsta-\nbility bound results. Section 4 contains the PWA sys-\ntemandinput-constrainedlinearsystemsexamplesthat\ndemonstrate the benefit of our main results. Section 5\ncontains the proof of our results. Our conclusions are\npresented in Section 6.\nNotationDenote byRn\u00d7mthe set of real matri-\nces of dimensionn\u2208Nandm\u2208N. The spaceRn\n2\nstands forRn\u00d71. For a matrixA\u2208Rn\u00d7m,|A|de-\nnotes its induced2-norm,|A| Fdenotes its Frobenius\nnorm, andB r(A) :={ \u02dcA\u2208Rn\u00d7m| |\u02dcA\u2212A| \u2264r}.\nFor a vectorv\u2208Rnand a positive definite matrix\nA\u2208Rn,|v| A=\u221a\nv\u22a4Avdenotes the weighted Eu-\nclidean norm. Denote the unit sphere embedded inRd\nbySd\u22121. Consider the functionf:R \u22650\u2192R\u22650. Given\nthe functiong:R \u22650\u2192R\u22650, we sayf(r) =O(g(r))if\nlimr\u2192\u221ef(r)\ng(r)<\u221e, andh(r) =o(r)iflim r\u2192\u221ef(r)\ng(r)= 0.\nConsider a setS. Thepower set ofSis denoted by2S.\nTheindicator function ofSis defined by1 S(x) := 1if\nx\u2208S, and1 S(x) := 0otherwise. Consider a collection\nof subsetsC \u22862S. The collectionC \u22862Sis called a\n\u03c3-field inSif it is closed under countable union and in-\ntersection, as well as under complement.\u03c3(C)denotes\nthe\u03c3-fieldgeneratedbyC,whichis theintersectionof all\n\u03c3-fields inScontainingC. Consider the matrix subsets\nX\u2286Y\u2286Rn\u00d7m. We sayXis anopen subset ofYif for\neveryx\u2208X, there existsr >0such that the open ball\nof radiusrcentred atxis a subset ofX.B(Y)denotes\ntheBorel\u03c3-fieldofY,whichisdefinedasB(Y) :=\u03c3(O)\nwhereO \u22862Yis the collection of all open subsets of\nY. Consider a measurable space(S,S)and a mapping\nf:S\u2192Rn\u00d7m. ThesetA\u2286SisS-measurableifA\u2208 S.\nGivenY\u2286T,f\u22121(Y) :={s\u2208S|f(s)\u2208Y}. We\nsay thatfisS-measurableiff\u22121(X)isS-measurable\nfor allX\u2208 B(Rn\u00d7m). Additionally, ifS\u2286Rp\u00d7q, we\nsayfis Borel measurableiffisB(S)-measurable.\nConsider a probability space(\u2126,F,P). We call a map-\npingX: \u2126\u2192Rn\u00d7marandom variableifXis mea-\nsurable, and additionally arandom vectorifm= 1,\nand ascalar random variableifn=m= 1. We\ndefine arandom sequenceas a sequence{X(i)} i\u2208I\nof random variablesX(i) : \u2126\u2192Rn\u00d7moveriin\nan index setI \u2286N. Consider a collection of ran-\ndom variablesX 1, X2, . . .taking values inX 1,X2, . . .\nand a predicateQ:X 1\u00d7 X 2\u00d7 \u00b7\u00b7\u00b7 \u2192 {true,false}.\nWith an abuse of notation,{Q(X 1, X2, . . .)}stands for\n{\u03c9\u2208\u2126|Q(X 1(\u03c9), X 2(\u03c9), . . .)}, andP(Q(X 1, X2, . . .))\nstandsforP({Q(X 1, X2, . . .)}).Thefunctionh:R \u22650\u2192\nR\u22650is said to ben-order sub-exponential(n-SE) if\nln(h(rn)) =o(r).Thefunction\u03b1:R \u22650\u2192R\u22650isinclass\nKif it is continuous, strictly increasing, and\u03b1(0) = 0.\nThe function\u03b1:R \u22650\u2192R\u22650is in classK \u221eif\u03b1(\u00b7)\u2208 K\nand unbounded, i.e.,lim s\u2192\u221e \u03b1(s) =\u221e. The function\n\u03b1:R \u22650\u2192R \u22650is in classKn-SE\n\u221eif\u03b1\u2208 K \u221eand\u03b1is\nn-SE. A functionh:R \u22650\u2192R\u22650is said to beasymptot-\nically polynomially bounded (APB)if there existsk\u2208N\nsuch thath(r) =O(rk). The function\u03b7:N 0\u2192R\u22650is\nin classLif it is non-increasing andlim t\u2192\u221e\u03b7(t) = 0.\nThe function\u03b2:R \u22650\u00d7N 0\u2192R \u22650is in classKLif\n\u03b2(\u00b7, t)\u2208 Kfor anyt\u2208N 0and\u03b2(r,\u00b7)\u2208 Lfor anyr\u22650.\n2 Problem Setup and Framework\nWe start by describing the stochastic system and adap-\ntive control framework considered in this work in Sec-tion 2.1. Afterwards, in Section 2.2, we make various as-\nsumptions that ensure stochastic properties of interest\nforoursystemarewell-defined,andallowustoderivees-\ntimation error and stability bounds in our main results.\n2.1 Discrete-Time Stochastic System and Adaptive\nControl Framework\nLet(\u2126,F,P)betheprobabilityspaceonwhichallofour\nrandom variables are defined, and denote expectation\nand variance byEandVarrespectively. Consider the\ndiscrete-time, stochastic, nonlinear system\nX(t+ 1) =f(X(t), U(t)) +\u03b8\u22a4\n\u2217\u03c8(X(t), U(t))\n+W(t+ 1), t\u2208N 0,(1)\nwithX(0) =x 0. Here, theprocess noise{W(t)} t\u2208N\nis a random sequence taking values inW\u2286Rn, and\n{X(t)} t\u2208N0and{U(t)} t\u2208N0are the sequence ofstates\nandcontrolstaking values inX\u2286RnandU\u2286Rmre-\nspectively.Moreover,f:X\u00d7U\u2192Rnisaknownnominal\nsystemmodel,\u03c8:X\u00d7U\u2192Rdis a known vector ofbasis\nfunctions,\u03b8 \u2217\u2208Rd\u00d7nis thetrue unknown system pa-\nrameter,andx 0\u2208Xistheinitialstate.Forconvenience,\nwe also defineg(x, u, w) :=f(x, u) +\u03b8\u22a4\n\u2217\u03c8(x, u) +w,\nand we only considergwith the codomainXto ensure\nsolutions are well-defined for all time.\nOur adaptive control framework for stabilising (1) is\nsummarised in Algorithm 1.\nAlgorithm 1Adaptive Control Framework\nHello\nInputs:f,\u03c8,\u03b1,\u00b5 s,\u03d10\u2208Rd\u00d7n,\u03b3 >0\nMeasureX(0)\nfort= 0,1, . . .do\nCompute estimate \u02c6\u03b8(t\u22121)following (3)\nSampleS(t)i.i.d.\u223c\u00b5 s\nCompute controlU(t)following (2)\nApplyU(t)to (1)\nMeasure stateX(t+ 1)from (1)\nend for\nWe now describe our framework in further detail. The\ncontrol sequence is generated via\nU(t) =\u03b1(X(t), S(t), \u02c6\u03b8(t\u22121)), t\u2208N 0.(2)\nwhere\u03b1:X\u00d7S\u00d7Rd\u00d7n\u2192Uis aknown control pol-\nicythatdesignsthatcontrolinputU(t)accordingtothe\nstateX(t),aninjectednoisetermS(t),andaparameter\nestimate \u02c6\u03b8(t\u22121). Intuitively, we can view\u03b1as a policy\nwe couldapply tostabilisethe systemwhenthetrue pa-\nrameter\u03b8 \u2217is used instead of a parameter estimate. In\nthis sense, we can view strategies following the frame-\nwork outlined in Algorithm 1 as a certainty equivalence\n3\nadaptivecontrollaw.ThiswillbeformalisedinSection3.\nThe random sequence ofexploratory noise{S(t)} t\u2208Nis\nsampled i.i.d. from a distribution\u00b5 swith the support\nS\u2286Rq, and is injected to help facilitate convergence of\nthe estimate \u02c6\u03b8(t). The sequence ofparameter estimates\n{\u02c6\u03b8(t)} t\u2208Ntaking values inRd\u00d7nare obtained via regu-\nlarised least squares (RLS) estimation:\n\u02c6\u03b8(t) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3\u03d10, t\u2208 {\u22121,0},\narg min\u03b8\u2208Rd\u00d7nPt\ns=1\f\fX(s)\u2212\u03b8\u22a4Z(s)\n\u2212f(X(s\u22121), U(s\u22121))\f\f2+\u03b3|\u03b8|2\nF, t\u2208N,(3)\nwhere{Z(t)} t\u2208Nis sequence ofregressorstaking values\ninRdandsatisfyingZ(t) =\u03c8(X(t\u22121), U(t\u22121)).More-\nover,\u03b3 >0is a user-chosen regularisation parameter,\nand\u03d1 0\u2208Rd\u00d7nis an initial deterministic parameter es-\ntimate. Note that the RLS estimates also satisfy\n\u02c6\u03b8(t) =G(t)\u22121tX\ns=1Z(s) (X(s)\u2212f(X(s\u22121), U(s\u22121)))\u22a4\nfort\u2208N, whereG(t) =Pt\ni=1Z(i)Z(i)\u22a4+\u03b3Iis known\nas theregularised Gramian.\nNote that{X(t)} t\u2208N0,{U(t)} t\u2208N0,{Z(t)} t\u2208Nand\n{\u02c6\u03b8(t)} t\u2208Nare defined via construction through (1)-(2),\nand depend on the noise sequences{W(t)} t\u2208Nand\n{S(t)} t\u2208N0. Since the former are sequences of mappings\nfrom\u2126toX,UandRn\u00d7drespectively, they caninfor-\nmallybe viewed as random sequences. However it is\nunclear from our description so far whether theyfor-\nmallysatisfy the required definition, and hence whether\nall stochastic properties of interest are well-defined. We\naddressthis concerninSection2.2by imposing assump-\ntions that ensure such requirements are satisfied.\n2.2 Standing Assumptions\nFirstly, we make the following regularity assumption on\nf,\u03c8and\u03b1. It helps ensure that all stochastic properties\nof interest for our system are well-defined.\nAssumption 1(Measurability of system mappings)\nThe nominal system modelf, basis functions\u03c8, and\npolicy\u03b1, are all Borel measurable.\nNext, we recall the definition of sub-Gaussian random\nvariables, which are random variables whose distribu-\ntions have tails that decay at least as fast as the Gaus-\nsian distribution.\nDefinition 1(sub-Gaussian random variable) Given a\nsub sigma-fieldG \u2286 Fand scalar random variableX,\nwe sayX| Gis\u03c32\nx-sub-Gaussian if for all\u03bb\u2208R,E[exp(\u03bbX)| G]\u2264exp(\u03bb2\u03c32\nx/2)1. IfXis anRd-valued\nrandomvector,wesayX| Gis\u03c32\nx-sub-Gaussianif\u03b6\u22a4X|\nGis\u03c32\nx-sub-Gaussian for all\u03b6\u2208 Sd\u22121. For both cases,\nwe sayXis\u03c32\nx-sub-Gaussian whenGis chosen as the\ntrivial sigma-field{\u2205,\u2126}.\nWe make the following assumption on{W(t)} t\u2208Nover\nthe probability space. It is related to independence, and\nensures the tails of its distribution decays sufficiently\nfast for parameter estimation.\nAssumption 2(Independentandsub-Gaussianprocess\nnoise)\n(1) The process noise sequence{W(t)} t\u2208Nis i.i.d. with\ndistribution denoted by\u00b5 w.\n(2) The sequences{W(t)} t\u2208Nand{S(t)} t\u2208N0are mu-\ntually independent.\n(3)W(t)is zero-mean\u03c32\nw-sub-Gaussian for anyt\u2208N,\ni.e.E[exp(\u03b3\u03b6\u22a4W(t))]\u2264exp(\u03b32\u03c32\nw/2)forany\u03b3\u2208R\nand\u03b6\u2208 Sn\u22121.\nLet\u03d5(t, \u03be,{u(i)}t\u22121\ni=0,{w(i)}t\ni=1)denote thedeterminis-\nticstatesolutionattimet\u2208N 0totheproblemdescribed\nin (1) given the initial state\u03be\u2208X, deterministic control\ninputsequence{u(i)}t\u22121\ni=0\u2208Utanddeterministicprocess\nnoise sequence{w(i)}t\ni=1\u2208Wt. We make the following\nassumption, which bounds the reachable states of this\ndeterministic solution as a function of energy-like esti-\nmates of the control inputs and process noise. It natu-\nrally restricts the instability of the system which helps\nestablishtheconvergenceofparameterestimates.,asre-\nquired for estimation in [18]. The continuous-time ana-\nlogue of this property is equivalent to a concept called\nforward completeness[3].\nAssumption 3(Sub-exponential input-to-state bound\non reachable states) There exist functions\u03c7 1, \u03c73, \u03c32\u2208\nK1-SE\n\u221e,\u03c74\u2208 K2-SE\n\u221e,\u03c72, \u03c31\u2208 K\u221eand a constantc 1\u22650\nsuch that\n\f\f\u03d5(t, \u03be,{u(i)}t\u22121\ni=0,{w(i)}t\ni=1)\f\f\u2264\u03c71(t) +\u03c7 2(|\u03be|)\n+\u03c73 t\u22121X\ni=0\u03c31(|u(i)|)!\n+\u03c74 tX\ni=1\u03c32(|w(i)|)!\n+c1\nforallt\u2208N,\u03be\u2208X,{u(i)}t\u22121\ni=0\u2208Utand{w(i)}t\ni=1\u2208Wt.\nWe assume the controls generated by the policy\u03b1sat-\nisfy a magnitude constraint. Such constraints are com-\nmonly satisfied in strategies like model predictive con-\ntrol (MPC), and are natural in most real-world appli-\ncations due to actuator saturation. They are useful for\n1Unless otherwise stated, inequalities involving random\nvariables should be interpretedsurely, i.e. given two random\nvariablesXandY,X\u2264Ystands forX(\u03c9)\u2264Y(\u03c9)for all\n\u03c9\u2208\u2126.\n4\nrestricting the growth of the states of the system during\ninstability, as required for estimation in [18].\nAssumption 4(Constrained controls) There exists\numax\u22650such that for anyx\u2208X,s\u2208Sand\u03d1\u2208Rd\u00d7n,\n|\u03b1(x, s, \u03d1)| \u2264u max.\nWe assume that the magnitude of the basis functions\u03c8\ngrow polynomially as a function of the states and con-\ntrols, which aids in bounding the regressor sequence, as\nrequired for estimation in [18].\nAssumption 5(APB basis functions) There exists an\nAPB function\u03c7 5:R\u22650\u2192R \u22650such that|\u03c8(x, u)| \u2264\n\u03c75\u0000\f\f[x\u22a4y\u22a4]\u22a4\f\f\u0001\n.\n3 Main Results\nWenowprovidethemaintheoreticalresultsofthiswork.\nInSection3.1wedefinetheregionalexcitationandglobal\nexcitationconditions,alongsidetheconceptofarobustly\npositiveinvariant(RPI) set. Under the assumption that\nregional excitation holds and an RPI set exists, we pro-\nvideboundsontheestimatonerrorforaclassofsystems\ncontrolled by our adaptive framework. We then define\ntheconceptofstochasticLyapunovfunctionsandglobal\nstochastic Lyapunov functions, and under the assump-\ntion that the former exists, we provide probabilistic sta-\nbility bounds for the adaptively controlled system. Sub-\nsequently, in Section 3.2, we provide high probability\nstability bounds for the system under the assumption of\nglobalexcitationandtheexistenceofaglobalstochastic\nLyapunov function.\n3.1 StabilityandEstimationwithRegionalControland\nExcitation\nFor any Borel measurable functionh:S\u00d7W\u2192R,\nwe denoteE[h(S, W)] :=R\nWR\nSh(s\u2032, w\u2032)d\u00b5s(s\u2032)d\u00b5w(w\u2032)\nandVar(h(S, W)) :=\nEh\n(h(S, W)\u2212E[h(S, W)])2i\n. They can be respectively\ninterpreted as the expectation and variance ofh(S, W)\nviewing(S, W)as random variables sampled from the\nproduct distribution\u00b5 s\u00d7\u00b5woverS\u00d7WMoreover,\ngiven a predicate2Q:S\u00d7W\u2192 {true,false}, we define\nP(Q(S, W)) :=E[1 {(s,w)\u2208S\u00d7W|Q(s,w)} (S, W)], which\ncan be interpreted as the probability ofQ(S, W)hold-\ning. This notation will be used to emphasise that some\nimportant objects can be constructed as a simple inte-\ngral with respect to only\u00b5 s\u00d7\u00b5w, which is useful from\na verification standpoint, in contrast toE,VarandP.\n2Recallthatapredicateisjustamappingfromsomeobjects\nto the truth-valuestrueandfalse. For example, givena\u2208R,\n\u201cx\u2264a\u201d isapredicatewithxasthefreevariable,whichtakes\nthe valuetrueifx\u2264aholds, andfalseifx > aholds.We now define regional and global excitation as follows,\nwhich we recall from [18].\nDefinition 2(Regional and global excitation) The fea-\nture map\u03c8and controller family\u03b1are said to be region-\nallyexcitedwithnoisedistribution\u00b5 sand\u00b5 woverX \u2286X\nwith finite constantsc PE, pPE>0if for all\u03d1\u2208Rd\u00d7n,\nx\u2208 Xand\u03b6\u2208 Sd\u22121,\nP\u0010\f\f\u03b6\u22a4\u03c8(x+W, \u03b1(x+W, S, \u03d1))\f\f2\u2265cPE\u0011\n\u2265pPE.\nFor convenience, we say(\u03c8, \u03b1, \u00b5 w, \u00b5s)is(X, c PE, pPE)-\nregionally excited. Moreover, we say(\u03c8, \u03b1, \u00b5 w, \u00b5s)is\n(cPE, pPE)-globallyexcitedif(\u03c8, \u03b1, \u00b5 w, \u00b5s)is(X, c PE, pPE)-\nregionally excited\nRecall that from the perspective of LS estimation, re-\ngional excitation ensures that for trajectories evolving\nthroughX,thecorrespondingregressorswillhaveanon-\ndegenerate distribution and will therefore beinforma-\ntiveforestimation,bywhichwemeanthattheminimum\neigenvalue of the GramianG(t)will be lower bounded\nwith high probability by a linearly growing function,\nwhichisknownaspersistencyofexcitation(PE).Global\nexcitation in Definition 2 is a strengthening of regional\nexcitation so that it holds over the entire state spaceX.\nInthegeneralcontextofnonlinearsystemidentification,\nsuch a condition may be restrictive, but stronger state-\nments on the estimation error can be made when the\nglobal excitation condition holds.\nAlongsidebeingusedtoestablishPE,recallthatregional\nexcitation is useful since it is a data-independent condi-\ntioncharacterisedentirelybythebasisfunctions\u03c8,con-\ntrol policy\u03b1, and the distribution of the process and ex-\nploratory noises\u00b5 w,\u00b5sentering the system. All of these\nobjects are assumed to be knowna priori, and therefore\nregionalexcitationinDefinition2canbereadilychecked\nwithout knowing the entirety of the system dynamics.\nThere are many different ways to verify regional excita-\ntion, one being to bound the first and second moments\nof\f\f\u03b6\u22a4\u03c8(x+W, \u03b1(x+W, S, \u03d1))\f\f2frombelowandabove\nrespectively, as in Lemma 1. This result is the same as\nLemma 1 from [18].\nLemma 1Considerfeaturemap\u03c8andcontrollerfamily\n\u03b1satisfyingAssumption1,andnoisedistributions\u00b5 w, \u00b5s\nsatisfying Assumption 2. Suppose there exists a subset\nX \u2286Xand constantsc PE1>0andc PE2\u22650such that\nfor all\u03d1\u2208Rd\u00d7n,x\u2208 Xand\u03b6\u2208 Sd\u22121,\nE\u0002\f\f\u03b6\u22a4\u03c8(x+W, \u03b1(x+W, S, \u03d1))\f\f\u0003\n\u2265cPE1,\nVar\u0000\f\f\u03b6\u22a4\u03c8(x+W, \u03b1(x+W, S, \u03d1))\f\f\u0001\n\u2264cPE2.\nThen,(\u03c8, \u03b1, \u00b5 s, \u00b5w)is(X, c PE, pPE)-regionally excited\n5\nwith\npPE:=1\n4\u0012cPE2\nc2\nPE1+ 1\u0013\u22121\n, c PE:=1\n4c2\nPE1.\nThroughout this section, we assume that regional exci-\ntation holds.\nAssumption 6(\u03c8, \u03b1, \u00b5 w, \u00b5s)is(X PE, cPE, pPE)-\nregionally excited for some setX PE\u2286Xand constants\ncPE, pPE>0.\nWe define theone-step predicted reachable set mapas\n\u0393(X) :={g(x, u,0)|x\u2208 X,|u| \u2264u max},X \u2286X.\nwhich is the set of reachable states fromX \u2286Xwhen\nthecontrolinputmagnitudeisboundedbyu maxandthe\nprocess noise is set to zero.\nNext,wedefinetheconceptofarobustlypositiveinvari-\nant (RPI) set.\nDefinition 3The setX \u2286Xis said to be a robustly\npositive invariant (RPI) set with error constant \u00af\u03d1 >0\nfor the systemgunder the influence of the policy\u03b1if\nfor allx\u2208 X,w\u2208W,s\u2208S, and\u03d1\u2208B \u00af\u03d1(\u03b8\u2217), it holds\nthatg(x, \u03b1(x, s, \u03d1), w)\u2208 X.Forconvenience,wesaythat\n(g, \u03b1)is(X, \u00af\u03d1, \u00b5w, \u00b5s)-RPI.\nIntuitively, a setXis RPI if starting from any statex\ninsideX, regardless of value of the process noisew\u2208W\nandinjectednoises\u2208S,iftheparameter\u03d1usedforcon-\ntrolissufficientlyclosetothetrueparameter\u03b8 \u2217suchthat\n|\u03d1\u2212\u03b8 \u2217| \u2264\u00af\u03d1, then the evolved stateg(x, \u03b1(x, s, \u03d1), w)\nremains insideX. Throughout Section 3, we assume the\nexistenceofsuchasetX RPIandanassociatederrorcon-\nstant \u00af\u03d1.\nAssumption 7There exists a subsetX RPI\u2286X\nand error bound \u00af\u03d1 >0such that(g, \u03b1, \u00b5 w, \u00b5s)is\n(XRPI,\u00af\u03d1, \u00b5w, \u00b5s)-RPI and\u0393(X RPI)\u2286 X PE.\nNote that in Assumption 7, we also require that\n\u0393(X RPI)\u2286 X PE. This connects the RPI setX RPIto the\nregion of excitationX PE, allowing us to establish PE\nwhen the state trajectoryX(t)evolves insideX RPI.\nRemark 1Inmanyadaptivecontrolworksbasedonro-\nbustcontroltechniqueslikerobustadaptiveMPC,knowl-\nedge of a bounded parameter set with nonempty interior\ncontaining\u03b8 \u2217is needed for controller design, such that\ntherobustcontrolalgorithmcanrendersomesetrobustly\npositive invariant accounting for all possible parameters\nin the bounded set (e.g. [1,2,12,17]). This is not what is\nrequired in this work. Even though \u00af\u03d1is required in As-\nsumption 7 for the estimation and stability results, it isnotrequiredtoimplementtheframeworkdescribedinAl-\ngorithm 1.\nBefore providing our main results, we introduce a few\nobjectsrequiredtostateourresult.Wedefinetheprocess\nnoise bound w:N 0\u00d7(0,1)\u2192R \u22650,state bound x:N\u00d7\n(0,1)\u00d7X\u2192R \u22650andregressorbound z:N\u00d7(0,1)\u00d7X\u2192\nR\u22650as\nw(t, \u03b4) :=(\n\u03c3wq\n2nln\u0000n\u03c02t2\n3\u03b4\u0001\n, t\u2208N,\n0, t= 0,\nx(t, \u03b4, x 0) :=\u03c7 1(t) +\u03c7 2(|x0|) +\u03c7 3(t\u03c31(umax)) +\u03c7 4(t\u03c32(w(t, \u03b4))) +c 1,\n(4)\nz(t, \u03b4, x 0) :=\u03c7 5\u0010p\nx(t\u22121, \u03b4, x 0)2+u2max\u0011\n,(5)\nwith\u03c7 1-\u03c74andc 1from Assumption 3, and and\u03c7 5from\nAssumption 5. Under Assumptions 2-5, these objects\nprovidehighprobabilityupperboundsonthemagnitude\nof{W(t)} t\u2208N,{X(t)} t\u2208N0and{Z(t)} t\u2208Nrespectively.\nNext, we introduce objects that provide probabilistic\nupper and lower semi-definite bounds on the regularised\nGramianG(t). We define theGramian upper boundas\n\u03b2max(t, \u03b4, x 0) :=tX\ni=1z2(i, \u03b4, x 0) +\u03b3,(6)\nwith\u03b3from (3). It is a high probability upper bound on\nthe maximum eigenvalue ofG(t), which is established in\nthe proof of Theorem 1.\nNext, we define thecontained time boundas\nTcontained (\u03b4, x 0) := sup\b\nT\u2208N|B x(T,\u03b4/3,x 0)(0)\u2229X\u2286 X RPI\t\n.\nTcontained (\u03b4, x 0)can be interpreted as a high probability\nlower bound on the horizon over which the stateX(t)\nremains insideX RPI. Moreover, define theburn-in time\nboundas\nTburn-in (\u03b4, x 0) := inf\u001a\nT\u2208N\f\f\f\ft\u22652\n(1\u2212ln(2))p PE\n\u00d7 \ndln \n1 +16Pt\ni=1z2(i, \u03b4/3, x 0)\ncPE3pPE(t\u22121)!\n+ ln\u0012\u03c02(t\u2212T+ 1)2\n2\u03b4\u0013!\n+ 1for allt\u2265T\u001b\n,\nIntuitively,T burn-in (\u03b4, x 0)can be viewed as a high prob-\nability upper bound on the time it takes for PE to start\nholding in the sense thatcPEpPE\n4(t\u22121) +\u03b3is a lower\nbound on\u03bb min(G(t)), assuming that the predicted state\n6\ng(X(t\u22121), U(t\u22121),0)remains insideX PE, which no-\ntably occurs ifX(t\u22121)is insideX RPIdue to Assump-\ntion 7.\nWe also defineT convergeas\nTconverge (\u03b4, x 0) := max (T burn-in (\u03b4, x 0),\ninf\b\nT\u2208N|e(t, \u03b4, x 0)\u2264\u00af\u03d1,\u2200t\u2265T\t\u0001\n.(7)\nTconverge (\u03b4, x 0)can be interpreted as a high probability\nupper bound on the time the parameter estimate \u02c6\u03b8(t)\nconverges into a \u00af\u03d1-ball around the true parameter\u03b8 \u2217,\nassumingthatthepredictedstateg(X(t\u22121), U(t\u22121),0)\nremains insideX PE.\nWe are now ready to state Theorem 1, which provides\nprobabilisticguaranteesontheestimationerror| \u02c6\u03b8t\u2212\u03b8\u2217|,\nand the invariance of the stateX twithin the setX RPI,\nuniformly over all time.\nTheorem 1(Regional estimation error bound and pos-\nitiveinvarianceguarantee)SupposeAssumptions1,2,3,\n4, 5, 6 and 7 are satisfied. Then, for anyx 0\u2208 XRPIand\n\u03b4\u2208(0,1), if\nTconverge (\u03b4, x 0) + 1\u2264T contained (\u03b4, x 0),(8)\nthen\nP\u0010\f\f\u02c6\u03b8(t)\u2212\u03b8 \u2217\f\f\u2264e(t, \u03b4, x 0),\u2200t\u2265T burn-in (\u03b4, x 0)\nandX(t)\u2208 X RPI,\u2200t\u22650\u0011\n\u22651\u2212\u03b4,(9)\nwhere the error bounde(t, \u03b4, x 0)is defined as\ne(t, \u03b4, x 0) :=1pcPEpPE\n4(t\u22121) +\u03b3\u0010\n\u03b31/2|\u03b8\u2217|F+(10)\n\u03c3wp\n2n(ln(3n/\u03b4) + (d/2) ln(\u03b2 max(t, \u03b4/3, x 0)\u03b3\u22121))\u0013\nand\u03b2 maxis defined in(6). Moreover, for anyx 0\u2208 XRPI\nand\u03b4\u2208(0,1),\n(1)lim t\u2192\u221ee(t, \u03b4, x 0) = 0;\n(2)T burn-in (\u03b4, x 0)<\u221e;\n(3)T converge (\u03b4, x 0)<\u221e.\nTheorem 1 provides 1) an upper bounde(t, \u03b4, x 0)on the\nestimationerror\f\f\f\u02c6\u03b8(t)\u2212\u03b8 \u2217\f\f\fthatholdsuniformlyoverall\nt\u2265T burn-in (\u03b4, x 0), and 2) a guarantee of the invariance\nofX(t)insideX RPIfor allt\u2208N 0, that are satisfied\nwith probability1\u2212\u03b4. These guarantees hold whenever\ncondition (8) is satisfied, for any\u03b4\u2208(0,1)andx 0\u2208X.\nCondition (8) ensures that with high probability, the\nestimate \u02c6\u03b8(t\u22121)will converge into a \u00af\u03d1-ball around\u03b8 \u2217beforeX(t)escapesX RPI. Then, since\u0393(X RPI)\u2286 X PE,\nby making use of Assumption 7 which guarantees RPI\nwhen the estimation error is smaller than \u00af\u03d1, it can be\ninductivelyestablished for subsequent time steps that\nPE holds, ensuring the estimation error remains smaller\nthan \u00af\u03d1, so the state remains insideX RPI, and so on,\nproducingtheinvarianceportionofthestatementin(9).\nSimultaneously, the error bound portion of (9) holds\nfor infinite time since the state trajectory is guaranteed\nto remain insideX RPIwith high probability, ensuring\nPE holds (see Remark 2 for a more detailed discussion).\nWe also observe that the error bound is asymptotically\nconvergent to zero in the sense thate(t, \u03b4, x 0)\u2192 \u221eas\nt\u2192 \u221e,whichisparticularlyinsightfulbecauseitmeans\nthat with probability at least1\u2212\u03b4, the the parameter\nestimate \u02c6\u03b8(t)willeventuallyenterandremaininsideany\narbitrarily small ball around\u03b8 \u2217.\nRemark 2Theorem1bearssimilaritieswithTheorem1\nfrom [18]. The formula for the error bounde(t, \u03b4, x 0)on\nthe estimation error| \u02c6\u03b8(t)\u2212\u03b8 \u2217|is the same in both re-\nsults, which means that the bound in this work is sim-\nilarly smaller in systems experiencing a large degree of\nexcitationandlessinstability.Moreover,bothguarantees\nrequire some kind of condition to be verified for a given\u03b4\nandx 0.Thekeydifferencebetweenthetwoisthattheer-\nror bound in Theorem 1 holds for infinite time, whereas\nthe one in [18] holds only over a finite interval. This oc-\ncurs because in Theorem 1, the RPI property of the con-\ntrolled system in Assumption 7 combined with condition\n(8)ensures the system remains insideX RPIfor all time,\nwhichguaranteesPEholdsoveraninfinitetimeinterval.\nOn the other hand, there is no RPI assumption in Theo-\nrem 1 from [18], such that we are unable to characterise\naprobabilisticeventwherethestatetrajectoryisremains\ninside a region guaranteeing PE for all time. Instead,\nTheorem 1 from [18] involves verifying a condition that\nenables the construction of a probabilistic event where\nthe lower boundcPEpPE\n4(t\u22121) +\u03b3on\u03bb min(G(t))starts\nholding beforeX(t)escapes the exciting region, allowing\nus to derive an error bound over a finite time interval.\nJust like Theorem 1 from [18], (8) is verifiable of-\nfline, meaning that it does not rely on any online\ndata collection. Moreover, althoughT converge (\u03b4, x 0)\n(andT burn-in (\u03b4, x 0)) are finite for any\u03b4\u2208(0,1)and\nx0\u2208 X RPI, without choosing a particular system, we\ncannot say whether or not the condition will be satis-\nfied. However, qualitatively, it is more likely to hold in\nsystems with a greater degree of excitation, and with\nless instability. This is becauseT burn-in (\u03b4, x 0)is smaller\nande(t, \u03b4, x 0)decays faster when z(t, \u03b4/3, x 0)grows\nslowly,andc PEandp PEarelarger,resultinginasmaller\nvalue forT converge (\u03b4, x 0). Moreover,T contained (\u03b4/3, x 0)\nis larger when z(t, \u03b4/3, x 0)grows slowly. On the other\nhand,itmaynotholdinsystemswithasmallerRPIset,\nsmall degree of excitation, and greater instability. Of\nparticular note is thatT converge (\u03b4, x 0)is smaller when\n7\n\u00af\u03d1is larger, andT contained (\u03b4, x 0)is larger whenX RPIis\nlarger. However, \u00af\u03d1andX RPIare intrinsically linked,\nand a tradeoff between them is likely to exist in specific\nproblems.\nWe now progress towards stating our main stability re-\nsult with regional control and excitation. Before doing\nso, we introduce some concepts. Given a Borel measur-\nable functionV:X \u2192R \u22650withX \u2286X, we define\n\u2206V(x, \u02dc\u03d1) :=E[V(g(x, W, \u03b1(x, S, \u02dc\u03d1+\u03b8 \u2217)]\u2212V(x)(11)\nforx\u2208 X, \u02dc\u03d1\u2208Rd\u00d7n. Note that \u02dc\u03d1in (11) does not\nrepresent the parameter estimate, but the estimation\nerror.\nNext, we introduce the notion of a stochastic Lyapunov\nfunction.\nDefinition 4(Stochastic Lyapunov function) The\nBorel measurable functionV:X RPI\u2192R \u22650over the\ndomainX RPI\u2286Xis said to be a stochastic Lyapunov\nfunction with the estimation error bound \u00af\u03d1, for the sys-\ntemgunder the influence of the policy\u03b1and the noise\ndistributions\u00b5 w,\u00b5s, if 1)(g, \u03b1)is(X RPI,\u00af\u03d1, \u00b5w, \u00b5s)-\nRPI, and 2) there exist\u03b1 1, \u03b12, \u03b13\u2208 K\u221e,\u03c33\u2208 K, and\n\u02dcd\u22650, such that\n\u03b11(|x|)\u2264V(x)\u2264\u03b1 2(|x|),\n\u2206V(x, \u02dc\u03d1)\u2264 \u2212\u03b1 3(|x|) + \u02dcd+\u03c3 3(|\u02dc\u03d1|),(12)\naresatisfiedforallx\u2208 X RPI,and \u02dc\u03d1\u2208B \u00af\u03d1(0),and\u03b1 3\u25e6\u03b1\u22121\n2\nislowerboundedbysomeconvexfunctioninK \u221e.Forcon-\nvenience, we say(g, \u03b1, \u00b5 w, \u00b5s)is(V,X RPI,\u00af\u03d1)-stochastic\nLyapunov. If(g, \u03b1, \u00b5 w, \u00b5s)is(V,X, \u00af\u03d1)-stochastic Lya-\npunov, we say that(g, \u03b1, \u00b5 w, \u00b5s)is(V, \u00af\u03d1)-global stochas-\ntic Lyapunov.\nIntuitively, the existence of the stochastic Lyapunov\nfunction in Definition 4 implies that if the parameter\u03d1\nused for control is sufficiently close to the true parame-\nter\u03b8\u2217such that| \u02dc\u03d1|=|\u03d1\u2212\u03b8 \u2217| \u2264\u00af\u03d1, then from any initial\nstatexinsideanRPIsetX RPI,weexpecttheclosed-loop\ntrajectory to decrease in magnitude, up to some value\nthat monotonically depends on \u02dcdand| \u02dc\u03d1|. On the other\nhand, aglobalstochastic Lyapunov function means that\nfrom any statexover the entire state spaceX, this\ndecreasing property is expected. The definition of the\nstochastic Lyapunov function in Definition 4 is similar\ntoexistingdefinitionsforstochasticLyapunovfunctions\nin nonlinear stochastic stability theory, in particular\nthe stochastic input-to-state Lyapunov function from\n[15]. The main difference is that the dissipation inequal-\nity (12) includes an additional determinisitc additive\ncomponent\u03c3 3(|\u02dc\u03d1|)representingtheinfluenceoftheesti-\nmation error. Another key difference is the requirement\nthat\u03b1 3\u25e6\u03b1\u22121\n2is lower bounded by some convex functioninK\u221e, which is trivially satisfied whenX RPIis bounded\nas in [15]. We assume the existence of a stochastic\nLyapunov function for the remainder of this section.\nAssumption 8ThereexistaBorelmeasurablefunction\nV:X RPI\u2192R\u22650overX RPI\u2286Xsatisfying\u0393(X RPI)\u2286\nXPE, and a constant \u00af\u03d1 >0, such that(g, \u03b1, \u00b5 w, \u00b5s)is\n(V,X RPI,\u00af\u03d1)-stochastic Lyapunov.\nWe now our main stability result in this section.\nTheorem 2(Probabilistic regional stability bound)\nSuppose Assumptions 1, 2, 3, 4, 5, 6, and 8 are satisfied.\nThen, for all\u03b4\u2208(0,1), there existsc 2\u22650such that for\nany initial statex 0\u2208 XRPI, if(\u03b4, x 0)satisfy\nTconverge (\u03b4/2, x 0) + 1\u2264T contained (\u03b4/2, x 0),(13)\nthen there exists\u03b7\u2208 Lsuch that\nP(|X(t)| \u2264\u03b7(t) +c 2)\u22651\u2212\u03b4(14)\nfor allt\u2208N 0.\nTheorem 2 says that, for any chosen failure probability\n\u03b4\u2208(0,1), there is some constantc 2\u22650such that for\nany initial statex 0\u2208 XRPI, if\u03b4andx 0satisfy condition\n(13), there exists some function\u03b7\u2208 Lsuch that magni-\ntude ofX(t)will be less than\u03b7(t) +c 2with probability\nat least1\u2212\u03b4. We can interpret this magnitude bound\nas astability boundsince it exhibits the properties of\nbothboundednessandconvergence\u2014 boundedness in\nthe sense that it is bounded from above by\u03b7(0) +c 2\nover all time (with\u03b7(0)dependent onx 0), and conver-\ngent in the sense that it converges toc 2regardless ofx 0.\nIntuitively, we can view\u03b7(t)as atransientcomponent,\nandc 2as asteady-statecomponent. This result requires\ncondition (13) to hold since it is essentially established\nby relying on the convergence of the estimate \u02c6\u03b8(t)into\na\u00af\u03d1-ball around\u03b8 \u2217, then using the existence of a Lya-\npunov function when the estimation error is sufficiently\nsmall in Assumption 8 to derive the stability bound. We\nrequire (13) rather than (8) due to the use of the union\nbound in the derivation of Theorem 2.\nRemark 3Although Theorem 2 only talks about the ex-\nistence of transient component\u03b7and steady-state com-\nponentc 2, they can be explicitly constructed. Given\u03b4\u2208\n(0,1),c 2can be constructed via\nc2=\u03b1\u22121\n1\u25e62\n\u03b4\u02dc\u03b3(2\u02dcd)(15)\nwhere\u03b1 1\u2208 K\u221eand\u02dcdare part of Definition 4, and\u02dc\u03b3\u2208\nK\u221eis constructed as\n\u02dc\u03b3(r) := 2 max(\u03b1\u22121\nv(r), r).\n8\nHere,\u03b1 visanyconvexfunctioninK \u221ethatlowerbounds\n\u03b13\u25e6\u03b1\u22121\n2, whose existence is required in Definition 4. On\nthe other hand, givenx 0\u2208 XRPIand\u03b4\u2208(0,1),\u03b7can be\nconstructed via\n\u03b7(t) := max\nt\u2032\u2265t\u02dc\u03b7(t\u2032)(16)\nwhere\u02dc\u03b7isx 0- and\u03b4-dependent and in turn constructed\nvia(17). In turn, it relies on\n\u03bb1(r) =r\u2212\u03b1 v(r) +\u03b1 v(r/2),\n\u03bb(r) =1\n2(r+ max\nr\u2032\u2208[0,1]\u03bb1(r\u2032)),\n\u03b21(r, t) =\u03b1\u22121\n1\u25e62\n\u03b4\u03bbt\u25e6\u03b12(r),\u03b22(r, t) =\u03b1\u22121\n1\u25e62\n\u03b4\u03bbt0+\u2308t/2\u2309\u25e6\u02dc\u03b3(2\u03c3 3(r)),\n\u03b72(t) =\u03b1\u22121\n1\u25e62\n\u03b4\u03bbt0+\u2308t/2\u2309\u25e6\u02dc\u03b3(2 \u02dcd),\n\u03b33(r) =\u03b1\u22121\n1\u25e62\n\u03b4\u02dc\u03b3(2\u03c3 3(r)).\nHere,\u03b1 1, \u03b12, \u03b13\u2208 K\u221eand\u03c3 3\u2208 Kare all from Defini-\ntion 4, xis from(4), andeis from(9). Note that the\nsteady-state componentc 2does not depend on the esti-\nmation error bounde(t, \u03b4/2, x 0), only on \u02dcd. Instead, this\ninformation is captured entirely in the transient compo-\nnent\u03b7(t).\n\u02dc\u03b7(t) =\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3x(Tconverge (\u03b4/2, x 0) + 1, \u03b4/6, x 0),0\u2264t\u2264T converge (\u03b4/2, x 0),\nmax\u0012\n\u03b21(x(T converge (\u03b4/2, x 0) + 1, \u03b4/6, x 0), t\u2212(T converge (\u03b4/2, x 0) + 1)),\n\u03b72(t\u2212(T converge (\u03b4/2, x 0) + 1))\n+\u03b22(max Tconverge (\u03b4/2,x 0)\u2264i\u2264T converge (\u03b4/2,x 0)+\u230a(t\u2212(T converge (\u03b4/2,x 0)+1))/2\u230b e(i, \u03b4/2, x 0),\nt\u2212(T converge (\u03b4/2, x 0) + 1)),\n\u03b33\u0000\nmax Tconverge (\u03b4/2,x 0)+1+\u230a(t\u2212(T converge (\u03b4/2,x 0)+1))/2\u230b\u2264i\u2264t\u22121 e(i, \u03b4/2, x 0)\u0001\u0013\n,\nt\u2265T converge (\u03b4/2, x 0) + 1,(17)\n3.2 Stability with Global Control and Excitation\nWe now assume global excitation and the existence of\na global stochastic Lyapunov function, and establish\nCorollary 1 under these assumptions.\nAssumption 9(\u03c8, \u03b1, \u00b5 w, \u00b5s)is(c PE, pPE)-globally ex-\ncited for some constantsc PE, pPE>0.\nAssumption 10There exist a Borel measurable func-\ntionV:X\u2192R \u22650, and a constant \u00af\u03d1 >0such that\n(g, \u03b1, \u00b5 w, \u00b5s)is(V, \u00af\u03d1)-global stochastic Lyapunov.\nCorollary 1(High probability global stability bound)\nSuppose Assumptions 1, 2, 3, 4, 5, 9, and 10 are sat-\nisfied. Then, for all\u03b4\u2208(0,1), there existsc 2\u22650such\nthat for allx 0\u2208X, there exists\u03b7\u2208 Lsuch that\nP(|X(t)| \u2264\u03b7(t) +c 2)\u22651\u2212\u03b4.(18)\nfor allt\u2208N 0.\nCorollary 1 is similar to Theorem 2, but considers the\nspecific case where global excitation holds and a global\nstochastic Lyapunov function exists. The key difference\nbetween the two is that under these strengthened as-\nsumptions, condition (8) no longer needs to be manu-ally verified, such that the statement in (18) can be es-\ntablished for anyx 0and\u03b4, resulting in a high probabil-\nity stability bound. The reason for this is explained in\nRemark 4. Moreover, the transient component\u03b7(t)and\nsteady-state componentc 2of the bound in (18) can also\nbe constructed via (15) and (16) from Remark 3.\nRemark 4Corollary 1 follows from Theorem 2 since\nunder Assumptions 9 and 10, Assumption 6 holds with\nXPE=Xand Assumption 8 holds withX RPI=X, and\nmoreoverT contained (\u03b4, x 0) =\u221e, such that condition(8)\nis automatically verified.\n4 Examples\nInExample1,weconsiderapiecewiseaffine(PWA)sys-\ntem with unknown parameters that is only regionally\ncontrollable and excited. We propose an adaptive con-\ntrol strategy based on the framework in Algorithm 1,\nand subsequently show that the assumptions required\nto establish Theorems 1 and 2 both hold. This provides\nprobabilisticguaranteesonthepositiveinvarianceofthe\nsystem on the excited and controllable region, alongside\nnon-asymptotic bounds on the estimation error and sta-\nbility bounds.\nIn Example 2, we propose an adaptive control strat-\n9\negyforinput-constrainedstochasticlinearsystemsbased\non the framework described in Algorithm 1, and subse-\nquently show how Corollary 1 can be used to establish\nhigh probability stability guarantees for the closed-loop\nsystem.\n4.1 Example 1: Piecewise-Affine System\nWe consider stochastic systems of the form\nX(t+ 1) =X(t) + 0.11 {|X(t)|\u2264\u00afx} U(t) +W(t), t\u2208N,\nX(0) =x 0\u2208R,(19)\nwhere{X(t)} t\u2208N0,{U(t)} t\u2208N0and{W(t)} t\u2208Nare ran-\ndom sequences taking values inR, andx 0is the initial\nstate. Here,\u00afx >0determines region where (19) is con-\ntrollable. We assume{W(t)} t\u2208Nis sampled i.i.d. from\nUniform(\u2212\u00afw,\u00afw)with\u00afw >0. The nominal systemf,\ntrue unknown parameter\u03b8 \u2217and basis function\u03c8in (1)\nthen correspond to\nf(x, u) = 0, \u03b8 \u2217=\"\n1\n0.1#\n, \u03c8(x, u) =\"\nx\n1{\u02dcx||\u02dcx|\u2264\u00afx} u#\n,\nand\u00b5 wis the probability measure associated with\nUniform(\u2212\u00afw,\u00afw), such thatW= [\u2212\u00afw,\u00afw].\nWe now describe the adaptive control strategy based on\nthe framework in Algorithm 1. Before doing so, given\nr >0andx\u2208Rn, let us define the radial saturation\nfunction assat r(x) :=xif|x| \u2264randsat r(x) =x\n|x|r\nif|x| \u2264r, and letM\u2020denote the pseudo-inverse of the\nmatrixM. Now, let\u00afu 1,\u00afu2>0be constants, and let\n\u00afumax= \u00afu 1+ \u00afu2. Moreover, let\u00b5 sbe the probability\nmeasure associated withUniform(\u2212\u00afs,\u00afs)where\u00afs >0,\nsuch thatS= [\u2212\u00afs,\u00afs]. The family of control policies\u03b1\nchosen for our framework is then given by\n\u03b1(x, s, \u03d1) := sat \u00afu1(\u2212\u03d1\u2020\n2\u03d11x) +s\nfors\u2208Sand\u03d1=h\n\u03d11\u03d12i\u22a4\n\u2208R2.\nNext, we verify the assumptions required to establish\nTheorems 1 and 2. In order to do so, we assume that the\nobjects in the problem setup satisfy the following:\n\u00afx\u2265\u00afw+ 0.1(u max+ \u00afu1),(20)\n0.1\u00afu 1>0.1\u00afs+ \u00afw.(21)\nWe start by verifying Assumption 1. We knowf(x, u) =\n0is Borel measurable since constants are Borel measur-\nable,\u03c8(x, u) =h\nx1{\u02dcx||\u02dcx|\u2264\u00afx} (x)ui\u22a4\nis Borel measurable\nsince the indicator function of a Borel measurable set isalso Borel measurable, and Borel measurability is pre-\nserved under multiplication and coordinate functions.\nMoreover,\u03b1(x, s, \u03d1) = sat \u00afu1(\u2212\u03d1\u2020\n2\u03d11x) +sis Borel mea-\nsurable sincesat \u00afu1(\u00b7)and the pseudo-inverse(\u00b7)\u2020of a\nmatrix are Borel measurable, and Borel measurability is\npreserved under multiplication, addition, and composi-\ntion (see [10] for these standard results).\nNext, Assumption 2 holds since{W(t)} t\u2208Nis i.i.d.,\n{W(t)} t\u2208Nand{S(t)} t\u2208N0are mutually independent,\nandW(t)is zero-mean and\u00afw2-sub-Gaussian for all\nt\u2208N(since it has a bounded support).\nWe now verify Assumption 3. Firstly, note that for any\ninitial state\u03be\u2208X, timet\u2208N 0, and deterministic input\nand noise sequences{u(i)}\u03c4\u22121\ni=0\u2208Utand{w(i)}t\ni=1\u2208\nWt, the deterministic trajectory\u03d5corresponding to the\nsystem (19) satisfies\n\f\f\u03d5(t, \u03be,{u(i)}t\u22121\ni=0,{w(i)}t\ni=1)\f\f\n\u2264 |\u03be|+ 0.1 t\u22121X\ni=0|u(i)|!\n+tX\ni=1|w(i)|\nThus, Assumption 3 is satisfied with\u03c7 2(\u00b7) = Id(\u00b7),\n\u03c73(\u00b7) = 0.1Id(\u00b7),\u03c7 4(\u00b7) =\u03c3 1(\u00b7) =\u03c3 2(\u00b7) = Id(\u00b7), and with\narbitrary\u03c7 1\u2208 K1-SE\n\u221e.\nNext, Assumption 4 holds since for allx\u2208X,s\u2208Sand\n\u03d1\u2208R,\n|\u03b1(x, s, \u03d1)| \u2264 |sat \u00afu1(\u2212\u03d1\u2020x) +s| \u2264\u00afu 1+ \u00afu2=umax.\nNote that Assumption 5 is satisfied with\u03c7 4(\u00b7) = Id(\u00b7),\nsince\n|\u03c8(x, u)|=|1 {\u02dcx||\u02dcx|\u2264\u00afx} (x)u| \u2264 |u|.\nWe now verify Assumption 6. The steps we take\nbear similarities to Example 1 from [18], where re-\ngional excitation was also established by making use\nof Lemma 1. There are some minor differences, pri-\nmarily due to the assumption that the process noise\ndistribution is bounded, and that the control inputs\napplied do not consist solely of injected noise, but also\nincluded a component computed from past data. This\nrequired us to make use of some steps inspired by Ex-\nample 2 in [18], which also included such a component.\nLetX PE:= [\u2212(\u00afx\u2212\u00afw),\u00afx\u2212\u00afw]. Supposex\u2208 X PE,\n\u03d1=h\n\u03d11\u03d12i\u22a4\n\u2208R2, and\u03b6\u2208 S1. Then,\nE\u0002\f\f\u03b6\u22a4\u03c8(x+W, \u03b1(x+W, S, \u03d1)\f\f\u0003\n=E\u0002\f\f\u03b61(x+W) +\u03b6 21{\u02dcx||\u02dcx|\u2264\u00afx} (x+W)\n\u00b7(sat \u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W)) +S)\f\f\fi\n10\n=Eh\f\f\f\u03b61(x+W) +\u03b6 2(sat \u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W)) +S)\f\f\fi\n(22)\nMoreover,\nEh\f\f\f\u03b61(x+W) +\u03b6 2(sat \u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W)) +S)\f\f\fi\n=E[E[|\u03b6 1(x+W)\n+\u03b62(sat \u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W)) +S)\f\f\f|Sii\n(23)\n\u2265Eh\f\f\fEh\n\u03b61(x+W)\n+\u03b62(sat \u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W)) +S)|Si\f\f\fi\n=Eh\f\f\f\u03b61x+\u03b6 2Eh\nsat\u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W))i\n+\u03b62S\f\f\fi\n(24)\n\u2265 |\u03b6 2|E[|S|] =|\u03b6 2|\u00afu2\n2(25)\nwhere (23) follows from the tower property, (24) follows\nfromtheindependenceofWandS,and(25)followsfrom\nthe optimality property of medians andE[|S|] =\u00afu2\n2.\nSimilarly, we have\nEh\f\f\f\u03b61(x+W) +\u03b6 2(sat \u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W)) +S)\f\f\fi\n=Eh\nEh\f\f\f\u03b61(x+W)\n+\u03b62(sat \u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W)) +S)\f\f\f|Wii\n(26)\n\u2265Eh\f\f\fEh\n\u03b61(x+W)+\n\u03b62(sat \u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W)) +S)|Wi\f\f\fi\n=Eh\f\f\f\u03b61(x+W) +\u03b6 2sat\u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W))\f\f\fi\n(27)\n\u2265 |\u03b6 1|E[|(x+W)|]\n\u2212 |\u03b6 2|Eh\f\f\fsat\u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W))\f\f\fi\n(28)\n\u2265 |\u03b6 1|E[|W|]\u2212 |\u03b6 2|u1 (29)\n=|\u03b6 1|\u00afw\n2\u2212 |\u03b6 2|u1 (30)\nwhere (26) follows from the tower property, (27) follows\nfromtheindependenceofWandS,(28)followsfromthe\nreversetriangleinequality,(29)followsfromtheoptimal-\nity property of medians and the fact thatsat \u00afu1is upper\nbounded by u1, and (30) follows fromE[|W|] =\u00afw\n2. Note\nthat if|\u03b6 2| \u2264\u00afw\n4\u00afu1+2 \u00afw, then|\u03b6 1| \u22651\u2212\u00afw\n4\u00afu1+2 \u00afw, and so\n|\u03b61|\u00afw\n2\u2212 |\u03b6 2|u1\u2265\u0010\n1\u2212\u00afw\n4\u00afu1+2 \u00afw\u0011\n\u00afw\n2\u2212\u00afw\u00afu1\n4\u00afu1+2 \u00afw=\u00afw\n4. More-\nover,if|\u03b6 2|>\u00afw\n4\u00afu1+2 \u00afw,then|\u03b6 2|u2\n2>\u00afw\u00afu2\n8\u00afu1+4 \u00afw.Combining\nthis with (22), (25) and (30), we have\nE\u0002\f\f\u03b6\u22a4\u03c8(x+W, \u03b1(x+W, S, \u03d1)\f\f\u0003\u2265max\u0012\n|\u03b62|u2\n2,|\u03b61|\u00afw\n2\u2212 |\u03b6 2|u1\u0013\n\u2265min\u0012\u00afw\n4,\u00afw\u00afu2\n8\u00afu1+ 4 \u00afw\u0013\n=:c PE1.\nOn the other hand, we have\nVar\u0010\f\f\f\u03b6\u22a4\u03c8(x+W,sat \u00afu1(\u2212\u03d1\u2020\n2\u03d11(x+W)) +S)\f\f\f\u0011\n=Var\u0010\f\f\f\u03b61(x+W) +\u03b6 21{\u02dcx||\u02dcx|\u2264\u00afx} (x+W)\n\u00b7(sat \u00afu1(\u2212\u03d1\u2020\n2\u03d11(x+W)) +S)\f\f\f\u0011\n=Var\u0010\f\f\f\u03b61(x+W) +\u03b6 2(sat \u00afu1(\u2212\u03d1\u2020\n2\u03d11(x+W)) +S)\f\f\f\u0011\n=Eh\u0010\n\u03b61W+\u03b6 2S+\u03b6 2\u0010\nsat\u00afu1(\u2212\u03d1\u2020\n2\u03d11(x+W))\n\u2212Eh\nsat\u00afu1(\u2212\u03d1\u2020\n2\u03d11(x+W))i\u0011\u00112\u0015\n\u22643\u0012\nEh\n(\u03b61W)2i\n+Eh\n(\u03b62S)2i\n+Eh\u0010\n\u03b62\u0010\nsat\u00afu1(\u2212\u03d1\u2020\n2\u03d11(x+W))\n\u2212Eh\nsat\u00afu1(\u2212\u03d1\u2020\n2\u03d11(x+W))i\u0011\u00112\u0015\u0013\n(31)\n\u22643 max\u0012\u00afw2\n3,u2\n2\n3+ 4u2\n1\u0013\n(32)\nwhere (31) follows from the QM-AM inequality, and\n(32) follows fromE\u0002\nW2\u0003\n\u2264\u00afw2\n3,E\u0002\nS2\u0003\n\u2264u2\n2\n3and\nEh\n(\u03b62(\u03b12(x+W, \u03d1)\u2212E[\u03b1 2(x+W, \u03d1)]))2i\n\u22644\u00afu2\n1.\nThus, using Lemma 1 we find that(\u03c8, \u03b1, \u00b5 s, \u00b5w)is\n(XPE, cPE, pPE)-regionally excitedwithc PE:=1\n4c2\nPE1\nandp PE:=1\n4\u0010\ncPE2\nc2\nPE1+ 1\u0011\u22121\n, such that Assumption 6 is\nsatisfied.\nNext, we verify that Assumption 7 holds. Before doing\nso, we note the following lemma holds (as a trivial con-\nsequence of Lemma 1 from [20].\nLemma 2ConsiderA\u2208Rn\u00d7nandB\u2208Rn\u00d7mwith\nn, m\u2208N. SupposeAis full rank,|A| \u22641and(A, B)is\n\u03ba-step reachable, that isrank(R \u03ba(A, B)) =nfor some\n\u03ba\u2208N.Let\u00afu >0beaconstant.Then,thereexistm, C >\n0suchthatforanyx\u2208Rnand\u02c6\u03d1=h\n\u02c6\u03d11\u02c6\u03d12i\u22a4\n\u2208\u00afBm(\u03b8\u2217),\n\f\f\fsat\u00afu(\u02c6\u03d1\u2020\n2\u02c6\u03d11x)\u2212sat \u00afu(R\u03ba(A, B)\u2020A\u03bax)\f\f\f\u2264C\f\f\f\u02c6\u03d1\u2212\u03d1 \u2217\f\f\f.\nNow, letX RPI:= [\u2212(\u00afx\u2212\u00afw\u22120.1u max),\u00afx\u2212\u00afw\u22120.1u max].\nMoreover, let \u00af\u03d1, C >0be such that0.1\u00afu 1\u22650.1(C \u00af\u03d1+\n\u00afs) + \u00afw, whose existence is verified via the assumption\n11\nin (21) and Lemma 2. Then, note that\u0393(X RPI)\u2286 X PE.\nFurthermore, for allx\u2208 X RPI,u\u2208U,s\u2208Sand \u02c6\u03d1=h\n\u02c6\u03d11\u02c6\u03d12i\u22a4\n\u2208\u00afB\u00af\u03d1(\u03b8\u2217), we have\n|g(x, \u03b1(x, s, \u02c6\u03d1\u2020\n2\u02c6\u03d11), w)|\n=|x+ 0.11 {\u02dcx||\u02dcx|\u2264\u00afx} (x)\u00b7(sat \u00afu1(\u2212\u02c6\u03d1\u2020\n2\u02c6\u03d11x) +s) +w|\n=|x+ 0.1(sat \u00afu1(\u2212\u02c6\u03d1\u2020\n2\u02c6\u03d11x) +s) +w|\n=|x+ 0.1sat \u00afu1(\u221210x)\n+ 0.1(s+ sat \u00afu1(\u2212\u02c6\u03d1\u2020\n2\u02c6\u03d11x)\u2212sat \u00afu1(\u221210x)) +w|\n\u2264 |x+ 0.1sat \u00afu1(\u221210x)|\n+|0.1(s+ sat \u00afu1(\u2212\u02c6\u03d1\u2020\n2\u02c6\u03d11x)\u2212sat \u00afu1(\u221210x)) +w|\n\u2264 |x+ 0.1sat \u00afu1(\u221210x)|+ 0.1(C \u00af\u03d1+ \u00afs) + \u00afw\n\u2264\u00afx\u2212\u00afw\u22120.1u max,\nwherethefinalinequalityfollowsfromthefactthatmak-\ning use of (20) and (21), we know that if|x| \u22640.1\u00afu 1\nand|x| \u2264\u00afx\u2212\u00afw\u22120.1u max, then|x+ 0.1sat \u00afu1(\u221210x)|+\n0.1(C \u00af\u03d1+ \u00afs) + \u00afw= 0.1(C \u00af\u03d1+ \u00afs) + \u00afw\u22640.1\u00afu 1\u2264\u00afx\u2212\u00afw\u2212\n0.1u max, and if|x|>0.1\u00afu 1and|x| \u2264\u00afx\u2212\u00afw\u22120.1u max,\nwe have|x+ 0.1sat \u00afu1(\u221210x)|+ 0.1(C \u00af\u03d1+ \u00afs) + \u00afw=|x| \u2212\n0.1\u00afu 1+ 0.1(C \u00af\u03d1+ \u00afs) + \u00afw\u2264 |x| \u2264\u00afx\u2212\u00afw\u22120.1u max. This\nimpliesg(x, \u03b1(x, s, \u02c6\u03d1), w)\u2208 X RPI. Therefore, we have\nestablished that(g, \u03b1, \u00b5 w, \u00b5s)is(X RPI,\u00af\u03d1, \u00b5w, \u00b5s)-RPI.\nWe now verify that Assumption 8 holds. Firstly,\nnote that we already established that(g, \u03b1, \u00b5 w, \u00b5s)is\n(XRPI,\u00af\u03d1, \u00b5w, \u00b5s)-RPI. Since we assumed (21), it triv-\nially follows that\nh:= ln(E[exp(0.1|S|)]) + ln(E[exp(|W|)])<0.1\u00afu 1.(33)\nFollowing similar steps to the proof of Lemma 2 from\n[20], we find that for allx\u2208 X RPIand\u02c6\u03d1=h\n\u02c6\u03d11\u02c6\u03d12i\u22a4\n\u2208\n\u00afB\u00af\u03d1(\u03b8\u2217),\nEh\nexp\u0010\n|x+ 0.11 {\u02dcx||\u02dcx|\u2264\u00afx} (x)\n\u00b7\u0010\n\u2212sat \u00afu1\u0010\n\u02c6\u03d1\u2020\n2\u02c6\u03d11x\u0011\n+S\u0011\n+W|\u0011\n\u22121i\n=Eh\nexp\u0010\n|x+ 0.1\u0010\n\u2212sat \u00afu1\u0010\n\u02c6\u03d1\u2020\n2\u02c6\u03d11x\u0011\n+S\u0011\n+W|\u0011\n\u22121i\n\u2264max\u0010\nexp\u0010\nh+ 0.1C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\n,\nexp\u0010\n|x| \u22120.1\u00afu 1+h+ 0.1C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\u0011\n\u22121\n= max\u0010\nexp\u0010\nh+ 0.1C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\n\u22121,\nexp(|x|) exp\u0010\n\u22120.1\u00afu 1+h+ 0.1C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\n\u22121\u0011\n\u2264max\u0010\nexp\u0010\nh+ 0.1C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\n\u22121,(exp(|x|)\u22121)\n\u00b7exp\u0010\n\u22120.1\u00afu 1+h+ 0.1C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\u0011\n(34)\u2264(exp(|x|)\u22121) exp\u0010\n\u22120.1\u00afu 1+h+ 0.1C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\n+ exp\u0010\nh+ 0.1C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\n\u22121\n= (exp(|x|)\u22121)\u2212\u0010\n1\u2212exp\u0010\n\u22120.1\u00afu 1\n+h+ 0.1C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\u0011\n(exp(|x|)\u22121)\n+ exp\u0010\nh+ 0.1C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\n\u22121\n\u2264(exp(|x|)\u22121)\u2212\u0000\n1\u2212exp\u0000\n\u22120.1\u00afu 1+h+C \u00af\u03d1\u0001\u0001\n\u00b7(exp(|x|)\u22121) + exp\u0010\nh+ 0.1C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\n\u22121(35)\n\u2264(exp(|x|)\u22121)\u2212\u0000\n1\u2212exp\u0000\n\u22120.1\u00afu 1+h+C \u00af\u03d1\u0001\u0001\n\u00b7(exp(|x|)\u22121) + exp(h)\u22121\n+ exp\u0010\n0.2C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\n\u22121,(36)\nwhere (34) follows from (33), (35) follows from\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u2264\u00af\u03d1, and (36) follows from the weak triangle\ninequality. Thus, Assumption 8 is satisfied withV,\u03b1 1,\n\u03b12,\u03b13chosen as\nV(x) = exp(|x|)\u22121, \u03b1 1(r) =\u03b1 2(r) = exp(r)\u22121,\n\u03b13(r) =\u0010\n1\u2212exp\u0010\n\u22120.1\u00afu 1+h+ 0.1C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\u0011\n\u00b7(exp(r)\u22121),\n\u02dcd= exp(h)\u22121, \u03c3 3(r) = exp(0.1Cr)\u22121,\nnotingthat\u03b1 3\u25e6\u03b1\u22121\n2(r)\u2265\u0000\n1\u2212exp\u0000\n\u22120.1\u00afu 1+h+C \u00af\u03d1\u0001\u0001\nr\nforr\u22650where the RHS of the inequality is clearly\nconvex.\nSince we have verified Assumptions 1-7, we know via\nTheorem 1 that if(\u03b4, x 0)satisfy (8), the probabilistic\nguarantees in (9) hold when applying the described\nadaptive control strategy to (19). Moreover, since we\nalso verified Assumption 8, we also know via Theorem 2\nthat if(\u03b4, x 0)satisfy (13), the probabilistic stability\nbounds in (14) hold. By keeping all other constants\nfixed and choosing\u00afxsufficiently large, both conditions\n(8) and (13) can be verified for the described system\nand adaptive control algorithm.\nFinally,wealsosimulatedtheclosed-loopadaptivePWA\nsystemover100trialswith x= 3000, u1= 0.9, u2= 0.1,\n\u00afw= 0.07,\u03b3= 0.0001andx 0= 0.5. The median and\n90thpercentileofthesystemstateover100samplepaths\nare shown in Figure 1. The figure highlights the sta-\nble behaviour of the adaptive system, supporting The-\norem 2. Note that with these parameters, the condi-\ntionsinTheorem2aresatisfied.However,T converge (\u03b4, x 0)\nandT contained (\u03b4, x 0)werefoundtobesignificantlylarger\nthan the times in Figure 1, reflecting the conservative-\nness of our analysis.\n12\nFig. 1. Median and90th percentile of|X(t)|over 100 trials.\n4.2 Example 2: Input-Constrained Stochastic Linear\nSystem\nWe consider stochastic linear systems of the form\nX(t+ 1) =AX(t) +BU(t) +W(t+ 1), t\u2208N 0,\nwithX(0) =x 0,wheretherandomsequences{X(t)} t\u2208N0,\n{U(t)} t\u2208N0and{W(t)} t\u2208Nare the states, controls, and\nprocess noise, taking values inRn,Rm, andRnrespec-\ntively,x 0\u2208Rnis the initial state, andA\u2208Rn\u00d7nand\nB\u2208Rn\u00d7mare the true, unknown, system matrices.\nWe assume that{W(t)} t\u2208Nis sampled i.i.d. from the\nnormal distributionN(0,\u03a3 w)with\u03a3 w\u227b0. Moreover,\nwe assume thatAis full rank and satisfies|A| \u22641, and\n(A, B)is\u03ba-step reachable, that is,rank(R \u03ba(A, B)) =n,\nwhereR \u03ba(\u02dcA,\u02dcB) :=h\n\u02dcB\u02dcA\u02dcB . . . \u02dcA\u03ba\u22121\u02dcBi\nis the\u03ba-step\nreachability matrix given \u02dcA\u2208Rn\u00d7nand \u02dcB\u2208Rn\u00d7m.\nThe\u03ba-step sub-sampled system can be written as the\nlinear system\n\u00afX(\u03c4+ 1) =AX(\u03ba(\u03c4+ 1)\u22121) +BU(\u03ba(\u03c4+ 1)\u22121)\n+W(\u03ba(\u03c4+ 1)\u22121)\n=A\u03ba\u00afX(\u03c4) +R \u2217\u00afU(\u03c4) + \u00afW(\u03c4+ 1)(37)\nfor\u03c4\u2208N 0, withR \u2217:=R \u03ba(A, B). Here,{ \u00afX(\u03c4)} t\u2208N0\nis the sub-sampled state sequence taking values inRn,\n{\u00afU(\u03c4)} \u03c4\u2208N 0is the control sequence for the sub-sampled\nsystem defined as\n\u00afU(\u03c4) :=h\nU(\u03ba(\u03c4+ 1)\u22121)\u22a4. . . U(\u03ba\u03c4)\u22a4i\u22a4\n,\nand{ \u00afW(\u03c4)} \u03c4\u2208Nis the process noise for the sub-sampled\nsystem defined as\n\u00afW(\u03c4) :=R \u03ba(A, I)h\nW(\u03ba(\u03c4+ 1)\u22121)\u22a4. . . W(\u03c4)\u22a4i\u22a4\n.Note that{ \u00afW(\u03c4)} \u03c4\u2208Nis an i.i.d. random sequence\nsampled from the distributionN(0,\u03a3 \u00afw), where\u03a3 \u00afw:=\nR\u03ba(A, I)(I \u03ba\u2297\u03a3w)R\u03ba(A, I)\u22a4with\u2297denoting the Kro-\nnecker product.\nWe consider the\u03ba-step sub-sampled system from (37)\nas the dynamical system to be controlled, such that the\nnominal systemf, unknown parameter\u03b8 \u2217, and basis\nfunctions\u03c8in (1) correspond to\nf(x, u) = 0, \u03b8 \u2217=h\nA\u03baR\u2217i\u22a4\n, \u03c8(x, u) =\"\nx\nu#\n,\nforx\u2208Rnandu\u2208R\u03bam. We make this choice because\nconsidering the sampled system is a common technique\nfor analyse the stability of input-constrained linear sys-\ntems with unbounded stochastic disturbances. This is\nreasonable because the stability of the original system\nis typically implied by the stability of the sub-sampled\nsystem. Moreover,\u00b5 wis the probability measure associ-\nated with the distributionN(0,\u03a3 \u00afw).\nWe now describe the adaptive control strategy based on\ntheframeworkinAlgorithm1.Firstly,let\u00afu 1\u2208(0, u max)\nbe a user-chosen parameter that specifies the level of\nthe saturated component of the policy\u03b1. Moreover, let\n\u00afu2=1\u221a\u03bam(umax\u2212\u00afu1),andwechoose\u00b5 sastheprobabil-\nity measure associated with the joint distribution of\u03bam\nindependent uniform distributions over[\u2212\u00afu 2,\u00afu2], such\nthatS= [\u2212\u00afu 2,\u00afu2]\u03bam. The family of control policies\u03b1\nchosen for our framework is then given by\n\u03b1(x, s, \u03d1) := sat \u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1x) +s\nfors\u2208Sand\u03d1=h\n\u03d11\u03d12i\u22a4\nwith\u03d1 1\u2208Rn\u00d7nand\u03d1 2\u2208\nRn\u00d7\u03bam.\nNext, we verify the assumptions required to establish\nCorollary 1. We start by verifying Assumption 1. We\nknowf(x, u) = 0is Borel measurable since constants\nare Borel measurable, and\u03c8(x, u) =h\nx\u22a4u\u22a4i\u22a4\nis Borel\nmeasurable since coordinate functions preserve Borel\nmeasurability.Moreover,\u03b1(x, s, \u03d1) = sat \u00afu1(\u03d1\u2020\n2\u03d1\u03ba\n1x)+sis\nBorelmeasurablesince1)addition,multiplication,com-\nposition, and coordinate functions preserve Borel mea-\nsurability, and 2) bothsat \u00afu1(\u00b7)and the pseudo-inverse\n(\u00b7)\u2020are Borel measurable functions.\nNext, Assumption 2 holds since{ \u00afW(\u03c4)} \u03c4\u2208Nis an i.i.d.\nsequence by construction,{ \u00afW(\u03c4)} \u03c4\u2208Nand{S(\u03c4)} \u03c4\u2208N 0\nare mutually independent, and \u00afW(\u03c4)is zero-mean and\n\u03bbmax(\u03a3\u00afw)-sub-Gaussian.\nWe now verify Assumption 3. Firstly, note that for any\ninitial state\u03be\u2208X, sub-sampled time\u03c4\u2208N 0, and deter-\nministic input and noise sequences for the sub-sampled\n13\nsystem{u(i)}\u03c4\u22121\ni=0\u2208U\u03c4and{w(i)}\u03c4\ni=1\u2208W\u03c4, the deter-\nministic trajectory\u03d5corresponding to the sub-sampled\nsystem (37) satisfies\n|\u03d5(\u03c4, \u03be,{u(i)}\u03c4\u22121\ni=0,{w(i)}\u03c4\ni=1)|\n\u2264 |A\u03ba\u03c4x0+\u03c4\u22121X\ni=0A\u03baiR\u2217u(\u03c4\u22121\u2212i) +\u03c4\u22121X\ni=0A\u03baiw(\u03c4\u2212i)|\n\u2264 |A\u03ba\u03c4x0|+|\u03c4\u22121X\ni=0A\u03baiR\u2217u(\u03c4\u22121\u2212i)|\n+|\u03c4\u22121X\ni=0A\u03baiR\u03ba(A, I)w(\u03c4\u2212i)|\n\u2264 |x 0|+|R \u2217|\u03c4\u22121X\ni=0|u(i)|+|R \u03ba(A, I)|\u03c4X\ni=1|w(i)|.\nThus, Assumption 3 is satisfied with\u03c7 2(\u00b7) = Id(\u00b7),\n\u03c73(\u00b7) =|R \u2217|Id(\u00b7),\n\u03c74(\u00b7) =|R \u03ba(A, I)|Id(\u00b7)and\u03c3 1(\u00b7) =\u03c3 2(\u00b7) = Id(\u00b7), and\nwith arbitrary\u03c7 1\u2208 K1-SE\n\u221e.\nNext, Assumption 4 holds since for allx\u2208X,s\u2208Sand\n\u03d1=h\n\u03d11\u03d12i\u22a4\n\u2208Rd\u00d7n,\n|\u03b1(x, s, \u03d1)|:=|sat \u00afu1(\u03d1\u2020\n2\u03d1\u03ba\n1x)|+|s| \u2264\u00afu 1+\u221a\u03bam\u00afu 2\n=umax\nNote that Assumption 5 is satisfied with\u03c7 4(\u00b7) = Id(\u00b7).\nWenowverifyAssumption9.Thestepswetaketoverify\nthis follow similarly to Example 2 in [18]. In both cases,\nLemma1isusedtoestablishregionalexcitation,andlin-\nearsystemswithboundedpoliciesandGaussianprocess\nnoise are considered. However, differences arise since we\nconsider multi-dimensional systems, rather than specif-\nically the double integrator which is a2-dimensional\nsystem. Supposex\u2208Rn,\u03d1\u2208R(n+\u03bam)\u00d7n, and\u03b6=\n[\u03b6\u22a4\n1\u03b62]\u22a4\u2208 Sn+\u03bam\u22121(where\u03b6 1\u2208Rnand\u03b6 2\u2208R\u03bam).\nThen,\nE\u0002\f\f\u03b6\u22a4\u03c8(x+W, \u03b1(x+W, S, \u03d1)\f\f\u0003\n=Eh\f\f\f\u03b6\u22a4\n1(x+W) +\u03b6\u22a4\n2(sat \u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W)) +S)\f\f\fi\n.\n(38)\nMoreover,\nEh\f\f\f\u03b6\u22a4\n1(x+W) +\u03b6\u22a4\n2(sat \u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W)) +S)\f\f\fi\n=E\u0002\nE\u0002\f\f\u03b6\u22a4\n1(x+W)\n+\u03b6\u22a4\n2(sat \u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W)) +S)\f\f\f|Sii\n(39)\u2265E\u0002\f\fE\u0002\n\u03b6\u22a4\n1(x+W)\n+\u03b6\u22a4\n2(sat \u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W)) +S)|Si\f\f\fi\n=E\u0002\f\f\u03b6\u22a4\n1x\n+\u03b6\u22a4\n2Eh\nsat\u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W))i\n+\u03b6\u22a4\n2S\f\f\fi\n(40)\n\u2265 |\u03b6 2|E[|S|] =|\u03b6 2|\u221a\u03bam\u00afu2\n2(41)\nwhere (39) follows from the tower property, and (40)\nfollows from the independence ofWandS. Moreover,\n(41)followsfromtheoptimalitypropertyofmediansand\nE[|S|]\u22651\u221a\u03bamE[|S|1] =1\u221a\u03bamP\u03bam\ni=1E[|S i|] =\u221a\u03bam\u00afu2\n2,\nwhereS idenotes theith coordinate ofSand we make\nuse ofE[|S i|] =\u00afu2\n2. Similarly, we have\nEh\f\f\f\u03b6\u22a4\n1(x+W) +\u03b6\u22a4\n2(sat \u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W)) +S)\f\f\fi\n=E\u0002\nE\u0002\f\f\u03b6\u22a4\n1(x+W)\n+\u03b6\u22a4\n2(sat \u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W)) +S)\f\f\f|Wii\n(42)\n\u2265Eh\f\f\fEh\n\u03b6\u22a4\n1(x+W)\n+\u03b6\u22a4\n2(sat \u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W)) +S)|Wi\f\f\fi\n=Eh\f\f\f\u03b6\u22a4\n1(x+W) +\u03b6\u22a4\n2sat\u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W))\f\f\fi\n(43)\n\u2265 |\u03b6 1|E[|(x+W)|]\n\u2212 |\u03b6 2|Eh\f\f\fsat\u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W))\f\f\fi\n(44)\n\u2265 |\u03b6 1|E[|W|]\u2212 |\u03b6 2|u1 (45)\n=|\u03b6 1|r\n2\u03bbmin(\u03a3\u00afw)n\n\u03c0\u2212 |\u03b6 2|u1 (46)\nwhere (42) follows from the tower property, (43) follows\nfrom the independence ofWandS, (44) follows from\nthe reverse triangle inequality, (45) follows from the\noptimality property of medians and the fact thatsat \u00afu1\nis upper bounded by u1, and (46) follows fromE[|W|] =\nEh\f\f\f\u03a31/2\n\u00afw\u03a3\u22121/2\n\u00afwW\f\f\fi\n\u2265p\n\u03bbmin(\u03a3\u00afw)Eh\f\f\f\u03a3\u22121/2\n\u00afwW\f\f\fi\n\u2265\nq\n\u03bbmin(\u03a3\u00afw)\nnEh\f\f\f\u03a3\u22121/2\n\u00afwW\f\f\f\n1i\n=\nq\n\u03bbmin(\u03a3\u00afw)\nnPn\ni=1Eh\f\f\f\u0010\n\u03a3\u22121/2\n\u00afwW\u0011\ni\f\f\fi\n=q\n2\u03bbmin(\u03a3\u00afw)n\n\u03c0,\nwhere\u0010\n\u03a3\u22121/2\n\u00afwW\u0011\nidenotes theith coordinate of\n\u03a3\u22121/2\n\u00afwWand we make use ofEh\f\f\f\u0010\n\u03a3\u22121/2\n\u00afwW\u0011\ni\f\f\fi\n=q\n2\n\u03c0\nsince\u03a3\u22121/2\n\u00afwWhas distributionN(0, I). Note\nthat if|\u03b6 2| \u2264\u221a\n\u03bbmin(\u03a3\u00afw)n\n\u00afu1\u221a\n2\u03c0+2\u221a\n\u03bbmin(\u03a3\u00afw)n, then|\u03b6 1| \u2265\n1\u2212\u221a\n\u03bbmin(\u03a3\u00afw)n\n\u00afu1\u221a\n2\u03c0+2\u221a\n\u03bbmin(\u03a3\u00afw)n, and so|\u03b6 1|q\n2\u03bbmin(\u03a3\u00afw)n\n\u03c0\u2212\n|\u03b62|u1\u2265\u0012\n1\u2212\u221a\n\u03bbmin(\u03a3\u00afw)n\n\u00afu1\u221a\n2\u03c0+2\u221a\n\u03bbmin(\u03a3\u00afw)n\u0013q\n2\u03bbmin(\u03a3\u00afw)n\n\u03c0\u2212\n\u221a\n\u03bbmin(\u03a3\u00afw)n\u00afu1\n\u00afu1\u221a\n2\u03c0+2\u221a\n\u03bbmin(\u03a3\u00afw)n=q\n\u03bbmin(\u03a3\u00afw)n\n2\u03c0. Moreover, if\n14\n|\u03b62|>\u221a\n\u03bbmin(\u03a3\u00afw)n\n\u00afu1\u221a\n2\u03c0+2\u221a\n\u03bbmin(\u03a3\u00afw)n, then|\u03b6 2|\u221a\u03bamu2\n2>\n\u221a\n\u03bbmin(\u03a3\u00afw)nm\u03ba\u00afu 2\n2\u221a\n2\u03c0\u00afu1+4\u221a\n\u03bbmin(\u03a3 \u00afw)n. Combining this with (38), (41)\nand (46), we have\nEh\f\f\f\u03b6\u22a4\n1(x+W) +\u03b6\u22a4\n2(sat \u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W)) +S)\f\f\fi\n\u2265max \n|\u03b62|\u221a\u03bam\u00afu2\n2,|\u03b61|r\n2\u03bbmin(\u03a3\u00afw)n\n\u03c0\u2212 |\u03b6 2|u1!\n\u2265min p\n\u03bbmin(\u03a3\u00afw)nm\u03ba\u00afu 2\n2\u221a\n2\u03c0\u00afu1+ 4p\u03bbmin(\u03a3 \u00afw)n,r\n\u03bbmin(\u03a3\u00afw)n\n2\u03c0!\n.\nOn the other hand, we have\nVar\u0010\f\f\f\u03b6\u22a4\u03c8(x+W,sat \u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W)) +S)\f\f\f\u0011\n\u2264Var\u0010\n\u03b6\u22a4\u03c8(x+W,sat \u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W)) +S)\u0011\n=Eh\u0010\n\u03b6\u22a4\n1W+\u03b6\u22a4\n2S+\u03b6\u22a4\n2\u0010\nsat\u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W))\n\u2212Eh\nsat\u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W))i\u0011\u00112\u0015\n\u22643\u0010\nEh\u0000\n\u03b6\u22a4\n1W\u00012i\n+Eh\u0000\n\u03b6\u22a4\n2S\u00012i\n+Eh\u0010\n\u03b6\u22a4\n2\u0010\nsat\u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W))\n\u2212Eh\nsat\u00afu1(\u2212\u03d1\u2020\n2\u03d1\u03ba\n1(x+W))i\u0011\u00112\u0015\u0013\n(47)\n\u22643\u0012\nmax\u0012\n\u03c32\nw,u2\n2\n3\u0013\n+ 4u2\n1\u0013\n,(48)\nwhere (47) follows from the QM-AM inequality, and\n(48) follows fromEh\u0000\n\u03b6\u22a4\n1W\u00012i\n=\u03b6\u22a4\n1E\u0002\nWW\u22a4\u0003\n\u03b61\u2264\n\u03bbmax(\u03a3\u00afw)andE\u0002\n(\u03b6\u22a4\n2S)2\u0003\n=\u03b6\u22a4\n2E\u0002\nSS\u22a4\u0003\n\u03b62=\u00afu2\n2\n3. Thus,\nusing Lemma 1 we find that(\u03c8, \u03b1, \u00b5 s, \u00b5w)is(c PE, pPE)-\nglobally excitedwithc PE:=1\n4c2\nPE1and\npPE :=1\n4\u0010\ncPE2\nc2\nPE1+ 1\u0011\u22121\n, wherec PE1 :=\nmin\u0012\u221a\n\u03bbmin(\u03a3\u00afw)nm\u03ba\u00afu 2\n2\u221a\n2\u03c0\u00afu1+4\u221a\n\u03bbmin(\u03a3 \u00afw)n,q\n\u03bbmin(\u03a3\u00afw)n\n2\u03c0\u0013\nand\ncPE2 := 3\u0010\nmax\u0010\n\u03c32\nw,u2\n2\n3\u0011\n+ 4u2\n1\u0011\nsuch that Assump-\ntion 9 is satisfied.\nWe now show that Assumption 10 is satisfied under the\nadditional constraint\nh:= ln(E[exp(|R \u2217S|)]) + ln(E[exp(|W|)])< \u03c3 min(R\u2217)\u00afu1.\n(49)\nTo see this, we first note under this constraint and using\nLemma2,weknowthereexist \u00af\u03d1, C >0suchthatforany\nx\u2208Rnand\u02c6\u03d1=h\n\u02c6\u03d11\u02c6\u03d12i\u22a4\n\u2208\u00afB\u00af\u03d1(\u03b8\u2217),bothh+|R \u2217|C\u00af\u03d1 <\u03c3min(R\u2217)\u00afu1and\f\f\fsat\u00afu1(\u02c6\u03d1\u2020\n2\u02c6\u03d11x)\u2212sat \u00afu1(R\u2020\n\u2217A\u03bax)\f\f\f\u2264\nC\f\f\f\u02c6\u03d1\u2212\u03d1 \u2217\f\f\fhold. Now, let \u00af\u03d1, Csatisfy this requirement.\nFollowing similar steps to the proof of Lemma 2 in [20],\nwe find that for allx\u2208Rnand\u02c6\u03d1=h\n\u02c6\u03d11\u02c6\u03d12i\u22a4\n\u2208\u00afB\u00af\u03d1(\u03b8\u2217),\nEh\nexp\u0010\n|A\u03bax+R \u2217\u0010\n\u2212sat \u00afu1\u0010\n\u02c6\u03d1\u2020\n2\u02c6\u03d11x\u0011\n+S\u0011\n+W|\u0011\n\u22121i\n\u2264 \u22121 + max\u0010\nexp\u0010\nh+|R \u2217|C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\n,\nexp\u0010\n|x| \u2212\u03c3 min(R\u2217)\u00afu1+h+|R \u2217|C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\u0011\n= max\u0010\nexp\u0010\nh+|R \u2217|C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\n\u22121,exp(|x|)\n\u00b7exp\u0010\n\u2212\u03c3min(R\u2217)\u00afu1+h+|R \u2217|C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\n\u22121\u0011\n\u2264max\u0010\nexp\u0010\nh+|R \u2217|C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\n\u22121,(exp(|x|)\u22121)\n\u00b7exp\u0010\n\u2212\u03c3min(R\u2217)\u00afu1+h+|R \u2217|C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\u0011\n(50)\n\u2264exp\u0010\n\u2212\u03c3min(R\u2217)\u00afu1+h+|R \u2217|C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\n\u00b7(exp(|x|)\u22121) + exp\u0010\nh+|R \u2217|C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\n\u22121\n= (exp(|x|)\u22121)\u2212(exp(|x|)\u22121)\n\u00b7\u0010\n1\u2212exp\u0010\n\u2212\u03c3min(R\u2217)\u00afu1+h+|R \u2217|C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\u0011\n+ exp\u0010\nh+|R \u2217|C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\n\u22121\n\u2264(exp(|x|)\u22121)\u2212(exp(|x|)\u22121)\n\u00b7\u0000\n1\u2212exp\u0000\n\u2212\u03c3min(R\u2217)\u00afu1+h+C \u00af\u03d1\u0001\u0001\n+ exp\u0010\nh+|R \u2217|C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\n\u22121(51)\n\u2264(exp(|x|)\u22121)\u2212(exp(|x|)\u22121)\n\u00b7\u0000\n1\u2212exp\u0000\n\u2212\u03c3min(R\u2217)\u00afu1+h+C \u00af\u03d1\u0001\u0001\n+ exp(2h)\u22121 + exp\u0010\n2|R\u2217|C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\n\u22121,(52)\nwhere (50) follows from (49), (51) follows from\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u2264\u00af\u03d1, and (52) follows from the weak triangle\ninequality. Thus, Assumption 10 is satisfied withV,\u03b1 1,\n\u03b12,\u03b13chosen as\nV(x) = exp(|x|)\u22121, \u03b1 1(r) =\u03b1 2(r) = exp(r)\u22121,\n\u03b13(r) = (exp(r)\u22121)\n\u00b7\u0010\n1\u2212exp\u0010\n\u2212\u03c3min(R\u2217)\u00afu1+h+|R \u2217|C\f\f\f\u02c6\u03d1\u2212\u03b8 \u2217\f\f\f\u0011\u0011\n,\n\u02dcd= exp(h)\u22121, \u03c3 3(r) = exp(2|R \u2217|Cr)\u22121,\nnoting that\u03b1 3\u25e6\u03b1\u22121\n2(r)\u2265\n(1\u2212exp(\u2212\u03c3 min(R\u2217)\u00afu1+h+Cm))rforr\u22650where\nthe RHS of the inequality is clearly convex.\nSince, we have shown that Assumptions 1, 2, 3, 4, 5 are\n9 satisfied, and 10 is also satisfied under the extra as-\n15\nsumption (49), the premise of Corollary 1 has been veri-\nfied.Thus,wehavesuccessfullyestablishedtheexistence\nof a high probability stability bounds for the learning-\nbasedadaptivecontrolproblemininput-constrainedlin-\near systems subject to Gaussian disturbances. We point\nout that high probability stability bounds were estab-\nlished in [20], but using an analysis specific to the prob-\nlem setup in (37). On the other hand, here we made use\nof the general analytical framework from Section 3 to\nestablish such bounds.\n5 Proofs\nWe now provide the proofs of all results in this paper.\nIn Section 5.1, we establish the estimation error bound\nandinvarianceguaranteeresultfromTheorem1.InSec-\ntion 5.2, we prove the probabilistic stability bound re-\nsults in Theorem 2 and Corollary 1.\n5.1 Estimation Error Bounds and Invariance Guaran-\ntee\nFirstly, we provide Lemma 3. It ensures that the states,\ncontrols, and parameter estimates, are all random se-\nquences, such that all stochastic properties of interest\nare well-defined. This result is the same as Lemma 2 in\n[18], which we refer to for the proof. We will not refer to\nLemma 3 directly in our proofs to simplify the exposi-\ntion.\nLemma 3Suppose Assumption 1 holds. Then,\n{X(t)} t\u2208N0,{U(t)} t\u2208N0,{Z(t)} t\u2208Nand{ \u02c6\u03b8(t)} t\u2208N, all\nsatisfy the definition of a random sequence.\nNext, we provide Lemma 4, which is a high probability,\nupper bound on the estimation error| \u02c6\u03b8(t)\u2212\u03b8 \u2217|as a\nfunction of\u03bb min(G(t))and\u03bb max(G(t)). This is the same\nas Lemma 7 in [18], which we refer to for the proof.\nLemma 4(Data-Dependent Least-Squares Error\nBounds) Suppose Assumptions 1 and 2 are satisfied.\nThen, for any\u03b4\u2208(0,1)andx 0\u2208X\nP\u0012\n|\u02c6\u03b8(t)\u2212\u03b8 \u2217| \u22641\n\u03bbmin(G(t))1/2\u0010\n\u03b31/2|\u03b8\u2217|F+\n\u03c3wp\n2n(ln(n/\u03b4) + (d/2) ln(\u03bb max(G(t))\u03b3\u22121))\u0011\n,\u2200t\u2208N\u0013\n\u22651\u2212\u03b4.\nIn order to move from the data-dependent bound in\nLemma 4 to a data-independent bound, we need to ob-\ntain a probabilistic upper bound on\u03bb max(G(t))and\nlowerboundon\u03bb min(G(t)).Theupperboundcanbeob-\ntained with the help of Lemma 5, which provides highprobability upper bounds on|Z(t)|and|X(t)|that hold\nuniformly over all time. This is the same as Lemma 4 in\n[18], which we refer to for the proof.\nLemma 5(High Probability State and Regressor\nBounds) Suppose Assumptions 1, 2, 3, 4 and 5 hold.\nThen, there exists APB function\u03c7 5:R\u22650\u2192R\u22650, such\nthat for anyx 0\u2208Xand\u03b4\u2208(0,1),\nP(|X(t\u22121)| \u2264 x(t\u22121, \u03b4, x 0)\nand|Z(t)| \u2264 z(t, \u03b4, x 0),\u2200t\u2208N)\u22651\u2212\u03b4,\nwithxand zdefined in(4)and(5)respectively.\nNext, we probabilistically lower bound\n\u03bbmin(Pt\ni=1Z(i)Z(i)\u22a4)using Lemma 6. It says that\nthere exists a high probability eventE 2, such that for all\ntime stepst\u2265T burn-in (3\u03b4, x 0), if 1) our sample belongs\nto this event, 2) the upper bound|Z(i)| \u2264 z(i, \u03b4, x 0)\nholds on the regressor for alli\u2264t, and 3) the one-step\npredicted stateE[X(i)|X(i\u22121), U(i\u22121)]remains in\nXPEfor alli\u2264t\u22121, then a linearly increasing lower\nbound on\u03bb min\u0010Pt\ni=1Z(i)Z(i)\u22a4\u0011\nholds \u2014 i.e. PE is\nholding. We interpret this as aregional PEresult due\nto the requirement that the one-step predicted state re-\nmains inX PE. Note that conditioning on the eventE 2is\nrequired due to the stochastic nature of the problem, in\nthe sense that PE also depends on the injected and pro-\ncess noise affecting the system. This result is the same\nas Lemma 5 in [18], which we refer to for the full proof.\nLemma 6(Regional Persistency of Excitation) Sup-\npose Assumptions 1, 2, 3, 4, 5 and 6 are satisfied. More-\nover, suppose that(\u03c8, \u03b1, \u00b5 w, \u00b5s)is(X PE, cPE, pPE)for\nsomeX PE\u2286X,c PE, pPE>0. Then, for anyx 0\u2208X\nand\u03b4\u2208(0,1), there exists an eventE 2\u2208 Fsatisfying\nP(E2)\u22651\u2212\u03b4, such that for anyt\u2265T burn-in (3\u03b4, x 0),\n\u03bbmin tX\ni=1Z(i)Z(i)\u22a4!\n\u2265cPEpPE\n4(t\u22121)\non the event3\nE2\u2229 {|Z(i)| \u2264 z(i, \u03b4, x 0),\u2200i\u2264t}\u2229\n{E[X(i)|X(i\u22121), U(i\u22121)]\u2208 X PE,\u2200i\u2264t\u22121}.\n3Weuse\u201contheevent\u201d torelateapredicateinvolvingaran-\ndomvariabletoaprobabilisticevent,withtheunderstanding\nit means that when an outcome belongs to that event, the\npredicate evaluated on the random variable is true. Specifi-\ncally, given a collection of random variablesX 1, X2, . . .tak-\ning values inX 1,X2, . . ., a predicateQ:X 1\u00d7 X 2\u00d7 \u00b7\u00b7\u00b7 \u2192\n{true,false}, and an eventE\u2208 F, we write \u201con the eventE,\nQ(X 1, X2, . . .)\u201d, or equivalently \u201cQ(X 1, X2, . . .)on the event\nE\u201d, ifE\u2286 {Q(X 1, X2, . . .)}.\n16\nWe now provide Proposition 1. It can be viewed as a\nresult similar to Theorem 1 that provides estimation er-\nror bounds and the guarantee of invariance of the state\nX(t)insideX RPI, but it also contains the information\nthat this event is a superset of another eventE 2\u2229E3\u2229E4\n(defined in Proposition 1). We provide this more infor-\nmative representation since it is useful for the stability\nanalysis later in Section 5.2.\nProposition 1SupposeAssumptions1,2,3,4,5,6and\n7 are satisfied. Then, for anyx 0\u2208 X RPIand\u03b4\u2208(0,1)\nsuch that(8)is satisfied, there exists an eventE 2\u2208 F\nsatisfyingP(E 2)\u22651\u2212\u03b4/3such that on the eventE 2\u2229\nE3\u2229 E4,\n|\u02c6\u03b8(t)\u2212\u03b8 \u2217| \u2264e(t, \u03b4, x 0)for allt\u2265T burn-in (\u03b4, x 0)\nandX(t)\u2208 X RPIfor allt\u2208N 0,(53)\nwhere the eventsE 3,E4\u2208 Fare defined as\nE3:=(\n|\u02c6\u03b8(t)\u2212\u03b8 \u2217| \u22641\n\u03bbmin(V(t))1/2\u0012\n\u03b31/2|\u03b8\u2217|F+\n\u03c3wp\n2n(ln(3n/\u03b4) + (d/2) ln(\u03bb max(V(t))\u03b3\u22121))\u0013\n,\u2200t\u2208N)\n,\nE4:={|X(t\u22121)| \u2264 x(t\u22121, \u03b4/3, x 0),\n|Z(t)| \u2264 z(t, \u03b4/3, x 0),\u2200t\u2208N}.\nand moreoverP(E 2\u2229 E3\u2229 E4)\u22651\u2212\u03b4. Furthermore, for\nanyx 0\u2208 XRPIand\u03b4\u2208(0,1),\n(1)lim t\u2192\u221ee(t, \u03b4, x 0) = 0;\n(2)T burn-in (\u03b4, x 0)<\u221e;\n(3)T converge (\u03b4, x 0)<\u221e.\nPROOF.Throughout this proof, supposex 0\u2208 X RPI,\n\u03b4\u2208(0,1)and (8) are satisfied. We will first prove (53),\nthen statements 1)-3) at the end.\nWe start by proving (53). LetE 2\u2208 Fbe an event such\nthat for allt\u2265T burn-in (\u03b4, x 0),\n\u03bbmin tX\ni=1Z(i)Z(i)\u22a4!\n\u2265cPE3pPE\n4(t\u22121)on the event\nE2\u2229 {|Z(i)| \u2264 z(i, \u03b4/3, x 0),\u2200i\u2264t}\u2229\n{E[X(i)|X(i\u22121), U(i\u22121)]\u2208 X PE,\u2200i\u2264t\u22121},(54)\nandP(E 2)\u22651\u2212\u03b4/3,wheretheexistenceofasatisfactory\nE2is established in Lemma 6, which we can make use of\ndue to Assumptions 1, 2, 3, 4, 5 and 6. Moreover, recall\nthatE 3,E4\u2208 Fare both defined in the statement of this\nproposition, and correspond to the event that that thedata-dependent estimation error bound from Lemma 4\nholds, and the event that the state and regressor bound\nfromLemma5holds,respectively.Wewillestablish(53)\nby separately proving that on the eventE 2\u2229 E3\u2229 E4,\n|\u02c6\u03b8(t)\u2212\u03b8 \u2217| \u2264e(t, \u03b4, x 0)(55)\nfor allt\u2208 {T burn-in (\u03b4, x 0), . . . , T converge (\u03b4, x 0)\u22121},\nand that on the eventE 2\u2229 E3\u2229 E4,\n|\u02c6\u03b8(t)\u2212\u03b8 \u2217| \u2264e(t, \u03b4, x 0)for allt\u2265T converge (\u03b4, x 0)\nandX(t)\u2208 X RPIfor allt\u2208N 0.(56)\nWe first establish (55). Note thatE 4\u2286 {X(i)\u2208\nXRPIfor alli\u2264T converge (\u03b4, x 0)\u22123}since\nTconverge (\u03b4, x 0)\u2264T contained (\u03b4, x 0)\u22121due to (8). This\nimplies that{X(i)\u2208 X RPIfor alli\u2264T converge (\u03b4, x 0)\u2212\n3} \u2286 {g(X(i), U(i),0)\u2208\u0393(X RPI)for alli\u2264\nTconverge (\u03b4, x 0)\u22123}\u2286{E[X(i)|X(i\u22121), U(i\u22121)]\u2208\nXPEfor alli\u2264T converge (\u03b4, x 0)\u22122}, where we used the\nfact thatE[X(i+ 1)|X(i), U(i)] =g(X(i), U(i),0)and\ng(X(i), U(i),0)\u2208\u0393(X RPI)on the event{X(i)\u2208 X RPI\ndue to Assumption 4. Making use of (54), it follows\nthat for allt\u2208 {T burn-in (\u03b4, x 0), . . . , T converge (\u03b4, x 0)\u22122},\nE2\u2229 E4\u2229 {E[X(i)|X(i\u22121), U(i\u22121)]\u2208 X PEfor alli\u2264\nTconverge (\u03b4, x 0)\u22122} \u2286 {\u03bb min(G(t))\u2265cPEpPE\n4(t\u22121)},\nand thereforeE 2\u2229 E3\u2229 E4\u2286 {|\u02c6\u03b8(t)\u2212\u03b8 \u2217| \u2264e(t, \u03b4, x 0)},\nwhich follows by the definition ofE 3,E4ande(t, \u03b4, x 0)\nfrom (10). Then, sinceE 2\u2229 E3\u2229 E4is independent oft,\nwe subsequently have established (55).\nIn order to establish (56), we first prove using an induc-\ntion argument that for allt\u2265T converge (\u03b4, x 0), on the\neventE 2\u2229 E3\u2229 E4,\n|\u02c6\u03b8(t)\u2212\u03b8 \u2217| \u2264e(t, \u03b4, x 0)andX(i)\u2208 X RPIfor alli\u2264t+ 1.\n(57)\nBase case:Note thatE 4\u2286 {X(i)\u2208 X RPIfor alli\u2264\nTconverge (\u03b4, x 0) + 1}since\nTconverge (\u03b4, x 0)\u2264T contained (\u03b4, x 0)\u22121due to (8).\nMoreover, we have{X(i)\u2208 X RPIfor alli\u2264\nTconverge (\u03b4, x 0) + 1} \u2286 {X(i)\u2208 X RPIfor alli\u2264\nTconverge (\u03b4, x 0)\u22122} \u2286 {g(X(i), U(i),0)\u2208\u0393(X RPI)i\u2264\nTconverge (\u03b4, x 0)\u22122} \u2286 {E[X(i)|X(i\u22121), U(i\u22121)]\u2208\nXPEfor alli\u2264T converge (\u03b4, x 0)\u22121}. Next, note that\nE2\u2229 E4\u2229 {E[X(i)|X(i\u22121), U(i\u22121)]\u2208 X PEfor alli\u2264\nTconverge (\u03b4, x 0)\u22121} \u2286 {\u03bb min(G(T converge (\u03b4, x 0)))\u2265\ncPEpPE\n4(Tconverge (\u03b4, x 0)\u22121)+\u03b3}making use of (54), and\nalso observe thatE 3\u2229 E4\u2229 {\u03bb min(G(T converge (\u03b4, x 0)))\u2265\ncPEpPE\n4(Tconverge (\u03b4, x 0)\u22121)+\u03b3} \u2286 {| \u02c6\u03b8(Tconverge (\u03b4, x 0))\u2212\n\u03b8\u2217| \u2264e(T converge (\u03b4, x 0), \u03b4, x 0)}. Thus, by combining\nthese set-theoretic inclusions together and making use\nof the definitions ofE 3,E4\u2208 Fande(t, \u03b4, x 0)from (10),\nwe find thatE 2\u2229 E3\u2229 E4\u2286 {|\u02c6\u03b8(Tconverge (\u03b4, x 0))\u2212\u03b8 \u2217| \u2264\n17\ne(Tconverge (\u03b4, x 0), \u03b4, x 0)andX(i)\u2208 X RPIfor alli\u2264\nTconverge (\u03b4, x 0) + 1}, completing the base case.\nInductive step:Supposet\u2265T converge (\u03b4, x 0), and that\nE2\u2229 E3\u2229 E4\u2286 {| \u02c6\u03b8(t)\u2212\u03b8 \u2217| \u2264e(t, \u03b4, x 0)andX(i)\u2208\nXRPIfor alli\u2264t+1}.Notethat{X(i)\u2208 X RPIfor alli\u2264\nt+ 1} \u2286 {X(i)\u2208 X RPIfor alli\u2264t\u22121} \u2286\n{g(X(i), U(i),0)\u2208\u0393(X RPI)for alli\u2264t\u22121} \u2286\n{E[X(i)|X(i\u22121), U(i\u22121)]\u2208 X PEfor alli\u2264t}. Next,\nnote thatE 2\u2229 E4\u2229 {E[X(i)|X(i\u22121), U(i\u22121)]\u2208\nXPEfor alli\u2264t} \u2286 {\u03bb min(G(t+1))\u2265cPEpPE\n4t+\u03b3}mak-\ninguseof(54),andalsoobservethatE 3\u2229E4\u2229{\u03bb min(G(t+\n1))\u2265cPEpPE\n4t+\u03b3} \u2286 {| \u02c6\u03b8(t)\u2212\u03b8 \u2217| \u2264e(t+1, \u03b4, x 0)}.Since\nt\u2265T converge (\u03b4, x 0), we havee(t, \u03b4, x 0)\u2264 \u00af\u03d1due to (7)\nand therefore{| \u02c6\u03b8(t)\u2212\u03b8 \u2217| \u2264e(t, \u03b4, x 0)} \u2286 {| \u02c6\u03b8(t)\u2212\u03b8 \u2217| \u2264\n\u00af\u03d1}making use of the definitions ofE 3,E4\u2208 Fand\ne(t, \u03b4, x 0)from (10). Next, note from Assumption 7 that\n{|\u02c6\u03b8(t)\u2212\u03b8 \u2217| \u2264\u00af\u03d1} \u2229 {X(t+ 1)\u2208 X RPI} \u2286 {X(t+ 2) =\ng(X(t+1), \u03b1(X(t+1), \u02c6\u03b8(t), S(t+1)), W(t+2))\u2208 X RPI}.\nThus, by combining these set-theoretic inclusions to-\ngether, we find thatE 2\u2229 E3\u2229 E4\u2286 {|\u02c6\u03b8(t+ 1)\u2212\u03b8 \u2217| \u2264\ne(t+ 1, \u03b4, x 0)andX(i)\u2208 X RPIfor alli\u2264t+ 2}, con-\ncluding the inductive step.\nSince we have established both the base case and the in-\nductive step, we have proven (57) via induction. More-\nover, sinceE 2\u2229 E3\u2229 E4is independent oftin (57), (56)\nfollows,and(53)subsequentlyfollowsbycombining(55)\nand (56).\nWealsofindthatP(E 2\u2229E3\u2229E4)\u22651\u2212\u03b4bymakinguseof\ntheunionbound,andthefactthatP(E 3)\u22651\u2212\u03b4/3from\nLemma 4 Assumptions 1 and 2,P(E 4)\u22651\u2212\u03b4/3from\nLemma 5 using Assumptions 1-5, andP(E 2)\u22651\u2212\u03b4/3\nfrom earlier in this proof.\nWe now establish statement 1)-3). The proof steps for\n1) are the same as the proof steps of statement 4) from\nCorollary 1 in [18]. Moreover, the proof of 2) is the same\nas the proof ofT burn-in (\u03b4, x 0)<\u221ein Theorem 1 from\n[18]. Finally,T converge (\u03b4, x 0)follows from 1) and 2).\nTheorem 1 easily follows as a consequence of Proposi-\ntion 1, but is more easily interpreted.\nProof of Theorem 1\nSupposex 0\u2208X,\u03b4\u2208(0,1)and (8) are satisfied. Let\nE2,E3,E4\u2208 Fbe defined the same as in Proposition 1.\nThen, we establish (9) as follows:\nP(|\u02c6\u03b8(t)\u2212\u03b8 \u2217| \u2264e(t, \u03b4, x 0)for allt\u2265T burn-in (\u03b4, x 0)\nandX(t)\u2208 X RPIfor allt\u2208N 0)\n\u2265P(E 2\u2229 E3\u2229 E4)(58)\u22651\u2212\u03b4,(59)\nwhere (58) and (59) both follow from Proposition 1.\nStatements1)-3)inTheorem1followdirectlyfromstate-\nments 1)-3) in Proposition 1.\n5.2 Stability Guarantees\nWe provide Proposition 2. It can be interpreted as a\nboundonthemagnitudeofthestatesX(t)oftheclosed-\nloop system (1) under the adaptive control framework\ndescribed in Algorithm 1, that depends on the probabil-\nity that the parameter estimates \u02c6\u03b8(t)remain in a\u00afe(t)-\nball around\u03b8 \u2217at each timet\u2265t 0(for arbitraryt 0\u2208N0\nand{\u00afe(t)} t\u2265t0).\nProposition 2Suppose Assumptions 1, 2, and 8 are\nsatisfied. Then, for all\u03b4\u2208(0,1), there exists\u03b2 1, \u03b22\u2208\nKL,\u03b3 3\u2208 K,\u03b7 2\u2208 Landc 2\u22650suchthatforanybounded\nsetX \u2286 X RPI,\nP\u0012\n|X(t)| \u2264max\u0012\n\u03b21(max\nx\u2208X|x|, t\u2212t 0),\n\u03b72(t\u2212t 0) +\u03b2 2\u0012\nmax\nt0\u22121\u2264i\u2264t 0+\u230a(t\u2212t 0)/2\u230b\u22121\u00afe(i), t\u2212t 0\u0013\n,\nc2+\u03b33\u0012\nmax\nt0+\u230a(t\u2212t 0)/2\u230b\u2264i\u2264t\u22121\u00afe(i)\u0013\u0013\u0013\n\u22651\u2212\u03b4\u2212P({X(t 0)\u2208 X RPI}c\n\u222a\b\n|\u02c6\u03b8(i)\u2212\u03b8 \u2217| \u2264\u00afe(i)for alli\u2265t 0\u22121\tc\n\u222a {X(i)\u2208 X RPIfor alli\u2265t 0}c)(60)\nfor allx 0\u2208X,t 0\u2208N 0,t\u2265t 0, and{\u00afe(i)} i\u2265t0\u2286R\u22650\nsatisfying e(i)\u2264 \u00af\u03d1for alli\u2265t 0\u22121\nPROOF.LetX RPI,V:X RPI\u2192R \u22650, and \u00af\u03b8 >0,\nbesuchthat(g, \u03b1, \u00b5 w, \u00b5s)is(V,X RPI,\u00af\u03d1)-stochasticLya-\npunov (with existence verified via Assumption 8), and\nlet\u03b1 1, \u03b12, \u03b13\u2208 K\u221e,\u03c33\u2208 K, and \u02dcd\u22650, all satisfy\nthe requirements in Definition 4 for(g, \u03b1, \u00b5 w, \u00b5s)to be\n(V,X RPI,\u00af\u03d1)-stochastic Lyapunov.\nThroughout this proof, supposeX \u2286 X RPI,x0\u2208X,\nt0\u2208N 0, and{\u00afe(i)} i\u2265t0\u2286R\u22650satisfies e(i)\u2264 \u00af\u03d1for all\ni\u2265t 0\u22121.Moreover,let{ \u02dc\u03b8(t)} t\u2265t0bearandomsequence\nsatisfying \u02dc\u03b8(t) = \u02c6\u03b8(t)\u2212\u03b8 \u2217for allt\u2265t 0\u22121.\nLet\u03b1 v\u2208 K\u221ebe a convex function satisfying\u03b1 v(r)\u2264\n\u03b13\u25e6\u03b1\u22121\n2(r)forr\u22650, as required in Definition 4. Then,\n\u03b1v(V(x))\u2264\u03b1 v\u25e6\u03b12(|x|)\u2264\u03b1 3(|x|)holdsforallx\u2208 X RPI.\n18\nTherefore, for alli\u2265t 0, on the event{| \u02c6\u03b8(i\u22121)\u2212\u03b8 \u2217| \u2264\n\u00afe(i\u22121)} \u2229 {X(i)\u2208 X RPI},\nE[V(X(i+ 1))|X(i), \u02c6\u03b8(i\u22121)]\u2212V(X(i))\n= \u2206V(X(i), \u02c6\u03b8(i\u22121))(61)\n\u2264 \u2212\u03b1 3(|X(i)|) + \u02dcd+\u03c3 3(|\u02c6\u03b8(i\u22121)\u2212\u03b8 \u2217|)(62)\n\u2264 \u2212\u03b1 v(V(X(i))) + \u02dcd+\u03c3 3(|\u02c6\u03b8(i\u22121)\u2212\u03b8 \u2217|)\n\u2264 \u2212\u03b1 v(V(X(i))) + \u02dcd+\u03c3 3(\u00afe(i\u22121)),(63)\nwhere (61) follows from (11) and (62) follows from As-\nsumption 8. For convenience, throughout the remainder\nof this proof, letd(i) = \u02dcd+\u03c3 3(\u00afe(i\u22121))fori\u2265t 0.\nNext, define \u02dcE(i) :={X(t 0)\u2208 X} \u2229 {| \u02c6\u03b8(j)\u2212\u03b8 \u2217| \u2264\u00afe(j\u2212\n1)andX(j)\u2208 X RPIfor allj\u2264i}fori\u2265t 0. Then, for\nalli\u2265t 0, we have\nE[1\u02dcE(i+1)V(X(i+ 1))]\u2264E[1 \u02dcE(i)V(X(i+ 1))]\n=E[1 \u02dcE(i)E[V(X(i+ 1))| { \u02c6\u03b8(j)}i\u22121\nj=t0\u22121,\n{X(j)}i\nj=t0]](64)\n=E[1 \u02dcE(i)E[V(X(i+ 1))| \u02c6\u03b8(i\u22121), X(i)]](65)\n\u2264E[1 \u02dcE(i)(V(X(i))\u2212\u03b1 v(V(X(i))) +d(i))](66)\n\u2264E[1 \u02dcE(i)V(X(i))]\u2212(E[1 \u02dcE(i)\u03b1v(V(X(i)))]) +d(i)\n\u2264E[1 \u02dcE(i)V(X(i))]\u2212(E[\u03b1 v(1\u02dcE(i)V(X(i)))])\n+d(i)(67)\n\u2264E[1{\u02dcE(i)}V(X(i))]\u2212\u03b1 v(E[1 \u02dcE(i)V(X(i))])\n+d(i)(68)\nwhere (64) follows from the tower property of condi-\ntional expectation and pulling out known factors, (65)\nfollows from the conditional independence ofX(i+ 1)\nand{ \u02c6\u03b8(j)}i\u22122\nj=t0\u22121,{X(j)}i\u22121\nj=t0given \u02c6\u03b8(i\u22121), X(i), (66)\nfollows from (63), (67) follows from the convexity of\u03b1 v\nandthefactthat\u03b1 v(0) = 0,and(68)followsviaJensen\u2019s\ninequality.\nNext, define\u02dc\u03b3(r) := 2 max(\u03b1\u22121\nv(r), r)forr\u22650, and\nnote that\u02dc\u03b3\u2208 K. We now establish that for alli\u2265t 0\nandl\u2264i, ifE[1 \u02dcE(i)V(X(i))]\u2264\u02dc\u03b3(max l\u2264j\u2264i d(j)), then\nE[1\u02dcE(i+1)V(X(i+ 1))]\u2264\u02dc\u03b3( max\nl\u2264j\u2264id(j)).(69)\nWe prove (69) via two cases.\nCase1:SupposeE[V(X(i))1 \u02dcE(i)]\u2264\u02dc\u03b3(max l\u2264j\u2264i d(j))/2.\nThen, we have\nE[1\u02dcE(i+1)V(X(i+ 1))]\n\u2264E[1 \u02dcE(i)V(X(i))]\u2212\u03b1 v(E[1 \u02dcE(i)V(X(i))]) + max\nl\u2264j\u2264id(j)\n(70)\u2264E[1 \u02dcE(i)V(X(i))] + max\nl\u2264j\u2264id(j)\n\u2264\u02dc\u03b3(max l\u2264j\u2264i d(j))\n2+ max\nl\u2264j\u2264id(j)(71)\n\u2264\u02dc\u03b3( max\nl\u2264j\u2264id(j)),(72)\nwhere (70) follows from (68), (71) follows since\nE[V(X(i))1 \u02dcE(i)]\u2264\u02dc\u03b3(max l\u2264j\u2264i d(j))/2, and (72) follows\nfrommax l\u2264j\u2264i d(j)\u2264\u02dc\u03b3(max l\u2264j\u2264i d(j))/2by the defini-\ntion of\u02dc\u03b3.\nCase 2:Suppose\u02dc\u03b3(max l\u2264j\u2264i d(j))/2\u2264\nE[V(X(i))1 \u02dcE(i)]\u2264\u02dc\u03b3(max l\u2264j\u2264i d(j)). Then,\nE[1\u02dcE(i+1)V(X(i+ 1))]\n\u2264E[1 \u02dcE(i)V(X(i))]\u2212\u03b1 v(E[1 \u02dcE(i)V(X(i))])\n+ max\nl\u2264j\u2264id(j)(73)\n\u2264\u02dc\u03b3( max\nl\u2264j\u2264id(j))\u2212\u03b1 v(\u02dc\u03b3(max l\u2264j\u2264i d(j))\n2)\n+ max\nl\u2264j\u2264id(j)(74)\n\u2264\u02dc\u03b3( max\nl\u2264j\u2264id(j))(75)\nwhere (73) follows from (68), (74) follows from the\nassumption that\n\u02dc\u03b3(max l\u2264j\u2264i d(j))/2\u2264E[V(X(i))1 \u02dcE(i\u22121)]\u2264\n\u02dc\u03b3(max l\u2264j\u2264i d(j)), and (75) the fact that\n\u2212\u03b1v(\u02dc\u03b3(max l\u2264j\u2264i d(j))\n2)\u2264 \u2212max l\u2264j\u2264i d(j)due to the\ndefinition of\u02dc\u03b3.\nThus, we have proven (69) via two cases.\nNext, define\u03bb 1(s) :=s\u2212\u03b1 v(s) +\u03b1 v(s/2), and\u03bb(s) :=\n(1/2)(s+max s\u2032\u2208[0,1] \u03bb1(s\u2032)). Note that\u03bb 1is continuous,\n\u03bb1(0) = 0, and\u03bb 1(s)< sfor alls >0, but not nec-\nessarily increasing. On the other hand, it can be shown\nby following similar steps to [16, Theorem B.15], that\n\u03bb\u2208 K \u221eand\n\u03bb1(s)\u2264\u03bb(s)< s(76)\nfor alls >0. We then have that for alli\u2265t 0andl\u2264i,\nifE[V(X(i))1 \u02dcE(i)]>\u02dc\u03b3(max l\u2264j\u2264i d(j)), then\nE[1\u02dcE(i+1)V(X(i+ 1))]\n\u2264E[1 \u02dcE(i)V(X(i))]\u2212\u03b1 v(E[1 \u02dcE(i)V(X(i))]) + max\nl\u2264j\u2264id(j)\n(77)\n\u2264E[1 \u02dcE(i)V(X(i))]\u2212\u03b1 v(E[1 \u02dcE(i)V(X(i))])\n+\u03b1v \nE[V(X(i))1 \u02dcE(i\u22121)]\n2!\n(78)\n\u2264\u03bb(E[1 \u02dcE(i)V(X(i))])(79)\n19\nwhere (77) follows from (68), (78) follows from\nE[V(X(i))1 \u02dcE(i)]>\u02dc\u03b3(max l\u2264j\u2264i d(j)), and the fact that\nmax l\u2264j\u2264i d(j)\u2264\u03b1 v(\u02dc\u03b3(max l\u2264j\u2264i d(j))\n2)by definition of\u02dc\u03b3,\nand (79) follows from (76).\nNow, by contraposition of (69), for alli\u2265t 0andl\u2264i,\nifE[1 \u02dcE(i+1)V(X(i+ 1))]>\u02dc\u03b3(max l\u2264j\u2264i d(j)), then\nE[1\u02dcE(i)V(X(i))]>\u02dc\u03b3( max\nl\u2264j\u2264id(j)).(80)\nLet\u03bbidenote the composition of\u03bbwith itselfitimes.\nFor alli\u2265t 0andl\u2208 {t 0, . . . , i}, ifE[V(X(i))1 \u02dcE(i)]>\n\u02dc\u03b3(max l\u2264j\u2264i d(j)), then it follows that\nE[V(X(i+ 1))1 \u02dcE(i+1)]\u2264\u03bb(E[V(X(i))1 \u02dcE(i)])(81)\n\u2264\u03bb2(E[V(X i\u22121)1\u02dcE(i\u22121)])(82)\n\u2264\u03bbi\u2212l+1(E[V(X l)1\u02dcE(l)])(83)\nwhere (81) follows from (79), (82) follows from (80) and\n(79), and (83) follows by iteratively repeating this pro-\ncessi+ 1times.\nSupposet\u2265t 0. For alll\u2208 {t 0, . . . , t}, we have\nE[V(X(t))1 \u02dcE(t)]\n\u2264max(\u03bbt\u2212l(E[V(X l)1\u02dcE(l)]),\u02dc\u03b3( max\nl\u2264j\u2264t\u22121d(j)))(84)\n\u2264max(\u03bbt\u2212l(max(\u03bbl\u2212t0E[V(X(t 0))1\u02dcE(t0)],\n\u02dc\u03b3( max\nt0\u2264j\u2264l\u22121d(j)))),\u02dc\u03b3( max\nl\u2264j\u2264t\u22121d(i)))(85)\n\u2264max(\u03bbt\u2212l(max(\u03bbl\u2212t0\u25e6\u03b12(max\nx\u2208X|x|),\n\u02dc\u03b3( max\nt0\u2264j\u2264l\u22121d(j)))),\u02dc\u03b3( max\nl\u2264j\u2264t\u22121d(j)))\n= max(\u03bbt\u2212t0\u25e6\u03b12(max\nx\u2208X|x|),\n\u03bbt\u2212l\u25e6\u02dc\u03b3( max\nt0\u2264j\u2264l\u22121d(j)),\u02dc\u03b3( max\nl\u2264j\u2264t\u22121d(j)))\nwhere (84) follows from (69) and (83), and so does (85).\nBy settingl\u2190t 0+\u230a(t\u2212t 0)/2\u230b, it follows that\nE[V(X(t))1 \u02dcE(t)]\n\u2264max \n\u03bbt\u2212t0\u25e6\u03b12\u0012\nmax\nx\u2208X|x|\u0013\n,\n\u03bbt\u2212t0\u2212\u230a(t\u2212t 0)/2\u230b\u25e6\u02dc\u03b3\u0012\nmax\nt0\u2264j\u2264t 0+\u230a(t\u2212t 0)/2\u230b\u22121d(j)\u0013\n,\n\u02dc\u03b3\u0012\nmax\nt0+\u230a(t\u2212t 0)/2\u230b\u2264j\u2264t\u22121d(j)\u0013!\n= max(\u03bbt\u2212t0\u25e6\u03b12(max\nx\u2208X|x|), \u03bbt\u2212t0\u2212\u230a(t\u2212t 0)/2\u230b\n\u25e6\u02dc\u03b3(\u02dcd+\u03c3 3( max\nt0\u22121\u2264j\u2264t 0+\u230a(t\u2212t 0)/2\u230b\u22121e(j))),\u02dc\u03b3(\u02dcd+\u03c3 3( max\nt0+\u230a(t\u2212t 0)/2\u230b\u2264j\u2264t\u22121e(j))))\n= max(\u03bbt\u2212t0\u25e6\u03b12(max\nx\u2208X|x|), \u03bbt0+\u2308(t\u2212t 0)/2\u2309\n\u25e6\u02dc\u03b3(\u02dcd+\u03c3 3( max\nt0\u22121\u2264j\u2264t 0+\u230a(t\u2212t 0)/2\u230b\u22121e(j))),\n\u02dc\u03b3(\u02dcd+\u03c3 3( max\nt0+\u230a(t\u2212t 0)/2\u230b\u2264j\u2264t\u22121e(j))))\nNext, suppose\u03b4\u2208(0,1), and let\nE1=(\nV(X(t))1 \u02dcE(t)\u22641\n\u03b4max \n\u03bbt\u2212t0\u25e6\u03b12\u0012\nmax\nx\u2208X|x|\u0013\n,\n\u03bbt0+\u2308(t\u2212t 0)/2\u2309\n\u25e6\u02dc\u03b3\u0012\n\u02dcd+\u03c3 3\u0012\nmax\nt0\u22121\u2264j\u2264t 0+\u230a(t\u2212t 0)/2\u230b\u22121e(j)\u0013\u0013\n,\n\u02dc\u03b3\u0012\n\u02dcd+\u03c3 3\u0012\nmax\nt0+\u230a(t\u2212t 0)/2\u230b\u2264j\u2264t\u22121e(j)\u0013\u0013!)\n.\nFrom Markov\u2019s inequality, we have\nP(E 1)\u22651\u2212\u03b4.(86)\nMoreover, on the eventE 1\u2229\u02dcE(t), we have\n|X(t)|\n\u2264\u03b1\u22121\n1(V(X(t)))(87)\n=\u03b1\u22121\n1(V(X(t))1 \u02dcE(t))\n\u2264\u03b1\u22121\n1 \n1\n\u03b4max \n\u03bbt\u2212t0\u25e6\u03b12\u0012\nmax\nx\u2208X|x|\u0013\n,\n\u03bbt0+\u2308(t\u2212t 0)/2\u2309\n\u25e6\u02dc\u03b3\u0012\n\u02dcd+\u03c3 3\u0012\nmax\nt0\u22121\u2264j\u2264t 0+\u230a(t\u2212t 0)/2\u230b\u22121e(j)\u0013\u0013\n,\n\u02dc\u03b3\u0012\n\u02dcd+\u03c3 3\u0012\nmax\nt0+\u230a(t\u2212t 0)/2\u230b\u2264j\u2264t\u22121e(j)\u0013\u0013!!\n\u2264max \n\u03b1\u22121\n1\u25e61\n\u03b4\u03bbt\u2212t0\u25e6\u03b12\u0012\nmax\nx\u2208X|x|\u0013\n,\n\u03b1\u22121\n1\u25e61\n\u03b4\u03bbt0+\u2308(t\u2212t 0)/2\u2309\n\u25e6\u02dc\u03b3\u0012\n\u02dcd+\u03c3 3\u0012\nmax\nt0\u22121\u2264j\u2264t 0+\u230a(t\u2212t 0)/2\u230b\u22121e(j)\u0013\u0013\n,\n\u03b1\u22121\n1\u25e61\n\u03b4\u02dc\u03b3\u0012\n\u02dcd+\u03c3 3\u0012\nmax\nt0+\u230a(t\u2212t 0)/2\u230b\u2264j\u2264t\u22121e(j)\u0013\u0013!\n\u2264max(\u03b1\u22121\n1\u25e61\n\u03b4\u03bbt\u2212t0\u25e6\u03b12(max\nx\u2208X|x|),\n\u03b1\u22121\n1\u25e61\n\u03b4\u03bbt0+\u2308(t\u2212t 0)/2\u2309\u25e6\u02dc\u03b3(2 \u02dcd) +\u03b1\u22121\n1\u25e61\n\u03b4\u03bbt0+\u2308(t\u2212t 0)/2\u2309\n\u25e6\u02dc\u03b3(2\u03c3 3( max\nt0\u22121\u2264j\u2264t 0+\u230a(t\u2212t 0)/2\u230b\u22121e(j))),\n20\n\u03b1\u22121\n1\u25e61\n\u03b4\u02dc\u03b3(2\u02dcd) +\u03b1\u22121\n1\n\u25e61\n\u03b4\u02dc\u03b3(2\u03c3 3( max\nt0+\u230a(t\u2212t 0)/2\u230b\u2264j\u2264t\u22121e(j))))(88)\n\u2264max(\u03b2 1(max\nx\u2208X|x|, t\u2212t 0), \u03b72(t\u2212t 0)\n+\u03b22( max\nt0\u22121\u2264j\u2264t 0+\u230a(t\u2212t 0)/2\u230b\u22121e(j), t\u2212t 0),\nc2+\u03b33( max\nt0+\u230a(t\u2212t 0)/2\u230b\u2264j\u2264t\u22121e(j)))(89)\nwhere (87) follows from Definition 4, (88) follows from\nthe weak triangle inequality forKfunctions in [9, Equa-\ntion6].Line(89)followsbysetting\u03b2 1(r, k) =\u03b1\u22121\n1\u25e61\n\u03b4\u03bbk\u25e6\n\u03b12(r),c 2=\u03b1\u22121\n1\u25e61\n\u03b4\u02dc\u03b3(2\u02dcd),and\u03b3 3(r) =\u03b1\u22121\n1\u25e61\n\u03b4\u02dc\u03b3(2\u03c3 3(r)),\nwhere\u03b2 1and\u03b3 3are clearly of classKLandKre-\nspectively, and moreover by letting\u03b7 2and\u03b2 2be class\nLandKLfunctions respectively that satisfy\u03b7 2(k) =\n\u03b1\u22121\n1\u25e61\n\u03b4\u03bbt0+\u2308k/2\u2309\u25e6\u02dc\u03b3(2 \u02dcd),\u03b2 2(r, k) =\u03b1\u22121\n1\u25e61\n\u03b4\u03bbt0+\u2308k/2\u2309\u25e6\n\u02dc\u03b3(2\u03c3 3(r)).\nTherefore, we find that\nP \n|X(t)| \u2264max \n\u03b21\u0012\nmax\nx\u2208X|x|, t\u2212t 0\u0013\n,\n\u03b72(t\u2212t 0) +\u03b2 2\u0012\nmax\nt0\u22121\u2264j\u2264t 0+\u230a(t\u2212t 0)/2\u230b\u22121e(j), t\u2212t 0\u0013\n,\nc2+\u03b33\u0012\nmax\nt0+\u230a(t\u2212t 0)/2\u230b\u2264j\u2264t\u22121e(j)\u0013!!\n\u2265P(E 1\u2229\u02dcE(t))(90)\n\u22651\u2212P(Ec\n1)\u2212P( \u02dcEc(t))(91)\n\u22651\u2212\u03b4\u2212P( \u02dcEc(t)).(92)\nwhere(90)followsfrom(89),(91)followsfromtheunion\nbound, and (92) follows from (86).\nWe now prove Theorem 2.\nProof of Theorem 2Suppose\u03b4\u2208(0,1), and let\n\u02dc\u03b4=\u03b4/2. Let \u00af\u03d1andV:X RPI\u2192R \u22650be such that\n(g, \u03b1, \u00b5 w, \u00b5s)is(V,X RPI,\u00af\u03d1)-stochastic Lyapunov. Let\n\u03b21, \u03b22\u2208 KL,\u03b3 3\u2208 K,\u03b7 2\u2208 Landc 2\u22650satisfy (60),\nwhere the existence of these objects is ensured since un-\nder our assumptions, Proposition 2 is satisfied. Now,\nsupposex 0\u2208 X RPI, and( \u02dc\u03b4, x0)satisfies condition (8)\nequivalent to (13) being satisfied). Then, using Propo-\nsition 1, we know there exists an eventE 2\u2208 Fsatisfy-\ning (53), withE 3,E4\u2208 Fdefined in Proposition 1. Next,\nnote that on the eventE 2\u2229 E3\u2229 E4,\n|\u02c6\u03b8(t)\u2212\u03b8 \u2217| \u2264e(t, \u02dc\u03b4, x0)for allt\u2265T burn-in (\u02dc\u03b4, x0)andX(t)\u2208 X RPIfor allt\u2208N 0.\nFor convenience of notation, letT 0=T converge (\u02dc\u03b4, x0).\nUsing this result, and the fact thate(t, \u02dc\u03b4, x0)\u2264 \u00af\u03d1for\nallt\u2265T 0by the definition ofT converged (\u02dc\u03b4, x0)in (7), we\napply Proposition 2 to find that for allt\u2265T 0+ 1,\nP\u0012\n|X(t)| \u2264max\u0012\n\u03b21(x(T0+ 1,\u02dc\u03b4/3, x 0), t\u2212(T 0+ 1)),\n\u03b22\u0012\nmax\n(T0+1)\u22121\u2264i\u2264(T 0+1)+\u230a(t\u2212(T 0+1))/2\u230b\u22121e(i,\u02dc\u03b4, x0),\nt\u2212(T 0+ 1)) +\u03b7 2(t\u2212(T 0+ 1)),\nc2+\u03b33\u0012\nmax\n(T0+1)+\u230a(t\u2212(T 0+1))/2\u230b\u2264i\u2264t\u22121e(i,\u02dc\u03b4, x0)\u0013\u0013\u0013\n\u22651\u2212 \u02dc\u03b4\u2212P({|X(T 0+ 1)| \u2264 x(T0+ 1,\u02dc\u03b4/3, x 0)}c\n\u222a\b\n|\u02c6\u03b8(i)\u2212\u03b8 \u2217| \u2264e(i, \u02dc\u03b4, x0)for alli\u2265(T 0+ 1)\u22121\tc\n\u222a {X(i)\u2208 X RPIfor alli\u2265(T 0+ 1)}c)\n\u22651\u2212 \u02dc\u03b4\u2212P((E 2\u2229 E3\u2229 E4)c)\u22651\u22122 \u02dc\u03b4(93)\nwhere the last inequalities follows fromE 2\u2229 E3\u2229\nE4\u2286 {|X(T 0+ 1)| \u2264 x(T0+ 1,\u02dc\u03b4/3, x 0)} \u2229\b\n|\u02c6\u03b8(i)\u2212\n\u03b8\u2217| \u2264e(i, \u02dc\u03b4, x0)for alli\u2265(T 0+ 1)\u22121\t\n\u2229 {X(i)\u2208\nXRPIfor alli\u2265(T 0+ 1)}, and the fact thatP(E 2\u2229 E3\u2229\nE4)\u22651\u2212 \u02dc\u03b4from Proposition 1.\nNext, define\u03b7as a function fromN 0toR\u22650satisfying\n\u03b7(t) := max t\u2032\u2265t\u02dc\u03b7(t\u2032)fort\u2208N 0, with\u02dc\u03b7(t)defined as\n\u02dc\u03b7(t)\n=\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3x(T0+ 1, \u03b4/3, x 0),0\u2264t\u2264T 0,\nmax\u0012\n\u03b21(x(T0+ 1, \u03b4/3, x 0), t\u2212(T 0+ 1)),\n\u03b22(max T0\u2264i\u2264T 0+\u230a(t\u2212(T 0+1))/2\u230b e(i, \u03b4, x 0),\nt\u2212(T 0+ 1)) +\u03b7 2(t\u2212(T 0+ 1)),\n\u03b33\u0000\nmax T0+1+\u230a(t\u2212(T 0+1))/2\u230b\u2264i\u2264t\u22121 e(i, \u03b4, x 0)\u0001\u0013\n,\nt\u2265T 0+ 1.\n(94)\nfor allt\u2208N 0, where clearly\u03b7\u2208 Lsince since\nlimt\u2192\u221e\u02dc\u03b3(t) = 0duetothefactthatlim t\u2192\u221ee(t,\u02dc\u03b4, x0) =\n0from Theorem 1. The conclusion follows by combining\n(93) with (94), and substituting \u02dc\u03b4with\u03b4/2.\nWe now provide the proof of Corollary 1, which follows\nfrom Theorem 2.\nProof of Corollary 1Since(\u03c8, \u03b1, \u00b5 w, \u00b5s)is\n(cPE, pPE)-globally excitedby assumption, it follows\n21\nthat(\u03c8, \u03b1, \u00b5 w, \u00b5s)is(X PE, cPE, pPE)-regionally excited\nwithX PE=X, satisfying Assumption 6. Moreover,\nnote that Assumption 7 is trivially satisfied with\nXRPI=X. It follows trivially thatT contained (\u03b4/2, x 0) =\nsup\b\nT\u2208N|B x(T,\u03b4/6,x 0)(0)\u2229X\u2286 X RPI\t\n=\u221e.\nMoreover, from Proposition 1, we know that\nTconverge (\u03b4/2, x 0)<\u221e. Next, note that(g, \u03b1, \u00b5 w, \u00b5s)is\n(V,\u00af\u03d1)-global stochastic Lyapunov by Assumption 10, so\n(g, \u03b1, \u00b5 w, \u00b5s)is(V,X, \u00af\u03d1)-stochasticLyapunov, satisfying\nAssumption 8. SinceT converge (\u03b4/2, x 0)< T contained =\n\u221e, we have verified condition (8) and hence the premise\nof Theorem 2, and therefore can establish (14), con-\ncluding the proof.\n6 Conclusion\nIn this work, we have provided a framework for AC\nin linearly parameterised stochastic systems, that com-\nbinesaparameterisedstabilisingpolicywithregularised\nleast squares (RLS) for parameter estimation. We de-\nrivednon-asymptoticerrorboundsfortheparameteres-\ntimatethatholdsforsufficientlylargetimewithpositive\nprobability under some assumptions, in particular, re-\nquiring that the policy will render the states of the sys-\ntempositivelyinvariantinaregionallyexcitingsetwhen\nthe parameter estimate is close to the true parameter.\nBy additionally assuming the existence of a stochastic\nLyapunov function over the invariant set, probabilistic\nstability bounds were then derived. These bounds were\nshown to exist for a regionally controllable PWA sys-\ntem example. Then, under the stricter assumption of\nglobal excitation and the existence of a global stochas-\ntic Lyapunov function with small parameter estimation\nerror, high probability stability bounds were derived.\nThe usefulness of this result was showcased on an input-\nconstrained linear system example.\nReferences\n[1] Veronica Adetola, Darryl DeHaan, and Martin Guay.\nAdaptive model predictive control for constrained nonlinear\nsystems.Systems & Control Letters, 58(5):320\u2013326, 2009.\n[2] Veronica Adetola and Martin Guay. Robust adaptive mpc\nfor constrained uncertain nonlinear systems.International\nJournal of Adaptive Control and Signal Processing,\n25(2):155\u2013167, 2011.\n[3] David Angeli and Eduardo D Sontag. Forward\ncompleteness, unboundedness observability, and their\nlyapunov characterizations.Systems & Control Letters, 38(4-\n5):209\u2013217, 1999.\n[4] Anuradha M Annaswamy and Alexander L Fradkov. A\nhistorical perspective of adaptive control and learning.\nAnnual Reviews in Control, 52:18\u201341, 2021.\n[5] Mouhacine Benosman. Learning-based adaptive control for\nnonlinear systems. In2014 European Control Conference\n(ECC), pages 920\u2013925. IEEE, 2014.[6] Nicholas M Boffi, Stephen Tu, and Jean-Jacques E Slotine.\nRegret bounds for adaptive nonlinear control. InLearning\nfor Dynamics and Control, pages 471\u2013483. PMLR, 2021.\n[7] Graham C Goodwin, Peter J Ramadge, and Peter E Caines.\nDiscrete time stochastic adaptive control.SIAM J. Control\nOptim., 19(6):829\u2013853, 1981.\n[8] Lei Guo. On critical stability of discrete-time adaptive\nnonlinear control.IEEE Transactions on Automatic Control,\n42(11):1488\u20131499, 1997.\n[9] Z P Jiang, Andrew R Teel, and Laurent Praly. Small-gain\ntheorem for iss systems and applications.Mathematics of\nControl, Signals and Systems, 7:95\u2013120, 1994.\n[10] Olav Kallenberg and Olav Kallenberg.Foundations of\nmodern probability, volume 2. Springer, 1997.\n[11] Iasson Karafyllis and Miroslav Krstic. Adaptive certainty-\nequivalence control with regulation-triggered finite-time\nleast-squares identification.IEEE Transactions on\nAutomatic Control, 63(10):3261\u20133275, 2018.\n[12] Johannes K\u00f6hler, Peter K\u00f6tting, Raffaele Soloperto, Frank\nAllg\u00f6wer, and Matthias A M\u00fcller. A robust adaptive model\npredictivecontrolframeworkfornonlinearuncertainsystems.\nInternational Journal of Robust and Nonlinear Control,\n31(18):8725\u20138749, 2021.\n[13] Chanying Li and James Lam. Stabilization of discrete-\ntime nonlinear uncertain systems by feedback based on ls\nalgorithm.SIAM Journal on Control and Optimization,\n51(2):1128\u20131151, 2013.\n[14] Zhaobo Liu and Chanying Li. Is it possible to stabilize\ndiscrete-time parameterized uncertain systems growing\nexponentially fast?SIAM Journal on Control and\nOptimization, 57(3):1965\u20131984, 2019.\n[15] Robert D McAllister and James B Rawlings. The stochastic\nrobustness of nominal and stochastic model predictive\ncontrol.IEEE Transactions on Automatic Control, 2022.\n[16] James Blake Rawlings, David Q Mayne, and Moritz Diehl.\nModel predictive control: theory, computation, and design,\nvolume 2. Nob Hill Publishing Madison, WI, 2017.\n[17] Andr\u00e1s Sasfi, Melanie N Zeilinger, and Johannes K\u00f6hler.\nRobust adaptive mpc using control contraction metrics.\nAutomatica, 155:111169, 2023.\n[18] Seth Siriya, Jingge Zhu, Dragan Ne\u0161i\u0107, and Ye Pu. Non-\nasymptotic bounds for closed-loop identification of unstable\nnonlinear stochastic systems.\n[19] Seth Siriya, Jingge Zhu, Dragan Ne\u0161i\u0107, and Ye Pu. Learning-\nbased adaptive control for stochastic linear systems with\ninput constraints.IEEE Control Systems Letters, 2022.\n[20] Seth Siriya, Jingge Zhu, Dragan Ne\u0161i\u0107, and Ye Pu. Stability\nbounds for learning-based adaptive control of discrete-\ntime multi-dimensional stochastic linear systems with input\nconstraints. In2023 62nd IEEE Conference on Decision and\nControl (CDC), pages 3802\u20133807. IEEE, 2023.\n[21] Chen Wei and Lei Guo. Prediction-based discrete-time\nadaptive nonlinear stochastic control.IEEE Transactions on\nAutomatic Control, 44(9):1725\u20131729, 1999.\n[22] LL Xie and Lei Guo. Adaptive control of a class of discrete-\ntime affine nonlinear systems.Systems & control letters,\n35(3):201\u2013206, 1998.\n22\n",
    "title": "A Framework for Adaptive Stabilisation of Nonlinear Stochastic Systems",
    "arxiv_id": "2511.17436v1",
    "category": "recent",
    "authors": [
      "Seth Siriya",
      "Jingge Zhu",
      "Dragan Ne\u0161i\u0107",
      "Ye Pu"
    ],
    "published": "2025-11-21",
    "num_pages": 22,
    "num_chars": 84469
  },
  {
    "text": "Multi-Agent Pointer Transformer: Seq-to-Seq Reinforcement Learning for\nMulti-Vehicle Dynamic Pickup-Delivery Problems\nZengyu Zou1, 4, Jingyuan Wang1, 2, 3, 4, *, Yixuan Huang1, 4, Junjie Wu2, 3\n1School of Computer Science and Engineering, Beihang University, Beijing, China\n2School of Economics and Management, Beihang University, Beijing, China\n3MIIT Key Laboratory of Data Intelligence and Management, Beihang University, Beijing, China\n4MOE Engineering Research Center of Advanced Computer Application Technology, Beihang University, China\n{zzy0001213, jywang, yixuanhuang, wujj}@buaa.edu.cn\nAbstract\nThis paper addresses the cooperative Multi-Vehicle Dy-\nnamic Pickup and Delivery Problem with Stochastic Re-\nquests (MVDPDPSR) and proposes an end-to-end centralized\ndecision-making framework based on sequence-to-sequence,\nnamed Multi-Agent Pointer Transformer (MAPT). MVD-\nPDPSR is an extension of the vehicle routing problem and a\nspatio-temporal system optimization problem, widely applied\nin scenarios such as on-demand delivery. Classical operations\nresearch methods face bottlenecks in computational complex-\nity and time efficiency when handling large-scale dynamic\nproblems. Although existing reinforcement learning meth-\nods have achieved some progress, they still encounter several\nchallenges: 1) Independent decoding across multiple vehicles\nfails to model joint action distributions; 2) The feature extrac-\ntion network struggles to capture inter-entity relationships;\n3) The joint action space is exponentially large. To address\nthese issues, we designed the MAPT framework, which em-\nploys a Transformer Encoder to extract entity representations,\ncombines a Transformer Decoder with a Pointer Network to\ngenerate joint action sequences in an AutoRegressive manner,\nand introduces a Relation-Aware Attention module to capture\ninter-entity relationships. Additionally, we guide the model\u2019s\ndecision-making using informative priors to facilitate effec-\ntive exploration. Experiments on 8 datasets demonstrate that\nMAPT significantly outperforms existing baseline methods\nin terms of performance and exhibits substantial computa-\ntional time advantages compared to classical operations re-\nsearch methods.\nCode\u2014 https://github.com/wszzyer/MAPT\n1 Introduction\nThe Pickup and Delivery Problem is a type of vehicle rout-\ning problem that has demonstrated its importance in many\nreal-world applications, such as online food delivery. In real-\nworld scenarios, the arrival time, origin, and destination of\nrequests are often unpredictable, and there are strict require-\nments for service response times. Therefore, efficiently plan-\nning vehicle routes based on real-time updated informa-\ntion is particularly important in solving this problem. This\n*Corresponding author\nCopyright \u00a9 2026, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.work focuses on solving the cooperative Multi-Vehicle Dy-\nnamic Pickup and Delivery Problem with Stochastic Re-\nquests (MVDPDPSR). Unlike the traditional Pickup and De-\nlivery Problem, in this problem, we need to route multiple\nvehicles, and the occurrence times of all requests are un-\nknown in advance. Moreover, the origin and destination of\neach request are only revealed after the request emerges.\nExisting methods for addressing dynamic vehicle rout-\ning problems fall into three categories:heuristic meth-\nods,classical operations research methods, andreinforce-\nment learning-based algorithms.Heuristic methodshave\nshown some success (Sheridan et al. 2013; Fikar 2018; An-\ndersson 2021) by routing vehicles through manually de-\nsigned heuristic rules, such as the nearest-distance rule.\nTheir strengths are simplicity and efficiency, fitting real-\ntime decision scenarios, but they have clear limits as they\ndepend heavily on manually designed rules and struggle\nto ensure optimality.Classical operations research meth-\nodstypically adopt the Rolling-Horizon paradigm, which\ntransforms dynamic problems into static ones. These static\nsubproblems are then solved using either exact algorithms\n(Lu and Dessouky 2004; Liu et al. 2018; Savelsbergh and\nSol 1998) or metaheuristics (Schilde, Doerner, and Hartl\n2011; Geiser, Hanne, and Dornberger 2020; Cai et al.\n2022). This paradigm requires accumulating a sufficient\nnumber of requests before initiating planning, which fails\nto meet the needs of real-time dynamic decision-making.\nAlso, both exact and metaheuristic algorithms have high\ncomputational complexity, greatly affecting their time effi-\nciency in practice. With technology development,reinforce-\nment learning-based algorithmshave been used for multi-\nvehicle routing. Multi-agent reinforcement learning meth-\nods have been proposed for the static multi-vehicle pickup\nand delivery problem (Zong et al. 2022; Berto et al. 2024).\nThey encode the current state with an Encoder, decode inde-\npendently for each vehicle with a Decoder, and use a conflict\nhandler in post-processing to solve vehicle conflicts.\nHowever, these multi-agent methods still face the follow-\ning issues: (1) Existing multi-agent reinforcement learning\nmethods decode each agent independently, failing to model\nthe joint probability distribution of agents\u2019 actions. Further-\nmore, the decisions generated by these methods may con-\nflict among agents (e.g., competing for the same request).\n(2) Existing feature extraction networks fail to capture rela-arXiv:2511.17435v1  [cs.LG]  21 Nov 2025\ntionships between entities (e.g., stations, vehicles, requests),\nwhich are key components of the MVDPDPSR. (3) The joint\naction space is often exponentially large, which prevents re-\ninforcement learning from exploring useful actions.\nTo tackle these remaining issues, we propose the Multi-\nAgent Pointer Transformer (MAPT), an end-to-end cen-\ntralized decision-making framework based on sequence-to-\nsequence to solve MVDPDPSR. Specifically, we model the\nMVDPDPSR problem as a Markov Decision Process, use a\nTransformer Encoder to extract entity representations, and\nthen combine a Transformer Decoder with a Pointer Net-\nwork to convert the embedding sequence into a joint action\nsequence in an AutoRegressive manner. To extract relation-\nships between entities, we propose a Relation-Aware Atten-\ntion module that embeds entity relationships and adds them\nto the attention matrix. To address the challenge of reinforce-\nment learning exploring a vast joint action space, we design\ninformative priors based on load balancing and station dis-\ntance, and fuse these priors with the action distribution de-\ncoded by the Decoder, enabling the model to select actions\nwith higher potential rewards.\nIn summary, the main contributions of our work are as\nfollows:\n\u2022 We formalize the MVDPDPSR problem as a Markov De-\ncision Process and design the Multi-Agent Pointer Trans-\nformer (MAPT) framework to decode joint actions and\nmodel the joint probability.\n\u2022 We design a Relation-Aware Attention module to capture\nvarious relationships between entities.\n\u2022 We design informative priors and integrate them into the\nmodel\u2019s decision distribution to facilitate effective explo-\nration.\n\u2022 We validate the effectiveness of MAPT on 8 datasets, and\nexperiments show that MAPT outperforms various base-\nlines and demonstrates significant computational time ad-\nvantages over classical operations research methods.\n2 Preliminaries\n2.1 Problem Formulation\nIn this section, we formally define the Multi-Vehicle Dy-\nnamic Pickup and Delivery Problem with Stochastic Re-\nquests(MVDPDPSR). A typical MVDPDPSR scenario in-\nvolves a fleet of vehicles (Multi-Vehicle) tasked with picking\nup and delivering cargos among a set of stations. The pickup\nand delivery requests arise dynamically over time, reflecting\nthe \u201cDynamic\u201d nature of the problem. Accordingly, stations,\nvehicles, and requests constitute the three fundamental com-\nponents of a MVDPDPSR. We provide their formal defini-\ntions below.\nDefinition 1(Stations).The stations in MVDPDPSR are\nrepresented as a fully connected weighted graphG=\n{N,E}. Here,N={n 1, . . . , n i, . . . , n I}denotes the set of\nstations, wheren irepresents thei-th station.E\u2208R|N|\u00d7|N|\nis the distance matrix, where each elemente ni,njindicates\nthe travel time from stationn ito stationn j. In the station\ngraphG, each station can serve as either an origin (pickup\nstation) or a destination (delivery station) for cargos.Definition 2(Vehicles).A MVDPDPSR scenario involves\nKvehicles, where each vehiclev kis represented by a tuple\nof four state variables:\nvk=\u27e8capk, spak, tok, dist k\u27e9.(1)\nThe meanings of these variables are as follows:\n\u2022capk(capacity): the total capacity of vehiclev k.\n\u2022spat\nk(space): the remaining available space of vehiclev k\nat timet. For notational brevity, we omit the superscript\ntime indextwhen unambiguous (e.g.spa k).\n\u2022tok(destination): the current destination station of the ve-\nhicle.\n\u2022dist k(distance): the remaining distance (travel time) to\nreach the current destination.\nIfdist k\u0338= 0, the vehicle is en route; otherwise, it is located\nat a station. Notably, the destination of a vehicle cannot be\nchanged while it is en route.\nDefinition 3(Requests).The objective of MVDPDPSR is\nto fulfillMcargo delivery requests, where each requestr m\nis represented by a tuple of six state variables:\nrm=\u27e8fromm, tom, val m, vol m, time m, state m\u27e9.\n(2)\nThe meanings of these variables are as follows:\n\u2022fromm(origin): the origin station of requestr m.\n\u2022tom(destination): the destination station of requestr m.\n\u2022val m(value): the profit associated with fulfilling the re-\nquest.\n\u2022vol m(volume): the cargo volume required for the request,\nwhich consumes vehicle space.\n\u2022time m(appearance time): the time at which the request\nbecomes visible to the system.\n\u2022state m(state): the current state of requestr m.\nA request becomes visible only when the current time\nreaches its appearance timetime m. The state vari-\nablestate m\u2208 {unassigned,picked,delivered}indicates\nwhether the request has not been picked up, has been picked\nup by a vehicle, or has been delivered.\nIn the MVDPDPSR scenario, the planning horizon is dis-\ncretized into time slicest= 0, . . . , T. At the initial time\nslicet= 0, all vehicles are located at their initial origin sta-\ntions, and their available space equals their full capacity. At\neach time slice, a scheduler performs the following actions:\n1. For every visible request in theunassignedstate at time\nt(i.e.time m\u2264t), assign a vehicle that is currently at\nthe same station and has sufficient space (spak\u2265vol m)\nto load the request\u2019s cargo. Update the state of the re-\nquest topickedand reduce the vehicle\u2019s available space\naccordingly:spak:=spak\u2212vol m.\n2. For each vehicle at a station (dist k= 0), select its next\ndestination station and dispatch it. Set the vehicle\u2019s travel\ndistancedist kto the distance between the current station\nand the selected destination.\n3. For every vehicle in transit (dist k\u0338= 0), decrease its re-\nmaining travel distance by one unit:dist k:=dist k\u22121.\n4. For each vehicle arriving at its destination (dist k= 0),\nunload all cargos whose destination matches the cur-\nrent station. Update the state of each delivered request\ntodeliveredand restore the vehicle\u2019s available space:\nspak:=spak+vol mfor each delivered request.\nIn this scenario, cargo delivery requests emerge dynamically\nover time, characterizing MVDPDPSR as adynamicpickup-\nand-delivery problem.\nA MVDPDPSR instance terminates when the planning\nhorizon reaches the upper limitT. The goal of scheduling\nis to maximize the total profit of requests completed within\nthe time limit while minimizing the aggregate travel cost.\n2.2 Markov Decision Process (MDP)\nMVDPDPSR is a sequential decision-making problem over\ntime, and we model its decision process as a Markov deci-\nsion process, defined as follows:\nDefinition 4(Observation/State).Since we have a central-\nized decision system, the system can fully observe all state\ninformation at the current time step. The state at time stept\nincludes the distance between stationse ni,nj, all vehicle in-\nformationv k, and all appeared requestsr mwheretime m\u2264\nt. Notably, within a centralized decision-making system, the\nrelationshiprel vk,rm\u2208 {unassigned,picked,delivered}\nbetween each vehiclev kand requestr mat timetmust be\nexplicitly tracked. This relationship indicates the request has\nnot been picked up, has been picked up by the vehicle, or has\nbeen delivered by the vehicle.\nDefinition 5(Action).The decision includes decisions for\nrequests and decisions for vehicles.\n\u2022 Request decisions. For the set ofunassignedrequests\nRt={r m|state m= unassigned\u2227time m\u2264t}, we\nneed to decide which vehicle to assign them to. The as-\nsigned vehicle\u2019s current location must be the same as the\nrequest\u2019s origin, i.e.,\nAt\nrm\u2208 {v k|dist k= 0\u2227to k=from m} \u222a {\u03c4},(3)\nwhere\u03c4denotes the action of temporarily deferring the\nrequest assignment. The assignment must ensure that the\nvehicle\u2019s capacity meets the requirements.\n\u2022 Vehicle decisions. For the set of vehicles that have reached\ntheir destinationsVt={v k|dist k= 0}, we need to\ndecide their next destination station, i.e.,\nAt\nvk\u2208 {n 1, . . . , n I}.(4)\nThe final joint action space is the Cartesian product of all\nsub-action spaces:\nAt=Y\u0000\b\nAt\nrm|rm\u2208 Rt\t\n\u222a\b\nAt\nvk|vk\u2208 Vt\t\u0001\n(5)\nDefinition 6(Transition).For vehiclesv k\u2208 Vt, we need\nto update their destination stationto kand remaining time to\nreach the new destination stationdist kbased onAt\nvk, and\nupdate their remaining space based on unloaded and newly\nloaded goods. For vehiclesv k/\u2208 Vt, the remaining time to\nreach their destination stations decreases by one unit. For\nall requests assigned to vehicles, theirstate mandrel vk,rm\nchanges topicked, and for all requests that have reached\ntheir destinations, theirstate mandrel vk,rmchanges to\ndelivered.Definition 7(Objective/Reward).The objective is to opti-\nmize the overall routing solution quality by maximizing the\nprofit from completed requests while accounting for vehicle\ntravel costs. Formally, we define the objective at timeTas:\nobjT=X\nt<TX\nk\uf8eb\n\uf8edX\nm\u2208Dt\nkvalm\u2212cost\u00b7etot\nk,tot+1\nk\uf8f6\n\uf8f8,(6)\nwhereDt\nkdenotes the set of requests delivered by vehiclev k\nby timet, andcostrepresents the cost per unit distance. The\nsingle-step reward at each decision point is then defined as\nthe incremental change in the objective value:\nrwdt=obj t\u2212obj t\u22121.(7)\n3 Methodology\n3.1 System Overview\nWe propose the Multi-Agent Pointer Transformer (MAPT),\nwhich is built on the Transformer (Vaswani et al. 2017)\nEncoder-Decoder architecture. At each time step of the\nMDP, the current state is fed into MAPT for decision-\nmaking. The Encoder processes the states of all entities and\nincorporates a Relation-Aware Attention module to capture\ninter-entity relationships. The Decoder generates actions for\neach entity in an AutoRegressive manner. To enhance ex-\nploration during reinforcement learning, we integrate infor-\nmative priors into the action selection process. The overall\narchitecture is illustrated in Figure 1.\n3.2 Encoder\nFeature Representation with Dense VectorsFirst, we\ntransform the raw input into dense vectors for unified rep-\nresentation. We compute some augmented inputs to enhance\nthe model\u2019s perception capabilities, namelyori ianddest i,\nwhich represent the number of requests originating from sta-\ntionn i, and the number of requests destined for stationn i,\nrespectively. Next, we concatenate the raw inputs of stations,\nrequests, and vehicles and pass them through a linear layer\nto transform them into dense vectors:\ndni= [ori i\u2225dest i]Wn,d ni\u2208Rhs(8)\ndvk= [cap k\u2225spa k\u2225dist k]Wv,d vk\u2208Rhs(9)\ndrm= [val m\u2225vol m]Wr,d rm\u2208Rhs(10)\nwhere\u2225denotes the concatenation operation for scalars, and\nhsis the model\u2019s hidden size. Additionally, we transform\nglobal information into a dense vector, wherem trepresents\nthe current number of requests, as follows:\ndg= [t\u2225m t]Wg,d g\u2208Rhs.(11)\nRelation-Aware AttentionAlthough there are some neu-\nral network models for solving the Vehicle Routing Prob-\nlem, these models cannot effectively capture the relation-\nships between entities. To overcome these limitations, this\nstudy designs a Relation-Aware Attention module aimed at\nincorporating relationship information between entities into\nthe embeddings. For any two entities (stations, requests,\nDecode  Stage Informative Prior Environment Encode  StageTransformer EncoderRelation-A ware AttentionKQ\nVRelation\nEmbedding\nSample\nAction\n        vehicles\nrequestsstations\n from\ntounassigned\npickeddestination\ndistdist\n distcalculate Informative Prior\nDot-Product\nSoftmax\nEmbeddingsAuto-\nRegressive\nTransformer\nDecoderInformative\nPrior\nDecoder Prob\nFused ProbInput seqseq'Markov Decision Process\nBased Planning\nplanning planning planning\nObservation/State at each timeFigure 1: The framework of MAPT. The blue arrow indicates that the elements in the sequence are generated in an AutoRe-\ngressive manner. The elements marked with * are the actions that need to be decoded.\nRelation Vehicle Request Station\nVehicle distanceunassigned\npicked\ndeliveredis destination\nisn\u2019t destination\nRequestunassigned\npicked\ndelivereddistancefrom\nto\nStationis destination\nisn\u2019t destinationfrom\ntodistance\nTable 1: Relation Types between Vehicle, Request and Sta-\ntion.\nvehicles), there are some relationships between them, as\nshown in Table 1. We design a relation encoding network\nRelation-Embedding(u, v)\u2208R. If the relationship between\nentitiesuandvis a distance relationship, we use a linear\nlayer to project it; for other relationships, we use a learnable\nparameter as their embedding. We denote the relationship\nmatrix between entities asR\u2208R|Q|\u00d7|K|, which is calcu-\nlated as follows:\nRu,v=Relation-Embedding(q u, kv),(12)\nwhereq urepresents theu-th entity in Query sequence (Q),\nandk vrepresents thev-th entity in Key sequence (K). Fol-\nlowing the definition of the standard Transformer (Vaswani\net al. 2017), we represent the Query sequence asQ, the Key\nsequence asK, the Value sequence asVand the head di-\nmension asd k. The Relation-Aware Attention is expressed\nas:\nRel-Aware-Attn(Q,K,V,R) = softmax\u0012QKT+R\u221adk\u0013\nV.\n(13)\nWe incorporate relationship information into the attention\nmechanism, enabling the model to capture relationships be-\ntween entities when generating entity embeddings.\nEncode with Relation-Aware AttentionWe use a stan-\ndard Transformer Encoder and replace its attention layerwith Relation-Aware Attention. We concatenate the dense\nvectors of all stations, requests, vehicles, and global infor-\nmation into a sequence and input them into the Encoder to\nobtain their respective embeddings:\n[or1, . . . ,o rM,ov1, . . . ,o vK,on1, . . . ,o nI,og] =\nEncoder([d r1, . . . ,d rM,dv1, . . . ,d vK,dn1, . . . ,d nI,dg]).\n(14)\n3.3 Decoder\nDecoder ModelWe use a standard Transformer Decoder\nand replace its attention layer with Relation-Aware At-\ntention. The cross-attention layer needs to perform cross-\nattention on the output of the Encoder to perceive all in-\nformation about the current problem. Additionally, we add\nlearnable positional encoding before the Decoder.\nAutoRegressive DecodingWe model the joint probabil-\nity distributionAtusing chain rule decomposition, which is\norder-agnostic:\nP(At) =MY\nmP(At\nrm|At\nr1...rm\u22121)\n\u00d7KY\nkP(At\nvk|At\nv1...vk\u22121, At\nr1...rM).(15)\nFrom this formula, we can see that each request needs to\nrefer to the decision results of previous requests when mak-\ning decisions; each vehicle needs to refer to the decision re-\nsults of previous vehicles and all requests when making de-\ncisions. Due to this sequential decision-making process, we\nadopt an AutoRegressive approach to generate the decision\nsequence and model it in a sequence-to-sequence frame-\nwork, as also observed by Wen et al. (2022). We combine the\nTransformer\u2019s ability to capture long-distance dependencies\nwith the Pointer Network\u2019s (Vinyals, Fortunato, and Jaitly\n2015) ability to handle entity selection, using the Trans-\nformer Decoder to decode actions.\nWe detail our decoding process, for notational conve-\nnience, we omit the superscript time indextwhen unam-\nbiguous. Let our sequence beseq, initially empty. When de-\ncoding the action for them-th requestr m, we first append it\ninto the sequence, which then becomes:\nseq = [r 1, Ar1, . . . , r m].(16)\nWe first convert entities in the sequence to their correspond-\ning embeddings to obtainseq\u2032(e.g., converting requestr mto\nits embeddingo rmwhich is output of the Transformer En-\ncoder). TheDecoder(seq\u2032)produces a hidden sequence of\nthe same length asseq\u2032. We take the last item of the hidden\nsequence,h rm, as the query vector and use the embeddings\nof all vehicles as the key vectors. By calculating the attention\nscores between them, we can quantify the request\u2019s prefer-\nence for each vehicle:\nPdec(Arm=vk) = softmax k(h\u22a4\nrm[ov1, . . . ,o vK]),(17)\nwheresoftmax k(\u00b7)denotes thek-th element of the softmax\noutput. We determineA rmby sampling fromPdec(Arm)or\nselecting the maximum probability vehicle. This decoding\nresult is then placed at the end ofseq, providing critical ref-\nerence information for subsequent decisions:\nseq = [r 1, Ar1, . . . , r m, Arm].(18)\nThis process can be repeated to decode all requests. We de-\ncode all vehicles in the same manner:\nPdec(Avk=ni) = softmax i(h\u22a4\nvk[on1, . . . ,o nI]).(19)\nAfter all decoding is completed, the elements in the se-\nquence are:\nseq = [r 1, Ar1, . . . , v 1, Av1, . . . , v K, AvK].(20)\n3.4 Informative Priors\nThe joint action space in MVDPDPSR is often exponen-\ntially large because of multiple vehicles, requests, and sta-\ntions, which prevents reinforcement learning from explor-\ning useful actions. To address this, we designed manually-\ncomputed informative priors for vehicle selection and desti-\nnation selection. By multiplying the probabilities output by\nthe Decoder with informative priors, we guide the agent to\nprioritize actions with higher potential rewards.\nInformative Priors for Vehicle SelectionIn a multi-\nvehicle scenario, the balance of vehicle load is crucial for\noverall delivery efficiency and resource utilization. Based on\nthis, we design a load-balancing informative prior, which\nis inversely proportional to the current vehicle load. The\nsmaller the current load of a vehicle, the higher its infor-\nmative prior weight in vehicle selection, thereby guiding the\nassignment of requests toward load balancing:\nPpri(Arm=vk) =spak\ncapk.(21)\nAlthough temporarily deferring request assignments may\nyield positive benefits (e.g., when the current vehicle load is\nhigh), we aim to avoid excessive occurrences of such cases,\nas this would severely affect the exploration efficiency dur-\ning reinforcement learning. Therefore, we set a small prob-\nability for action\u03c4(defined in Eq. (3)):\nPpri(Arm=\u03c4) =\u03b2.(22)\nThe vehicle selection probability after informative priors\u2019\nguidance is:\nP(A rm=vk) =Pdec(Arm=vk)\u00b7Ppri(Arm=vk).(23)Informative Priors for Destination SelectionFor the\ndestination station selection of vehiclev k, the informative\npriorPpri(Avk=ni)is determined as follows:\n\u2022 If stationn ihas requests to be delivered,Ppri(Avk=\nni) = 1.\n\u2022 If stationn ihas requests to be picked up,Ppri(Avk=\nni) =0.1\u00d7E\netok,ni, where Edenotes the average value of all\nelements in the distance matrixE. This formulation pri-\noritizes stations that are closer in distance. Since the num-\nber of stations with pickup requests is significantly larger\nthan those with delivery requests, we use a smaller co-\nefficient (0.1) to balance the vehicle\u2019s priorities between\npickup and delivery tasks.\n\u2022 For stationsn ithat do not fall into the above two cate-\ngories,Ppri(Avk=ni) = 0.\nThe destination selection probability after informative pri-\nors\u2019 guidance is:\nP(A vk=ni) =Pdec(Avk=ni)\u00b7Ppri(Avk=ni).(24)\n3.5 Optimization via PPO\nWe use Proximal Policy Optimization (Schulman et al.\n2017) to train MAPT. Our value function is defined as\nV(st) =MLP(o g), where MLP transforms global informa-\ntion into a scalar. We use Generalized Advantage Estimation\n(Schulman et al. 2015) to balance the bias and variance of\nadvantage estimation:\n\u02c6AGAE\nt =\u221eX\nl=0(\u03b3\u03bb)l(rwd t+l+\u03b3V(s t+l+1 )\u2212V(s t+l)).\n(25)\nThe Actor loss is defined as\n\u03c1t(\u03b8) =\u03c0\u03b8(At|st)\n\u03c0\u03b8old(At|st),(26)\nLCLIP(\u03b8) =E th\nmin\u0000\n\u03c1t(\u03b8)\u02c6AGAE\nt,\nCLIP(\u03c1 t(\u03b8),1\u2212\u03f5,1 +\u03f5) \u02c6AGAE\nt\u0001i\n,(27)\nwhere\u03c0 \u03b8(At|st)is the joint probability defined in Eq. (15).\nThe Critic loss is defined as\nLCritic= (V(s t)\u2212(r t+1+\u03b3V(s t+1)))2.(28)\nThe final total loss is\nL=LCLIP+\u03b1LCritic.(29)\n4 Experiments\n4.1 Experimental Scenarios\nTo validate the effectiveness of the MAPT framework, we\nconducted experiments on 8 datasets, including both syn-\nthetic and real-world datasets, for comprehensive evaluation.\nA brief description of the datasets is provided below.\n\u2022Synthetic Dataset: We generated synthetic datasets with\ndifferent scales of stationsI\u2208 {20,50,300}, correspond-\ning to dataset suffixes-S,-L, and-XL, respectively. For\nthese datasets, the number of requests and vehicles were\nScenario synth-S synth-S-cost synth-L synth-L-cost dhrd-tpe dhrd-sg dhrd-se synth-XL\nMetric Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191\nOR Tools 182.1 0.54 117.4 0.52 N/A N/A N/A N/A N/A N/A N/A N/A 55.3 0.45 N/A N/A\nSA 162.5 0.52 108.1 0.52 641.1 0.28 339.3 0.29 244.9 0.31 136.7 0.30 50.4 0.41 1531.9 0.31\nGA 108.7 0.30 58.3 0.29 329.4 0.14 89.5 0.12 64.1 0.08 40.9 0.13 21.3 0.17 863.8 0.17\nSA*133.0 0.42 81.2 0.42 563.1 0.25 284.8 0.24 223.3 0.28 139.6 0.30 56.6 0.46 1650.9 0.34\nGA*75.2 0.21 39.4 0.19 219.9 0.09 88.0 0.09 53.8 0.07 39.0 0.13 18.4 0.15 543.2 0.11\nMAPDP*174.9 0.53 82.0 0.49 177.6 0.08 65.2 0.08 287.5 0.36 247.2 0.35 72.9 0.56 807.0 0.16\nPARCO*178.8 0.53 86.6 0.52 177.6 0.08 65.2 0.08 391.2 0.49 267.5 0.46 64.2 0.49 1834.7 0.36\nNearest 189.2 0.57 124.1 0.57 962.6 0.41 592.3 0.41 309.2 0.39 194.5 0.40 39.8 0.32 2641.1 0.53\nMAPDP 157.5 0.41 70.0 0.42 958.1 0.37 385.2 0.37 135.4 0.17 64.9 0.18 19.3 0.16 2543.1 0.50\nMAPT 275.2 0.83 192.1 0.84 1875.1 0.80 1348.6 0.81 697.2 0.87 376.1 0.71 78.9 0.64 3227.5 0.65\nTable 2: Overall performance comparison on synthetic datasets and real-world datasets. The best result in each column is\nbolded. \u201dN/A\u201d indicates that the algorithm cannot provide results within an acceptable time frame.\nScenario synth-S synth-S-cost synth-L synth-L-cost dhrd-tpe dhrd-sg dhrd-se synth-XL\nMetric Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191\nMAPT 275.2 0.83 192.1 0.84 1875.1 0.80 1348.6 0.81 697.2 0.87 376.1 0.71 78.9 0.64 3227.5 0.65\nw/o Rel 270.9 0.82 190.7 0.83 1869.9 0.80 1338.4 0.81 692.1 0.87 365.3 0.69 77.1 0.63 3136.8 0.63\nw/o AR 217.2 0.65 171.1 0.77 1604.0 0.68 1074.5 0.69 611.4 0.76 258.1 0.50 49.4 0.40 1905.8 0.38\nw/o Priors 175.2 0.54 92.7 0.54 1089.5 0.47 529.1 0.48 511.6 0.64 250.9 0.49 56.6 0.46 1375.0 0.27\nTable 3: Results of ablation studies. The best result in each column is bolded.\nset as(M, K) = (110,5),(550,15), and(550,50), re-\nspectively. Vehicle capacity was fixed at 3. Travel cost per\nunit distance is set to 0 or 0.3, with the latter case marked\nby an additional suffix-cost. Requests were uniformly\nsampled from stations, with appearance times sampled\nfromUniform(1, T), whereTwas set to 58 or 128. The\nprofit for each request was the distance between its origin\nand destination.\n\u2022Real-World Dataset: We used the DHRD dataset (As-\nsylbekov et al. 2023), which contains food delivery re-\nquests from three cities: Taipei (tpe), Singapore (sg),\nand Stockholm (se). Each city was treated as an inde-\npendent dataset withI= 36stations. The correspond-\ning numbers of requests and vehicles were(M, K) =\n(800,20),(700,15), and(200,3)for the three cities, re-\nspectively. Vehicle capacity was set to 6. The dataset was\ndivided into 76 days for training/validation and 14 days\nfor testing.\n4.2 Performance Evaluation\nBaselinesWe evaluate our MAPT against several base-\nlines, including Rolling-Horizon algorithms, static algo-\nrithms, rule-based algorithms, and MDP-based algorithms.\n\u2022Rolling-Horizon Algorithms:The Rolling-Horizon\nparadigm addresses dynamic problems by decomposing\nthem into static subproblems within a sliding time win-\ndow. For solving these static subproblems, we employ\nOR Tools,Simulated Annealing (SA), andGenetic\nAlgorithm (GA).\u2022Static Algorithms:These methods assume complete fore-\nknowledge of all requests (though unrealistic), solving the\nproblem statically without Rolling-Horizon.MAPDP*\n(Zong et al. 2022) is a MARL approach with Encoder-\nDecoder for static problems, adapted to handle dis-\ntance matrix inputs.PARCO*(Berto et al. 2024) is a\nTransformer-based RL framework with parallel decision-\nmaking, modified to support distance matrix inputs.\nSA*/GA*are static versions of our Rolling-Horizon SA\nand GA with full request information.\n\u2022Rule-Based Algorithms:These methods use predefined\nrules for scheduling decisions.Nearestis a greedy ap-\nproach that always selects the closest available request for\npickup or delivery.\n\u2022MDP-Based Algorithms:At each step of the MDP, we\nsolve the problem statically usingMAPDP(Zong et al.\n2022) as described in the Static Algorithms section, and\nonly execute the first step of the solution to ensure appli-\ncability to dynamic problems.\nOverall ComparisonThe performance results on syn-\nthetic and real-world datasets are summarized in Table 2, us-\ning the objective (Obj.) and request completion rate (Comp.)\nas metrics (higher is better). MAPT significantly outper-\nforms all baselines across all datasets. MAPT also surpasses\nthe state-of-the-art MAPDP, which fails to model multi-\nvehicle joint probabilities and lacks our informative priors.\nAs can be seen from Table 4, the decision time of MAPT\nis significantly shorter than that of exact and meta-heuristic\nalgorithms.\nScenario synth-S synth-S-cost synth-L synth-L-cost synth-XL\nOR Tools 401.727 459.143 N/A N/A N/A\nSA 23.684 23.255 166.660 164.102 613.920\nGA 16.654 16.448 103.147 99.864 354.789\nNearest 0.059 0.060 0.117 0.119 0.148\nMAPDP 0.794 0.827 31.981 32.544 33.217\nMAPT 1.420 1.416 8.300 7.888 27.434\nTable 4: Comparison of inference times on the synthetic\ndataset (in seconds), which is the average running time of\neach instance in the dataset.\n4.3 Ablation Studies\nWe conducted ablation studies on all datasets to validate the\neffectiveness of key components:\n\u2022w/o Relation-Aware Attention (Rel): Replaces the\nRelation-Aware Attention module with the original atten-\ntion mechanism.\n\u2022w/o AutoRegressive Decoding (AR): Decodes each ac-\ntion independently and employs a conflict handler to re-\nsolve conflicts where request assignments exceed vehicle\ncapacity.\n\u2022w/o Informative Priors (Priors): Removes informative\npriors guidance for vehicle and destination selection.\nThe results are shown in Table 3. As can be seen from\nthe table, AutoRegressive Decoding and Informative Priors\nsignificantly improve performance across all datasets, and\nRelation-Aware Attention also contributes to some extent.\nThe ablation experiments demonstrate that all three pro-\nposed components enhance the final performance, validating\ntheir effectiveness.\n4.4 Sensitivity Analysis\nWe conduct a sensitivity analysis on the hyperparameter\u03b2as\ndefined in Eq. (22). We vary\u03b2from 0 to 0.3 in increments of\n0.02 and observe the resulting changes in the final objective.\nThe results on the 4 representative datasets are illustrated\nin Figure 2. As shown in the figure, when\u03b2is small, the\nperformance is better and more stable, while larger values of\n\u03b2lead to a gradual decline in performance. However, in the\ndhrd-tpe and dhrd-se datasets, using a small but non-zero\u03b2\nimproves performance. This indicates that slightly deferring\nrequest allocation can be beneficial in these cases.\n5 Related Work\n5.1 Dynamic Pickup and Delivery Problems\nDynamic Pickup and Delivery Problems have been studied\nthrough exact, heuristic, metaheuristic, and learning-based\nmethods (Cai et al. 2023). Exact methods such as MILP\ncan yield optimal results but are restricted to small instances\n(Liu et al. 2018; Savelsbergh and Sol 1998). Heuristic meth-\nods (Fikar 2018; Fikar, Hirsch, and Gronalt 2018; Anders-\nson 2021) are efficient for large-scale dynamic problems but\ncannot ensure optimality. Metaheuristics (Schilde, Doerner,\nand Hartl 2011; Tirado and Hvattum 2017; Novaes et al.\n0.0 0.1 0.2\n\u03b2010002000Objective\n(a) synthetic-L\n0.0 0.1 0.2\n\u03b2020004000Objective\n (b) synthetic-XL\n0.0 0.1 0.2\n\u03b2550600650700Objective\n(c) dhrd-tpe\n0.0 0.1 0.2\n\u03b2707580Objective\n (d) dhrd-se\nFigure 2: Sensitivity analysis of\u03b2across datasets.\n2015) improve scalability but still struggle in highly dy-\nnamic settings. Reinforcement learning methods have been\nexplored (Li et al. 2021; Yu et al. 2025), but they mainly\nfocus on single-vehicle operations. Hybrid approaches first\nlearn edge costs using neural networks and then apply tra-\nditional optimization techniques (Jiang et al. 2023b; Wang,\nWu, and Zhao 2021; Wang et al. 2019; Wang, Lin, and Li\n2025; Wu et al. 2019). Other studies have addressed ride-\non-demand problems with dynamic pricing (Guo et al. 2019,\n2020, 2023), as well as dynamic metro scheduling (Wang\net al. 2022). There are also entity representation learning\nmethods that use GNNs, hyper-GNNs, contrastive learning,\ntransfer learning, or LLM enhancement and can be applied\nto spatio-temporal system optimization (Wu et al. 2020;\nYang et al. 2025; Han et al. 2025; Jiang et al. 2023a; Li et al.\n2025; Cheng et al. 2025; Zhang et al. 2024).\n5.2 RL-based Methods for Vehicle Routing\nProblems\nReinforcement Learning has been applied to both single and\nmultiple Vehicle Routing Problems. For single-vehicle prob-\nlems, attention-based models (Vinyals, Fortunato, and Jaitly\n2015; Bello et al. 2016; Nazari et al. 2018; Kool, Van Hoof,\nand Welling 2018) and GNN-based variants (Fellek et al.\n2023; Heydaribeni et al. 2023) have been proposed. For\nmulti-vehicle problems, multi-agent RL methods (Zhang\net al. 2020; Zong et al. 2022; Zhang, Qi, and Guan 2023)\nhave been developed but mainly for static settings. Recent\nRL studies for dynamic settings (Anuar et al. 2022; Xiang\net al. 2024) have emerged, but they simplify the original\nproblem by introducing additional assumptions.\n6 Conclusion\nThis paper presents MAPT to solve the cooperative Multi-\nVehicle Dynamic Pickup and Delivery Problem with\nStochastic Requests. MAPT utilizes the Transformer\u2019s\nEncoder-Decoder architecture combining with Pointer Net-\nwork to generate joint action sequences in an AutoRegres-\nsive manner. By incorporating the Relation-Aware Attention\nmodule and informative priors, the framework achieves a\nbetter performance. Experiments show that MAPT outper-\nforms existing baselines in solution quality across synthetic\ndatasets and real-world datasets.\n7 Acknowledgments\nThis work is supported by the National Key R&D Pro-\ngram of China (2023YFC3304700). Prof. Jingyuan Wang\u2019s\nwork was partially supported by the National Natural Sci-\nence Foundation of China (No. 72222022, 72171013) and\nthe Fundamental Research Funds for the Central Uni-\nversities (JKF-2025017226182). Prof. Junjie Wu\u2019s work\nwas partially supported by the National Natural Science\nFoundation of China (72242101, 72031001), the Out-\nstanding Young Scientist Program of Beijing Universities\n(JWZQ20240201002), and the Artificial Intelligence Tech-\nnology R&D Center for Exploration and Development of\nChina National Petroleum Corporation (2024-KFKT-22).\nReferences\nAndersson, T. 2021. A Comparative Study on a Dynamic\nPickup and Delivery Problem: Improving routing and order\nassignment in same-day courier operations.\nAnuar, W. K.; Lee, L. S.; Seow, H.-V .; and Pickl, S. 2022. A\nmulti-depot dynamic vehicle routing problem with stochas-\ntic road capacity: An MDP model and dynamic policy for\npost-decision state rollout algorithm in reinforcement learn-\ning.Mathematics, 10(15): 2699.\nAssylbekov, Y .; Bali, R.; Bovard, L.; and Klaue, C. 2023.\nDelivery Hero Recommendation Dataset: A Novel Dataset\nfor Benchmarking Recommendation Algorithms. InPro-\nceedings of the 17th ACM Conference on Recommender Sys-\ntems, 1042\u20131044.\nBello, I.; Pham, H.; Le, Q. V .; Norouzi, M.; and Bengio,\nS. 2016. Neural combinatorial optimization with reinforce-\nment learning.arXiv preprint arXiv:1611.09940.\nBerto, F.; Hua, C.; Luttmann, L.; Son, J.; Park, J.; Ahn, K.;\nKwon, C.; Xie, L.; and Park, J. 2024. PARCO: Learning Par-\nallel Autoregressive Policies for Efficient Multi-Agent Com-\nbinatorial Optimization.arXiv preprint arXiv:2409.03811.\nCai, J.; Zhu, Q.; Lin, Q.; Li, J.; Chen, J.; and Ming, Z.\n2022. An efficient multi-objective evolutionary algorithm\nfor a practical dynamic pickup and delivery problem. In\nInternational conference on intelligent computing, 27\u201340.\nSpringer.\nCai, J.; Zhu, Q.; Lin, Q.; Ma, L.; Li, J.; and Ming, Z. 2023.\nA survey of dynamic pickup and delivery problems.Neuro-\ncomputing, 126631.\nCheng, J.; Wang, J.; Zhang, Y .; Ji, J.; Zhu, Y .; Zhang, Z.;\nand Zhao, X. 2025. Poi-enhancer: An llm-based seman-\ntic enhancement framework for poi representation learning.\nInProceedings of the AAAI conference on artificial intelli-\ngence, volume 39, 11509\u201311517.\nFellek, G.; Farid, A.; Gebreyesus, G.; Fujimura, S.; and\nYoshie, O. 2023. Deep Graph Representation Learning to\nSolve Vehicle Routing Problem. In2023 International Con-\nference on Machine Learning and Cybernetics (ICMLC),\n172\u2013180. IEEE.\nFikar, C. 2018. A decision support system to investigate\nfood losses in e-grocery deliveries.Computers & Industrial\nEngineering, 117: 282\u2013290.Fikar, C.; Hirsch, P.; and Gronalt, M. 2018. A decision sup-\nport system to investigate dynamic last-mile distribution fa-\ncilitating cargo-bikes.International Journal of Logistics Re-\nsearch and Applications, 21(3): 300\u2013317.\nGeiser, T.; Hanne, T.; and Dornberger, R. 2020. Best-match\nin a set of single-vehicle dynamic pickup and delivery prob-\nlem using ant colony optimization. InProceedings of the\n2020 the 3rd International Conference on Computers in\nManagement and Business, 126\u2013131.\nGuo, S.; Chen, C.; Wang, J.; Ding, Y .; Liu, Y .; Xu, K.; Yu,\nZ.; and Zhang, D. 2020. A force-directed approach to seek-\ning route recommendation in ride-on-demand service us-\ning multi-source urban data.IEEE Transactions on Mobile\nComputing, 21(6): 1909\u20131926.\nGuo, S.; Chen, C.; Wang, J.; Liu, Y .; Xu, K.; Yu, Z.; Zhang,\nD.; and Chiu, D. M. 2019. Rod-revenue: Seeking strategies\nanalysis and revenue prediction in ride-on-demand service\nusing multi-source urban data.IEEE Transactions on Mobile\nComputing, 19(9): 2202\u20132220.\nGuo, S.; Shen, Q.; Liu, Z.; Chen, C.; Chen, C.; Wang, J.;\nLi, Z.; and Xu, K. 2023. Seeking based on dynamic prices:\nHigher earnings and better strategies in ride-on-demand ser-\nvices.IEEE Transactions on Intelligent Transportation Sys-\ntems, 24(5): 5527\u20135542.\nHan, C.; Wang, J.; Wang, Y .; Yu, X.; Lin, H.; Li, C.; and Wu,\nJ. 2025. Bridging traffic state and trajectory for dynamic\nroad network and trajectory representation learning. InPro-\nceedings of the AAAI Conference on Artificial Intelligence,\nvolume 39, 11763\u201311771.\nHeydaribeni, N.; Zhan, X.; Zhang, R.; Eliassi-Rad, T.; and\nKoushanfar, F. 2023. HypOp: Distributed Constrained Com-\nbinatorial Optimization leveraging Hypergraph Neural Net-\nworks.arXiv preprint arXiv:2311.09375.\nJiang, J.; Pan, D.; Ren, H.; Jiang, X.; Li, C.; and Wang, J.\n2023a. Self-supervised trajectory representation learning\nwith temporal regularities and travel semantics. In2023\nIEEE 39th international conference on data engineering\n(ICDE), 843\u2013855. IEEE.\nJiang, W.; Zhao, W. X.; Wang, J.; and Jiang, J. 2023b. Con-\ntinuous trajectory generation based on two-stage GAN. In\nProceedings of the AAAI conference on artificial intelli-\ngence, volume 37, 4374\u20134382.\nKool, W.; Van Hoof, H.; and Welling, M. 2018. At-\ntention, learn to solve routing problems!arXiv preprint\narXiv:1803.08475.\nLi, X.; Luo, W.; Yuan, M.; Wang, J.; Lu, J.; Wang, J.; L \u00a8u, J.;\nand Zeng, J. 2021. Learning to optimize industry-scale dy-\nnamic pickup and delivery problems. In2021 IEEE 37th in-\nternational conference on data engineering (ICDE), 2511\u2013\n2522. IEEE.\nLi, Y .; Wang, J.; Yu, X.; Wang, P.; and Huang, Q. 2025.\nCross City Traffic Flow Generation via Retrieval Aug-\nmented Diffusion Model. InThe Thirty-ninth Annual Con-\nference on Neural Information Processing Systems.\nLiu, S.; Tan, P. H.; Kurniawan, E.; Zhang, P.; and Sun, S.\n2018. Dynamic scheduling for pickup and delivery with time\nwindows. In2018 IEEE 4th World Forum on Internet of\nThings (WF-IoT), 767\u2013770. IEEE.\nLu, Q.; and Dessouky, M. 2004. An exact algorithm for the\nmultiple vehicle pickup and delivery problem.Transporta-\ntion Science, 38(4): 503\u2013514.\nNazari, M.; Oroojlooy, A.; Snyder, L.; and Takac, M. 2018.\nReinforcement Learning for Solving the Vehicle Routing\nProblem. In Bengio, S.; Wallach, H.; Larochelle, H.; Grau-\nman, K.; Cesa-Bianchi, N.; and Garnett, R., eds.,Advances\nin Neural Information Processing Systems, volume 31. Cur-\nran Associates, Inc.\nNovaes, A. G.; Bez, E. T.; Burin, P. J.; and Arag \u02dcao Jr, D. P.\n2015. Dynamic milk-run OEM operations in over-congested\ntraffic conditions.Computers & Industrial Engineering, 88:\n326\u2013340.\nSavelsbergh, M.; and Sol, M. 1998. Drive: Dynamic routing\nof independent vehicles.Operations Research, 46(4): 474\u2013\n490.\nSchilde, M.; Doerner, K. F.; and Hartl, R. F. 2011. Meta-\nheuristics for the dynamic stochastic dial-a-ride problem\nwith expected return transports.Computers & operations\nresearch, 38(12): 1719\u20131730.\nSchulman, J.; Moritz, P.; Levine, S.; Jordan, M.; and\nAbbeel, P. 2015. High-dimensional continuous control\nusing generalized advantage estimation.arXiv preprint\narXiv:1506.02438.\nSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and\nKlimov, O. 2017. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347.\nSheridan, P. K.; Gluck, E.; Guan, Q.; Pickles, T.; Balc\u0131og,\nB.; Benhabib, B.; et al. 2013. The dynamic nearest neighbor\npolicy for the multi-vehicle pick-up and delivery problem.\nTransportation Research Part A: Policy and Practice, 49:\n178\u2013194.\nTirado, G.; and Hvattum, L. M. 2017. Improved solutions to\ndynamic and stochastic maritime pick-up and delivery prob-\nlems using local search.Annals of Operations Research,\n253: 825\u2013843.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017.\nAttention is All you Need. In Guyon, I.; Luxburg, U. V .;\nBengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and\nGarnett, R., eds.,Advances in Neural Information Process-\ning Systems, volume 30. Curran Associates, Inc.\nVinyals, O.; Fortunato, M.; and Jaitly, N. 2015. Pointer net-\nworks.Advances in neural information processing systems,\n28.\nWang, J.; Lin, Y .; and Li, Y . 2025. GTG: Generalizable\nTrajectory Generation Model for Urban Mobility. InPro-\nceedings of the AAAI Conference on Artificial Intelligence,\nvolume 39, 834\u2013842.\nWang, J.; Wu, N.; and Zhao, W. X. 2021. Personalized route\nrecommendation with neural network enhanced search al-\ngorithm.IEEE Transactions on Knowledge and Data Engi-\nneering, 34(12): 5910\u20135924.Wang, J.; Wu, N.; Zhao, W. X.; Peng, F.; and Lin, X. 2019.\nEmpowering A* search algorithms with neural networks for\npersonalized route recommendation. InProceedings of the\n25th ACM SIGKDD international conference on knowledge\ndiscovery & data mining, 539\u2013547.\nWang, Z.; Pan, Z.; Chen, S.; Ji, S.; Yi, X.; Zhang, J.; Wang,\nJ.; Gong, Z.; Li, T.; and Zheng, Y . 2022. Shortening pas-\nsengers\u2019 travel time: A dynamic metro train scheduling ap-\nproach using deep reinforcement learning.IEEE Transac-\ntions on Knowledge and Data Engineering, 35(5): 5282\u2013\n5295.\nWen, M.; Kuba, J.; Lin, R.; Zhang, W.; Wen, Y .; Wang, J.;\nand Yang, Y . 2022. Multi-Agent Reinforcement Learning is\na Sequence Modeling Problem. In Koyejo, S.; Mohamed,\nS.; Agarwal, A.; Belgrave, D.; Cho, K.; and Oh, A., eds.,\nAdvances in Neural Information Processing Systems, vol-\nume 35, 16509\u201316521. Curran Associates, Inc.\nWu, N.; Wang, J.; Zhao, W. X.; and Jin, Y . 2019. Learn-\ning to effectively estimate the travel time for fastest route\nrecommendation. InProceedings of the 28th ACM Interna-\ntional Conference on Information and Knowledge Manage-\nment, 1923\u20131932.\nWu, N.; Zhao, X. W.; Wang, J.; and Pan, D. 2020. Learn-\ning effective road network representation with hierarchical\ngraph neural networks. InProceedings of the 26th ACM\nSIGKDD international conference on knowledge discovery\n& data mining, 6\u201314.\nXiang, C.; Wu, Z.; Tu, J.; and Huang, J. 2024. Centralized\nDeep Reinforcement Learning Method for Dynamic Multi-\nVehicle Pickup and Delivery Problem With Crowdshippers.\nIEEE Transactions on Intelligent Transportation Systems.\nYang, Y .; Wang, J.; Yu, X.; and Tang, Y . 2025. HygMap:\nRepresenting All Types of Map Entities via Heterogeneous\nHypergraph. InProceedings of the Thirty-Fourth Inter-\nnational Joint Conference on Artificial Intelligence, 9438\u2013\n9446.\nYu, X.; Wang, J.; Yang, Y .; Huang, Q.; and Qu, K. 2025.\nBIGCity: A universal spatiotemporal model for unified tra-\njectory and traffic state data analysis. In2025 IEEE 41st In-\nternational Conference on Data Engineering (ICDE), 4455\u2013\n4469. IEEE.\nZhang, K.; He, F.; Zhang, Z.; Lin, X.; and Li, M. 2020.\nMulti-vehicle routing problems with soft time windows: A\nmulti-agent reinforcement learning approach.Transporta-\ntion Research Part C: Emerging Technologies, 121: 102861.\nZhang, W.; Wang, J.; Yang, Y .; et al. 2024. VecCity:\nA taxonomy-guided library for map entity representation\nlearning.arXiv preprint arXiv:2411.00874.\nZhang, Z.; Qi, G.; and Guan, W. 2023. Coordinated multi-\nagent hierarchical deep reinforcement learning to solve\nmulti-trip vehicle routing problems with soft time windows.\nIET Intelligent Transport Systems, 17(10): 2034\u20132051.\nZong, Z.; Zheng, M.; Li, Y .; and Jin, D. 2022. Mapdp: Co-\noperative multi-agent reinforcement learning to solve pickup\nand delivery problems. InProceedings of the AAAI confer-\nence on artificial intelligence, volume 36, 9980\u20139988.\nA Example Problem Instance\nFigure 3 presents a representative problem instance in our\nstudy. This instance consists of 2 pickup stations and 4 de-\nlivery stations, where each request is associated with one\npickup station and one delivery station. It also includes 2\nvehicles (yellow and blue) and 5 requests \u2014 each request\nis identified by a pair of pickup (p n) and delivery (d n) sta-\ntions (withn= 1,2,3,4,5), and has timestamps like9:00,\n9:10, etc. The yellow vehicle follows a sequential route:\np1\u2192p 2\u2192d 1\u2192d 2\u2192p 3\u2192d 3\u2014 as visualized by the\norange arrows connecting these stations in the figure. It first\nhandles early requestsp 1(pickup at9:00) andp 2(pickup\nat9:10), then completes their deliveries (d 1at9:20,d 2at\n9:30). Whenp 3(pickup at9:40) dynamically arrives later,\nthe yellow vehicle is scheduled tore-route back to the top-\nleft pickup stationto fulfill this new request \u2014 highlighting\nhow our problem requires adapting todynamically arriv-\ning requests. Meanwhile, the blue vehicle executes another\nsequence:p 4\u2192d 4\u2192p 5\u2192d 5\u2014 as shown by the blue\narrows. Here,p 4(pickup at9:00) andd 4(delivery at9:10)\nare processed first, followed byp 5(pickup at9:20) andd 5\n(delivery at9:30).\nWhen the blue vehicle reaches the upper-middle pickup\nstation, Request 4 (e.g.,p 4, the pickup for Request 4) is as-\nsigned to it, while Request 2 (e.g.,p 2, the pickup for Request\n2) is deferred and then assigned to the yellow vehicle upon\nits subsequent arrival \u2014 demonstrating the core challenge of\nvehicle-request assignment: we need to decide which vehi-\ncle each request should be assigned to.\nB Difference between Rolling Horizon and\nMarkov Decision Process\nRolling Horizon is often used in classical operations re-\nsearch methods. It transforms dynamic problems into static\nsubproblems by accumulating a sufficient number of re-\nquests over a time window before planning begins (top of\nFig. 4). During this accumulation already-arrived requests\nstay unprocessed causing unavoidable time waste. These\nstatic subproblems are then solved with exact algorithms\nor metaheuristics but their high computational complexity\nfurther hinders time efficiency. In contrast Markov Deci-\nsion Process (MDP) triggers planning at each time step (bot-\ntom of Fig. 4) and makes decisions right as requests arrive.\nThe former fails to meet real-time dynamic decision-making\nneeds due to its batch-based approach while the latter pri-\noritizes real-time adaptation through frequent smaller-scale\nplanning.\nC Dataset Details\nIn the MVDPDPSR problem addressed in this paper, the\nproblem scale is not solely determined by the number of sta-\ntions but also depends on the number of requests, as we need\nto determine the assignment relationship between requests\nand vehicles. In the largest dataset, the number of requests\nreaches 800, which already exceeds the problem scale of\nmost prior studies on static pickup-and-delivery problems,\nsuch as (Zong et al. 2022). Below, we provide details of the\np1 9:00\np3 9:40\np5 9:20p2 9:10\np4 9:00 d1 9:20\nd2 9:30 d3 9:50\nd5 9:30\nd4 9:10\nVehicle\nDelivery Station\nPickup Station\np1\u2192p2\u2192d1\u2192d2\u2192p3\u2192d3\np4\u2192d4\u2192p5\u2192d5p1pickup of request 1\nd1delivery of request 1Figure 3: Schematic illustration of the MVDPDPSR prob-\nlem.\ncumulative\nrequests\nplanning planningcumulative\nrequests\nplanning planning planning planning planning planning\nRolling\nHorizon\nMarkov\nDecision\nProcess\nFigure 4: Comparison between Rolling Horizon and Markov\nDecision Process paradigms. The Rolling Horizon paradigm\nrequires accumulating a sufficient number of requests be-\nfore invoking a static solver for vehicle routing optimization,\nwhile the Markov Decision Process paradigm enables real-\ntime decision-making as requests arrive.\nsynthetic datasets and the real-world datasets. Their statis-\ntics are summarized in Table 5.\n\u2022Synthetic Dataset: We first synthesize some datasets to\nvalidate the effectiveness of the algorithm. We gen-\neratedI\u2208 {20,50,300}stations, withI= 20,\n50, and300corresponding to dataset suffixes-S,-L,\nand-XL, respectively. The distance between stations is\nsampled fromUniform{0,1, . . . ,10}for-Sdatasets,\nfromUniform{0,1, . . . ,30}for-Ldatasets and from\nUniform{0,1, . . . ,20}for-XLdatasets. Then, the short-\nest path algorithm is applied. The capacity of each vehi-\ncle was set to 3, and the number of vehicles wasK\u2208\n{5,15,50}. Travel cost per unit distance is set to 0 or 0.3,\nwith the latter case marked by an additional suffix-cost.\nThe origin and destination of each request were uniformly\nsampled from theIstations, and the request\u2019s appearance\ntime was sampled fromUniform{1,2, . . . , T}, whereTis\nset to 58 for-Sdatasets and 128 for-Land-XLdatasets.\nThe profit for each request was the distance between its\norigin and destination.\n\u2022Real-World Dataset: We used the DHRD(Assylbekov\net al. 2023) dataset, which consists of millions of food\ndelivery requests from three cities: Taipei (tpe), Singa-\npore (sg), and Stockholm (se). We treated each city as\nan independent dataset. In this dataset, we treated each\ngeographical area as a station, with areas divided by 5-\ncharacter geohash granularity. After excluding stations\nwith almost no requests, each dataset hadI= 36stations.\nThe distance between two areas was defined as the min-\nimum number of areas that needed to be traversed. The\ncapacity of each vehicle was set to 6, and the number of\nvehicles for the three cities was set toK\u2208 {20,15,3}. In\nthis dataset, we did not consider thecostof vehicle travel,\nonly the total profit of completed requests. The origin and\ndestination of each request were the stations correspond-\ning to their respective geographical areas. We divided a\nday intoT= 48time slots, corresponding toTtime steps\nin the simulation environment. The time step of a request\u2019s\nappearance was determined by its real-time slot. The profit\nfor each request is set to 1. We divided the dataset into 76\ndays for training and validation, and 14 days for testing.\nD Computational Complexity Analysis\nThe computational efficiency of MAPT is crucial for its\npractical application in dynamic vehicle routing scenar-\nios, where real-time decision-making is essential. Our ar-\nchitecture maintains the fundamental structure of Trans-\nformer models while introducing minimal additional over-\nhead through the Relation-Aware Attention and Informa-\ntive Prior modules. The encoder stage processes all enti-\nties (stations, requests, and vehicles) as a single sequence.\nFor each encoder block with hidden dimensionH, the com-\nplexity is dominated by the self-attention mechanism and\nfeed-forward network operations. The self-attention com-\nputation scales asO((I+M+K)2H)whereI,M, and\nKrepresent the number of stations, requests, and vehicles\nrespectively, while the feed-forward network contributes\nO((I+M+K)H2)per layer.\nDuring decoding, we employ the KV-Cache technique\ncommonly used in large language models to optimize au-\ntoregressive generation. This approach caches previously\ncomputed key-value pairs, significantly reducing redun-\ndant computations when processing sequential decisions.\nEach decoder block involves three main components: self-\nattention over the partially constructed solution (complexity\nO((M+K)2H)), cross-attention between the decoder in-\nputs and encoder outputs (O((M+K)(I+M+K)H)),\nand the feed-forward network operations (O((M+K)H2)).\nSo the overall computational complexity of MAPT is\nO(L[(I+M+K)2H+ (I+M+K)H2]), whereLrep-\nresents the number of transformer layers. While this demon-\nstrates quadratic scaling with respect to the number of enti-\nties, the parallel processing capabilities of modern GPUs en-\nable efficient computation even for large problem instances.\nThis efficiency is particularly important for our target appli-\ncation, where the system must respond to dynamic requestsScenio Stations Reqs. Veh. T Cost Profit\nsynthetic-S 20 110 5 58 0 distance\nsynthetic-S-cost 20 110 5 58 0.3 distance\nsynthetic-L 50 550 15 128 0 distance\nsynthetic-L-cost 50 550 15 128 0.3 distance\nsynthetic-XL 300 550 50 128 0 distance\ndhrd-tpe 36 800 20 48 0 1\ndhrd-sg 36 700 15 48 0 1\ndhrd-se 36 200 3 48 0 1\nTable 5: Statistics of experimental datasets.costis the cost\nper unit distance;profitis the profit per request (either\ndistance-based or fixed).\nin real-time while maintaining solution quality.\nE Training Algorithm\nThe main part of our training algorithm follows the original\nProximal Policy Optimization (PPO) algorithm (Schulman\net al. 2017), and we have incorporated some learning rate\nschedule strategies. We summarize our training process in\nAlgorithm 1.\nF Implementation Details\nFor the model architecture, we employ a Transformer with\n6 encoder layers and 2 decoder layers. The hidden size is\nset to 128, and the number of attention heads is 2. Regard-\ning the optimizer configuration, we utilize the Adam opti-\nmizer with a learning rate schedule that includes a warm-up\nphase and a decay phase. Specifically, the learning rate in-\ncreases linearly during the first quarter of the total training\nsteps, reaching the target learning rate, and subsequently un-\ndergoes linear decay for the remaining steps. For the Prox-\nimal Policy Optimization (PPO) algorithm, we set the dis-\ncount factor\u03b3= 0.99, the clipping coefficient\u03f5= 0.2,\nand the Generalized Advantage Estimation (GAE) parame-\nter\u03bb= 0.99. Critic loss coefficient\u03b1is set to 1. The number\nof PPO epochs is fixed at 1. The hyperparameter\u03b2is set to\n0.03. During training, we monitor the model\u2019s performance\non the validation set and retain the best-performing model\nas the final model. Additional dataset-specific hyperparame-\nters are provided in Table 6. All experiments are conducted\nusing Python 3.10 and PyTorch 2.5.\nG Difference between Non-AutoRegressive\nand AutoRegressive Decoding\nWe use a simple VRP example to illustrate the difference\nbetween the Non-AutoRegressive method with independent\ndecoding and our AutoRegressive method in terms of decod-\ning, as shown in Figure 5. Suppose two vehicles,v 1andv 2,\nare at the same location and need to collaboratively visit sta-\ntionsn 1. . . n 4. The optimal solution is achieved when both\nvehicles move left and right respectively, while a suboptimal\nsolution is obtained when they move up and down respec-\ntively. Let the actions of the two vehicles beA v1andA v2,\n1 2\n4 31 2\n4 31 2\n4 3\nNon-AutoRegressive Decodingsample action\nfrom \nand handcraft Conflict\nHandler\n(Non-dif ferentiable)\n1 2\n4 31 2\n4 31 2\n4 3\nsample action\nfrom \n (masked) sample\naction fromWorse Solution\nBetter Solution\nAutoRegressive DecodingFigure 5: Comparison between Non-AutoRegressive Decoding(up) and AutoRegressive Decoding(down).\nDataset Total Rollout MiniBatch LR\nsynth-S 16K 256 256 1e-4\nsynth-S-cost 16K 256 256 1e-4\nsynth-L 4K 64 64 2e-5\nsynth-L-cost 4K 64 64 5e-5\nsynth-XL 4K 64 64 2e-5\ndhrd-se 8K 256 256 2e-5\ndhrd-tpe 2K 64 64 2e-5\ndhrd-sg 2K 64 64 1e-5\nTable 6: Dataset Specific Hyper-parameters.Totalindicates\nthe overall training budget.Rolloutspecifies the experi-\nence collection per iteration.Minibatchdetermines the SGD\nbatch size.LRcontrols the policy update step size.\nrepresenting the next station they move to at the first step,\nand\u03c0represent a scheduling policy. However, for the Non-\nAutoRegressive independent decoding method, it can only\nmodel the probability distribution of each vehicle\u2019s individ-\nual actions\u03c0(A v1|v1, v2)and\u03c0(A v2|v1, v2), but fails\nto model the joint probability distribution of the two vehi-\ncles\u2019 actions\u03c0(A v1, Av2|v1, v2). Consequently, it cannot\nguarantee that both vehicles move left and right respectively.\nMoreover, since a conflict handler is applied afterward, its\ngradient cannot be backpropagated, making it impossible to\noptimize the neural network parameters using conflict in-\nformation when conflicts occur, which limits the capability\nof such methods. For AutoRegressive decoding, we first de-\ncode the actionA v1ofv1according to\u03c0(A v1=n 1. . . n 4|\nv1, v2), then useA v1as an additional condition to decode the\nactionA v2ofv2, i.e.,\u03c0(A v2=n 1. . . n 4|v1, v2, Av1). At\nthis point, the policy can determine the direction ofv 2basedon whetherA v1is left or right, thus achieving the optimal\nsolution. Additionally, we can mask outA v1when decoding\nAv2to avoid conflicts.\nH Does MAPT Overly Rely on Informative\nPrior?\nTo verify whether MAPT excessively relies on the Infor-\nmative Prior, we compare the experimental results between\nMAPT and using only the Informative Prior as the decision-\nmaking algorithm, as shown in Table 7. In synthetic datasets,\nproblem instances are randomly generated, making it diffi-\ncult to form complex and realistic distributions. As a result,\nthe generated problem instances tend to be simpler, lead-\ning to cases where the difference between the Informative\nPrior and MAPT is small. However, for real-world datasets,\nour model (MAPT) shows significant improvements over\nthe Informative Prior, with enhancement rates of 14% for\ndhrd-tpe, 16% for dhrd-sg, and 19% for dhrd-se. In Figure\n6, we show the distribution of request origins and destina-\ntions in the dhrd-tpe dataset, which differs significantly from\nthe Uniform distribution of synthetic data. Reinforcement\nlearning can learn more complex distributions based on the\nInformative Prior, which is why its improvement on real-\nworld datasets is more significant than on synthetic datasets.\nMoreover, even for the large-scale synthetic-XL dataset, our\nmodel achieves a notable 37% improvement over the Infor-\nmative Prior. These results demonstrate that our model does\nnot overly rely on Informative Priors.\nI Generalization\nSince our decision-making does not strictly depend on the\nnumber of vehicles and requests, we expect our model to\nScenario synth-S synth-S-cost synth-L synth-L-cost dhrd-tpe dhrd-sg dhrd-se synth-XL\nMetric Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191\nInfor Prior 267.1 0.81 185.0 0.81 1858.9 0.79 1305.5 0.79 614.1 0.77 323.7 0.62 66.1 0.54 2340.7 0.47\nMAPT 275.2 0.83 192.1 0.84 1875.1 0.80 1348.6 0.81 697.2 0.87 376.1 0.71 78.9 0.64 3227.5 0.65\nTable 7: Performance comparison between Informative Prior and MAPT.\nAlgorithm 1: PPO Training Algorithm for MAPT\n1:Initialize:\n2:Policy network\u03c0 \u03b8with parameters\u03b8.\n3:Replay bufferB.\n4:EnvironmentE, episode lengthT.\n5:Learning rate scheduler with warmup and linear de-\ncay.\n6:Rollout iterationsK, rollout threadsN.\n7:foriterationk= 1,2, . . . , Kdo\n8:forthreadn= 1,2, . . . , Ndo\n9:Reset environmentEand initialize observationss 0.\n10:forstept= 0,1,2, . . . , T\u22121do\n11:Collect Data:\n12:Sample actionsa tfrom policy\u03c0 \u03b8(at|st).\n13:Execute actionsa tin environmentE.\n14:Observe next states t+1and rewardr t.\n15:Store transition(s t, at, rt, st+1)in bufferB.\n16:end for\n17:end for\n18:Compute Advantages:\n19:Compute advantages using Generalized Advantage\nEstimation (25).\n20:forepoche= 1,2, . . . , Edo\n21:Optimize Surrogate Objective:\n22:Sample mini-batch of transitions from bufferB.\n23:Compute critic loss by (28) and policy loss by\n(27).\n24:Update policy\u03c0 \u03b8using gradient descent on joint\nloss (29).\n25:end for\n26:Update Old Policy:\n27:\u03b8 old\u2190\u03b8\n28:update learning rate through scheduler.\n29:end for\nhave some generalization ability. To this end, we conducted\na series of experiments:\n\u2022synthetic-S\u2192synthetic-XLrepresents the validation of\nwhether a model trained on small-scale data can be used\non large-scale problems.\n\u2022synthetic-S\u2192dhrd-serepresents the validation of\nwhether a model trained on data from one distribution can\ngeneralize to other distributions.\nThe generalization results are shown in Table 8. The exper-\niments show that models trained on small synthetic dataset\ncan generalize to real-world dataset and large-scale dataset\nto some extent. This is due to two reasons: first, the use of\nTransformer, a model structure with good generalization ca-\n(a)\n (b)\nFigure 6: Heatmap of the distribution of request origins(a)\nand destinations(b) in the dhrd-tpe dataset, where darker col-\nors indicate higher quantities.\nScenario synth-S dhrd-se synth-XL\nMetric Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191 Obj.\u2191Comp.\u2191\nsynth-S 275.2 0.83 74.4 0.61 2470.2 0.50\ndhrd-se 117.4 0.36 78.9 0.64 1480.0 0.30\nsynth-XL 109.8 0.34 52.6 0.42 3227.5 0.65\nTable 8: Generalization of MAPT. The row index indicates\nthat the model is trained on the corresponding dataset, and\nthe column index indicates that the model is evaluated on the\ncorresponding dataset.\npabilities, and second, the inclusion of informative priors in\nthe model, which provides guidance in unseen distributions.\nIt is worth noting that when tested on real-world datasets\nand large-scale datasets, our method still outperforms the\npure Informative Priors approach (see Tables 7 and 8), prov-\ning that our deep model has also learned some transferable\nknowledge, rather than relying solely on the Informative Pri-\nors for generalization. Although the model trained on the\ndhrd-se dataset performed best on this dataset, it performed\npoorly when generalized to synthetic datasets. We speculate\nthat this is because the request OD distribution and station\nembeddings learned from the dhrd-se dataset are not suitable\nfor synthetic datasets. However, there is still much to explore\nin the field of generalization as part of our future research.\nJ Baselines Implementation\nRolling HorizonThe Rolling Horizon Policy addresses\nthe dynamic nature of the Multi-Vehicle Dynamic Pickup\nand Delivery Problem by breaking it into smaller, static sub-\nproblems within a sliding time window, enabling the use\nof traditional optimization methods. At each decision point,\nthe policy determines a planning horizon based on the re-\nmaining time frames and the maximum allowable planning\nhorizon, ensuring computational tractability. The maximum\nDataset Horizon Replan\nsynth-S 20 10\nsynth-S-cost 20 10\nsynth-L 60 30\nsynth-L-cost 60 30\nsynth-XL 40 20\ndhrd-se 30 15\ndhrd-tpe 30 15\ndhrd-sg 30 15\nTable 9: Rolling Horizon Hyper-parameters.Horizonin-\ndicates the maximum allowable planning horizon.Replan\nspecifies the re-planning interval.\nallowable planning horizon used for each dataset is shown\nin Table 9, columnHorizon. The problem within this win-\ndow is formulated as a static instance, where vehicles, re-\nquests, and constraints (e.g., vehicle capacity, request ap-\npearance times, and preloaded requests) are fixed for the du-\nration of the window. A solver, chosen from a set of avail-\nable methods (e.g.,OR-Tools,Genetic Algorithm(GA), or\nSimulated Annealing(SA)), is then applied to optimize the\nassignment of requests to vehicles and their routes within\nthe window. The solver\u2019s solution is translated into action-\nable decisions, such as assigning requests to vehicles and\ndetermining the next hop for each vehicle, while ensuring\nfeasibility with respect to constraints like vehicle capacity\nand request uniqueness. The policy periodically re-evaluates\nand re-plans at predefined intervals to adapt to new informa-\ntion and changing conditions. The re-planning interval used\nfor each dataset is shown in Table 9, columnReplan. By it-\neratively solving these static subproblems and updating the\nsolution as the horizon shifts, the Rolling Horizon Policy ef-\nfectively handles the dynamic and stochastic nature of the\nproblem, balancing computational efficiency with solution\nquality.\nOR-ToolsWe address the Multi-Vehicle Dynamic Pickup\nand Delivery Problem with Stochastic Requests using the\nGoogle OR-Tools CP-SAT solver. The problem involves a\nfleet of vehiclesK, a set of requestsM, and a set of stations\nNover a time horizonT. Each vehiclek\u2208Khas a start-\ning locationstart k, a capacitycapacity k, and a join time\njointime kindicating when it becomes available for the first\ndecision-making. Requestsm\u2208Mappear at timeappear m\nand must be picked up fromfrom mand delivered toto m.\nSome requests may be pre-loaded on vehicles, denoted by\npreload k,m, and vehicles are en route until their join time,\nduring which they cannot perform any actions. For the con-\nvenience of modeling, we assume that the volume of each\nrequest is 1.\nDecision Variables:\n\u2022pickup k,m,t : Binary variable indicating whether vehiclek\npicks up requestmat timet.\n\u2022delivery k,m,t : Binary variable indicating whether vehicle\nkdelivers requestmat timet.\n\u2022load k,t: Integer variable representing the load of vehiclekat timet.\n\u2022loc k,t: Integer variable representing the location of vehicle\nkat timet.\n\u2022noact k,t: (No Action) Binary variable indicating whether\nvehiclekis not performing any action at timet, e.g. the\nvehicle is en route.\nConstraints:\n\u2022Pre-Loaded Requests: For pre-loaded requests, the\npickup action is enforced att=\u22121:\npickup k,m,\u22121 =preload k,m\u2200k\u2208K, m\u2208M(30)\nload k,\u22121=X\nmpreload k,m\u2200k\u2208K(31)\n\u2022Load Dynamics: The load of each vehicle is updated\nbased on pickup and delivery actions:\nload k,t=load k,t\u22121 +X\nm\u2208Mpickup k,m,t\u2212\nX\nm\u2208Mdelivery k,m,t\u2200k\u2208K, t\u2208[0, T](32)\n\u2022Location Dynamics: The location of each vehicle is up-\ndated based on pickup, delivery, and no-action variables:\nlock,t=\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3start k ift\u2264jointime k\nlock,t\u22121 ifnoact k,t\u22121 = 1\ntom ifdelivery k,m,t = 1\nfrom mifpickup k,m,t = 1\n\u2200k\u2208K, t\u2208[0, T], m\u2208M(33)\n\u2022No-Action Constraints: If no pickup or delivery occurs\nat timet, the no-action variable is set to 1:\nnoact k,t= 1\u21d0\u21d2\nX\nm\u2208Mpickup k,m,t +X\nm\u2208Mdelivery k,m,t = 0\n\u2200k\u2208K, t\u2208[0, T](34)\n\u2022Request Assignment: Each request must be picked up\nand delivered by at most one vehicle:\nX\nk\u2208KX\nt\u2208[\u22121,T]pickup k,m,t\u22641\u2200m\u2208M(35)\nX\nk\u2208KX\nt\u2208[\u22121,T]delivery k,m,t\u22641\u2200m\u2208M(36)\n\u2022Pickup-Delivery Sequence: A request must be picked up\nbefore it can be delivered:\nX\nt\u2032\u2208[\u22121,t]pickup k,m,t\u2032= 1 =\u21d2delivery k,m,t = 1\n\u2200k\u2208K, m\u2208M, t\u2208[\u22121, T](37)\n\u2022Capacity Constraints: The load of each vehicle must not\nexceed its capacity:\nload k,t\u2264capacity k\u2200k\u2208K, t\u2208[0, T](38)\n\u2022Time-Distance Constraints: The time between consecu-\ntive actions must be sufficient to travel between locations:\nt1\u2212t0\u2265dist lock,t0,lock,t1\n\u2200k\u2208K, t 0, t1\u2208[0, T], t 0< t1(39)\n\u2022Join Time Constraints: Vehicles cannot perform any ac-\ntions before their join time:\nnoact k,t= 1\u2200k\u2208K, t\u2208[0, jointime k\u22121](40)\nObjective Function: The objective is to maximize the total\nprofit of delivered requests while minimizing travel costs:\nMaximizeX\nm\u2208MX\nk\u2208KX\nt\u2208[0,T]delivery k,m,t\u00b7value m\n\u2212X\nk\u2208KX\nt\u2208[0,T\u22121]costlock,t,lock,t+1(41)\nSimulated AnnealingThe Simulated Annealing (SA) al-\ngorithm for solving the Multi-Vehicle Dynamic Pickup and\nDelivery Problem with Stochastic Requests starts by gener-\nating an initial solution through random request assignments\nto vehicles, ensuring constraints such as initial positions,\ndisplacement, location consistency, request uniqueness, ca-\npacity, request appearance times, and vehicle start times are\nsatisfied. To construct a neighbor solution, the algorithm ran-\ndomly selects a request and removes its current assignment\nfrom the vehicle\u2019s path. Then the selected request is reas-\nsigned to a new vehicle, and both pickup and delivery times\nare randomly selected within their respective feasible win-\ndows, ensuring the pickup occurs after the request\u2019s appear-\nance time and the delivery occurs after the pickup plus the\ntravel time. The annealing process iteratively explores the\nsolution space by accepting better solutions or probabilisti-\ncally accepting worse solutions based on the current temper-\nature, which gradually decreases according to a predefined\ncooling rate. The initial temperature is set to 1000, the final\ntemperature is set to 1, the cooling rate is set to 0.99, and the\nmaximum number of iterations is set to 5000.\nGenetic AlgorithmThe Genetic Algorithm (GA) for\nsolving the Multi-Vehicle Dynamic Pickup and Delivery\nProblem with Stochastic Requests begins by generating an\ninitial population of feasible solutions, where requests are\nrandomly assigned to vehicles while ensuring constraints\nsuch as initial positions, displacement, location consistency,\nrequest uniqueness, capacity, request appearance times, and\nvehicle start times are satisfied. Each individual in the pop-\nulation represents a solution, and its fitness is evaluated us-\ning an exponential transformation of the objective function,\nwhich combines the total profit of served requests and the\ncost of vehicle movements. The algorithm evolves the pop-\nulation through selection, crossover, and mutation opera-\ntions. During selection, individuals are chosen probabilis-\ntically based on their fitness, favoring better solutions. The\ncrossover operation randomly selects a request and swaps\nits assignment between two parent solutions, ensuring fea-\nsibility by updating vehicle locations and loads. The muta-\ntion operation randomly selects a request and reallocates itto a different vehicle or adjusts its pickup and delivery times,\nmaintaining feasibility through constraint checks. The pop-\nulation iteratively evolves over a predefined number of gen-\nerations, with the best solution being tracked and updated\nbased on fitness. The population size is set to 10, and the\nnumber of generations is set to 500.\n",
    "title": "Multi-Agent Pointer Transformer: Seq-to-Seq Reinforcement Learning for Multi-Vehicle Dynamic Pickup-Delivery Problems",
    "arxiv_id": "2511.17435v1",
    "category": "recent",
    "authors": [
      "Zengyu Zou",
      "Jingyuan Wang",
      "Yixuan Huang",
      "Junjie Wu"
    ],
    "published": "2025-11-21",
    "num_pages": 15,
    "num_chars": 70934
  },
  {
    "text": "SMILE: A Composite Lexical-Semantic Metric for Question-\nAnswering Evaluation\nShrikant Kendre\u2217skendre@salesforce.com\nSalesforce AI Research\nAustin Xu\u2217austin.xu@salesforce.com\nSalesforce AI Research\nHonglu Zhouhonglu.zhou@salesforce.com\nSalesforce AI Research\nMichael S. Ryoo mryoo@salesforce.com\nSalesforce AI Research\nShafiq Joty\u2020sjoty@salesforce.com\nSalesforce AI Research\nJuan Carlos Niebles\u2020jniebles@salesforce.com\nSalesforce AI Research\nAbstract\nTraditional evaluation metrics for textual and visual question answering, like ROUGE, ME-\nTEOR, and Exact Match (EM), focus heavily on n-gram based lexical similarity, often\nmissing the deeper semantic understanding needed for accurate assessment. While mea-\nsures like BERTScore and MoverScore leverage contextual embeddings to address this lim-\nitation, they lack flexibility in balancing sentence-level and keyword-level semantics and\nignore lexical similarity, which remains important. Large Language Model (LLM) based\nevaluators, though powerful, come with drawbacks like high costs, bias, inconsistency, and\nhallucinations. To address these issues, we introduceSMILE: Semantic Metric Integrating\nLexical Exactness, a novel approach that combines sentence-level semantic understanding\nwith keyword-level semantic understanding and easy keyword matching. This composite\nmethod balances lexical precision and semantic relevance, offering a comprehensive eval-\nuation. Extensive benchmarks across text, image, and video QA tasks show SMILE is\nhighly correlated with human judgments and computationally lightweight, bridging the\ngap between lexical and semantic evaluation. Code and evaluation scripts are available\nathttps://github.com/SalesforceAIResearch/smile-metric-qna-eval\n1 Introduction\nQuestion answering (QA) is an essential task used to measure the progress of language-based models. Across\ntext, image, and video domains, the primary measure of model performance on QA benchmarks is accuracy,\nwhich is typically computed viaexact (or easy) match (EM): A model response is deemed correct if the\nground-truth answer, typically annotated by humans, exactly matches (or can be found within) the model\nresponse. As recent models have grown to be more capable language generators, model answers have grown\nmore nuanced, making EM overly stringent (Wang et al., 2023a). A reasonable attempt to mitigate these\nissues is to employ N-gram based metrics typically used for text generation evaluation, such as ROUGE (Lin,\n\u2217Equal contribution.\n\u2020Equal senior contribution.\n1arXiv:2511.17432v1  [cs.CL]  21 Nov 2025\n0 50 100 150 200 250 300\nInference time (ms/sample)0.00.20.40.60.81.0Pearson correlation\nw/ human annotationsLow-latency,\nhigh-performing\nevaluation metrics\nROUGE-L1\nsBERT\nSMILE (ours)METEOR\nGPT-4oBERTScore\nGPT-3.5Figure 1:SMILE offers high-performance, low-latency QA evaluation, breaking the trade-off be-\ntween cost and performance. Performance is averaged across human-annotated samples from benchmarks\nfor natural language (\u00d7), image (\u25b3), and video (\u25e6) domains.\n2004) or METEOR (Banerjee & Lavie, 2005), or embedding-based metrics like BERTScore (Zhang et al.,\n2019), MoverScore (Zhao et al., 2019), and BLEURT (Sellam et al., 2020), to assess similarity between\nthe predicted response and the ground-truth (Rajpurkar et al., 2016; Bajaj et al., 2016; Dunn et al., 2017;\nKo\u010disk` y et al., 2018; Yang et al., 2018a). While such metrics capture high-level similarity between the model\nresponse and ground-truth, they may miss fine-grained details crucial to answer correctness (e.g., \u201cThe cat\nisona chair\u201d vs. \u201cThe cat isundera chair\u201d) that result in lower correlation with human judgments (Ma\u00f1as\net al., 2024).\nConcurrently, due to their strong language comprehension abilities, large language models (LLMs) have\nbeen deployed as automatic evaluators for text generation. This approach, broadly known asLLM-as-judge,\nfunctions by either prompting more capable LLMs, like GPT-4o, or finetuning smaller LLMs specifically\nfor evaluation. LLM-as-judge is appealing as LLMs can adapt to different evaluation criteria and generate\nexplanations. Consequently, recent methods and benchmarks (Jacovi et al., 2025; Wang et al., 2024a) now\nemploy judges as evaluators in QA settings.\nHowever, using judge models for evaluation increasescosts. For practitioners and developers with limited re-\nsources, repeatedly querying pay-per-use API models to evaluate large datasets (5K+ samples) or dedicating\nlimited compute to hosting an evaluation server can be impractical for rapid development, which may drive\nthem to use lesser but faster metrics. Beyond resource demands, generative evaluators also exhibit relatively\nhigh latency (see Figure 1) and are susceptible to hallucinations, as we qualitatively show in Section 3.\nThisworkrevisitsembedding-basedapproachesforautomaticQAevaluationandintroducesSemanticMetric\nIntegrating Lexical Exactness (SMILE), a lightweight yet high-performing framework for grading QA tasks.\nSMILE aims to retain the efficiency of embedding-based evaluators while addressing their limitations, such as\nlack of fine-grained response understanding. To do so, SMILE comprises two subscores: A semantic subscore\nto assess overall response content, and a keyword subscore to reward lexical exactness. Overall, SMILE offers\na best-of-both-worlds evaluation solution: AsFigure 1 shows, itcorrelates with human annotators as strongly\nas GPT-4o. Additionally, SMILE core components can be precomputed for fast lookup, resulting in a 9x\nspeedup compared to API queries. SMILE\u2019s lightweight design allows it to run on CPU during evaluation,\nrequiring minimal GPU VRAM to perform aone timeevaluation dataset preprocessing step. Additionally,\nSMILE offers interpretability through its composite structure with two subscores. While SMILE can be\napplied in many settings, our study focuses on factoid QA tasks. Our contributions are:\n2\n(1)We revisit the promise of embedding-based automatic evaluation metrics for QA tasks and propose\nSMILE, which utilizes both sentence and keyword level similarity scores to evaluate based on holistic and\nfine-grained content.\n(2)We construct a 225-sample human annotated test set from nine (text, image, video) QA datasets, labeled\nby 4 domain experts. This set is used to benchmark SMILE and other metrics based on their correlation\nwith human judgments.\n(3)We demonstrate that SMILE can serve as a lightweight drop-in replacement for more powerful LLM-as-\njudge models across modalities.\n(4)Extensive ablation studies demonstrate the necessity of each of SMILE\u2019s components.\nIn all, our experiments demonstrate that SMILE is alightweightyethigh-performingautomatic eval-\nuation metric for QA settings. Code and evaluation scripts are available athttps://github.com/\nSalesforceAIResearch/smile-metric-qna-eval\n2 Background and related work\nThis work addresses three QA modalities: Natural language QA (NLQA), visual QA (VQA), and video QA\n(VidQA). Across these, a modelfreceives an input(q, c)and generates a textual answery=f(q;c). The\ninput consists of a questionqand contextc, wherecis text for NLQA, an image for VQA, and a video for\nVidQA. The task of the model is to produce a natural language answeryto the questionqbased on the\ngiven contextc.\nFormodelevaluation,weadoptareference-basedprotocol,assumingahuman-annotatedground-truthanswer\ny\u22c6is available for each input(q, c). Given a model responsey, the goal is to determine its correctness.\nSpecifically, we aim to design an evaluatorjthat produces an evaluation scores=j(y, y\u22c6)based onyand\ny\u22c6. Thissource-freesetup, where evaluation occurs without access to(q, c), aligns with the original exact\nmatch (EM) evaluation setup. Source-free evaluation may represent aneasierevaluation setting, as prior\nwork indicates LLM-based evaluators struggle when contextcis included (Xu et al., 2025).\nFor practitioners prioritizing accuracy, the evaluation scorescan be a binary correct/incorrect label. For\ndetailed failure analysis, a finer-grained score (e.g. 0-5 scale) may be preferred. Regardless of the format,\nthe score should be convertible to a binary label, typically via a straightforward threshold (e.g., scores\u22643\nas incorrect, scores\u22654as correct for a 0-5 scale) (Maaz et al., 2024; Wang et al., 2024a). We now review\nexisting metrics and evaluators.\nText generation metrics.QA benchmarks have typically used EM or ROUGE (Lin, 2004) to assess model\noutputs, e.g., (Yang et al., 2018b; Rajpurkar et al., 2016; Dunn et al., 2017; Fan et al., 2019). As model\nresponses grew more nuanced, n-gram metrics such as BLEU (Papineni et al., 2002) and METEOR (Banerjee\n& Lavie, 2005) were adopted in QA settings (Bajaj et al., 2016). Despite better correlation with human\nannotations than EM, n-gram metrics have been shown to be insufficient for modern QA tasks (Chen et al.,\n2019).\nEmbedding-based metrics.Embedding models, like BERT (Devlin et al., 2019) and finetuned variants,\ne.g., (Reimers & Gurevych, 2019; Gao et al., 2021), are trained to measure semantic similarity. As such,\nembedding-based metrics are a natural step in overcoming the limitations of EM and n-gram-based met-\nrics (Chen et al., 2019), with notable methods like BERTScore (Zhang et al., 2019), BARTScore (Yuan\net al., 2021), BLEURT (Sellam et al., 2020) being used as evaluators in benchmarks (Ao et al., 2024). Re-\ncent work (Bulian et al., 2022; Risch et al., 2021; Lee et al., 2020) developed metrics specifically for the QA\nsetting.\nLLM-as-judge evaluators.The LLM-as-judge paradigm initially utilized frontier LLMs for automatic\nevaluation (Wang et al., 2023b; Liu et al., 2023c; Fu et al., 2024; Chiang & Lee, 2023). However, biases\nin prompted evaluators hold were soon identified (Panickssery et al., 2024; Wang et al., 2023c; Park et al.,\n2024), leading to the finetuning of smaller models for evaluation (Kim et al., 2024; Li et al., 2023a; Zheng\net al., 2024; Wang et al., 2023d; Shiwen et al., 2024). Recent efforts focus on training for diverse evaluation\nprotocols (Vu et al., 2024; Wang et al., 2024b), such as pairwise, single rating, and binary classification.\nApplying LLM-as-judges specifically to QA grading is a recent development (Ma\u00f1as et al., 2024), with many\n3\nTGIFTextVQADocVQAHotpotQA\n[...] Uphill in the narrow vale are the Bridgewater monument and the Ashridge estate. It is noted for its picturesque setting and has been referred to as a \"chocolate-box\" village due to its traditional appearance. [...]Ivinghoe Beacon is a prominent hill and landmark in the Chiltern Hills, standing 233 m (757 ft) above sea level. It is situated close to the village of Ivinghoe in Buckinghamshire, the Ashridge Estate, and the villages of Aldbury and Little Gaddesden in Hertfordshire and is managed and owned by the National Trust. [...]Question: Is this a grocery store?Answer: yesSynthetic Answer: It is indeed a grocery store.Model output:  Yes, the image shows a grocery store with customers and a variety of products, including bottles and fresh produce. This indicates that the store provides a range of food and household items for customers to purchase. The presence of a customer suggests that [...]Question: what goes of the track while attempting to take a corner at speed ?Answer: truckSynthetic Answer: The truck tends to go o\ufb00 the track while attempting to take a corner at speed.Model output: The truck in the video is attempting to take a corner at high speed and loses control, causing it to spin out of the track and crash into the dirt. This results in a dramatic scene with dust and [...]Question: What is the \"Conversion Rate\" for \"Event\"?Answer: 8Synthetic Answer: The conversion rate for an event is 8.Model output:| Column | Min | Max | Average || --- | --- | --- | --- || Conversion Rate | 0.00 | 100.00 | 33.18 |The \"Conversion Rate\" for \"Event\" is 33.18%. This is based on the cal- [...]Question: What was the name of the hill that was the seat of an estate uphill Aldbury?Synthetic Answer: The Chiltern Hills served as the seat of an estate uphill in Aldbury.Answer: Chiltern HillsModel output: Bridgewater monumentHuman: 1GPT-4o: 0 \u274c\nSMILE: 1 \u2705\nHuman: 0GPT-4o: 1 \u274c\nSMILE: 0 \u2705\nHuman: 1sBERT: 0 \u274c\nSMILE: 1 \u2705\nHuman: 0sBERT: 1 \u274c\nSMILE: 0 \u2705\nSMILE subscores:Semantic subscore ss : 0.678Keyword subscore sl : 0.333SMILE score sSMILE :0.506SMILE binned score:2Max sim words: 3318SMILE subscores:Semantic subscore ss : 0.721Keyword subscore sl : 1.000SMILE score sSMILE  :0.860SMILE binned score:5Max sim words: yesSMILE subscores:Semantic subscore ss : 0.336Keyword subscore sl : 0.193SMILE score sSMILE :0.265SMILE binned score:1Max sim words: bridgewater monumentSMILE subscores:Semantic subscore ss: 0.810Keyword subscore sl:1.000SMILE score sSMILE :0.905SMILE binned score:5Max sim words:   truckJudge models may hallucinate even on a simple example.Judge models may hallucinate when output is verboseEmbedding-based metrics miss short responses in long outputsEmbedding metrics often overrate relevant but wrong answersFigure 2:Example failure cases for existing methods.Columns 1-2illustrate LLM-as-judge fail-\nures: hallucination even on simple verification (column 1), and incorrect yet concrete responses (column 2).\nColumns 3-4illustrate embedding-based model failures with lengthy (column 3) and relevant but incorrect\n(column 4) responses. SMILE correctly identifies each case and exposes expanded, interpretable subscores\n(bottom): semantic alignment (s s), keyword/faithfulness (s l), overall score (s SMILE), a binned decision score,\nand the top max-similarity words, providing transparent evidence for why a response is accepted or rejected.\nScores are shown on a 0/1 scale for comparison. [...] denotes omitted content for brevity.\nbenchmarks (Krishna et al., 2024; Jacovi et al., 2025) and studies (Maaz et al., 2024; Liu et al., 2023b)\nemploying API models.\n3 When do existing evaluation methods and metrics fail?\nBefore introducing SMILE, we present a qualitative case study highlighting the failure modes of LLM-as-\njudge and embedding-based metrics in QA evaluation. Our analysis of 225 human-evaluated model responses\nreveal a common failure point: verbose or generic model outputs, consistent with Ma\u00f1as et al. (2024); Luo\net al. (2021).\nSurprisingly, we also find that using a powerful LLM like GPT-4o does not guarantee accurate evaluations.\nWe highlight two representative failure modes in Figure 2 (columns 1-2). GPT-4o was prompted to generate\na binary accuracy label, as well a 1-5 score (see Section A for prompt). For ease of comparison, we convert\nSMILEscorestoabinaryaccuracyindicator. AprimaryconcernwithusingLLMsasjudgesishallucinations.\nFigure 2 (column 1) shows that even for relatively simple samples, GPT-4o may hallucinate an incorrect\nlabel. Figure 2 (column 2) is an example ofconcretenessbias, a known judge model bias (Park et al., 2024).\nHere, GPT-4o is tricked by a response that includes concrete artifacts, like the table the model generated,\neven if it incorrect response.\nSeeking an efficient evaluator, we also analyzed failure modes of embedding-based approaches. Semantic\nsimilarity metrics like BERTScore exhibited well-known limitations (Zhang et al., 2019). Figure 2 (columns\n4\nIt says on   the jet body  friendly low faresSemantic score (0.96)Embedding KW score (1)SMILEFriendly low fares The advertisement on the jet says \u201cFriendly low fares\u201d(1) Preprocessing: Distributional alignment with SLM(2) Evaluation: Computing SMILE with semantic + keyword scores\nWhat does this jet advertise as having?\nThe advertisement on the jet says \u201cFriendly low fares\u201dFriendly low fares Exact match KW score (1)Keyword score (1)\nCosine similarity Embedding modelGoldanswer\nSynthetic answer\nSLMEvaluated model\nSynthetic answer generator\nSLM\n(0.98)Figure 3:Overview of SMILE. SMILE evaluates QA outputs in two steps using a synthetic answer\ngenerator and an embedding model. (1)One-timepreprocessing generates stylistically aligned synthetic\nanswers using a small language model (SLM). (2) SMILE computes two sub-scores: a semantic score based\non the synthetic answer, and a keyword subscore that combines exact match with embedding comparisons\nof model response N-grams to the gold answer. This subscoring balances semantic and lexical evaluation.\n3-4) highlights two key examples: column 3 shows how overly verbose model responses can easily misled\nsemantic similarity metrics, as much of the output is irrelevant to the simple \u201cyes\u201d response. This can be\nviewed this asdistributional misalignment(Agrawal et al., 2022): increasingly high-quality model outputs\nare often lengthier, contrasting with the typical short answers in factoid QA benchmarks. Conversely, when\nmodel responses are short but semantically relevant, these metrics are prone to false positives, as illustrated\nin Figure 2 (column 4). The limitations of embedding-based and LLM-as-judge methods motivated SMILE-a\nhallucination-free evaluation metric presented next.\n4 The SMILE metric\nOur analysis in Section 3 pinpointed two critical limitations of embedding-based approaches: (1) adistribu-\ntional gapbetween verbose model responses and concise ground-truth answers, and (2) alack of fine-grained\nunderstandingdue to their semantic focus. SMILE directly addresses these issues with two key innovations:\n(1) Synthetic answer generation to bridge the stylistic distribution gap, and (2) targeted sub-scores capturing\nboth semantic and lexical similarity between model responses and ground-truth.\nBridging the stylistic distribution gap.As shown in Section 3, assessing directly based on the ground\ntruthy\u22c6, which is typically short for short-form (factoid) QA (e.g., a single word or short phrase), may be\nsub-optimal, as model responses tend to be more verbose. Motivated by past work that have used LLMs\nto perform other kinds of zero-shot distribution alignment (Gao et al., 2023; Xu et al., 2024), we utilize an\nLLM to generate a synthetic model response from the ground-truth. Our key insight is that for short-form\nQA tasks, alightweightmodel (e.g., 3B parameter) can be deployed as a synthetic answer generatorg.\nSpecifically, the generatorgtakes as input the original questionqand ground truth answery\u22c6and outputs\na synthetic answer\u02dcy=g(y\u22c6, q), which aligns stylistically with model responses, but reflects the ground-\ntruth answer content. As a concrete example from our evaluation setup, for input question \u201cWhat is the\nConversion Rate for Event?\u201d and ground-truth \u201c8\u201d, a generated synthetic answer is \u201cThe conversion rate of\nan event is 8\u201d. We emphasize thatsynthetic answer generation is independent of the model being evaluated\nand is performed only once, prior to test-time, per evaluation set. As a result, synthetic answers may be\nstored and used for any subsequent evaluations.\nIntegrating semantic and lexical similarity.The core idea of SMILE is to measure bothsemantic\nandlexicalsimilarity between the model response and the ground-truth using an embedding modele. We\ncalculate a semantic similarity score, which we denotes s, as\nss(y,\u02dcy;e) =sim(e(y), e(\u02dcy)),(1)\n5\nwhere,sim(x, y) = (1+\u27e8x, y\u27e9/\u2225x\u2225 2\u2225y\u22252)/2,whichisalinearlytransformedcosinesimilaritythatlieswithinan\ninterpretable interval of[0,1]. As we show in Section 5.4, generating synthetic answers bridges the stylistic\ndistribution gap between ground-truth answers and model responses enough to make semantic similarity\nmeaningful. However, Section 3 shows that this semantic similarity score alone is insufficient to capture\nthe nuances of evaluation. As a result, we additionally compute a lexical similarity score, which we denote\ns\u2113\u2208[0,1], as\ns\u2113(y, y\u22c6;e) =1\n2/parenleft\uf8ecig\nEM(y, y\u22c6) + max\ni{sim(e(N i[y]), e(y\u22c6))}/parenright\uf8ecig\n(2)\nwhereEM(y, y\u22c6)\u2208{0,1}score between the predictionyand ground-truthy\u22c6andN i[y]denotes thei-th\nn-gram of responsey. In computings \u2113, we take advantage of the fact thaty\u22c6is typically a short phrase to\ncompute two complementary scores. The easy match sub-scoreEMserves as a preliminary check for lexical\nanswer correctness. However, as noted in prior work (Wang et al., 2023a; Luo et al., 2021), string matching\nmay be too stringent for synonym-like answers (e.g., \u201ccat\u201d vs. \u201ckitten\u201d). As a result, we loosen the necessity\nfor string matches via the maximum n-gram embedding similarity score, which serves as a continuous-valued\nmeasure of lexical exactness.\nEvaluationwithSMILE.Withoursemanticandlexicalscorescomputed, wecannowcomputetheSMILE\nscore, denoteds SMILE\u2208[0,1]:\nsSMILE(y, y\u22c6;e, w) =1\n2(w\u00b7s s(y,\u02dcy;e) + (1\u2212w)\u00b7s \u2113(y, y\u22c6;e)),(3)\nwherew\u2208(0,1)is some user-specified weight to balance the two subscores. This weighting mechanism\nallows practitioners to express their preferences: Those who are more inclined towards exact match may\nplace higher weight ons \u2113, whereas those who value higher responses whose meaning is closest with the\nground-truth may place a higher weight ons s.\n4.1 Optimizations for test-time speed-up\nSMILE offers significant speed advantage over LLM-as-judge methods, as extracting representations from\nlightweight embedding models like BERT (Devlin et al., 2019) is far faster than generating natural language\noutputs. This speed advantage can be further enhanced bypre-computingand storing representations for\nsynthetic answerse(\u02dcy)and keyword representationse(y\u22c6)before evaluation. By storinge(\u02dcy)ande(y\u22c6), only\nthe model response representationse(y)ande(N i[y])need to be calculated during test time.\n4.2 Interpretability of SMILE scores\nSMILE\u2019ssemanticandlexicalsubscoresprovidepractitionerswithmoreinterpretableandactionablefeedback\nthan other metrics. These subscores enable monitoring of model performance along two complementary axes:\nsemantic content, a holistic measure of response relevance, and lexical exactness, a finer-grained measure\nof response quality. Importantly, SMILE allows evaluation not only at the instance-level but also at the\npopulation-level. Aggregatings sands \u2113across all test samples reveals a model\u2019sgeneralstrengths and\nweaknesses. This contrasts with LLM-as-judge methods, which offer more specificinstance-levelnatural\nlanguage feedback, making it hard to extract overall insights. See examples in Section C.\n5 Experiments and results\nBenchmarks and generator models.We assessed SMILE on established benchmarks across three do-\nmains: NLQA, VQA, and VidQA. To ensure diverse evaluation, we included three benchmarks per domain:\nMRQA (Fisch et al., 2019), HotpotQA (Yang et al., 2018b), and MuSiQue (Trivedi et al., 2022) for NLQA,\nTextVQA (Singh et al., 2019), DocVQA (Mathew et al., 2020), and POPE (Li et al., 2023b) for VQA,\nand TGIF (Jang et al., 2017), MSVD (Xu et al., 2017), and MSRVTT (Xu et al., 2016) for VidQA. For\nHotpotQA and MuSiQue, we used the standardized setup from ContextualBench (Nguyen et al., 2024). We\ngenerated responses using the following models for each domain: GPT-4o (Hurst et al., 2024) for NLQA,\nLLaVA-1.5 for VQA 7B (Liu et al., 2023a;b) , and Qwen2.5-VL 3B Instruct (Bai et al., 2025) for VidQA.\n6\nThese models were selected for their strong capabilities in producing high-quality textual responses, forming\nthe basis of our analysis. We also evaluate on QA-Eval (Wang et al., 2023a), a large-scale NLQA dataset\n(\u223c10k samples) based on Natural Question (NQ) and TriviaQA (TQ), with responses from GPT-3.5 and\nGPT-4o. This setup enables robust comparison of SMILE against LLM judges at scale.\nData annotation efforts.To evaluate QA metrics, we assessed their alignment with human judgments\nusing agolden evaluation set. We constructed this set by sampling model outputs from the nine benchmarks\n(three per domain), randomly selecting 25 input-output pairs per dataset for annotation. Four annotators\n(authors of the paper with native level English) evaluated the generated outputs based on a predefined\nrubric, considering correctness, relevance, and clarity. Given potential ambiguity, annotators used a 3 point\nscale: clearly incorrect, unclear, clearly correct. To check annotation quality, we calculated Krippendorff\u2019s\nalpha (Krippendorff, 2011), achieving a score of0.71, indicating substantial inter-annotator agreement.\nThis high agreement confirms the reliability of our annotations, so we proceed with it as the basis of our\nevaluation.\nBaselines and metrics.We compared SMILE with established metrics, including traditional NLP mea-\nsures: ROUGE-L, METEOR and Exact and Easy Match; alongside embedding-based similarity metrics:\nBERTScore (with Roberta-large) and sBERT cosine similarity1. Following Maaz et al. (2024), we also em-\nployed GPT-4o and GPT-3.5-Turbo as judge models, prompting them for a 0-5 score and a binary yes/no\nprediction. For all baselines, we provide detailed implementation details in Section A, including judge model\nprompts.\nSMILE implementation.We choose Llama-3.2-3B-Instruct as our synthetic answer generatorgand\nember-V12asourembeddingmodele. Thiscombinationiscomputationallylightweight: the335Mparameter\nember-v1 can run inference on a CPU, and generating responses with the 3B Llama model requires<10GB\nof VRAM. Furthermore, our ablation study (Section 5.4) shows that larger models offer only marginal\nperformance improvements, highlighting SMILE\u2019s inherent lightweight nature. SMILE scores, similar to\nGPT-4o, are discretized into six bins (0\u20135), with scores\u22654considered correct. The N-gram value is\ndynamically set based on ground truth answer length, and the parameterwfixed to0.3.\n5.1 Main experimental results\nUsing our golden evaluation set, we compare SMILE against existing baseline metrics. To holistically assess\nevaluators-human agreement, we computed Pearson correlation and Kendall\u2019s Tau-b from Human Accuracy.\nPearson correlation and Kendall\u2019s Tau measure agreement with human annotations on the instance level,\nranging from \u20131 (perfect disagreement) to +1 (perfect agreement). Kendall\u2019s Tau-b, focuses on ranking\nconsistency and accounts for ties in the data. We report results in two evaluation settings to separate\nformattingeffectsfrommetricbehavior:Origusestheoriginalground-truthreferences(thecommonpractical\nsetup), andSynuses SMILE\u2019s synthetic canonicalized references.\nPearson correlation results are presented in Table 1. SMILE consistently outperforms other evaluation\nmetrics across tasks, achieving the highest overall correlation with human evaluations. Notably, SMILE\nsignificantly surpasses GPT-4o and GPT-3.5, despite their prominence as LLM-as-judge evaluators. Across\nall tasks, SMILE\u2019s positive correlation scores are significantly closer to 1 than most competitors, indicating\nstrong agreement with human evaluations and validating the robustness of our approach.\nKendall\u2019s Tau-b results, presented in Table 2, establish SMILE\u2019s superior correlation with human rankings.\nQuantitatively, SMILE outperforms all competing metrics, further validating its effectiveness. SMILE sur-\npasses GPT-4o and GPT-3.5, underscoring its exceptional ability to rank generated responses in a way that\nclosely mirrors human annotated rankings.\nFinally, we evaluate SMILE and GPT-3.5/4o as evaluators on QA-Eval, using two prompting variants:\n(1) original prompt (based on Maaz et al. (2024)), and (2) extract-style prompt (asks LLM to extract\nshort answer first). As shown in Table 3, SMILE consistently outperforms GPT-3.5 and closely matches\n1https://huggingface.co/sentence-transformers/all-roberta-large-v1\n2https://huggingface.co/llmrails/ember-v1\n7\nMetric Ref. Video QA: Qwen2.5 Visual QA: llava 1.5 7B Language QA: GPT-4o\nTGIF MSVD MSRVTT TextVQA DocVQA POPE MRQA HotpotQA MUSIQUE Overall\nExact MatchOrig nan nan nan nan nan 0.099 nan 0.109 0.147 0.118\nSyn nan nan nan nan nan 0.099 nan 0.109 0.147 0.118\nEasy MatchOrig 0.793 0.379 0.290 0.795 0.375 0.451 0.676 0.657 0.890 0.590\nSyn nan nan nan nan nan 0.143 0.129 0.185 0.185 0.161\nROUGE-LOrig 0.603 0.442 0.217 0.596 0.705 0.001 0.361 0.603 0.445 0.441\nSyn 0.625 0.372 -0.340 0.081 0.228 0.518 0.284 0.042 -0.053 0.283\nMETEOROrig 0.663 0.488 0.438 0.667 0.747 0.086 0.528 0.664 0.611 0.544\nSyn 0.617 0.451 -0.311 0.264 0.314 0.414 0.283 0.051 -0.046 0.306\nBERTScoreOrig 0.421 0.331 0.110 0.398 0.677 0.164 0.310 0.620 0.411 0.382\nSyn 0.674 0.497 -0.247 0.206 0.345 0.434 0.223 0.094 0.194 0.324\nsBERTOrig 0.472 0.592 0.315 0.602 0.852 -0.164 0.352 0.664 0.363 0.486\nSyn 0.806 0.579 0.020 0.419 0.426 0.541 0.037 0.203 0.195 0.358\nBLEURTOrig -0.101 0.256 -0.004 0.483 0.705 -0.488 0.354 0.558 0.454 0.378\nSyn 0.692 0.517 -0.093 0.517 0.504 0.366 0.174 0.158 0.136 0.351\nMoverscoreOrig 0.613 0.390 0.278 0.338 0.671 0.053 0.242 0.379 0.231 0.355\nSyn 0.629 0.388 -0.331 0.296 0.426 0.414 0.190 0.001 0.074 0.305\nGPT-3.5Orig 0.825 0.609 0.350 0.663 0.803 0.422 0.810 0.668 0.525 0.631\nSyn 0.7420.8450.059 0.584 0.702 0.210 0.389 0.234 0.310 0.453\nGPT-4oOrig 0.778 0.6870.627 0.8590.761 0.699 0.294 0.678 0.764 0.683\nSyn 0.8980.766 0.358 0.844 0.9040.874 0.9440.774 0.690 0.784\nSMILEOrig 0.800 0.564 0.552 0.7840.9140.469 0.8600.952 0.950 0.761\nSyn 0.847 0.607 0.496 0.796 0.909 0.763 0.839 0.902 0.948 0.790\nTable 1:Pearson correlation with human judgments (\u2191) across Video, Visual, and Language\nQA.For each metric we report two settings:Orig(evaluated against original ground-truth references) and\nSyn(evaluated against SMILE\u2019s synthetic references), shown for all datasets and Overall. SMILE achieves\nthe strongest overall correlation across modalities.\nGPT-4o. Notably, LLMs degrade under the extract prompt, highlighting SMILE\u2019s robustness and prompt\nindependence.\nImpact of Synthetic Canonicalized References.To analyze the impact of canonicalized references,\nwe compare results usingOrigandSynanswers. Across datasets and modalities, traditional metrics stagnate\nor degrade underSyn, whereas SMILE remains stable and typically strongest. Canonicalization therefore\ndoes not \u201crescue\u201d baseline metrics; SMILE\u2019s agreement with humans is robust to the choice of reference.\nShort-answer examples illustrate theSyn-induced lexical-overlap bias: underSyn, traditional metrics tend to\nover-score verbose or even contradictory responses because increased surface overlap with the canonicalized\nreference can mask factual errors. On TGIF, generic non-answers receive higher scores; on POPE, fluent\ncontradictions (e.g., \"no bowl\" versus \"contains a bowl\") are similarly over-scored. These cases show that\ncanonicalization alone cannot fix metrics that privilege surface overlap and fluency over semantic correctness.\nSMILE mitigates this by combining sentence-level semantic similarity with keyword-level exactness and an\noptional exact-match component in a weighted aggregation that captures both intent and factual content.\nQualitative examples in Figure 2 illustrate these patterns across TGIF, DocVQA, TextVQA, and HotpotQA\n5.2 SMILE as a drop-in replacement for GPT-4o\nBuilding on SMILE\u2019s alignment with human judgment, we now demonstrate its capability to supplant GPT-\n4o as an evaluation metric. To do so, we compare model accuracy derived from SMILE scores against that\n8\nMetric Ref. Video QA: Qwen2.5 Visual QA: llava 1.5 7B Language QA: GPT-4o\nTGIF MSVD MSRVTT TextVQA DocVQA POPE MRQA HotpotQA MUSIQUE Overall\nExact MatchOrig nan nan nan nan nan 0.100 nan 0.109 0.147 0.119\nSyn nan nan nan nan nan 0.100 nan 0.109 0.147 0.119\nEasy MatchOrig 0.765 0.414 0.295 0.773 0.361 0.420 0.676 0.657 0.890 0.583\nSyn nan nan nan nan nan 0.145 0.129 0.185 0.185 0.161\nROUGE-LOrig 0.598 0.509 0.248 0.614 0.696 0.162 0.390 0.496 0.554 0.474\nSyn 0.591 0.296 -0.264 0.052 0.182 0.381 0.223 0.014 -0.070 0.230\nMETEOROrig 0.592 0.511 0.446 0.644 0.686 0.161 0.396 0.458 0.582 0.497\nSyn 0.522 0.339 -0.255 0.261 0.218 0.294 0.222 0.028 -0.046 0.243\nBERTScoreOrig 0.347 0.240 0.060 0.322 0.526 0.125 0.289 0.455 0.440 0.312\nSyn 0.618 0.411 -0.187 0.131 0.290 0.408 0.171 0.043 0.185 0.272\nsBERTOrig 0.393 0.505 0.230 0.435 0.662 -0.102 0.289 0.455 0.382 0.384\nSyn 0.674 0.394 -0.043 0.305 0.336 0.374 0.051 0.142 0.231 0.283\nBLEURTOrig 0.019 0.231 -0.034 0.566 0.526 -0.238 0.323 0.469 0.520 0.325\nSyn 0.571 0.419 -0.068 0.435 0.463 0.306 0.170 0.028 0.104 0.285\nMoverscoreOrig 0.506 0.342 0.170 0.226 0.426 -0.011 0.341 0.355 0.104 0.276\nSyn 0.646 0.300 -0.221 0.226 0.327 0.328 0.187 -0.057 -0.012 0.256\nGPT-3.5Orig 0.738 0.487 0.291 0.625 0.690 0.441 0.638 0.439 0.570 0.547\nSyn 0.6680.7590.033 0.549 0.642 0.209 0.341 0.261 0.300 0.418\nGPT-4oOrig 0.686 0.5800.575 0.7800.6580.676 0.281 0.488 0.767 0.610\nSyn 0.7970.638 0.272 0.728 0.754 0.660 0.614 0.688 0.589 0.638\nSMILEOrig 0.765 0.559 0.485 0.730 0.841 0.607 0.805 0.765 1 0.729\nSyn 0.773 0.595 0.463 0.6520.9200.607 0.805 0.7650.981 0.729\nTable 2:Kendall\u2019s Tau-b (\u2191) ranking agreement with human judgments across Video, Visual,\nand Language QA.For each metric we report two settings:Orig(original ground-truth references) and\nSyn(SMILE\u2019s synthetic references). Across all datasets and Overall SMILE attains the highest ranking\nagreement.\nGPT 3.5 GPT-4o\nNQ TQ NQ TQ Overall\nGPT-3.5, original prompt 0.756 0.849 0.713 0.706 0.756\nGPT-4o, original prompt 0.865 0.913 0.815 0.806 0.850\nGPT-3.5, extract prompt 0.478 0.572 0.413 0.440 0.476\nGPT-4o, extract prompt 0.831 0.898 0.783 0.774 0.821\nSMILE 0.829 0.889 0.786 0.760 0.816\nTable 3:Pearson Correlation with human judgment on QAEval.SMILE shows strong agreement\nwith human annotations, outperforming GPT-3.5 and roughly matching GPT-4o.\nfrom GPT-4o-based evaluation each benchmark\u2019scomplete test-set.We find that SMILE exhibits the lowest\noverall deviation among all tested methods, as summarized in Table 4. This compelling result strongly\nsuggests SMILE is a reliable and direct alternative to resource-intensive LLM-as-judge approaches like GPT-\n4o.\n5.3 Cost effective Model selection with SMILE\nSelecting optimal checkpoints during ML model training is crucial for maximizing performance on down-\nstream tasks. Traditionally, this selection process relies on not-so reliable metrics like METEOR and\n9\nVideo QA: Qwen2.5 Visual QA: llava 1.5 7B Language QA: GPT-4o\nTGIF MSVD MSRVTT TextVQA DocVQA POPE MRQA HotpotQA MuSiQue Overall\nGPT-4o 0.705 0.657 0.503 0.436 0.191 0.783 0.920 0.909 0.700 0.645\nExact Match -0.705 -0.657 -0.503 -0.424 -0.187 -0.782 -0.877 -0.884 -0.681 0.633\nEasy Match -0.025 -0.217 -0.106 -0.056-0.044 -0.021 -0.083 -0.152 -0.135 0.093\nROUGE-L -0.705 -0.657 -0.503 -0.413 -0.179 -0.773 -0.648 -0.493 -0.553 0.547\nMETEOR -0.705 -0.657 -0.503 -0.405 -0.166 -0.783 -0.592 -0.509 -0.454 0.530\nBERTScore 0.294 0.340 0.497 0.562 0.805 0.216 0.0080.091 0.300 0.354\nsBERT -0.705 -0.657 -0.503 -0.379 -0.145 -0.780 -0.566 -0.399 -0.504 0.515\nSMILE -0.032 -0.241 0.132 0.0210.104 -0.041 -0.008 -0.016 0.005 0.067\nTable 4:Deviation from GPT-4o accuracyacross Video, Visual, and Language QA tasks, usingcomplete\ntest sets. SMILE exhibits the smallest deviation among evaluators, closely aligning with GPT-4o.\nMetrics Checkpoint 1 Checkpoint 2\nRank Cost($) Rank Cost($)\nGPT-4o 1 12 2 11.99\nGPT-3.5-turbo 1 12 2 12.00\nMETEOR 1 - 2 -\nsBERT 2 - 1 -\nSMILE 1- 2-\nTable5: CheckpointselectiononTGIFvideoQA(Videomodel): Rank(1=best)andapproximateevaluation\ncost (USD) per metric. SMILE ranks checkpoints similarly to GPT metrics, but without inference costs. \"-\"\ndenotes methods without API inference cost.\nROUGE, or expensive metrics such as LLM-based judge evaluations. In this experiment, we use SMILE\nto identify the best checkpoint. Specifically, we select two intermediate checkpoints(with similar perfor-\nmance) from the Video model and evaluate their performance on the TGIF benchmark. The evaluation is\nconducted using five metrics: GPT-4o, GPT-3.5-turbo, METEOR, sBERT and SMILE.\nAs per Table 5, our findings demonstrate thatSMILE selects the same optimal checkpoint (i.e. checkpoint 1)\nas GPT-4o and GPT-3.5-turbo. This alignment highlights SMILE\u2019s effectiveness, emphasizing its capability\nto provide reliable checkpoint selection without incurring additional evaluation cost.\nAn advantage of SMILE is its substantial reduction in evaluation costs compared to GPT-based models.\nGPT-4o and GPT-3.5 cost\u2019s around $12 for each checkpoint evaluation on TGIF, and the cost increases\nas more checkpoints and evaluation benchmarks are added. In contrast, SMILE has almost no extra cost.\nTherefore, adopting SMILE not only maintains performance accuracy but also significantly lowers monetary\noverhead, making it a highly efficient and scalable solution for checkpoint selection.\n5.4 Ablations\nThissectionpresentsanablationstudyofSMILEcenteredonthreekeyperspectives: (1)Component analysis,\nsystematically removing steps (synthetic answer generation, semantic similarity score, keyword score) to\ndemonstrate their individual importance, (2)Model scaling, examining the impact of using larger models\nfor both synthetic answer generation and embedding, (3)Hyperparameter tuning, analyzing the effect of the\nweightwin SMILE. Results are detailed in Table 6 and Figure 5.\nComponent analysis.SMILE comprises three key components: (1) semantic similarity, (2) lexical ex-\nactness, and (3) distribution alignment via a lightweight language model. Table 6 (top) summarizes the\ncontribution of these components to SMILE\u2019s robust performance. Experiments demonstrate thatboth key-\nword and sentence similarity scores are essential. Removing keyword scores significantly reduces Pearson\ncorrelation, underscoring the critical role of lexical exactness in QA evaluation. Conversely, relying solely\n10\nVideo QA Visual QA Language QA Overall\nSMILE 0.650 0.823 0.896 0.790\nw/o keyword scores 0.628 0.775 0.941 0.782\nw/o sentence scores 0.463 0.533 0.249 0.415Component\nAblationw/o synthetic answers 0.639 0.722 0.921 0.761\nSMILE 0.650 0.823 0.896 0.790\nEmbedding: GTE 7B 0.647 0.824 0.947 0.806Model\nAblationSyn. answer: GPT-3.5-Turbo 0.636 0.802 0.930 0.790\nTable 6:Component and model ablations.Performance is assessed by Pearson correlation. Keyword\nscores are the primary contributor, highlighting the importance of lexical exactness. Embedding model\nscaling yields marginal (<2%) gains.\n0 100 200 300 400 500\nLength of responses0.00.10.20.3FrequencyGold answers\nModel outputs\nSynthetic answers\nFigure 4: Length of gold answers, model outputs, and synthetic answers, across all domains and benchmarks\nin characters. Synthetic answers more align better with model outputs in terms of output length, enabling\nbetter semantic evaluation.\non keyword scores neglects global structure, degrading performance notably in VidQA and VQA. Synthetic\nanswers are also crucial, particularly for verbose model predictions in VidQA and VQA. Figure 4 illus-\ntrates the effect of synthetic answer generation, which effectively maps extremely short gold answers to\nlonger model outputs. Combining sentence scores, keyword scores, and synthetic answers yields robust and\naccurate evaluation across domains.\nModel scaling.A key advantage of SMILE is that it offers the ability toefficientlyrun evaluation. Our\nmodel choices in Section 5 demonstrate this: SMILE at inference time requires only a 355M parameter em-\nbedding model, and pre-generating synthetic answers requires only a 3B generative model. Table 6 (bottom)\nfurther establishes SMILE\u2019s lightweight nature: increasing model capacity yields minimal performance gains,\nif at all. Our model ablation focused on the synthetic answer generation model and the embedding model.\nUsing GPT-3.5-Turbo instead of Llama-3.2-3B-Instruct for synthetic answers yielded comparable correla-\n0.2 0.4 0.6 0.8\nWeight w0.00.20.40.60.81.0Pearson correlation\nNLQA\nVQAVidQA\nOverall\nFigure 5: Sweep ofw, which trades off lexical exactness for semantic similarity aswincreases. SMILE\nexhibits relatively stable aggregate performance forw\u22640.5.\n11\ntion with human judgments, indicating thateffective synthetic answer generation is achievable with smaller\nlightweight models. Replacing ember-v1 with the substantially larger GTE-7B (Li et al., 2023c) embedding\nmodel resulted in only a marginal performance gain of less than 2%, despite a 20\u00d7increase in model size.\nThis indicates thatSMILE remains effective even with lightweight embedding models.\nHyperparameter tuning.SMILE\u2019s lone hyperparameterwallows practitioners to precisely decide the\nimpact of the semantic and keyword subscores. Specifically, aswincreases, more importance is given to\nthe semantic subscore. As we show in Figure 5, overall performance is relatively stable forw\u22640.5before\nsmoothly decreasing. This aligns with results from our component ablation study in Table 6: The keyword\nsubscores alone exhibited relatively strong performance, while the semantic subscore fared worse. However,\nSMILEhyperparameterchoiceisrelativelyforgiving, withanychoiceofwthatslightlyupweightsthekeyword\nsubscore likely to perform well.\n6 Conclusion\nWe introduce SMILE, a novel, lightweight QA evaluation metric that overcomes limitations of existing meth-\nods by integrating semantic and lexical analysis. Its efficiency addresses the high cost, biases, and inconsis-\ntencies of LLM-based evaluators. Benchmarking across text, image, and video QA demonstrates SMILE\u2019s\nstrongcorrelationwithhumanjudgment, surpassingtraditionalmetricsandLLMjudgeslikeGPT-4o. Itsde-\nsign also offers interpretability, and ablation studies validate the importance of its components. In summary,\nSMILE provides a robust, efficient, and interpretable solution for QA evaluation across modalities, effectively\nbalancing lexical precision and semantic relevance as a promising alternative to costly LLM evaluations.\nLimitations\nAlthough SMILE offers a lightweight, interpretable, and scalable alternative to LLM-based evaluators, it\ncomes with certain limitations. (1) SMILE is designed for source-free evaluation and does not access the\ncontext. Although efficient, this may cause it to miss context-dependent errors. (2) The metric relies on\nsynthetic answers to align ground-truths with model outputs. The quality of these synthetic answers can\naffect the the scoring, especially in long-form or open-ended responses. (3) Our evaluation is limited to\nfactoid QA tasks. SMILE\u2019s effectiveness on complex reasoning, multi-hop, or conversational QA remains\nunexplored. (4) SMILE includes a weighting parameter to balance lexical and semantic components, which\nmay require tuning for specific tasks or domains.\nReferences\nAishwarya Agrawal, Ivana Kaji\u0107, Emanuele Bugliarello, Elnaz Davoodi, Anita Gergely, Phil Blunsom, and\nAida Nematzadeh. Reassessing evaluation practices in visual question answering: A case study on out-of-\ndistribution generalization.arXiv preprint arXiv:2205.12191, 2022.\nJunyi Ao, Yuancheng Wang, Xiaohai Tian, Dekun Chen, Jun Zhang, Lu Lu, Yuxuan Wang, Haizhou Li,\nand Zhizheng Wu. Sd-eval: A benchmark dataset for spoken dialogue understanding beyond words.arXiv\npreprint arXiv:2406.13340, 2024.\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie\nWang, Jun Tang, et al. Qwen2. 5-vl technical report.arXiv preprint arXiv:2502.13923, 2025.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. Ms marco: A human generated machine reading\ncomprehension dataset.arXiv preprint arXiv:1611.09268, 2016.\nSatanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved corre-\nlation with human judgments. InProceedings of the acl workshop on intrinsic and extrinsic evaluation\nmeasures for machine translation and/or summarization, pp. 65\u201372, 2005.\n12\nJannis Bulian, Christian Buck, Wojciech Gajewski, Benjamin Boerschinger, and Tal Schuster. Tomayto,\ntomahto. beyond token-level answer equivalence for question answering evaluation.arXiv preprint\narXiv:2202.07654, 2022.\nAnthony Chen, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Evaluating question answering eval-\nuation. InProceedings of the 2nd workshop on machine reading for question answering, pp. 119\u2013124,\n2019.\nCheng-Han Chiang and Hung-Yi Lee. Can large language models be an alternative to human evaluations?\nInProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pp. 15607\u201315631, 2023.\nJacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeepbidirectional\ntransformers for language understanding. InProceedings of the 2019 conference of the North American\nchapter of the association for computational linguistics: human language technologies, volume 1 (long and\nshort papers), pp. 4171\u20134186, 2019.\nMatthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, and Kyunghyun Cho. Searchqa:\nA new q&a dataset augmented with context from a search engine.arXiv preprint arXiv:1704.05179, 2017.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. Eli5: Long form\nquestion answering.arXiv preprint arXiv:1907.09190, 2019.\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. Mrqa 2019 shared task:\nEvaluating generalization in reading comprehension.arXiv preprint arXiv:1910.09753, 2019.\nJinlan Fu, See Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. InProceedings\nof the 2024 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long Papers), pp. 6556\u20136576, 2024.\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. Precise zero-shot dense retrieval without relevance\nlabels. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pp. 1762\u20131777, 2023.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings.\narXiv preprint arXiv:2104.08821, 2021.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card.arXiv preprint arXiv:2410.21276,\n2024.\nAlon Jacovi, Andrew Wang, Chris Alberti, Connie Tao, Jon Lipovetz, Kate Olszewska, Lukas Haas, Michelle\nLiu, Nate Keating, Adam Bloniarz, et al. The facts grounding leaderboard: Benchmarking llms\u2019 ability\nto ground responses to long-form input.arXiv preprint arXiv:2501.03200, 2025.\nYunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal\nreasoning in visual question answering. InProceedings of the IEEE conference on computer vision and\npattern recognition, pp. 2758\u20132766, 2017.\nSeungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig,\nMoontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized\nin evaluating other language models.arXiv preprint arXiv:2405.01535, 2024.\nTom\u00e1\u0161 Ko\u010disk` y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and\nEdward Grefenstette. The narrativeqa reading comprehension challenge.Transactions of the Association\nfor Computational Linguistics, 6:317\u2013328, 2018.\nKlaus Krippendorff. Computing krippendorff\u2019s alpha-reliability, 2011.\n13\nSatyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upad-\nhyay, and Manaal Faruqui. Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation,\n2024. URLhttps://arxiv.org/abs/2409.12941.\nHwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Joongbo Shin, and Ky-\nomin Jung. Kpqa: A metric for generative question answering using keyphrase weights.arXiv preprint\narXiv:2005.00192, 2020.\nJunlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. Generative judge for\nevaluating alignment.arXiv preprint arXiv:2310.05470, 2023a.\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object\nhallucination in large vision-language models.arXiv preprint arXiv:2305.10355, 2023b.\nZehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general\ntext embeddings with multi-stage contrastive learning.arXiv preprint arXiv:2308.03281, 2023c.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. InText summarization branches\nout, pp. 74\u201381, 2004.\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning,\n2023a.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. InNeurIPS, 2023b.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation\nusing gpt-4 with better human alignment. InProceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, pp. 2511\u20132522, 2023c.\nManLuo, ShailajaKeyurSampat, RileyTallman, YankaiZeng, ManuhaVancha, AkarshanSajja, andChitta\nBaral. \u2018just because you are right, doesn\u2019t mean i am wrong\u2019: Overcoming a bottleneck in development\nand evaluation of open-ended vqa tasks. InProceedings of the 16th Conference of the European Chapter\nof the Association for Computational Linguistics: Main Volume, 2021.\nMuhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards\ndetailed video understanding via large vision and language models. InProceedings of the 62nd Annual\nMeeting of the Association for Computational Linguistics (ACL 2024), 2024.\nOscar Ma\u00f1as, Benno Krojer, and Aishwarya Agrawal. Improving automatic vqa evaluation using large\nlanguage models. InProceedings of the AAAI Conference on Artificial Intelligence, 2024.\nMinesh Mathew, Dimosthenis Karatzas, R Manmatha, and CV Jawahar. Docvqa: A dataset for vqa on\ndocument images. corr abs/2007.00398 (2020).arXiv preprint arXiv:2007.00398, 2020.\nXuan-Phi Nguyen, Shrey Pandit, Senthil Purushwalkam, Austin Xu, Hailin Chen, Yifei Ming, Zixuan Ke,\nSilvioSavarese, CaimingXong, andShafiqJoty. Sfr-rag: Towardscontextuallyfaithfulllms.arXiv preprint\narXiv:2409.09916, 2024.\nArjun Panickssery, Samuel R Bowman, and Shi Feng. Llm evaluators recognize and favor their own genera-\ntions.arXiv preprint arXiv:2404.13076, 2024.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation\nof machine translation. InProceedings of the 40th annual meeting of the Association for Computational\nLinguistics, pp. 311\u2013318, 2002.\nJunsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, and Sanghyuk Choi. Offsetbias: Leveraging\ndebiased data for tuning evaluators.arXiv preprint arXiv:2407.06551, 2024.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for\nmachine comprehension of text.arXiv preprint arXiv:1606.05250, 2016.\n14\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks.arXiv\npreprint arXiv:1908.10084, 2019.\nJulian Risch, Timo M\u00f6ller, Julian Gutsch, and Malte Pietsch. Semantic answer similarity for evaluating\nquestion answering models.arXiv preprint arXiv:2108.06130, 2021.\nThibault Sellam, Dipanjan Das, and Ankur P Parikh. Bleurt: Learning robust metrics for text generation.\narXiv preprint arXiv:2004.04696, 2020.\nTu Shiwen, Zhao Liang, Chris Yuhao Liu, Liang Zeng, and Yang Liu. Skywork critic model series.https:\n//huggingface.co/Skywork, September 2024. URLhttps://huggingface.co/Skywork.\nAmanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and\nMarcus Rohrbach. Towards vqa models that can read. InProceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 8317\u20138326, 2019.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop ques-\ntions via single-hop question composition.Transactions of the Association for Computational Linguistics,\n10:539\u2013554, 2022.\nTu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, and Yun-Hsuan Sung. Foun-\ndational autoraters: Taming large language models for better automatic evaluation.arXiv preprint\narXiv:2407.10817, 2024.\nCunxiang Wang, Sirui Cheng, Qipeng Guo, Yuanhao Yue, Bowen Ding, Zhikun Xu, Yidong Wang, Xiangkun\nHu, Zheng Zhang, and Yue Zhang. Evaluating open-qa evaluation.Advances in Neural Information\nProcessing Systems, 36:77013\u201377042, 2023a.\nJiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu,\nand Jie Zhou. Is chatgpt a good nlg evaluator? a preliminary study. InProceedings of EMNLP Workshop,\npp. 1, 2023b.\nJiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating\nlarge video description models.arXiv preprint arXiv:2407.00634, 2024a.\nPeifeng Wang, Austin Xu, Yilun Zhou, Caiming Xiong, and Shafiq Joty. Direct judgement preference\noptimization, 2024b.\nPeiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and\nZhifang Sui. Large language models are not fair evaluators.arXiv preprint arXiv:2305.17926, 2023c.\nYidong Wang, Zhuohao Yu, Wenjin Yao, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya\nJiang, Rui Xie, Jindong Wang, et al. Pandalm: An automatic evaluation benchmark for llm instruction\ntuning optimization. InThe Twelfth International Conference on Learning Representations, 2023d.\nAustin Xu, Will Monroe, and Klinton Bicknell. Large language model augmented exercise retrieval for\npersonalized language learning. InProceedings of the 14th Learning Analytics and Knowledge Conference,\npp. 284\u2013294, 2024.\nAustin Xu, Srijan Bansal, Yifei Ming, Semih Yavuz, and Shafiq Joty. Does context matter? contextu-\naljudgebench for evaluating llm-based judges in contextual settings.arXiv preprint arXiv:2503.15620,\n2025.\nDejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video\nquestion answering via gradually refined attention over appearance and motion. InACM Multimedia,\n2017.\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video\nand language. InProceedings of the IEEE conference on computer vision and pattern recognition, pp.\n5288\u20135296, 2016.\n15\nAn Yang, Kai Liu, Jing Liu, Yajuan Lyu, and Sujian Li. Adaptations of rouge and bleu to better evaluate\nmachine reading comprehension task.arXiv preprint arXiv:1806.03578, 2018a.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In\nConference on Empirical Methods in Natural Language Processing (EMNLP), 2018b.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore: Evaluating generated text as text generation.\nAdvances in neural information processing systems, 34:27263\u201327277, 2021.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text\ngeneration with bert.arXiv preprint arXiv:1904.09675, 2019.\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, and Steffen Eger. Moverscore:\nText generation evaluating with contextualized embeddings and earth mover distance.arXiv preprint\narXiv:1909.02622, 2019.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, et al. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.\nAdvances in Neural Information Processing Systems, 36, 2024.\n16\nA Additional SMILE and baseline implementation details\nA.1 Prompt templates\nAs described in Section 4, we prompt the synthetic answer generator with the original question and ground-\ntruth answer and task it with generating a synthetic answer. We provide the full prompt below.\n### Synthetic answer generation prompts\n## System prompt:\nYou are an intelligent chatbot designed for generating answer as a sentence from question-answer pairs.\nYour task is to generate a single sentence answer using the question and the answer already provided. Here\u2019s\nhow you can accomplish the task:\n------\n##INSTRUCTIONS:\n- Look at the provided answer.\n- Generate a short single sentence response using the question and the answer.\n- Response SHOULD ALWAYS USE THE WORDS FROM ANSWER provided.\n- DO NOT USE THE QUESTION AS IT IS IN THE RESPONSE.\n- Return only the response and nothing else.\n## User prompt\nPlease phrase a short single sentence answer using question-answer pair only:\nQuestion: {<question>}\nAnswer: {<answer>}\nDO NOT PROVIDE ANY OTHER OUTPUT APART FROM A SINGLE SHORT SENTENCE.\nTo prompt GPT-4o and GPT-3.5 as judge models, we utilize prompts adopted from Maaz et al. (2024), as\ndescribed in Section 5. We provide full prompts below.\n### Original prompt: GPT-4o/GPT-3.5-Turbo judge prompts\n## System prompts\nYou are an intelligent chatbot designed for evaluating the correctness of generative outputs for\nquestion-answer pairs.\nYour task is to compare the predicted answer with the correct answer and determine if they match meaningfully.\nHere\u2019s how you can accomplish the task:\n------\n##INSTRUCTIONS:\n- Focus on the meaningful match between the predicted answer and the correct answer.\n- Consider synonyms or paraphrases as valid matches.\n- Evaluate the correctness of the prediction compared to the answer.\n## User prompt\nPlease evaluate the following video-based question-answer pair:\nQuestion: {<question>}\nCorrect Answer: {<answer>}\nPredicted Answer: {<model_output>}\nProvide your evaluation only as a yes/no and score where the score is an integer value between 0 and 5, with 5\nindicating the highest meaningful match.\nPlease generate the response in the form of a Python dictionary string with keys \u2019pred\u2019 and \u2019score\u2019, where\nvalue of \u2019pred\u2019 is a string of \u2019yes\u2019 or \u2019no\u2019 and value of \u2019score\u2019 is in INTEGER, not STRING.\nDO NOT PROVIDE ANY OTHER OUTPUT TEXT OR EXPLANATION. Only provide the Python dictionary string.\nFor example, your response should look like this: {{\u2019pred\u2019: \u2019yes\u2019, \u2019score\u2019: 4}}.\n### Extract prompt: GPT-4o/GPT-3.5-Turbo judge prompts\n## System prompts\nYou are an expert evaluator for video-based question answering systems. Your task is to judge the factual\naccuracy of a predicted answer by comparing it to a correct answer. You will follow a structured evaluation\napproach to ensure consistency:\n------\n## INSTRUCTIONS:\nStep 1: Extract the key facts from the Correct Answer.\nStep 2: In case the Correct Answer is a list, choose the best answer that matches the Predicted Answer.\nStep 3: Extract the key facts from the Predicted Answer.\nStep 4: Compare the two sets of facts and determine how consistent they are.\n- Consider paraphrasing, synonyms, and partial overlaps.\n- Ignore grammatical errors.\n- Penalize hallucinated or contradicted information.\nStep 4: Based on the comparison, assign a factual accuracy score between 0 and 5 (INTEGER only), where:\n17\n5 = Fully accurate and aligned\n4 = Mostly accurate, minor omissions or paraphrasing\n3 = Partially correct but with notable missing or incorrect info\n2 = Limited accuracy, mostly incorrect or unrelated\n1 = Completely inaccurate\n0 = No relation or total hallucination\nRespond strictly in the following format:\n{\u2019score\u2019: X, \u2019pred\u2019:Y} where X is an integer between 0 and 5 and Y is a either \u2019yes\u2019(X>3) or \u2019no\u2019(X<=3). Do not\ninclude any explanation or extra text.}\n## User prompt\nEvaluate the following video-based QA pair:\nQuestion: {<question>}\nCorrect Answer: {<answer>}\nPredicted Answer: {<model_output>}\nReturn your evaluation following the instructions above.\nDO NOT PROVIDE ANY OTHER OUTPUT TEXT OR EXPLANATION. Only provide the Python dictionary string.\nA.2 SMILE text processing\nAs a part of text pre-processing, we perform standard text normalization on words present in ground truth\nanswers and predictions. We first convert each string to lower case and remove all punctuation. Then,\neach word is lemmatized using POS-aware lemmatization to capture accurate base forms. If the resulting\nprocessed word is empty after these steps, the original lower-case word is retained.\nA.3 Metrics conversion to accuracy\nFor all evaluated baselines and metrics, we must convert from scores to binary correct or incorrect accuracy\nlabels. ROUGE, METEOR, BERTScore, sBERT, and SMILE all output continuous-valued scores between\n0 and 1. We apply a threshold of 0.67, considering anything above the threshold to be correct and anything\nbelow to be incorrect. The choice of 0.67 is the same as considering anything with a score of 4 or above to be\ncorrect after converting the continuous [0,1] score to a 0-5 scale with uniform binning. For GPT-3.5-Turbo\nand GPT-4o, the model is prompted to output a yes/no label for correctness, which we use directly.\nB Data annotation details\nB.1 Annotation instructions\nAnnotators were given with detailed instructions on how to annotate responses. We adopted a 3 point scale:\nclearly incorrect,unclear, andclearly correct. We defined each of these categories as follows:\nClearly incorrect:The model definitively produces a response that is incorrect.\nUnclear:The model response cannot be confirmed correct from the ground-truth answer.\nClearly correct:The model response can be explicitly verified as correct using the ground-truth answer.\nWe also defined edge case behavior:\nExtraneous information:If the model response correctly answers the question, but includes other infor-\nmation that may or may not be factual, we consider the responseclearly correct. As a concrete example,\nfor question \u201cWhat brand of soda is in this picture\u201d with ground-truth \u201cCoca-Cola\u201d, we consider the model\nresponse \u201cCoca-Cola is in this picture. It is the most popular soda in the world by unit sales and has over\n60 different flavors\u201d to be correct, even though it contains extraneous factually verifiable information.\nSynonyms or ambiguous subjects:We consider a model response that answers the question using an\nambiguous subject to beunclear. As a concrete example, for question \u201cwho describes a video game??\u201d\nwith ground-truth \u201cman\u201d, we consider the model response \u201cperson\u201d to be unclear, as it does not describe in\nsufficient detail the person.\n18\nB.2 Annotation aggregation and conversion to accuracy labels\nWe collected responses from four annotators. To aggregate individual annotations into a single label, we\nutilized majority vote, employing random tie-breaking as needed. To form final accuracy labels, we consider\nclearly correctresponses to be accurate and considerunclearandclearly incorrectresponses to be\ninaccurate.\nC SMILE Interpretability Examples\nAs discussed in Section 4.2, we provide detailed SMILE subscores in Figure 2. In the TGIF example from\nFigure 2, the model output shows a high semantic scores s(Equation (1)), reflecting strong relevance to the\nsynthetic answer. The lexical relevance scores l(Equation (2)) is also high, indicating a perfect overlap with\nthe ground truth. To clarify which word contributes most to the keyword score, we also return the word(s)\nwith the maximum similarity (\u201cmax sim words\u201d). These components together offer actionable insights into\nmodel strengths and weaknesses, helping guide targeted improvements.\nD Supplement ablation results\nIn this section, we present additional plots to supplement our Model Ablation described in Section 5.4.\nSpecifically, we include scatter plots and distribution plots to further illustrate the performance difference\nwhen varying model choices for synthetic answer generation and embedding.\nD.1 Synthetic answer generation ablation\nReferring to Figure 6, we see a very strong linear correlation between the two sets of generated synthetic\nanswers and thus backs our claim thatgenerating synthetic answers is a fairly simple taskas mentioned in\nSection 5.4. Figure 7, further bolster our claim, and highlights that the \u2019avg score\u2019 distribution remains very\nsimilar, hence we see a marginal difference in the performance as reported in table 6.\nD.2 Embedding model ablation\nFigure 8 and Figure 9 provides insight into the performance variation observed in Table 6, highlighting that\nkeyword scores exhibit greater sensitivity to the choice of embedding model compared to sentence scores.\n19\nFigure 6: Distribution analysis of SMILE sentence embedding scores across different synthetic answer sets.\nA strong linear relationship is observed between the two synthetic answer sets, indicating that synthetic\nanswers can reliably be generated using any state-of-the-art generation model.\n20\nFigure 7: Distribution analysis of \u2019SMILE avg scores\u2019 across different synthetic answer sets. We see a very\nsimilar score distribution, highlighting the fact the performance remains very similar.\n21\nFigure 8: Analyzing Sentence score distributions using different embedding models. Sentence scores show\nstronger linear correlation, indicating that it is robust to change in embedding model.\n22\nFigure 9: Analyzing Keyword score distributions using different embedding models. Keyword scores show\na linear correlation, but has some added noise, indicating that it is more sensitive to changes in embedding\nmodels.\n23\n",
    "title": "SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation",
    "arxiv_id": "2511.17432v1",
    "category": "recent",
    "authors": [
      "Shrikant Kendre",
      "Austin Xu",
      "Honglu Zhou",
      "Michael Ryoo",
      "Shafiq Joty",
      "Juan Carlos Niebles"
    ],
    "published": "2025-11-21",
    "num_pages": 23,
    "num_chars": 64821
  },
  {
    "text": "Towards fully differentiable neural ocean model with\nVeros\nEtienne Meunier1Said Ouala2Hugo Frezat3Julien Le Sommer3Ronan Fablet2\n1Inria Paris, France2IMT Atlantique, France3IGE, Grenoble, France\nAbstract\nWe present a differentiable extension of the VEROS ocean model H\u00e4fner et al.\n[2018], enabling automatic differentiation through its dynamical core. We describe\nthe key modifications required to make the model fully compatible with JAX\u2019s\nautodifferentiation framework and evaluate the numerical consistency of the result-\ning implementation. Two illustrative applications are then demonstrated: (i) the\ncorrection of an initial ocean state through gradient-based optimization, and (ii)\nthe calibration of unknown physical parameters directly from model observations.\nThese examples highlight how differentiable programming can facilitate end-to-end\nlearning and parameter tuning in ocean modeling. Our implementation is available\nonline .1\n1 Introduction\nEarth system models (ESMs) are widely used to study climate changes resulting from both an-\nthropogenic and natural perturbations. Over the past years, significant advances have been made\nthrough the development of new numerical schemes, refined physical parameterizations, and the use\nof increasingly powerful computers. Despite these advances, tuning ESMs to accurately reproduce\nhistorical data remains largely a manual process Hourdin et al. [2017], and persistent errors and\nbiases continue to challenge their accuracy.\nRecent advances in scientific machine learning motivated the development of new learning-based\nmethods for the design and calibration of ESMs Irrgang et al. [2021], Nguyen et al. [2023], Ouala et al.\n[2024], Dheeshjith et al. [2025]. In this context, one promising way of building climate models that\nare informed with Machine Learning (ML) is based on Differentiable Programing (DP) paradigms\nGelbrecht et al. [2023], Sapienza et al. [2024]. In particular, DP methods allow for designing\ncalibration methods that can be trained end-to-end Frezat et al. [2022], Kochkov et al. [2024] which,\nin turn, has the potential to unify and solve various challenges in climate modeling including, model\nbias correction, parameter tuning and the design of subgrid-scale parameterizations that are informed\nby observations. However, most existing climate models are implemented in programming languages\nthat do not natively support automatic differentiation. Moreover, they are often based on design\nchoices aimed towards memory efficiency (e.g., in-place operations), which prevents the use of\nautomatic differentiation methods.\nDespite these limitations, recent studies have explored the development of dynamical cores that\nnatively support automatic differentiation and can be trained end-to-end [e.g. Kochkov et al., 2024,\nfor atmospheric simulations]. In this work, we take a step in this direction by exploring ocean model\ncalibration within a differentiable programming framework. Specifically, we use the VERsatile\nOcean Simulator (VEROS) H\u00e4fner et al. [2018], which contains a JAX backend, to evaluate the\nability of using DP to ocean model tuning. We show how we adapted VEROS to support JAX\u2019s\nautomatic differentiation and present examples demonstrating the use of both forward and reverse\nmode differentiation for state and parameter calibration.\n1https://github.com/Etienne-Meunier/Veros-Autodiff\nPreprint.arXiv:2511.17427v1  [cs.LG]  21 Nov 2025\n2 Differentiability properties in Veros\n2.1 Modifications to Veros\nTo enable differentiation through the Veros step function, we implemented several key modifications\nto ensure compatibility with JAX\u2019s automatic differentiation framework.\nFunctionally pure step function.The original Veros step function employs in-place operations that\ndirectly modify state variables, which violates the functional purity requirements of JAX automatic\ndifferentiation. To address this incompatibility, we developed a wrapper function that creates a\ncopy of the input state and returns the updated state without modifying the original input. This\napproach ensures functional purity for both forward- and reverse-mode gradient computations. We\nalso implemented a selective differentiation wrapper that freezes all variables except for a specified\nsubset, limiting gradient computation to only the parameters of interest.\nHandling functions with singular gradients.While most operations in Veros are inherently\ndifferentiable, certain functions exhibit undefined or singular gradients that can lead to numerical\ninstabilities. For example, the square root function f(x) =\u221axhas an undefined gradient at x= 0 .\nTo address this, we preserve the original forward computation but define custom backward passes.\nSpecifically, for the square root, we implement the regularized gradient f\u2032(x)\u225c(2p\nmax(x, \u03b5))\u22121\nwhere \u03b5 >0 is a small regularization parameter that prevents singularities at the origin. JAX\u2019s custom\ngradient functionality enables this selective modification of backward passes.\nComputational graph preservation.We eliminated in-place type conversions that could break the\ncomputational graph required for automatic differentiation. These modifications primarily affected\ndiagnostic routines. Additionally, we replaced standard Python assert statements with JAX\u2019s\ncheckifyframework2to maintain compatibility with JAX transformations.\n2.2 Forward behaviour\nWe rigorously verified that our modifications preserve the original forward simulation behavior.\nAll control flow structures and operations were replaced with JAX-compatible equivalents where\nnecessary, and when adjustments were required, only gradient definitions were modified while\npreserving forward computations unchanged. To validate forward compatibility, we compared\ntemperature fields after 3000 integration steps between our modified implementation and the original\nVeros code. The relative error was approximately 10\u221210, which we attribute to floating-point precision\ndifferences between backends. The computational performance of forward simulation remains\nvirtually unchanged, as the core numerical operations are identical to the original implementation.\n2.3 Backward behaviour\nWe validated the correctness of computed gradients through comparison with finite-difference ap-\nproximations. For a composite function \u2113\u225cg(s(x)) :RS\u2192R , where srepresents the Veros step\nfunction andgis a scalar aggregation operator, we compute the gradient validation error:\nE=\r\r\r\r\u2207x\u2113\f\f\nx=w\u00b7k\u2212\u2113(w+\u03b5k)\u2212\u2113(w\u2212\u03b5k)\n2\u03b5\r\r\r\r\n2\u2113\u225cg(s(x)) :RS\u2192R\nHere,\u2207x\u2113is computed using automatic differentiation, kis a random unit vector, wis the evaluation\npoint, and \u03b5\u224810\u22124is the finite-difference step size. Over single-step Veros evaluations, this\nvalidation yields errors on the order of 10\u22127, confirming the accuracy of our gradient implementation.\nAdditional details regarding accuracy across multiple time steps are provided in the Appendix.\n2https://docs.jax.dev/en/latest/debugging/checkify_guide.html\n2\nFigure 1: Results from the initial field reconstruction experiment. Left: Loss and distance metrics\nduring optimization. Right: Temperature field snapshots showing the reference field, initial perturbed\nfield, and final reconstructed field after optimization.\n2.4 Toy experiment: gradient-based correction of initial states\nTo validate our gradient implementation and demonstrates practical utility for data assimilation, we\ndesign an inverse problem that reconstructs an original temperature field from a perturbed initial\ncondition through optimization across multiple Veros steps.\nSetup :Starting from a reference initial temperature field Tref(t= 0) , we create a perturbed version\nby adding a spatial Gaussian perturbation centered in the domain. We then minimize the L2 loss\nbetween temperature fields after l= 4 integration steps: L(T 0) =\u2225S(4)(T0)\u2212S(4)(Tref)\u22252\n2where\nSrepresents Veros integration step and T0is the optimized initial field. Gradient descent is used:\nT(k+1)\n0 =T(k)\n0\u2212\u03b1\u2207 T0L(T(k)\n0).\nResults :Figure 1 shows a successful field reconstruction. The left panel displays the loss function\n(L(T 0)) and distance metric ( \u2225T(k)\n0\u2212T ref\u22252\n2) during optimization\u2014both converge to near-zero. The\nright panel shows temperature snapshots: reference, perturbed initial, and final reconstructed fields,\nvisually confirming successful recovery of the original distribution.\n3 Application to model calibration problems\n3.1 Idealised Antarctic Circumpolar Current (ACC) model configuration\nWe use the ACC model configuration from the VEROS setup gallery at an eddy-permitting resolution\nof 1/4\u00b0. The configuration consists of a zonally re-entrant channel representing an idealized ACC,\nconnected to a northern ocean basin enclosed by land, representing the Atlantic ocean. The model\ndomain extends from 0\u00b0 to 60\u00b0E and from 40\u00b0S to 40\u00b0N, with 15 vertical levels resolving the\nwater column down to 2000 m depth. The circulation is forced by a meridionally-varying eastward\nwind stress over the ACC region, combined with a surface temperature relaxation. More details on\nsimulation parameters are provided in the appendix.\nThis simulation setup is used to demonstrate the potential of automatic differentiation for parameter\ncalibration in ocean models. We focus on the lateral viscosity ( Ah) and bottom friction coefficients\n(rbot), as these parameters strongly influence resolved mesoscale variability. Bottom friction is a\nprimary energy sink for mesoscale kinetic energy, which alters the vertical structure of resolved\neddies, and can change their amplitude and scales Arbic and Scott [2008]. Lateral viscosity controls\nsmall-scale dissipation and numerical stability, therefore affecting eddy lifetimes, spectral slopes\nand the cascade of energy/enstrophy Pearson et al. [2017]. The calibration of these two parameters\nprovide a framework to mimic calibration experiments based on satellite observations.\nA reference simulation is first generated using prescribed values for these parameters (see Tab. 1 for\ndetails). We then demonstrate that automatic differentiation enables the recovery of these reference\nvalues when starting from mis-specified parameter choices, relying solely on observations of the\nbarotropic streamfunction (BSF), which, in this idealized setup, serves as a proxy for large-scale\ncirculation features that are typically inferred from satellite observations of sea surface height (SSH).\n3\n3.2 Sensitivity analysis for parameter calibration\nThe left panel of Fig. 2 shows the gradient of the loss function with respect to both parameters,\ncomputed using forward-mode automatic differentiation. The loss is defined as the mean squared\nerror between the simulated and reference BSF. The gradient field is visualized using a quiver plot,\nwhere arrows indicate the direction and magnitude of the steepest descent in parameter space. Overall,\nthe gradient magnitudes differ significantly between the two parameters. The loss function exhibits\nstronger sensitivity to variations in Ah(horizontal direction) compared to rbot(vertical direction),\nas shown by the predominance of gradient vectors in the horizontal orientation for most regions of\nthe parameter space. This anisotropy suggests that lateral viscosity has a more direct influence on\nthe large-scale circulation patterns captured by the BSF. Beyond this scale difference in gradient\nmagnitudes, the direction of the gradient vectors generally points toward a convergence region that is\nconsistent with the location of the true parameter values (marked by red crosses), which suggests that\ngradient-based optimization is suitable for recovering the true values of these parameters.\nBarotropic streamfunction (m3/s)Latitude (\u00b0N)\nLongitude (\u00b0E)6120 \nReference runUncalibrated model simulationTuned model simulation\nFigure 2:Left:Loss landscape for the calibration task. Color indicates the loss for each combination\nof parameter and vector field gradients direction.Right:Results of calibration experiment. ACC\ntransport (Sverdrup) and Snapshots of the barotropic streamfunction.\n3.3 Calibration experiment analysis\nIn this experiment, we evaluate the ability of using gradient-based optimization to tune model\nparameters using observations of the BSF. As highlighted in right panel of Fig. 2, the uncalibrated\nmodel uses values of Ahandrbotthat result in quantitative changes in the ACC transport and in the\ndistribution of mesoscale eddies, illustrated by the BSF anomaly relative to the long-term mean.\nOverall, the gradient-based calibration yields parameter values that are very close to the reference\nvalues, and the resulting calibrated simulation closely reproduces the reference run, both in terms of\nACC transport strength and the spatial distribution of mesoscale eddies.\n4 Conclusion and future work\nIn this work, we introduce modifications to the Veros ocean simulator to enable automatic differentia-\ntion capabilities. We demonstrate gradient computation through two examples: correcting an initial\nstate and calibrating physical parameters based on a reference simulation. This provides essential\ntools for online learning of parameterizations and model correction in ocean modeling.\nOur implementation successfully computes accurate gradients through the complex nonlinear dy-\nnamics of Veros in the demonstrated scenarios, opening possibilities for data assimilation, parameter\nestimation, and physics-informed machine learning in oceanography. The validation experiments\nconfirm gradient accuracy and practical utility, though further investigation is needed to assess\nperformance over extended simulation horizons.\nFuture work will focus on optimizing gradient computation performance, particularly memory usage\nand processing time for long rollouts. Key directions include implementing implicit differentiation\nschemes Blondel et al. [2022] through iterative solvers in vertical mixing schemes, which currently\nrepresent computational bottlenecks. We also plan to explore hybrid approaches combining automatic\ndifferentiation with adjoint sensitivity methods Pontryagin [2018] for improved efficiency in long-\nhorizon problems, while conducting experiments to validate gradient accuracy over extended temporal\nscales.\n4\nAcknowledgments and Disclosure of Funding\nThis study is a contribution to AI4PEX (Artificial Intelligence and Machine Learning for Enhanced\nRepresentation of Processes and Extremes in Earth System Models), a project supported by the\nEuropean Union\u2019s Horizon Europe research and innovation programme under grant agreement no.\n101137682. This study also contributes to EDITO Model Lab, a project supported by the European\nUnion\u2019s Horizon Europe research and innovation programme under grant agreement no. 101093293.\nThis study has received funding from Agence Nationale de la Recherche - France 2030 as part of the\nPEPR TRACCS programme under grant number ANR-22-EXTR-0006 and ANR-22-EXTR-0007.\nReferences\nBrian K. Arbic and Robert B. Scott. On quadratic bottom drag, geostrophic turbulence, and oceanic\nmesoscale eddies.Journal of Physical Oceanography, 38(1):84\u2013103, January 2008. ISSN 0022-\n3670. doi: 10.1175/2007jpo3653.1. URLhttp://dx.doi.org/10.1175/2007JPO3653.1.\nMathieu Blondel, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan Hoyer, Felipe Llinares-L\u00f3pez,\nFabian Pedregosa, and Jean-Philippe Vert. Efficient and modular implicit differentiation.Advances\nin neural information processing systems, 35:5230\u20135242, 2022.\nSurya Dheeshjith, Adam Subel, Alistair Adcroft, Julius Busecke, Carlos Fernandez-Granda, Shubham\nGupta, and Laure Zanna. Samudra: An ai global ocean emulator for climate.Geophysical Research\nLetters, 52(10):e2024GL114318, 2025.\nHugo Frezat, Julien Le Sommer, Ronan Fablet, Guillaume Balarac, and Redouane Lguensat. A\nposteriori learning for quasi-geostrophic turbulence parametrization.Journal of Advances in\nModeling Earth Systems, 14(11):e2022MS003124, 2022.\nMaximilian Gelbrecht, Alistair White, Sebastian Bathiany, and Niklas Boers. Differentiable pro-\ngramming for earth system modeling.Geoscientific Model Development, 16(11):3123\u20133135,\n2023.\nDion H\u00e4fner, Ren\u00e9 L\u00f8we Jacobsen, Carsten Eden, Mads RB Kristensen, Markus Jochum, Roman\nNuterman, and Brian Vinter. Veros v0. 1\u2013a fast and versatile ocean simulator in pure python.\nGeoscientific Model Development, 11(8):3299\u20133312, 2018.\nFr\u00e9d\u00e9ric Hourdin, Thorsten Mauritsen, Andrew Gettelman, Jean-Christophe Golaz, Venkatramani\nBalaji, Qingyun Duan, Doris Folini, Duoying Ji, Daniel Klocke, Yun Qian, et al. The art and\nscience of climate model tuning.Bulletin of the American Meteorological Society, 98(3):589\u2013602,\n2017.\nChristopher Irrgang, Niklas Boers, Maike Sonnewald, Elizabeth A Barnes, Christopher Kadow,\nJoanna Staneva, and Jan Saynisch-Wagner. Towards neural earth system modelling by integrating\nartificial intelligence in earth system science.Nature Machine Intelligence, 3(8):667\u2013674, 2021.\nDmitrii Kochkov, Janni Yuval, Ian Langmore, Peter Norgaard, Jamie Smith, Griffin Mooers, Milan\nKl\u00f6wer, James Lottes, Stephan Rasp, Peter D\u00fcben, et al. Neural general circulation models for\nweather and climate.Nature, pages 1\u20137, 2024.\nTung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K Gupta, and Aditya Grover. Climax:\nA foundation model for weather and climate.arXiv preprint arXiv:2301.10343, 2023.\nSaid Ouala, Bertrand Chapron, Fabrice Collard, Lucile Gaultier, and Ronan Fablet. Online calibration\nof deep learning sub-models for hybrid numerical modeling systems.Communications Physics, 7\n(1):402, 2024.\nBrodie Pearson, Baylor Fox-Kemper, Scott Bachman, and Frank Bryan. Evaluation of scale-aware\nsubgrid mesoscale eddy models in a global eddy-rich model.Ocean Modelling, 115:42\u201358, July\n2017. ISSN 1463-5003. doi: 10.1016/j.ocemod.2017.05.007. URL http://dx.doi.org/10.\n1016/j.ocemod.2017.05.007.\nLev Semenovich Pontryagin.Mathematical theory of optimal processes. Routledge, 2018.\n5\nFacundo Sapienza, Jordi Bolibar, Frank Sch\u00e4fer, Brian Groenke, Avik Pal, Victor Boussange, Patrick\nHeimbach, Giles Hooker, Fernando P\u00e9rez, Per-Olof Persson, et al. Differentiable programming for\ndifferential equations: A review.arXiv preprint arXiv:2406.09699, 2024.\n6\nAppendix\n4.1 Experimental configuration\nOur experiments utilize Veros with the JAX backend and the scipy_jax solver for linear systems.\nWe enable the energetically consistent kinetic energy (EKE) parameterization while disabling both\nthe turbulent kinetic energy (TKE) and neutral diffusion parameterizations to focus on the core\ndifferentiable dynamics.\n4.2 Gradient validation\nTo assess the effect of repeated time-stepping on gradient accuracy, we compute\u2202L\n\u2202rbot, where rbot\nis the bottom drag coefficient tuned in our first experiment. As shown in Fig. 4, the discrepancy\nbetween gradient estimation methods increases with the number of time steps.\nFigure 3: Gradient of the loss with respect to rbotand the accuracy of automatic differentia-\ntion methods over iterations.Left:Evolution of \u2202L/\u2202r botfor different methods.Right:Ac-\ncuracy of forward-mode (JVP) and reverse-mode (VJP) automatic differentiation, computed as\n1\u2212|autodiff grad\u2212finite diff|/|finite diff| , highlighting the agreement with numerical gradients over\niterations.\n4.3 Gradient computational cost\nWe analyze the computational overhead of gradient computation by measuring the cost of vector-\nJacobian products (VJP) through Veros integration steps using the experimental setup from Section 2.4.\nFigure 4 demonstrates that gradient computation cost scales linearly with the number of integration\nsteps.\nFigure 4: Gradient computation time using vector-Jacobian products (VJP) for optimizing the initial\ntemperature field with respect to a loss function evaluated after multiple Veros integration steps.\n7\n4.4 ACC model configuration\nTable 1: Model configuration parameters for the idealized ACC simulation. Parameters marked with\n* are the calibration targets in our experiments. For detailed descriptions of these variables, refer to\nthe VEROS documentation.\nParameter Value\nDomain configuration\nResolution 1/4\u00b0\nDomain extent 0\u00b0\u201360\u00b0E, 40\u00b0S\u201340\u00b0N\nVertical levels 15\nMaximum depth 2000 m\nTime stepping\n\u2206tmom 1200 s\n\u2206ttracer 1200 s\nRun length 4 years\nFriction and mixing parameters\nA\u2217\nh 3435.5036038313715m2/s\nr\u2217\nbot 10\u22125s\u22121\nKiso 62.5 m2/s\nKGM 62.5 m2/s\nEddy kinetic energy (EKE)\nck(EKE) 0.4\nc\u03b5(EKE) 0.5\nEKE max 625 m2/s2\nLmin 1 m\nTurbulent kinetic energy (TKE)\nck(TKE) 0.1\nc\u03b5(TKE) 0.7\n\u03b1TKE 30.0\n\u03bamin\nM 2\u00d710\u22124m2/s\n\u03bamin\nH 2\u00d710\u22125m2/s\n8\n",
    "title": "Towards fully differentiable neural ocean model with Veros",
    "arxiv_id": "2511.17427v1",
    "category": "recent",
    "authors": [
      "Etienne Meunier",
      "Said Ouala",
      "Hugo Frezat",
      "Julien Le Sommer",
      "Ronan Fablet"
    ],
    "published": "2025-11-21",
    "num_pages": 8,
    "num_chars": 20439
  },
  {
    "text": "Self-Supervised Learning by Curvature Alignment\nBenyamin Ghojogh1M.Hadi Sepanj2Paul Fieguth2\nAbstract\nSelf-supervised learning (SSL) has recently ad-\nvanced through non-contrastive methods that cou-\nple an invariance term with variance, covariance,\nor redundancy-reduction penalties. While such\nobjectives shape first- and second-order statistics\nof the representation, they largely ignore the lo-\ncal geometry of the underlying data manifold. In\nthis paper, we introduceCurvSSL, a curvature-\nregularized self-supervised learning framework,\nand its RKHS extension,kernel CurvSSL. Our\napproach retains a standard two-view encoder\u2013\nprojector architecture with a Barlow Twins-style\nredundancy-reduction loss on projected features,\nbut augments it with a curvature-based regular-\nizer. Each embedding is treated as a vertex whose\nknearest neighbors define a discrete curvature\nscore via cosine interactions on the unit hyper-\nsphere; in the kernel variant, curvature is com-\nputed from a normalized local Gram matrix in\nan RKHS. These scores are aligned and decorre-\nlated across augmentations by a Barlow-style loss\non a curvature-derived matrix, encouraging both\nview invariance and consistency of local manifold\nbending. Experiments on MNIST and CIFAR-\n10 datasets with a ResNet-18 backbone show\nthat curvature-regularized SSL yields competi-\ntive or improved linear evaluation performance\ncompared to Barlow Twins and VICReg. Our\nresults indicate that explicitly shaping local ge-\nometry is a simple and effective complement to\npurely statistical SSL regularizers.\nBenyamin Ghojogh and M.Hadi Sepanj contributed equally to\nthis work.1Artificial Intelligence Scientist, Waterloo, Ontario,\nCanada2Vision and Image Processing Group, Systems Design\nEngineering, University of Waterloo, Ontario, Canada. Corre-\nspondence to: Benyamin Ghojogh <bghojogh@uwaterloo.ca >,\nM.Hadi Sepanj <mhsepanj@uwaterloo.ca >, Paul Fieguth\n<paul.fieguth@uwaterloo.ca>.1. Introduction\nSelf-supervised learning (SSL) has become a central\nparadigm for visual representation learning, replacing ex-\nplicit labels with surrogate objectives defined over aug-\nmented views of the same image (Sepanj et al., 2025a; Rani\net al., 2023). Contrastive methods such as InfoNCE-based\napproaches (Sepanj & Fiegth, 2025; Chen et al., 2020) max-\nimize agreement between positive pairs while repelling neg-\natives, whereas recent non-contrastive methods (Grill et al.,\n2020; Bardes et al., 2022; Sepanj & Fieguth, 2024; Sep-\nanj et al., 2025b) avoid explicit negatives by combining an\ninvariance term with variance, covariance, or redundancy-\nreduction penalties. Architectures such as Barlow Twins\n(Zbontar et al., 2021) and VICReg (Bardes et al., 2022)\nenforce that two augmentations of the same sample pro-\nduce highly correlated embeddings along the diagonal of a\ncross-correlation matrix, while off-diagonal terms and per-\ndimension variances are controlled to prevent dimensional\ncollapse.\nThese approaches, however, still treat the representation\nspace primarily as a flat Euclidean vector space and reg-\nularize it using first- and second-order statistics (means,\nvariances, cross-correlations). From a geometric point of\nview, high-dimensional data are often assumed to concen-\ntrate around a lower-dimensional manifold embedded in\nfeature space. Standard SSL objectives encourage different\naugmentations of the same input to map to nearby points on\nthis manifold and to occupy decorrelated feature dimensions\nglobally, but they do not explicitly control thelocalgeome-\ntry of the learned manifold. In particular, two augmentations\nmay be close in Euclidean distance yet induce different local\nneighborhoods, tangent directions, or higher-order bending\nof the manifold. As a consequence, embeddings can satisfy\ninvariance and redundancy-reduction constraints while still\ndistorting the local structure that underlies nearest-neighbor\nretrieval, clustering, or semi-supervised learning.\nIn differential and discrete geometry, curvature quantifies\nhow a surface or manifold bends in a neighborhood of a\npoint. For polyhedra, classical constructions based on angu-\nlar defect measure how much the sum of face angles around\na vertex deviates from 2\u03c0(Descartes, 1890; Markvorsen,\n1996; Coxeter, 1973; Richeson, 2019; Hilton & Pedersen,\n1982). The sharper the corner, the larger the defect. A\n1arXiv:2511.17426v1  [cs.LG]  21 Nov 2025\nSelf-Supervised Learning by Curvature Alignment\nclosely related discrete viewpoint for data is to imagine\neach point as a vertex of a hypothetical polyhedron whose\nfaces are spanned by its knearest neighbors (Ghojogh et al.,\n2020). By translating neighbor differences to the origin,\nnormalizing them onto a unit hypersphere, and aggregating\ncosine similarities between neighbor directions, one obtains\na scalarcurvature scorethat reflects how sharply the local\nneighborhood bends around that point. This construction\ncan be further generalized to reproducing kernel Hilbert\nspaces (RKHS) (Gretton, 2013; Sepanj et al., 2025b) by ex-\npressing inner products and norms through a kernel function,\nand normalizing the corresponding local Gram matrix.\nMotivated by this geometric perspective, we propose\ncurvature-regularized self-supervised learning (CurvSSL)\nalong with its kernel versionkernel CurvSSL. This method is\na simple non-contrastive SSL objective that integrates curva-\nture into the learning signal. We retain a standard two-view\nencoder\u2013projector architecture and a redundancy-reduction\nterm in the spirit of Barlow Twins (Zbontar et al., 2021),\nwhich encourages diagonal cross-correlation between the\nprojected embeddings of two augmentations while driving\noff-diagonal correlations toward zero. On top of this, we\ntreat each projected embedding as a vertex in representation\nspace, compute a discrete curvature score from its knearest\nneighbors via cosine interactions on the unit hypersphere,\nand use these scores to define an additional, geometry-aware\nregularizer. At the batch level, we align curvature across aug-\nmentations of the same samples and decorrelate curvature\npatterns across different samples using a Barlow Twins-style\nloss on a curvature-derived matrix. In this way, the objec-\ntive does not only enforce invariance and low redundancy\nin the coordinates of the embeddings, but also promotes\nconsistency and diversity in the local bending of the learned\nmanifold.\nOverall, CurvSSL can be viewed as a curvature-regularized\nvariant of non-contrastive SSL: the backbone loss still en-\nforces view-invariance and redundancy reduction, yet the\nrepresentation is further constrained to preserve local man-\nifold geometry as captured by discrete curvature in the\nembedding space (and, in an extension, kernelized curva-\nture in an RKHS). We show experimentally that this sim-\nple curvature-aware modification of a ResNet-based SSL\npipeline yields competitive representations, indicating that\nexplicitly shaping local geometry is a promising comple-\nment to purely statistical regularizers.\n2. Background on Polyhedron Curvature and\nAngular Defect\nApolytopeis a geometrical object in Rdwhose faces are\nplanar. The special cases of polytope in R2andR3are called\npolygonandpolyhedron, respectively. Some examples for\npolyhedron are cube, tetrahedron, octahedron, icosahedron,and dodecahedron with four, eight, and twenty triangular\nfaces, and twelve flat faces, respectively (Coxeter, 1973).\nConsider a polygon where \u03c4jand\u00b5jare the interior and\nexterior angles at the j-th vertex; we have \u03c4j+\u00b5j=\u03c0.\nA similar analysis holds in R3for Fig. 1-a. In this figure,\na vertex of a polyhedron and its opposite cone are shown\nwhere the opposite cone is defined to have perpendicular\nfaces to the faces of the polyhedron at the vertex. The\nintersection of a unit sphere centered at the vertex and the\nopposite cone is shown in the figure. This intersection\nis a geodesic on the unit sphere. According to Thomas\nHarriot\u2019s theorem proposed in 1603 (Markvorsen, 1996),\nif this geodesic on the unit sphere is a triangle, its area is\n\u00b51+\u00b5 2+\u00b5 3\u2212\u03c0= 2\u03c0\u2212(\u03c4 1+\u03c42+\u03c43). The generalization\nof this theorem from a geodesic triangular polygon ( 3-gon)\nto ank-gon is (Markvorsen, 1996):\n\u00b51+\u00b7\u00b7\u00b7+\u00b5 k\u2212k\u03c0+ 2\u03c0= 2\u03c0\u2212kX\na=1\u03c4a,(1)\nwhere the polyhedron haskfaces meeting at the vertex.\nRen\u00b4e Descartes\u2019sangular defectat a vertex xof a polyhe-\ndron is (Descartes, 1890):\nD(x) := 2\u03c0\u2212kX\na=1\u03c4a.(2)\nThe total defect of a polyhedron is defined as the summation\nof the defects over the vertices. It can be shown that the\ntotal defect of a polyhedron withvvertices,eedges, andf\nfaces is:\nD:=vX\ni=1D(x i) = 2\u03c0(v\u2212e+f).(3)\nThe term v\u2212e+f is Euler-Poincar \u00b4e characteristic of the\npolyhedron (Richeson, 2019; Hilton & Pedersen, 1982);\ntherefore, the total defect of a polyhedron is equal to its\nEuler-Poincar \u00b4e characteristic. According to Fig. 1-b, the\nsmaller\u03c4angles result in sharper corner of the polyhedron.\nTherefore, we can consider the angular defect as thecurva-\ntureof the vertex.\n3. Curvature Calculation for Data Points\n3.1. Curvature Calculation in the Input Space\nThe main idea of thecurvature calculation of data points\nis as follows (Ghojogh et al., 2020). Every data point is\nconsidered to be the vertex of a hypothetical polyhedron (see\nFig. 1-a). For every point, we find its k-Nearest Neighbors\n(k-NN). The kneighbors of the point (vertex) form the k\nfaces of a polyhedron meeting at that vertex. Then, the\nmore curvature that point (vertex) has, the more anomalous\n2\nSelf-Supervised Learning by Curvature Alignment\nFigure 1. (a) Polyhedron vertex, unit sphere, and the opposite cone,\n(b) large and small curvature, (c) a point and its neighbors normal-\nized on a unit hyper-sphere around it.\nit is because it is far away (different) from its neighbors.\nTherefore, we define acurvature score, denoted by c, which\nis proportional to the curvature or angular effect.\nSince, according to the equation of angular effect, the cur-\nvature is proportional to negative summation of angles, we\ncan consider the curvature score to be inversely proportional\nto the summation of angles. Without loss of generality, we\nassume the angles are in the range [0, \u03c0] (otherwise, we take\nthe smaller angle). The less the angles between two edges\nof the polyhedron, the more their cosine. As the curvature\nscore is inversely proportional to the angles, we can use\ncosine for the curvature score:\nc(xi)\u221d1\n\u03c4a\u221dcos(\u03c4 a).(4)\nWe define the curvature score to be the summation of cosine\nof the angles of the polyhedron faces meeting at that point:\nc(xi) :=kX\na=1cos(\u03c4 a) =kX\na=1\u02d8x\u22a4\na\u02d8xa+1\n||\u02d8xa||2||\u02d8xa+1||2,(5)\nwhere \u02d8xa:=x a\u2212xiis the a-th edge of the polyhedron\npassing through the vertex xi,xais the a-th neighbor of xi,\nand\u02d8xa+1denotes the next edge sharing the same polyhe-\ndron face with \u02d8xawhere \u02d8xk+1=\u02d8x1.\nNote that finding the pairs of edges which belong to the\nsame face is difficult and time-consuming so we relax this\ncalculation to the summation of the cosine of angles between\nall pairs of edges meeting at the vertexx i:\nc(xi) :=k\u22121X\na=1kX\nb=a+1\u02d8x\u22a4\na\u02d8xb\n||\u02d8xa||2||\u02d8xb||2,(6)\nwhere \u02d8xa:=x a\u2212xi,\u02d8xb:=x b\u2212xi, andxaandxb\ndenote the a-th and b-th neighbors of xi. In Eq. (6), we haveomitted the redundant angles because of symmetry of inner\nproduct. Note that the Eq. (6) implies that we normalize the\nkneighbors of xito fall on the unit hyper-sphere centered\natxiand then compute their cosine similarities (see Fig.\n1-c).\nThe mentioned relaxation is valid for the following reason.\nTake two edges meeting at the vertex xi. If the two edges\nbelong to the same polyhedron face, the relaxation is ex-\nact. Consider the case where the two edges do not belong\nto the same face. These two edges are connected with a\nset of polyhedron faces. If we tweak one of the two edges\nto increase/decrease the angle between them, the angle of\nthat edge with its neighbor edge on the same face also in-\ncreases/decreases. Therefore, the changes in the additional\nangles of relaxation are consistent with the changes of the\nangles between the edges sharing the same faces.\n3.2. Curvature Calculation in the RKHS\nThe pattern of curvature of data points might not be linear.\nTherefore, we use thekernel curvatureto work on data\nin the RKHS (Ghojogh et al., 2020). In kernel curvature\ncalculation, the two stages of finding k-NN and calculating\nthe curvature score are performed in RKHS. Let \u03d5:X \u2192 H\nbe the pulling function mapping the data x\u2208 X to the\nRKHS H. In other words, x7\u2192\u03d5(x) . Let tdenote the\ndimensionality of the RKHS, i.e., \u03d5(x)\u2208Rtwhilex\u2208Rd.\nNote that we usually have t\u226bd . The kernel over two\nvectors x1andx2is the inner product of their pulled data\n(Hofmann et al., 2008; Ghojogh et al., 2023):\nR\u220bk(x 1,x2) :=\u03d5(x 1)\u22a4\u03d5(x 2).(7)\nThe Euclidean distance in the RKHS is (Sch \u00a8olkopf, 2001):\n||\u03d5(x i)\u2212\u03d5(x j)||2=q\nk(xi,xi)\u22122k(x i,xj) +k(x j,xj).\n(8)\nUsing this distance, we find the k-NN of the dataset in the\nRKHS.\nAfter findingk-NN in the RKHS, we calculate the score in\nthe RKHS. We pull the vectors \u02d8xaand\u02d8xbto the RKHS so\n\u02d8x\u22a4\na\u02d8xbis changed to k(\u02d8xa,\u02d8xb) =\u03d5( \u02d8xa)\u22a4\u03d5(\u02d8xb). LetKi\u2208\nRk\u00d7kdenote the kernel matrix of neighbors of xiwhose\n(a, b) -th element is k(\u02d8xa,\u02d8xb). The vectors in Eq. (6) are\nnormalized. In the RKHS, this is equivalent to normalizing\nthe kernel (Ah-Pine, 2010; Ghojogh et al., 2023):\nk\u2032(\u02d8xa,\u02d8xb) :=k(\u02d8xa,\u02d8xb)p\nk(\u02d8xa,\u02d8xa)k(\u02d8xb,\u02d8xb).(9)\nIfK\u2032\ni\u2208Rk\u00d7kdenotes the normalized kernel Ki, the kernel\ncurvature score in the RKHS is:\nc(xi) :=k\u22121X\na=1kX\nb=a+1K\u2032\ni,ab,(10)\n3\nSelf-Supervised Learning by Curvature Alignment\nwhere K\u2032\ni,abdenotes the (a, b) -th element of the normalized\nkernelK\u2032\ni.\n4. CurvSSL and Kernel CurvSSL\n4.1. Network and Data Settings\nThe neural network for self-supervised learning contains an\nencoder f\u03b8followed by a projection head g. LetX \u2282Rd\nbe the input space, f\u03b8:X \u2192Rdhan encoder, and g:\nRdh\u2192Rdza projection head. Suppose T(x) denotes the\ndistribution of training data. For every training data instance,\nwe draw two stochastic augmentations (x,x\u2032)\u223c T(x) and\npass them through the encoder and the projection head:\nh=f \u03b8(x)\u2208Rdh,z=g(h)\u2208Rdz,\nh\u2032=f\u03b8(x\u2032)\u2208Rdh,z\u2032=g(h\u2032)\u2208Rdz.(11)\nEvery mini-batch, with sizeb, is{(z i,z\u2032\ni)}b\ni=1.\n4.2. Loss Function\nWe now describe the proposed self-supervised objective.\nAs before, let {(zi,z\u2032\ni)}b\ni=1denote the projected embed-\ndings of two augmentations of a mini-batch of size b, where\nzi=g(f \u03b8(xi))andz\u2032\ni=g(f \u03b8(x\u2032\ni))\u2208Rdz. Our loss has\ntwo components: (i) a redundancy-reduction term on the em-\nbedding coordinates, in the spirit of Barlow Twins, and (ii) a\ncurvature-based term that aligns and decorrelates curvature\npatterns across the batch.\n4.2.1. REDUNDANCY REDUCTION IN EMBEDDING SPACE\nWe first normalize the projected embeddings per feature\ndimension:\n\u02dczi=zi\u2212\u00b5z\n\u03c3z+\u03b5, \u02dcz\u2032\ni=z\u2032\ni\u2212\u00b5\u2032\nz\n\u03c3\u2032z+\u03b5,(12)\nwhere the division is element-wise, \u00b5z,\u03c3z\u2208Rdzare the\nbatch-wise mean and standard deviation of {zi}b\ni=1, and\nsimilarly for \u00b5\u2032\nz,\u03c3\u2032\nzand{z\u2032\ni}b\ni=1;\u03b5 >0 is a small constant\nfor numerical stability. We then form the cross-correlation\nmatrix:\nC\u2208Rdz\u00d7dz,C uv:=1\nbbX\ni=1\u02dczi,u\u02dcz\u2032\ni,v,(13)\nwhere Cuvis the (u, v) -th element of Cand\u02dczi,udenotes\ntheu-th component of \u02dczi. Following Barlow Twins (Zbontar\net al., 2021), we enforce that the diagonal entries of Care\nclose to 1(strong agreement between views in each feature)\nwhile off-diagonal entries are close to 0(low redundancy\nbetween different features):\nLemb:=dzX\nu=1(Cuu\u22121)2+\u03bb embdzX\nu,v=1\nu\u0338=vC2\nuv,(14)where \u03bbemb>0 controls the strength of the off-diagonal\npenalty.\n4.2.2. CURVATURE-BASED REGULARIZATION\nIn addition to redundancy reduction at the coordinate level,\nwe regularize thelocal geometryof the learned manifold via\ncurvature. For each embedding zi, we compute a discrete\ncurvature score c(zi)by treating zias a vertex of a hypo-\nthetical polyhedron whose faces are spanned by its k-nearest\nneighbors in the embedding space. Let {zi,a}k\na=1denote\nthese neighbors, and define edge vectors \u02d8zi,a:=z i,a\u2212zi.\nNormalizing these edges onto the unit hypersphere and ag-\ngregating the pairwise cosine similarities between neighbor\ndirections yields the curvature score (see Eq. (6)):\nc(zi) :=k\u22121X\na=1kX\nb=a+1\u02d8z\u22a4\ni,a\u02d8zi,b\n\u2225\u02d8zi,a\u22252\u2225\u02d8zi,b\u22252,(15)\nwhich measures how sharply the local neighborhood around\nzibends. The kernel curvature score is (see Eq. (10)):\nc(zi) :=k\u22121X\na=1kX\nb=a+1K\u2032\ni,ab,(16)\nwhere K\u2032\ni,abis the (a, b) -th element of the normalized ker-\nnel kernelK\u2032\nibetween \u02d8zi,aand\u02d8zi,b.\nEqs. (15) and (16) can be used for curvature scores in\nCurvSSL and kernel CurvSSL loss functions, respectively.\nWe compute analogous curvature scores c(z\u2032\ni)for the second\nview.\nStacking the curvature scores into vectors c=\n[c(z 1), . . . , c(z b)]\u22a4andc\u2032= [c(z\u2032\n1), . . . , c(z\u2032\nb)]\u22a4\u2208Rb,\nwe first normalize them across the batch:\n\u02dcc=c\u2212\u00b5 c1\n\u03c3c+\u03b5, \u02dcc\u2032=c\u2032\u2212\u00b5\u2032\nc1\n\u03c3\u2032c+\u03b5,(17)\nwhere \u00b5c, \u03c3c\u2208R are the mean and standard deviation of c,\n\u00b5\u2032\nc, \u03c3\u2032\ncare those of c\u2032, the1\u2208Rbis the all-ones vector, and\n\u03b5 >0 is again a small constant. We then form a curvature-\nderived matrix:\nM\u2208Rb\u00d7b,M ij:=1\nb\u02dcci\u02dcc\u2032\nj,(18)\nwhere Mijdenotes the (i, j) -th element of M, which plays\nan analogous role to the cross-correlation matrix C, but now\nat thesamplelevel in terms of curvature. We encourage the\ncurvature of matched augmentations to agree (diagonal en-\ntries of Mclose to 1) and the curvature patterns of different\nsamples to be decorrelated (off-diagonals close to0):\nLcurv:=bX\ni=1(Mii\u22121)2+\u03bb curvbX\ni,j=1\ni\u0338=jM2\nij,(19)\n4\nSelf-Supervised Learning by Curvature Alignment\nwhere \u03bbcurv>0 controls the strength of curvature-based\nredundancy reduction.\n4.2.3. TOTAL OBJECTIVE AND KERNEL EXTENSION\nOur final self-supervised loss is a weighted sum of the\nembedding-level and curvature-level terms:\nL:=L emb+\u03b1 curvLcurv,(20)\nwhere \u03b1curv>0balances the influence of curvature regular-\nization. In the Euclidean case, i.e., CurvSSL, c(\u00b7)is given\nby Eq. (15). In the kernel curvature variant, i.e., kernel\nCurvSSL, Eq. (16) is used forc(\u00b7).\nThe proposed objective enforces view invariance and re-\ndundancy reduction at the level of embedding coordinates,\nwhile simultaneously shaping the local manifold geometry\nthrough curvature alignment and curvature-based decorrela-\ntion across the batch.\n5. Experiments\nWe empirically evaluate the proposed curvature-regularized\nself-supervised learning on two standard benchmarks,\nMNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky &\nHinton, 2009), using a ResNet backbone (He et al., 2016)\nand a two-stage protocol: (i) self-supervised pretraining\nwith the proposed CurvSSL and kernel CurvSSL objectives,\nand (ii) frozen-encoder linear evaluation. In addition, we\nvisualize the learned representations with UMAP (McInnes\net al., 2018) to inspect the geometry induced by curvature\nregularization.\n5.1. Experimental Setup\nDatasets.We consider MNIST (LeCun et al., 1998) and\nCIFAR-10 (Krizhevsky & Hinton, 2009) as two represen-\ntative image datasets of increasing difficulty. MNIST con-\nsists of grayscale handwritten digits (10 classes), while\nCIFAR-10 contains natural RGB images with more com-\nplex intra-class variability. For SSL pretraining, we use only\nthe training split of each dataset. For linear evaluation, we\nuse the standard training and test splits.\nNetwork and training details.For both datasets, we\nadopt a ResNet-18 (He et al., 2016) encoder f\u03b8followed\nby a two-layer MLP projection head gthat maps encoder\nfeatures to a dz-dimensional projection space. The self-\nsupervised model is trained using the curvature-regularized\nloss (20), where the embedding-level redundancy reduc-\ntionLembis instantiated as a Barlow Twins objective (14),\nand the curvature-level term Lcurvuses the discrete cur-\nvature score (15) or (16) with k-nearest neighbors in the\nprojected space. We use dz= 128 ,k= 10 neigh-\nbors, a mini-batch size of 256, and train the SSL modelTable 1. Linear evaluation accuracy (%) on MNIST and CIFAR-\n10 using a frozen ResNet-18 encoder pretrained with the SSL\nobjectives.\nMethod MNIST CIFAR-10\nVicReg (Bardes et al., 2022)95.9 74.5\nBarlow Twins (Zbontar et al., 2021)94.9 73.6\nCurvSSL (ours)97.9 75.1\nKernel CurvSSL (ours)98.4 76.5\nfor100epochs for MNIST and 500epochs for CIFAR-\n10 using Adam optimizer (Kingma, 2014) with learning\nrate10\u22123and weight decay 10\u22124. The curvature and em-\nbedding weights (\u03bbemb, \u03bbcurv, \u03b1curv)are selected once and\nreused across datasets. In all experiments, we simply set\n\u03bbemb=\u03bb curv=\u03b1 curv= 1. For kernel CurvAlign, radial\nbasis function (RBF) kernel function was employed.\nData augmentations.For MNIST, we follow common\npractice for SSL on digit images, applying random resized\ncrops, small rotations, and grayscale-to-RGB conversion,\nfollowed by per-channel normalization. For CIFAR-10, we\nadopt a standard augmentation pipeline with random resized\ncrops, horizontal flips, color jitter, random grayscale, and\nper-channel normalization. Two independent augmented\nviews are sampled for each image in a mini-batch and passed\nthrough the shared encoder\u2013projector.\n5.2. Linear Evaluation\nTo assess the quality of the learned representations, we\nperform linear evaluation following the standard protocol.\nAfter SSL pretraining, we freeze the encoder f\u03b8and discard\nthe projection head g. A small classifier consisting of a\nlinear layer with one hidden layer and batch normalization\n(as described in Section 3) is trained on top of the frozen\nencoder features using cross-entropy loss. Only the classifier\nparameters are updated; the encoder weights remain fixed.\nWe train the linear classifier for a fixed number of epochs\n(e.g., 50) with SGD and report top-1 test accuracy on\nMNIST and CIFAR-10. The results are summarized in\nTable 1. Overall, CurvSSL and kernel CurvSSL achieve\ncompetitive linear probe performance on both datasets, indi-\ncating that enforcing both redundancy reduction and curva-\nture consistency produces representations that transfer well\nto supervised classification. On CIFAR-10, which is more\nchallenging, we observe that the curvature term does not\nprevent the model from learning discriminative features and\ncan improve class separation compared to using redundancy\nreduction alone. Moreover, as expected, kernel CurvSSL\nperforms better than CurvSSL because of handling nonlin-\nearities better through RKHS.\n5\nSelf-Supervised Learning by Curvature Alignment\n(a)CurvSSL (Euclidean).\n(b)Kernel CurvSSL.\nFigure 2. UMAP visualization of encoder features on MNIST after\ncurvature-regularized SSL. Points are colored by ground-truth digit\nclass.\n5.3. UMAP Visualization of Learned Representations\nBeyond scalar accuracy, we study the geometry of the\nlearned representations via UMAP embeddings. For each\ndataset, we extract features from the frozen encoder on a\nheld-out split (train or test) and project them to two di-\nmensions using UMAP with a fixed configuration (e.g.,\nnneighbors = 15 ,min dist= 0.1 ). We visualize either the en-\ncoder features hor the projected features z, coloring points\nby ground-truth class labels.\nFigure 2 compares Euclidean CurvSSL and its kernel variant\non MNIST. Clusters corresponding to different digits are\nwell separated, with relatively smooth transitions between\nnearby classes (e.g., visually similar digits such as \u20183\u2019 and\n\u20185\u2019). The curvature regularization encourages local neighbor-\n(a)CurvSSL (Euclidean).\n(b)Kernel CurvSSL.\nFigure 3. UMAP visualization of encoder features on CIFAR-10\nafter curvature-regularized SSL. Points are colored by ground-truth\nclass.\nhoods to be geometrically consistent across augmentations,\nwhich manifests as tighter and more coherent class clusters\nin the UMAP plot.\nAs depicted in Fig. 3 for CIFAR-10, both CurvSSL and Ker-\nnel CurvSSL produce embeddings that form more complex\nstructures, reflecting the higher intra-class variability of nat-\nural images. Nonetheless, we observe that classes occupy\ndistinct regions with meaningful local neighborhoods.\n6. Conclusion\nWe proposed geometry-aware self-supervised objectives,\nnamed CurvSSL and kernel CurvSSL, that augment a Bar-\nlow Twins-style redundancy reduction loss with curvature-\nbased regularizations. By treating each embedding as a\n6\nSelf-Supervised Learning by Curvature Alignment\nvertex with a discrete curvature score computed from its\nk-nearest neighbors on the unit hypersphere, and coupling\nthese scores across augmentations and samples via a curva-\nture\u2013Barlow loss, our method encourages both view invari-\nance and consistency of local manifold geometry.\nOn MNIST and CIFAR-10, curvature-regularized SSL\nyields competitive linear evaluation accuracy and well-\nstructured UMAP embeddings, suggesting that explicitly\nshaping local geometry complements standard invariance\nand redundancy-reduction terms. The method is simple to\nintegrate into existing two-view pipelines and admits a ker-\nnel extension, making it a practical starting point for further\ngeometric SSL work on larger datasets, architectures, and\nmanifold-sensitive tasks such as semi-supervised learning\nand retrieval.\nReferences\nAh-Pine, J. Normalized kernels as similarity indices. In\nPacific-Asia Conference on Knowledge Discovery and\nData Mining, pp. 362\u2013373. Springer, 2010.\nBardes, A., Ponce, J., and LeCun, Y . VICReg: Variance-\ninvariance-covariance regularization for self-supervised\nlearning. InInternational Conference on Learning Rep-\nresentations, 2022.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\nsimple framework for contrastive learning of visual rep-\nresentations. InInternational conference on machine\nlearning, pp. 1597\u20131607. PmLR, 2020.\nCoxeter, H. S. M.Regular polytopes. Courier Corporation,\n1973.\nDescartes, R. Progymnasmata de solidorum elementis.Oeu-\nvres de Descartes, X:265\u2013276, 1890.\nGhojogh, B., Karray, F., and Crowley, M. Anomaly detec-\ntion and prototype selection using polyhedron curvature.\nInCanadian Conference on Artificial Intelligence, pp.\n238\u2013250. Springer, 2020.\nGhojogh, B., Crowley, M., Karray, F., and Ghodsi, A. Back-\nground on kernels.Elements of Dimensionality Reduction\nand Manifold Learning, pp. 43\u201373, 2023.\nGretton, A. Introduction to RKHS, and some simple kernel\nalgorithms.Adv. Top. Mach. Learn. Lecture Conducted\nfrom University College London, 16(5-3):2, 2013.\nGrill, J.-B., Strub, F., Altch \u00b4e, F., Tallec, C., Richemond, P.,\nBuchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z.,\nGheshlaghi Azar, M., et al. Bootstrap your own latent-a\nnew approach to self-supervised learning.Advances in\nneural information processing systems, 33:21271\u201321284,\n2020.He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. InProceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 770\u2013778, 2016.\nHilton, P. and Pedersen, J. Descartes, Euler, Poincare,\nPolya and polyhedra.S \u00b4eminaire de Philosophie et\nMath \u00b4ematiques, (8):1\u201317, 1982.\nHofmann, T., Sch \u00a8olkopf, B., and Smola, A. J. Kernel meth-\nods in machine learning.The annals of statistics, pp.\n1171\u20131220, 2008.\nKingma, D. P. Adam: A method for stochastic optimization.\narXiv preprint arXiv:1412.6980, 2014.\nKrizhevsky, A. and Hinton, G. Learning multiple layers of\nfeatures from tiny images. Technical report, University\nof Toronto, ON, Canada, 2009.\nLeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient-\nbased learning applied to document recognition.Proceed-\nings of the IEEE, 86(11):2278\u20132324, 1998.\nMarkvorsen, S. Curvature and shape. InYugoslav Geo-\nmetrical Seminar, Fall School of Differential Geometry,\nYugoslavia, pp. 55\u201375, 1996.\nMcInnes, L., Healy, J., and Melville, J. Umap: Uniform\nmanifold approximation and projection for dimension\nreduction.arXiv preprint arXiv:1802.03426, 2018.\nRani, V ., Nabi, S. T., Kumar, M., Mittal, A., and Kumar,\nK. Self-supervised learning: A succinct review.Archives\nof Computational Methods in Engineering, 30(4):2761\u2013\n2775, 2023.\nRicheson, D. S.Euler\u2019s Gem: The Polyhedron Formula and\nthe Birth of Topology, volume 64. Princeton University\nPress, 2019.\nSch\u00a8olkopf, B. The kernel trick for distances. InAdvances\nin neural information processing systems, pp. 301\u2013307,\n2001.\nSepanj, H. and Fieguth, P. Aligning feature distributions\nin VICReg using maximum mean discrepancy for en-\nhanced manifold awareness in self-supervised represen-\ntation learning.Journal of Computational Vision and\nImaging Systems, 10(1):13\u201318, 2024.\nSepanj, M. H. and Fiegth, P. SinSim: Sinkhorn-regularized\nSimCLR.arXiv preprint arXiv:2502.10478, 2025.\nSepanj, M. H., Ghojogh, B., and Fieguth, P. Self-supervised\nlearning using nonlinear dependence.IEEE Access, 13:\n190582\u2013190589, 2025a.\n7\nSelf-Supervised Learning by Curvature Alignment\nSepanj, M. H., Ghojogh, B., and Fieguth, P. Kernel VICReg\nfor self-supervised learning in reproducing kernel Hilbert\nspace.arXiv preprint arXiv:2509.07289, 2025b.\nZbontar, J., Jing, L., Misra, I., LeCun, Y ., and Deny, S.\nBarlow twins: Self-supervised learning via redundancy\nreduction. InInternational conference on machine learn-\ning, pp. 12310\u201312320. PMLR, 2021.\n8\n",
    "title": "Self-Supervised Learning by Curvature Alignment",
    "arxiv_id": "2511.17426v1",
    "category": "recent",
    "authors": [
      "Benyamin Ghojogh",
      "M. Hadi Sepanj",
      "Paul Fieguth"
    ],
    "published": "2025-11-21",
    "num_pages": 8,
    "num_chars": 28857
  },
  {
    "text": "arXiv:2511.17421v1  [cs.CV]  21 Nov 2025Preventing Shortcut Learning in Medical Image Analysis through\nIntermediate Layer Knowledge Distillation from Specialist Teachers\nChristopherBoland1,2,Sotirios A. Tsaftaris2,Sonia Dahdouh1,\n1Canon Medical Research Europe, Edinburgh, EH6 5NP, UK\n2School of Engineering, The University of Edinburgh, Edinburgh, EH9 3FG, UK\nAbstract\nDeep learning models are prone to learning shortcut solutions to problems using spuriously correlated yet irrelevant\nfeatures of their training data. In high-risk applications such as medical image analysis, this phenomenon may prevent\nmodels from using clinically meaningful features when making predictions, potentially leading to poor robustness and\nharm to patients. We demonstrate that different types of shortcuts\u2014those that are diffuse and spread throughout\nthe image, as well as those that are localized to specific areas\u2014manifest distinctly across network layers and can,\ntherefore, be more effectively targeted through mitigation strategies that target the intermediate layers. We propose a\nnovel knowledge distillation framework that leverages a teacher network fine-tuned on a small subset of task-relevant\ndata to mitigate shortcut learning in a student network trained on a large dataset corrupted with a bias feature.\nThrough extensive experiments on CheXpert, ISIC 2017, and SimBA datasets using various architectures (ResNet-18,\nAlexNet, DenseNet-121, and 3D CNNs), we demonstrate consistent improvements over traditional Empirical Risk\nMinimization, augmentation-based bias-mitigation, and group-based bias-mitigation approaches. In many cases, we\nachieve comparable performance with a baseline model trained on bias-free data, even on out-of-distribution test data.\nOur results demonstrate the practical applicability of our approach to real-world medical imaging scenarios where bias\nannotations are limited and shortcut features are difficult to identify a priori.\nKeywords\nAlgorithmic Bias, Shortcut Learning, Knowledge Distillation, Spurious Correlations\nArticle informations\n\u00a92025 Boland, Christopher and Tsaftaris, Sotirios and Dahdouh, Sonia. License: CC-BY 4.0\nCorresponding author: christopher.boland@mre.medical.canon\n1. Introduction\nNeuralnetworks frequently demonstrate a preference\nfor the path of least resistance during training, a\nphenomenon termed \u201csimplicity bias\u201d (Shah et al.,\n2020). This tendency can lead these models to rely on fea-\ntures that, while strongly correlated with class labels in their\ntraining datasets, are irrelevant to the task. Such features,\noften referred to as \u201cshortcuts\u201d or \u201cspurious correlations\u201d,\nyield an effective decision rule-set within the distribution\nof the training dataset but one which fails to generalize to\ndata beyond this distribution (Geirhos et al., 2020). For\nexample, a model trained to identify cows in images may\nlearn to detect grassy backgrounds rather than learning to\nunderstand what a cow looks like, if most training images\nshow cows in grassy pastures. When presented with im-\nages of cows in novel contexts, such as on a beach, the\nmodel\u2019s prediction accuracy declines significantly (Beery\net al., 2018). Because the decision rules learned by thesesystems prioritize such spurious features over robust, task-\nrelevant ones, they fail to generalize to data where the\nspurious features are not available. In contrast, a system\nthat is trained to leverage reliable and task-relevant visual\nfeatures should exhibit consistent performance even amidst\nshifts in data distribution.\nIn high-risk applications such as disease diagnosis, where\nclinical decisions rely on the accurate identification of subtle\nand often hard-to-detect disease features, shortcut learning\nrepresents a risk to patient safety. Consider pneumotho-\nrax detection: popular chest X-ray datasets often contain\nimages acquired post-treatment, once patients have been\nfitted with treatment devices like chest drains, which are\nvisible in X-ray images. Naturally, this creates a correlation\nbetween the presence of the treatment device and the dis-\nease label, which a network can learn, incorrectly, to use as\na predictive feature of disease presence. Consequently, the\nmodel is less accurate at detecting disease in patients who\nhave not yet been treated (Murali et al., 2022). Similarly,\n447\nBoland, Tsaftaris, and Dahdouh, 2025\nmodels trained to detect atelectasis in chest X-rays can\nlearn to leverage the presence of ECG cables as a predictive\nfeature (Olesen et al., 2024). Disease detection models\noften rely inappropriately on such confounding features in\naddition to more subtle features including image acquisition\nprotocol or even demographic characteristics (Souza et al.,\n2024; Konz and Mazurowski, 2024; Seyyed-Kalantari et al.,\n2021). Sources of shortcut features in medical data are\nnumerous, and their interactions are complex - exacerbating\nthe challenge of monitoring and accounting for bias. This\nis magnified by the inconsistencies in metadata collection\nand labeling practices across datasets and healthcare insti-\ntutions, making it impractical to track and account for all\npotential spurious features of the data.\nEmerging regulatory frameworks underscore the impor-\ntance of these challenges. The European Union AI Act,\ndue to come into effect in 2026, establishes comprehensive\nrequirements for AI systems in high-risk domains like health-\ncare. The act mandates rigorous testing and monitoring of\nAI systems to identify and mitigate potential biases. Simi-\nlarly, the World Health Organization\u2019s guidelines for AI in\nhealthcare emphasize the need to safeguard patient safety\nand guarantee equitable treatment outcomes. The FDA\u2019s\nguidelines for AI and machine learning systems in healthcare\napplications necessitate detailed information regarding the\nmetrics employed and how they ensure patient safety. Addi-\ntionally, the guidelines request clarity on how to address any\nnew or previously unidentified sources of bias that may arise,\nas well as details on how to disclose potential biases that\ncould impact the model\u2019s effectiveness to users (FDA et al.,\n2023). FDA approval for many AI systems in healthcare\noften requires a demonstration of \u201cpractical equivalence\u201d\nto existing systems performing the same task, including\nevidence that the system\u2019s safety is on par with that of\ncurrent processes (Petrick et al., 2023). These frameworks\nhighlight the risks of deploying systems that may perpetuate\nor amplify existing healthcare disparities through learned\nbiases. This regulatory landscape creates an urgent need\nfor systematic approaches to identify and mitigate shortcut\nlearning in medical AI systems.\nCurrent approaches to shortcut mitigation can be cate-\ngorized according to their intervention point in the model\ndevelopment pipeline. Data-centric techniques address bias\nduring pre-processing, where training data distributions are\nmodified through resampling, reweighting, or augmentation\nto reduce imbalances with respect to bias features (Wu\net al., 2023; Li and Vasconcelos, 2019; Ahmed et al., 2022;\nZhang et al., 2022; Liu et al., 2021; Wang et al., 2024;\nYun et al., 2019). Model-centric techniques include (1)\nin-processing methods, which incorporate additional loss\nterms or penalties during training to discourage reliance on\nspurious features (Sagawa et al., 2019; M \u00a8uller et al., 2023;\nZhang et al., 2022; Boland et al., 2024a) and (2) post-processing approaches, which attempt to remove learned\nbiases from already trained models through fine-tuning or\npruning (Xue et al., 2024; Wu et al., 2022; Ghadiri et al.,\n2024; Bayasi et al., 2024). A critical limitation across\nmany of these methods is their dependency on accurate\nbias annotations for all training data. The assumption of\naccess to comprehensive and reliable bias labels presents\nsignificant practical challenges in medical contexts, where\nthe sources of bias are often numerous, interrelated, and\ndifficult to identify a priori. Even when bias sources are\nknown, obtaining accurate labels across diverse healthcare\ninstitutions with inconsistent metadata collection practices\nis prohibitively resource-intensive, limiting the real-world\napplicability of these approaches (Banerjee et al., 2023).\nConsequently, there is a need for methods that can address\nor reduce this burden of bias annotation while maintaining\nmitigation efficacy.\nRecently, knowledge distillation (KD) has shown poten-\ntial as a promising in-processing approach for preventing\nbias learning (Boland et al., 2024a; Cha et al., 2022; Bassi\net al., 2024; Kenfack et al., 2024). KD was originally pro-\nposed as a model compression technique where a smaller\nstudent network learns to mimic the predictions of a larger\nteacher network through an additional loss term that mini-\nmizes the divergence between the student model\u2019s outputs\nand those of the teacher model (Hinton, 2015). In the\ncontext of shortcut learning, a teacher trained on carefully\ncurated data might help guide a student away from spurious\ncorrelations present in larger, potentially biased datasets.\nTraditional knowledge distillation approaches typically focus\nonly on matching the final layer outputs. However, several\nworks have demonstrated that learned biases can be de-\ntected in the intermediate layers of neural networks and can\neven be localized to specific network layers (Boland et al.,\n2024b; Glocker et al., 2023; Stanley et al., 2025). Distilla-\ntion approaches for debiasing that target the intermediate\nlayers of the network may be able to mitigate biases more\neffectively.\nIn our previous work (Boland et al., 2024a), we in-\ntroduced an oracle-guided training approach to mitigate\nshortcut learning using a \u201cspecialized teacher\u201d, a model\ntrained specifically on task-relevant, bias-free data. While\ndemonstrating promising results, this approach relied on\nmatching batch-wise class probability distributions for knowl-\nedge transfer\u2014a technique sensitive to batch composition.\nHere, we significantly extend this foundation through several\nmethodological improvements. We replace batch-wise prob-\nability matching with sample-level Kullback-Leibler (KL)\ndivergence between teacher and student predictions. This\nmore principled approach provides direct guidance for each\nsample. We also extend the original framework to incorpo-\nrate knowledge distillation in the final classification layer,\ncomplementing the intermediate layer guidance. Further-\n448\nPreventing Shortcut Learning through Intermediate Layer Knowledge Distillation\nmore, we significantly expand the experimental validation\nwith extensive evaluation on out-of-distribution (OOD) test\nsets, systematic analysis of partial-layer distillation, uti-\nlization of compact teacher architectures to guide larger\nstudent networks, and evaluation of our method\u2019s efficacy\nwhen training data is corrupted with multiple, simultaneous\nshortcuts. All of these extensions serve to demonstrate the\nenhanced generalizability and practical applicability of our\napproach. Experiments across several network architecture\ndesigns, such as AlexNet, ResNet-18, DenseNet-121, and\na lightweight 3D CNN, and over multiple medical image\nanalysis tasks in different modalities, demonstrate that our\napproach is not modality-, task-, or architecture-specific. To\nfurther strengthen the viability of the approach in real-world\nscenarios, we validate our method on a recently released\nsynthetic brain MRI dataset featuring subtle structural bias\nfeatures, which are hard to detect upon simple inspection\n(Stanley et al., 2023).\nOur proposed approach utilizes a teacher model trained\non a small, carefully curated dataset to guide a student\nnetwork\u2019s learning on larger, potentially biased datasets.\nBy distilling knowledge at intermediate network layers, we\nencourage the student to learn robust, task-relevant fea-\ntures rather than relying on spurious correlations. Our\ncontributionsare as follows:\n1.We demonstrate that intermediate-layer knowledge distil-\nlation from a teacher fine-tuned on a small amount of\nunbiased, task-relevant data effectively mitigates shortcut\nlearning of a student trained on bias-corrupted data and\nleads to improved generalization as demonstrated through\nvalidation on out-of-distribution (OOD) test data.\n2.We provide empirical evidence that distillation at interme-\ndiate network layers significantly improves bias mitigation\ncompared to final-layer distillation alone.\n3.We show that fine-tuning the teacher on task-relevant\ndata leads to performance gains and reductions in bias\ncompared to alternative distillation approaches, such as\nusing a teacher pre-trained on ImageNet data or through\nconfidence regularization of the intermediate layers.\n4.We establish that compact teacher architectures (e.g.,\nAlexNet) can effectively guide larger student networks\nwith different architectures (e.g., ResNet-18), critical for\nreal-world deployment where much larger models, which\nwould overfit to small, bias-free training subsets, are likely\nto be used.\n2. Related Works\nThis section reviews relevant literature in two key areas re-\nlated to our work: approaches that address shortcut learningin deep neural networks and knowledge distillation tech-\nniques that can be leveraged for bias mitigation. We first\nexplore various shortcut mitigation strategies and then ex-\namine how knowledge distillation can be adapted to address\nthis challenge.\n2.1 Shortcut mitigation\nShortcut mitigation techniques are grouped according to\nwhether they modify the training data or the model\u2019s learn-\ning process. We review both data-centric and model-centric\napproaches, highlighting their respective strengths and limi-\ntations.\n2.1.1 Data-centric techniques\nData-centric approaches address bias at the source by mod-\nifying training data distributions. Common approaches\ninclude up-sampling and down-sampling the dataset to re-\nmove the imbalance in the data with respect to the bias\nfeatures, or re-weighting the loss to reduce the influence of\nthe bias (Wang et al., 2020; Sagawa et al., 2019). Such\napproaches require bias labels and sufficient data diversity\nafter resampling or augmentation. In contrast, methods\nlike Just Train Twice (JTT) (Liu et al., 2021) and Discover\nand Cure (Wu et al., 2023) assign pseudo-labels of the\nbias feature to identify potentially biased samples before\nup-sampling or reweighting, avoiding the need for explicit\nbias annotations. These approaches estimate bias through\nmodel accuracy patterns or feature space representations.\nBeyond basic resampling and re-weighting approaches,\nadvanced data augmentation techniques have emerged as\npowerful tools for disrupting potential shortcuts. Cutout\n(Zhong et al., 2020) introduces random occlusions by mask-\ning image regions, forcing models to learn more distributed\nrepresentations. Mixup (Zhang et al., 2017) creates syn-\nthetic training examples by interpolating between image\npairs and their labels, reducing overfitting to training arti-\nfacts. CutMix (Yun et al., 2019) combines these approaches\nby replacing removed regions with patches from other train-\ning images.\nWhile effective for natural images, these augmentation\nstrategies face limitations in medical contexts. Disease\nfeatures in medical images are often subtle and localized,\nunlike the prominent objects in natural image datasets.\nRandom augmentations risk occluding critical diagnostic\nfeatures, and they fail to target specific shortcut features\nsystematically.\nAhmed et al. (2022) propose the use of a comprehensive\npre-processing pipeline for pneumonia detection in chest X-\nrays involving normalization, region-of-interest (ROI) crop-\nping, rotations, etc. Through evaluation with both IID\nand OOD test data, they validate that the influence of\nbiases in the training data is significantly reduced compared\n449\nBoland, Tsaftaris, and Dahdouh, 2025\nto a model trained without applying this pre-processing.\nWhile this is relatively straightforward to implement, such\nan approach requires domain-specific tuning, knowledge of\npossible shortcut sources, and task-specific domain knowl-\nedge to inform some of the augmentation strategies, such\nas ROI cropping.\n2.1.2 Model-centric techniques\nModel-centric techniques mitigate learned biases by adjust-\ning the model\u2019s weights and learning process, rather than\ntargeting the training data itself. These can be further\nbroken down into in-process techniques, which are applied\nat training time, and post-process techniques, which are\napplied after training is complete.\nAdversarial training methods (Correa et al., 2024; Zhang\net al., 2018) introduce competing objectives to discourage\nreliance on biased features. However, these often require\nexplicit labels for the bias sources and the competing ob-\njectives can introduce instability. Feature disentanglement\ntechniques (M \u00a8uller et al., 2023) attempt to separate task-\nrelevant from spurious features but also often require explicit\nbias labels and rely on the assumption that such features\nare entirely task irrelevant, which may not always hold in\npractice.\nPost-processing methods like pruning (Wu et al., 2022)\nand fine-tuning (Xue et al., 2024) attempt to remove short-\ncuts after training. Such approaches are particularly useful\nwhen it is not possible to re-train the model, for example,\nwhen the full, original training data is not available.\n2.2 Knowledge Distillation\nWhile not originally developed for bias mitigation, knowl-\nedge distillation-inspired approaches to bias mitigation have\nshown promise in recent years.\n2.2.1 Traditional Knowledge Distillation\nKnowledge distillation, originally proposed as a method for\nmodel compression (Hinton, 2015), has recently shown ef-\nfectiveness in mitigating bias learning (Boland et al., 2024a;\nCha et al., 2022; Bassi et al., 2024; Kenfack et al., 2024).\nAdopting a student-teacher training regime, knowledge from\na large, well-trained teacher network is \u201cdistilled\u201d into a\nsmaller student network. Typically, this process incorporates\nan additional loss term that quantifies the divergence in pre-\ndicted class probabilities between the two models (Bucilu \u02c7a\net al., 2006).\n2.2.2 Distillation for bias mitigation\nTian et al. (2024) demonstrate that distillation from a\nteacher trained on a balanced subset of training data can\neffectively mitigate learned biases in a student networktrained on the full, biased dataset. However, they assume\naccess to labels that accurately portray the source of bias in\nall of the teacher\u2019s training data, and they focus exclusively\non the alignment of features in the final network layer,\nwhich may allow for more effective bias mitigation. Chai\net al. (2022) propose training a teacher model to overfit on\nits training data, and using its softened logits as training\nlabels for a student model. The authors demonstrate that\nthis soft labeling approach effectively functions as an error-\nbased re-weighting mechanism that can improve fairness\nmetrics without explicit demographic data. However, such\nan approach may not effectively capture all bias sources in\nthe training data.\nWhile traditional knowledge distillation focuses on final\nlayer outputs, recent work has explored distillation at the\nintermediate layers. Cha et al. (2022) propose MIRO. Uti-\nlizing a large pre-trained network as an \u201coracle\u201d network,\nthe authors formulate domain generalization as maximizing\nmutual information between the \u201coracle\u201d model\u2019s represen-\ntations and a target model\u2019s. Similarly, Bassi et al. (2024)\npropose \u201cexplanation distillation\u201d as a technique to prevent\nshortcut learning in deep neural networks. Their approach\ndistills explanations from a teacher model pre-trained on\na massive, diverse dataset, but not necessarily one with\ntask-specific knowledge. This lack of knowledge pertaining\nto the specific task of the student in the teacher network\nlimits its ability to guide the student network to robust,\ntask-relevant features.\nBoland et al. (2024a) introduced an oracle-guided train-\ning approach for shortcut mitigation that does not require\nexplicit bias labels for the full training dataset of the student,\nnor does it make assumptions about bias characteristics.\nOur work builds upon this foundation through methodolog-\nical improvements for enhanced robustness, exploration of\nknowledge distillation applied only to subsets of the student\nnetwork\u2019s intermediate layers, and the use of low-capacity\nteacher networks to guide high-capacity students.\n3. Methods\nOur proposed approach addresses the challenge of shortcut\nlearning through a novel teacher-student knowledge distil-\nlation framework that guides feature learning at multiple\nnetwork depths (Figure 1). Central to our approach is\nthe observation that the influence of shortcut learning is\ndetectable in a network\u2019s intermediate layers, suggesting\nthat effective mitigation strategies should target the entire\nnetwork rather than just the final output (Boland et al.,\n2024b).\nIn this section, we present our method for measuring\nintermediate-layer model confidence. We then detail our\nknowledge distillation approach for mitigating shortcut learn-\ning, followed by our experimental setup, including datasets,\n450\nPreventing Shortcut Learning through Intermediate Layer Knowledge Distillation\nsynthetic shortcut designs, and evaluation protocols.\n3.1 Model confidence and shortcut learning\nModels trained on biased data tend to exhibit overconfi-\ndence in their predictions (Utama et al., 2020). Shortcut\nfeatures in a model\u2019s training data provide an easier decision\nrule-set with which to infer class. These simple features\nallow the model to achieve high confidence with less effort\n(Ao et al., 2023). Prior work has demonstrated that these\nspurious features lead to detectable changes in the inter-\nnal behavior of the network (Boland et al., 2024b). We\nare interested in (a) the confidence with which a trained\nnetwork infers class through the internal layers, (b) how\ntraining on shortcut-corrupted data changes this behavior,\nand (c) if knowledge distillation from an unbiased teacher\ncan mitigate shortcut learning. To understand this, we\nintroduce classification probes (linear classification heads,\nconsisting of an average pooling layer and a single fully\nconnected layer) which are attached to the intermediate\nlayers of both the student and teacher networks. After the\nnetwork finishes training, these probes are fine-tuned on the\ndownstream task. Once trained, the probes offer insight\ninto a model\u2019s ability to infer the true class at different\ndepths of the network, in addition to facilitating knowledge\ndistillation from the teacher network\u2019s intermediate layers\nto the student\u2019s.\n3.2 Measuring Model Confidence\nTo quantify a model\u2019s confidence over a batch at each\nlayer, we follow prior work and consider the output logits of\nthe classification probes (Taha et al., 2022). The sigmoid\nof the output logits ranges from0to1, indicating the\nlikelihood that the input belongs to the positive (1) or\nnegative (0) class. We quantify model confidence C(X)\nas the deviation from maximum uncertainty (0 .5), where\nhigher values indicate greater prediction certainty. This is\nillustrated in Equation 1, where f(x)represents the sigmoid\noutput of the model for inputx.\nC(X) =/summationdisplay\nx\u2208X|f(x)\u22120.5|(1)\n3.3 Mitigation of shortcut learning via knowledge\ndistillation\nOur training scheme (Fig. 1) aims to mitigate shortcut\nlearning by preventing the student model from becoming\noverconfident through the use of shortcut features. The\nstudent model is trained to minimize the cross-entropy loss\non a biased dataset while matching the teacher network\u2019s\nclass probabilities at each layer.3.3.1 Teacher-Student Architecture\nThe \u201cspecialist teacher\u201d model is defined as a network\ntrained on a small, carefully curated subset of the full train-\ning dataset. This subset is manually selected to contain\nbalanced class representation and to be free of the bias\nfeatures present in the student\u2019s training data. Unlike tra-\nditional knowledge distillation approaches that use large,\ngeneral-purpose teachers, our specialized teacher possesses\ntask-specific knowledge while avoiding the spurious correla-\ntions that contaminate larger datasets.\nImportantly, all samples used to train the teacher model\nare excluded from the student\u2019s training dataset to prevent\nleakage between the teacher and the student\u2019s training data.\nFor the teacher model, we follow a standard training pro-\ncedure: the network is trained to completion, then frozen\nbefore the classification probes are fine-tuned on the down-\nstream task. We use separate optimizers for updating the\nnetwork parameters and the probe parameters to prevent\nunintended interactions between their learning objectives.\nThe student model is trained on the biased dataset\nusing both the standard classification loss and the knowl-\nedge distillation from the teacher. At each epoch, after the\nstudent model\u2019s parameters have been updated, the net-\nwork\u2019s encoder and final classification head are frozen, and\nthe probes are fine-tuned on the downstream classification\ntask. This maintains the probes\u2019 ability to classify based on\nthe student\u2019s currently learned feature embeddings while\npreventing undesired interaction between the probe training\nand the student\u2019s feature learning.\nWe encourage alignment between teacher and student\nby minimizing the KL divergence between the output proba-\nbility distributions of each model\u2019s intermediate layer classifi-\ncation probes. Following other intermediate-layer knowledge\ndistillation literature (Haidar et al., 2021; Bassi et al., 2024),\nwe also apply knowledge distillation loss on the output of\nthe network\u2019s final classification head.\n3.3.2 Loss Functions\nThe training loss of the student is described in Eq. 3, where\nLtotal is the total loss,LCEis the Cross Entropy (CE) loss,\nLKDis the knowledge distillation loss between the teacher\nand student probes, and \u03bbiis a weight applied to each\nloss to allow the trade-off between each objective to be\nmanaged. For simplicity, we set all weights equal to 1. KL\ndivergence loss is defined in Eq. 2 where we have two sets of\nintermediate layer predictions, S={p1,p2,...,p n}andT=\n{q1,q2,...,q n}whereSrepresent the set of intermediate\nlayer outputs of the student network, Trepresents the\nteacher, and \u03b1irepresents the weight of the distillation loss\nto theithlayer of the student.\n451\nBoland, Tsaftaris, and Dahdouh, 2025\n\u2112CE\ud835\udcb3\u2112KDTeacher (frozen)\nStudent\nFCN\nFigure 1: Overview of the proposed student-teacher training method. The teacher network, trained on clean data, guides\nthe student model\u2019s learning process through the distillation of task-specific knowledge to the intermediate layers.\nLKD=n/summationdisplay\ni\u03b1iDKL(pS\ni||qT\ni)(2)\nLtotal=\u03bb 1\u00b7LCE+\u03bb 2\u00b7LKD (3)\n3.4 Experimental Setup\n3.4.1 Datasets\nTable 1: Composition of positive/negative class samples in\ntrain, validation, and test splits of our datasets.\nDataset Train Valid Test\nCheXpert 1457/1457 365/406 600/300\nISIC 560/560 140/146 207/393\nSimBA 1291/1292 323/323 530/544\nMIMIC n/a n/a 500/500\nFitzpatrick17k n/a n/a 69/69\nWe evaluate our proposed method using three medical\nimaging datasets of different modalities and tasks. For each,\nwe enforce class balancing by downsampling the majority\nclass and combine the original train/validation splits. New\nsplits are generated when we run k-fold cross-validation.\nTable 1 summarizes the composition of positive/negative\nclass samples across train, validation, and test splits for\neach dataset.\u2022CheXpert (Irvin et al., 2019): a large-scale chest radiog-\nraphy dataset comprised of224 ,316chest X-rays from\n65,240patients and14disease labels. In our experiments,\nwe consider the task of pneumothorax detection. We\nuse a subset of the full CheXpert dataset containing an\nequal number of pneumothorax-positive and no finding-\npositive images. Our final training dataset consists of\n2,914images.\n\u2022ISIC 2017 (Codella et al., 2018): a popular skin lesion\nimage dataset from the International Skin Imaging Col-\nlaboration containing2 ,000dermoscopic images. We\nperform binary classification between malignant lesions\n(melanoma/seborrheic keratosis) and benign lesions. Our\ntraining split contains1 ,120images after class balancing.\n\u2022SimBA (Stanley et al., 2023): a fully synthetic brain\nMRI dataset which allows evaluation on 3D medical data\nwith controlled biases. It contains simulated structural\nchanges associated with the class label alongside artifi-\ncially introduced morphological deformations as potential\nshortcuts. We also utilize the original version of the\ndataset without any bias features added.\nTo assess the generalization of trained models, we in-\nclude two out-of-distribution (OOD) datasets for evaluation:\n\u2022MIMIC (Goldberger et al., 2000; Johnson et al., 2024,\n2019): for CheXpert evaluation, we use a class-balanced\nevaluation set composed of pneumothorax-positive and no\n452\nPreventing Shortcut Learning through Intermediate Layer Knowledge Distillation\nfinding samples from the MIMIC dataset, another large-\nscale chest radiograph dataset acquired from a different\ninstitution.\n\u2022Fitzpatrick17k (Groh et al., 2021, 2022): for ISIC evalua-\ntion, we leverage a second dermatological image dataset.\n3.4.2 Synthetic shortcuts\n(a) (b) (c) (d)\nFigure 2: ISIC skin lesion image augmented with synthetic\nshortcuts: (a) original; (b) noise; (c) square (constant\nlocation); (d) square (random location). The noise effect\nhas been amplified here for illustrative purposes.\nInspired by research highlighting common shortcut sources\nin medical image analysis datasets such as acquisition de-\nvices, scanning protocols, hospital tags, and medical devices,\nwe design a controlled environment for the empirical\nevaluation of our approach. We introduce synthetic bias\nfeatures into our ISIC and CheXpert training datasets that\nallow us to assess the generalizability of our method across\ndiverse types of bias. We design several experimental setups\nfeaturing a single bias feature and multiple, concurrent bias\nfeatures. We augment our datasets with one of three unique\nsynthetic bias features (Figure 2):\n1.Diffuse:leveraging random, uniform noise patterns as a\nspurious signal spread throughout the image. The noise\nis generated using a uniform distribution with values\nbetween 0 and 0.15 applied to each pixel. Such shortcut\nfeatures are designed to simulate those that may be\ncaused by acquisition devices and scanning protocols\n(Ong Ly et al., 2024).\n2.Localized:introducing small square shapes to the image,\nsimilar to other work (Dagaev et al., 2023), we aim\nto simulate more localized shortcut features of various\ncomplexities seen in the literature, such as hospital tags\nand treatment devices (Olesen et al., 2024). We test two\nvariants:\n(a)Constant location:the square appears in a fixed\nspot.\n(b)Random location:the location of the square varies\namong images.\nIn our training splits, the shortcut features are correlated\nwith the class label (Figure 3). We vary the prevalence of\nDisease Disease No disease No diseaseTrain Validation/T est\nBias feature No bias featureFigure 3: Illustrative representation of the synthetic shortcut\nfeature distribution in our train, validation, and test splits\nin the CheXpert and ISIC datasets.\nthe shortcut feature (the degree of its correlation with the\nclass label in the training data) to assess its influence on\ntraining and mitigation efforts. In the validation and test\nsplits, shortcut features are balanced across both classes.\nIn cases with two simultaneous shortcut features, each is\ncorrelated with a different class label.\nNotably, in the case of the SimBA dataset, all data\nsplits exhibit the same bias prevalence. Experiments on\nSimBA allow us to validate the efficacy of our method when\nit is not possible to access an unbiased validation set.\n3.5 Evaluation metrics and statistical analysis\nWe evaluate our experiments by considering both overall\nperformance metrics and bias-specific metrics. For clas-\nsification performance, we report the Area Under the Re-\nceiver Operating Characteristic Curve (AUC), providing a\nthreshold-free measure of discriminative ability across all\ndatasets. Similar to recent research investigating bias and\nshortcut learning in medical image analysis, we quantify the\nimpact of shortcut features on model predictions through\nTrue Positive Rate disparity (\u2206TPR) (Glocker et al., 2023;\nStanley et al., 2025).\u2206TPR directly measures the model\u2019s\nability to maintain consistent sensitivity across bias-aligned\nand bias-contrasting groups, a critical requirement for clin-\nical deployment where missed diagnoses (false negatives)\ncarry severe consequences. We used a 0.5 classification\nthreshold in all cases as the natural decision boundary for\nbinary classification.\nWe define bias-aligned samples as those whose combi-\nnation of class label and shortcut feature presence matches\nthe class-bias correlation established in the training data.\n453\nBoland, Tsaftaris, and Dahdouh, 2025\nMeanwhile, bias-contrasting samples represent those whose\ncombination of class label and shortcut feature presence\nopposes the class-bias correlation in the training data. Sta-\ntistical significance between performance differences is as-\nsessed using paired t-tests with Bonferroni correction to\naccount for multiple comparisons where appropriate.\n3.6 Benchmark methods\nWe compare our approach with several established short-\ncut learning mitigation methods. The network trained on\nthe original, clean dataset without shortcut features is our\nBaseline. The network trained on the shortcut-corrupted\ndataset with standard cross-entropy optimization is referred\nto asERM. We compare to four augmentation-based ap-\nproaches:CutOut(Zhong et al., 2020),MixUp(Zhang\net al., 2017),CutMix(Yun et al., 2019), as well as the use\nof random rotation (up to15\u25e6) and horizontal flip augmenta-\ntions(Aug). We also compare to two popular group-based\nmethods:GroupDRO (GDRO)(Sagawa et al., 2019) and\nJust Train Twice (JTT)(Liu et al., 2021)\nFor each method, we implement configurations following\nthe authors\u2019 recommendations without any specific fine-\ntuning or adjustments made for our data and use identical\narchitecture backbones for fair comparison.\n3.7 Implementation Details\nIn all experiments, we utilize an AdamW optimizer with\nweight decay of0 .1and train our models with a learning\nrate of1\u00d7e\u22124. All intermediate layer classification probes\nare trained with a learning rate of0 .1. For our 2D datasets\n(ISIC, CheXpert, MIMIC, and Fitzpatrick17k) images are\nre-sized to ImageNet resolution,224 \u00d7224, while for SimBA,\nwe resize to96\u00d796\u00d796. We do not apply any rotation or\nflipping augmentations by default. We set the maximum\nnumber of training epochs to 1000 with early stopping af-\nter 15 epochs if there is no improvement in the validation\nloss. In none of our experiments does training run for the\ncomplete 1000 epochs without reaching the early stop con-\ndition. We use 5-fold cross-validation, with consistent test\nsets across folds. Our experimental setup utilizes Python\nand PyTorch, and we train on an NVIDIA RTX 2080 Ti\nand Tesla V100s.\nFor the 3D experiments on SimBA data, we employ\na lightweight 3D CNN consisting of five convolutional\nblocks, each containing a 3D convolutional layer (kernel\nsize 3\u00d73\u00d73), batch normalization, and Sigmoid activation.\nClassification probes are attached after each convolutional\nblock, consisting of 3D global average pooling followed by\na linear layer.4. Results\nOur experimental evaluation examines several key aspects\nof the proposed approach. We first investigate how short-\ncut learning manifests in intermediate network layers, then\nevaluate our knowledge distillation method against alter-\nnative approaches. We also analyze the impact of partial\nlayer distillation, the effectiveness of compact teacher ar-\nchitectures, and performance on realistic structural biases\nin 3D medical data. In the following work, our teacher\nnetworks are trained on a subset of 20% of the full, original\ntraining data. The samples in this subset are removed from\nthe student network\u2019s training data. We later explore the\nefficacy of our teacher with fewer training data.\n4.1 Core method validation\nWe begin our experimental evaluation by establishing fun-\ndamental evidence for our approach: first demonstrating\nhow shortcut learning manifests in neural networks, then\nvalidating our knowledge distillation method\u2019s effectiveness\nacross diverse experimental conditions.\n4.1.1 Shortcut learning manifests distinctly across network\nlayers\nConsidering the complexity of many medical image anal-\nysis tasks, we expect a well-trained model using clinically\nrelevant features to exhibit lower confidence than a model\nrelying on easy shortcut features. Additionally, we might\nexpect that the confidence of a model reliant on shortcut\nfeatures will increase in earlier layers, aligning with the\nexpectation that the deeper layers of the network capture\nmore sophisticated features (Baldock et al., 2021).\nTo test this, we train two ResNet-18 models on CheX-\npert following Empirical Risk Minimization (ERM), where\nwe simply aim to optimize cross-entropy loss. One is trained\non the original dataset without any synthetic biases, while\nthe other is trained on the same dataset augmented with\nsynthetic shortcut features associated with the disease class.\nIn this case, the shortcut features have a 100% prevalence\nrate (perfectly correlated with the disease class). After train-\ning, we fine-tune our classification probes for each model.\nEach model is evaluated on our held-out test set, and the\noutput of the probes are used to evaluate the influence of\nthe bias feature on the network\u2019s predictive behavior.\nFigure 4 illustrates the per-layer confidence of each\nmodel. In line with our hypothesis, the model trained on\nthe biased data becomes overconfident compared to the\nbaseline trained on clean data. In the case of our diffuse\nnoise shortcut, we observe this to an extreme degree in\nthe earliest layers, while the localized shortcuts don\u2019t result\nin a large degree of overconfidence until the later layers.\nThis is likely because the diffuse shortcut is composed of\n454\nPreventing Shortcut Learning through Intermediate Layer Knowledge Distillation\n1 3 5 7 9 11 13 15 17\nLayer0.00.10.20.30.40.5ConfidenceNoise\nModel\nClean training data\nBiased training data0.00.10.20.30.40.5Constant Square\n1 3 5 7 9 11 13 15 17\nLayer0.00.10.20.30.40.5Random SquareConfidence per layer across shortcut types\nFigure 4: Intermediate-layer confidence of two ResNet-18 models trained on near-identical training sets. Confidence\nbands represent the standard deviation over 5-fold cross-validation. Both networks are trained on the CheXpert dataset\nwith a learning rate of1 e\u22124. Intermediate layer classification probes have a learning rate of0 .1. The training data of\none model has been corrupted with various synthetic shortcut features, while the training data of the other has not.\nlow-level features that require very little disambiguation by\nthe network, unlike the localized shortcuts. We observe\nsimilar patterns in other datasets and network architectures\nthat we evaluate.\n4.1.2 Intermediate-layer knowledge distillation mitigates\nshortcut reliance\nHaving established the layer-specific nature of shortcut learn-\ning, we now demonstrate that our knowledge distillation\nframework, utilizing a teacher trained on a small curated\ndataset, effectively prevents students from developing short-\ncut dependencies across multiple datasets and bias types.\nWe evaluate our approach across the CheXpert and ISIC\ndatasets and all student models are trained on data aug-\nmented with a synthetic bias feature with different degrees\nof correlation with the class label (prevalence). We compare\nthe performance of our model to several augmentation-\nbased debiasing approaches, as well as Just Train Twice\n(JTT) and GroupDRO.\nAcross all tested bias types and degrees of correlation\nwith class labels, our student network demonstrates the\nmost consistently low bias in its predictions, as measured by\n\u2206TPR between bias-aligned and bias-contrasting samples\n(Table 2). In several cases, the TPR disparity is reduced\nsuch that it is comparable to our clean baseline. Our\nmethod remains similarly effective in reducing the bias evenas its prevalence increases, while most other methods we\nevaluate worsen in effectiveness at higher prevalence rates.\nNotably, the majority of methods show significantly reduced\nefficacy in mitigating diffuse shortcuts across both datasets.\nContrastingly, our approach is consistently effective across\nlocalized and diffuse shortcuts. We highlight a consistent\ndrop in efficacy for the noise bias feature in the CheXpert\ndataset with all methods, including ours. We suspect that\nthis is due to useful textural information being corrupted\nby the noise shortcut. We also typically find that our\napproach achieves better overall AUC compared to other\nmethods, particularly at higher prevalence rates. As shortcut\nprevalence decreases from 95% to 75%, all methods show\nimproved\u2206TPR, which is expected since weaker biases\nwill provide less misleading signal during training. However,\neven at lower prevalence rates, our approach maintains its\nadvantage over other methods.\n4.1.3 Generalization to clean and out-of-distribution data\nA critical test of any deep neural network is whether it has\nlearned a robust, generalizable set of decision rules. Here\nwe evaluate our approach across three distinct evaluation\nscenarios: (1) a biased test set where shortcuts are present\nbut distributed equally across classes, such that they are\nno longer useful predictive features; (2) a clean test set\nfeaturing none of the synthetic bias features present in\n455\nBoland, Tsaftaris, and Dahdouh, 2025\nTable 2:\u2206TPR\u2193between bias-aligned and bias-contrasting samples for a ResNet-18 trained on data with various bias\nprevalence rates. Results are presented as Mean \u00b1Std over 5-fold cross-validation. Models are marked asbestand\nsecond-best . When the difference between first and second best is statistically significant ( p <0.05according to a\npaired t-test), the best-performing model is highlighted with *.\nPrev.ModelCheXpert ISIC\n(%) Noise Square (C) Square (R) Noise Square (C) Square (R)\n0 Baseline 0.131\u00b10.0510.020\u00b10.0080.015\u00b10.0130.409\u00b10.0950.056\u00b10.0150.050\u00b10.017\n100ERM 1.000\u00b10.0001.000\u00b10.0000.991\u00b10.0081.000\u00b10.0000.777\u00b10.0930.844\u00b10.168\nMixUp 0.987\u00b10.0260.998\u00b10.0030.854\u00b10.1190.998\u00b10.004 0.875\u00b10.1520.754\u00b10.157\nCutOut 0.999\u00b10.0011.000\u00b10.0000.832\u00b10.1121.000\u00b10.0000.448\u00b10.0650.277\u00b10.064\nCutMix 0.993\u00b10.0050.503\u00b10.073 0.126\u00b10.022 1.000\u00b10.0000.359\u00b10.0630.116\u00b10.053\nAug 0.957\u00b10.092 0.979\u00b10.0130.984\u00b10.0061.000\u00b10.0000.161\u00b10.052 0.515\u00b10.133\nOurs0.377\u00b10.185*0.079\u00b10.017*0.035\u00b10.013*0.068\u00b10.055*0.034\u00b10.016*0.074\u00b10.042\n95ERM 0.939\u00b10.0280.912\u00b10.0860.791\u00b10.0930.959\u00b10.0190.861\u00b10.0360.703\u00b10.103\nMixUp 0.927\u00b10.1050.987\u00b10.0080.693\u00b10.1030.952\u00b10.0370.936\u00b10.0350.757\u00b10.066\nCutOut 0.950\u00b10.0500.912\u00b10.0690.636\u00b10.1310.946\u00b10.0470.579\u00b10.2490.439\u00b10.146\nCutMix 0.975\u00b10.0300.325\u00b10.048 0.142\u00b10.063 0.922\u00b10.079 0.311\u00b10.1070.134\u00b10.040\nAug 0.899\u00b10.072 0.779\u00b10.1500.813\u00b10.0610.935\u00b10.0540.205\u00b10.079 0.549\u00b10.115\nGDRO 0.986\u00b10.0100.978\u00b10.0150.702\u00b10.0910.967\u00b10.0140.800\u00b10.0350.477\u00b10.029\nJTT 0.982\u00b10.0110.946\u00b10.0310.673\u00b10.0650.946\u00b10.0310.793\u00b10.0430.505\u00b10.073\nOurs0.372\u00b10.110*0.089\u00b10.023*0.047\u00b10.0270.077\u00b10.069*0.100\u00b10.0390.052\u00b10.012*\n85ERM 0.745\u00b10.1450.735\u00b10.1130.423\u00b10.0630.274\u00b10.037 0.376\u00b10.0940.294\u00b10.083\nMixUp 0.699\u00b10.1150.677\u00b10.1310.336\u00b10.0280.490\u00b10.0910.523\u00b10.1000.407\u00b10.075\nCutOut 0.810\u00b10.0720.656\u00b10.2140.374\u00b10.1040.511\u00b10.2270.471\u00b10.1580.262\u00b10.115\nCutMix 0.733\u00b10.1070.211\u00b10.083 0.097\u00b10.017 0.653\u00b10.1200.214\u00b10.0720.082\u00b10.031\nAug 0.664\u00b10.190 0.517\u00b10.1970.526\u00b10.1320.677\u00b10.2060.115\u00b10.053 0.317\u00b10.081\nGDRO 0.759\u00b10.0320.624\u00b10.0600.267\u00b10.0830.697\u00b10.0850.335\u00b10.0850.097\u00b10.053\nJTT 0.719\u00b10.0350.533\u00b10.0340.378\u00b10.1210.703\u00b10.1250.350\u00b10.0730.158\u00b10.051\nOurs0.348\u00b10.151*0.106\u00b10.0510.059\u00b10.0440.077\u00b10.109*0.057\u00b10.0370.067\u00b10.083\n75ERM 0.445\u00b10.1550.387\u00b10.0640.201\u00b10.0870.253\u00b10.054 0.273\u00b10.0570.186\u00b10.077\nMixUp 0.380\u00b10.120 0.331\u00b10.0670.156\u00b10.0360.470\u00b10.1490.282\u00b10.0950.183\u00b10.036\nCutOut 0.460\u00b10.0780.361\u00b10.1110.168\u00b10.0440.265\u00b10.1880.140\u00b10.0360.165\u00b10.120\nCutMix 0.526\u00b10.0950.138\u00b10.032 0.055\u00b10.0200.330\u00b10.0820.153\u00b10.0480.070\u00b10.061\nAug 0.446\u00b10.1310.408\u00b10.1580.345\u00b10.0440.324\u00b10.1420.149\u00b10.0630.256\u00b10.066\nGDRO 0.507\u00b10.0190.329\u00b10.0150.114\u00b10.0240.444\u00b10.0640.183\u00b10.0230.057\u00b10.010\nJTT 0.452\u00b10.0460.261\u00b10.0350.146\u00b10.0480.459\u00b10.0770.204\u00b10.0290.086\u00b10.037\nOurs0.364\u00b10.1820.127\u00b10.0590.061\u00b10.028 0.066\u00b10.040* 0.145\u00b10.088 0.063\u00b10.024\nour training sets; and (3) out-of-distribution (OOD) test\nsets to evaluate generalization. This allows us to assess\nboth the method\u2019s ability to ignore spurious features and\nits capacity to learn robust, generalizable, and clinically\nrelevant features. Our findings are highlighted in Table 3.\nComparisons on the bias-corrupted test set allow valida-\ntion of how well each model learned to ignore the presence of\nthe shortcuts at inference. Across all shortcut types on both\ndatasets, we find that our method consistently achieves the\nbest overall AUC and consistently matches or even outper-\nforms the clean baseline evaluated on the shortcut-corrupted\ntest data. We highlight that the clean baseline consistently\nsees a significant drop in performance when evaluated on\nnoise-corrupted data. We hypothsize that this is likely a\nresult of the degradation of useful texture-related informa-\ntion in the test set, combined with inherent bias of CNNarchitectures towards textural information (Geirhos et al.,\n2018).\nInterestingly, we find that all models see significantly\nimproved performance when tested on the clean test data.\nThis supports previous findings that biases in the training\ndata do not necessarily prevent models from learning under-\nlying causal features (Stanley et al., 2025; Glocker et al.,\n2023); but can lead them to preferentially rely on the spuri-\nously correlated features when they are available. Notably,\nacross all shortcut types, our model tested on the clean\ndataset achieves performance that is competitive with the\nbaseline. By comparison, most other tested methods fail to\nsee an improvement in AUC on the clean test set when the\ntraining data was augmented with the noise shortcut. We\nhighlight this as further evidence of the power of a teacher\nmodel fine-tuned on a small amount of task-relevant data\n456\nPreventing Shortcut Learning through Intermediate Layer Knowledge Distillation\nTable 3: AUC\u2191for ResNet-18. We compare our approach to four popular augmentation-based de-biasing techniques.\nShortcuts here have a 100% correlation with the task label, so group-based methods (GroupDRO and JTT) are omitted\nfrom these comparisons. Results are presented as Mean \u00b1Std over 5-fold cross-validation. Models are marked asbest\nand second-best . When the difference between first and second best is statistically significant ( p<0.05according to a\npaired t-test), the best-performing model is marked *.\nTest set ModelCheXpert ISIC\nNoise Square (C) Square (R) Noise Square (C) Square (R)\nBiasedBaseline 0.709\u00b10.0240.755\u00b10.0130.752\u00b10.0150.749\u00b10.0240.809\u00b10.0190.808\u00b10.017\nERM 0.489\u00b10.0120.533\u00b10.0070.554\u00b10.0060.521\u00b10.0110.600\u00b10.0110.612\u00b10.007\nMixUp 0.509\u00b10.0070.539\u00b10.0080.550\u00b10.0100.490\u00b10.0260.555\u00b10.0280.585\u00b10.011\nCutOut 0.498\u00b10.0290.548\u00b10.0100.584\u00b10.0080.521\u00b10.023 0.627\u00b10.0130.639\u00b10.006\nCutMix 0.529\u00b10.007 0.680\u00b10.025 0.758\u00b10.015 0.516\u00b10.0150.753\u00b10.038 0.781\u00b10.020\nAug 0.483\u00b10.0090.585\u00b10.0170.550\u00b10.0130.518\u00b10.0060.731\u00b10.0170.631\u00b10.010\nOurs0.689\u00b10.044*0.747\u00b10.008*0.761\u00b10.010.775\u00b10.023*0.777\u00b10.0240.805\u00b10.016\nCleanBaseline 0.754\u00b10.0140.754\u00b10.0140.754\u00b10.0140.811\u00b10.0190.811\u00b10.0190.811\u00b10.019\nERM 0.491\u00b10.0290.599\u00b10.0200.704\u00b10.0150.498\u00b10.038 0.672\u00b10.0380.745\u00b10.015\nMixUp 0.587\u00b10.0210.581\u00b10.0150.649\u00b10.0250.402\u00b10.0180.565\u00b10.0370.652\u00b10.036\nCutOut 0.527\u00b10.0630.604\u00b10.0120.741\u00b10.0070.485\u00b10.0890.722\u00b10.0210.758\u00b10.013\nCutMix 0.608\u00b10.017 0.743\u00b10.037 0.776\u00b10.0110.495\u00b10.0400.791\u00b10.0370.797\u00b10.016\nAug 0.507\u00b10.0370.711\u00b10.0440.703\u00b10.0150.444\u00b10.0120.777\u00b10.0210.727\u00b10.032\nOurs0.741\u00b10.010*0.749\u00b10.0090.763\u00b10.011 0.767\u00b10.028* 0.778\u00b10.024 0.807\u00b10.016\nOODBaseline 0.737\u00b10.0140.737\u00b10.0140.737\u00b10.0140.677\u00b10.0240.677\u00b10.0240.677\u00b10.024\nERM 0.461\u00b10.042 0.548\u00b10.0150.688\u00b10.0270.645\u00b10.0310.557\u00b10.0120.556\u00b10.052\nMixUp 0.534\u00b10.037 0.546\u00b10.0350.633\u00b10.0380.567\u00b10.0350.539\u00b10.0330.534\u00b10.036\nCutOut 0.424\u00b10.0660.571\u00b10.0490.730\u00b10.013 0.635\u00b10.0160.585\u00b10.0080.583\u00b10.020\nCutMix 0.526\u00b10.0400.692\u00b10.013 0.724\u00b10.0210.650\u00b10.0200.561\u00b10.0340.529\u00b10.043\nAug 0.426\u00b10.0270.683\u00b10.0180.692\u00b10.0080.670\u00b10.012 0.635\u00b10.028 0.618\u00b10.026\nOurs0.733\u00b10.018*0.759\u00b10.022*0.763\u00b10.008*0.727\u00b10.0570.697\u00b10.030*0.666\u00b10.042\nto prevent a student from being corrupted by the spurious\nfeature.\nFinally, the OOD test sets serve to evaluate the robust-\nness and generalizability of the decision rules learned by the\nnetwork. Here, we see that our student network consistently\nmatches the performance of the clean baseline across both\ndatasets and all bias features, consistently outperforming\nall other approaches.\nThese findings collectively support our hypothesis that\ntask-relevant knowledge distillation across intermediate net-\nwork layers can effectively guide models toward learning\nmore robust and clinically relevant features.\n4.1.4 Effectiveness against multiple concurrent shortcuts\nOur proposed approach has demonstrated promise in the\nmitigation of synthetic shortcuts. However, prior experi-\nments purposefully represent a highly controlled shortcut\nenvironment. Only a single synthetic shortcut is present\nin the training data. Realistically, spurious features are\nunlikely to be constrained to a single source, particularly\nin large datasets. It is important, therefore, that any bias\nmitigation approach is able to mitigate multiple sources of\nbias simultaneously.We augment our training data with two simultaneous\nshortcuts, one correlated with the positive class and the\nother with the negative class. The predictive strength\nof these shortcuts is varied across different training sets.\nAs seen in Figure 5, our method remains effective in the\npresence of multiple bias sources in the training data, and\nacross all tested prevalence rates, consistently outperforming\nall other methods. In the majority of cases, we find that\nour student model remains competitive with the baseline\nmodel trained on entirely clean data.\n4.1.5 Validation on realistic 3D structural biases\nWhile our previous experiments focused primarily on 2D\nimage classification with synthetic shortcuts, we now extend\nour analysis to a more realistic scenario featuring subtle\nstructural biases that more closely resemble real-world med-\nical imaging artifacts. To evaluate our approach in this\nrealistic context, we leverage the SimBA dataset \u2014 a syn-\nthetic brain MRI dataset designed specifically to study bias\nin 3D medical image analysis (Stanley et al., 2024). The\nSimBA dataset features subtle morphological deformations\nthat correlate with disease labels at a 65% prevalence rate.\nThese localized structural modifications represent a more\n457\nBoland, Tsaftaris, and Dahdouh, 2025\n75 85 950.500.550.600.650.700.750.80AUC\nISIC\n75 85 95\nCheXpertPerformance of models trained on data corrupted with two simultaneous shortcuts\nBias feature prevelance rate (%)JTT\nGDRO\nOurs\nAugmentation\nCutMix\nMixUp\nCutout\nBaseline\nFigure 5: Performance of ResNet-18 trained on ISIC and CheXpert datasets featuring multiple simultaneous shortcuts.\nThe green line represents a model trained on a training set before augmenting with synthetic shortcuts. We compare our\nstudent with a specialized teacher to JTT and GroupDRO.\nERM (No Bias) Ours ERM (Morphology Bias)0.000.050.100.150.200.25 TPR\n****** TPR of Models trained on SiMBA dataset\nFigure 6:\u2206TPR of 3D CNN models trained on SimBA\ndata. ERM (No Bias) is trained on data without any bias\nfeatures. Ours and ERM (Morphology Bias) are trained\non data augmented with a synthetic morphological bias\nfeature. All models are evaluated on test data featuring\nthe morphology bias. ** indicates a statistically significant\ndifference in\u2206TPR according to a paired t-test with Bon-\nferroni correction.\nnuanced and challenging form of bias compared to our\nprevious experiments with artificial shortcuts.\nImportantly, a key methodological distinction in these\nexperiments is that all data splits in SimBA (training, val-\nidation, and test) exhibit the same bias prevalence rate.\nThis differs from our previous synthetic shortcut experi-\nments, where validation data contained balanced shortcut\ndistributions. The absence of bias-balanced validation datacreates a significantly more challenging scenario that closely\nmirrors real-world clinical settings, where validation data\noften shares the same biases as training data.\nWe train a lightweight 3D CNN with linear classification\nprobes attached after each convolutional layer. Our teacher\nmodel is trained on a 20% subset of the unbiased training\ndata, while the student model is trained on the biased\ndataset. All volumes were resampled to 96\u00d796\u00d796 voxels.\nFigure 6 presents the performance comparison between\nthree models: our student model guided by a teacher fine-\ntuned on some task-relevant data (Ours), a model trained\non the full unbiased dataset (ERM (No Bias)), and a model\ntrained on the biased dataset using standard Empirical\nRisk Minimization (ERM (Morphology Bias)). The results\ndemonstrate clear performance differences among these\napproaches.\nStatistical analysis using repeated measures ANOVA\nconfirms significant differences between the models. Subse-\nquent pairwise comparisons using paired t-tests with Bon-\nferroni correction reveal statistically significant differences\nin\u2206TPR between the ERM model trained on the mor-\nphologically biased dataset and both alternative models.\nNotably, while a statistically significant difference in\u2206TPR\nremains between our model and ERM (No Bias), the dis-\nparity difference is significantly reduced compared to ERM\n(Morphology Bias).\nOur findings demonstrate that our approach can effec-\ntively mitigate bias even without the benefit of a balanced\nvalidation set to guide the training process. This is sig-\nnificant for real-world medical imaging applications, where\nobtaining bias-balanced validation data is often infeasible.\nThese results further validate the applicability of our method\n458\nPreventing Shortcut Learning through Intermediate Layer Knowledge Distillation\nto complex 3D medical imaging tasks featuring realistic bias\npatterns, suggesting broader potential for clinical applica-\ntions.\n4.2 Method design and optimization\nHaving validated our core approach, we now explore key\ndesign choices that optimize its effectiveness and practical\napplicability.\n4.2.1 Partial layer distillation preserves student\nperformance\nWhile our initial implementation applied knowledge distilla-\ntion across all batch normalization layers of our ResNet-18\nstudents, this comprehensive approach might over-constrain\nthe students\u2019 learning process. We investigate whether\nmore selective application of distillation can maintain or\neven enhance performance. We systematically evaluate dis-\ntillation applied at varying numbers of intermediate layers\nin a ResNet-18 network, from all 17 intermediate layers to\n0 intermediate layers (final classification head only).\nFor partial-layer configurations, we employ a random\nsampling approach where we independently select nlayers\nfrom both the student and teacher networks during each\ntraining epoch. Importantly, these selections are made in-\ndependently, meaning the specific layers chosen may differ\nbetween networks. We pair the selected layers sequen-\ntially based on their relative depth to establish meaningful\nknowledge transfer despite potentially different architectural\npositions.\nTable 4 reveals key insights about the value of our\nintermediate-layer distillation. We note that applying distil-\nlation at fewer intermediate layers (5 \u22129) leads to compa-\nrable performance to distillation applied at all intermediate\nlayers (17), both in terms of AUC and\u2206TPR. In some\ncases on the ISIC dataset, we see improvements in the\nAUC when the loss is applied at fewer layers. We hypothe-\nsize that in these cases, the reduced regularization of the\nKD loss facilitates an improved ability of the network to\nlearn task-relevant features without sacrificing the useful\nguidance away from spurious features. Importantly, when\ndistillation is applied solely at the final classification head\nand not in the intermediate layers ( n= 0), performance\ndeclines significantly and bias increases significantly across\nall experiments. This dramatic deterioration highlights the\ncritical role of intermediate-layer guidance in mitigating\nshortcut learning.\n4.2.2 Low-capacity teachers effectively guide larger\nstudent networks\nTraining the teacher using a small, curated subset of data\ncan pose challenges when applied to significantly largermodels. In this study, we examine whether a low-capacity\nmodel can effectively serve as a teacher for a higher-capacity\nstudent. Specifically, we distill knowledge from an AlexNet\nteacher to a ResNet-18 student, and from a ResNet-18\nteacher to a DenseNet-121 student. To apply knowledge\ndistillation from a low-capacity teacher, we follow a similar\nprotocol to Section 4.2.1. We randomly sample nlayers\nfrom the student network each epoch, where nis equal to\nthe number of classification probes in the teacher model.\nThenlayers of the student network are paired sequentially\nwith the classification probes of the teacher.\nWe train our student on our datasets augmented with\nsynthetic biases and present these results in Table 5. We\ncompare our student to an identical network trained follow-\ning a standard Cross Entropy optimization protocol (ERM).\nEven with a small teacher network, knowledge distillation\nfrom the intermediate layers proves capable of effectively\nmitigating the influence of shortcuts present in the student\ntraining data.\nIn real-world applications, it is more likely that larger\nmodels, such as DenseNet-121\u2014often considered state-of-\nthe-art\u2014are employed instead of smaller networks such as\na ResNet-18. Training a much larger teacher network on\na very limited clean subset increases the likelihood that\nthe teacher will overfit to its training data, negatively im-\npacting its ability to guide the student network toward\nrobust and generalizable features. The efficacy of compact\nteacher networks is, therefore, significant for the practical\nimplementation of our approach.\n4.2.3 Task-specific teacher fine-tuning outperforms\nalternative approaches\nWe propose that a teacher network fine-tuned on a small\nsubset of task-relevant data can provide sufficient insight\nto deter a student network from learning bias features.\nHere, we validate this choice. We consider two alternative\napproaches to our proposed fine-tuned teacher to evaluate\nthe importance of task-specific knowledge in the teacher:\n1.ImageNet pre-trained teacher:We use a teacher net-\nwork pre-trained on the ImageNet dataset without any\ntask-specific fine-tuning. This teacher possesses general\nvisual recognition capabilities from training on diverse\nnatural images but lacks task-specific or domain-specific\nmedical imaging knowledge. Knowledge distillation is\nperformed identically as with our fine-tuned teacher, with\nKL divergence minimization between corresponding inter-\nmediate layers of the student and the pre-trained teacher.\nThis comparison helps us understand whether general\nvisual features from a diverse dataset are sufficient for\nguiding the student away from shortcuts or if task-specific\nknowledge is essential.\n459\nBoland, Tsaftaris, and Dahdouh, 2025\nTable 4: Performance of a ResNet-18 student network tested on the shortcut-corrupted test sets with our distillation\nloss. Distillation loss is applied at different numbers of intermediate layers between 0 and 17. When loss is applied\nat 0 intermediate layers, we only apply KD between the student and teacher\u2019s final outputs. Results are presented as\nMean\u00b1Std over 5-fold cross-validation. Models are markedbestand second-best .\n# layersCheXpert ISIC\nNoise Square (C) Square (R) Noise Square (C) Square (R)\nAUC\u2191170.694\u00b10.0340.742\u00b10.0090.762\u00b10.0080.754\u00b10.0190.761\u00b10.0350.754\u00b10.023\n13 0.688\u00b10.0340.746\u00b10.0070.762\u00b10.005 0.780\u00b10.0180.767\u00b10.012 0.780\u00b10.011\n9 0.687\u00b10.0340.747\u00b10.008 0.756\u00b10.0140.768\u00b10.0130.762\u00b10.0330.783\u00b10.015\n5 0.689\u00b10.044 0.747\u00b10.0080.762\u00b10.0100.775\u00b10.023 0.777\u00b10.0240.807\u00b10.016\n0 0.606\u00b10.0080.640\u00b10.0150.684\u00b10.0140.632\u00b10.0190.667\u00b10.0130.713\u00b10.017\n\u2206TPR\u2193170.272\u00b10.070.083\u00b10.024 0.028\u00b10.0200.091\u00b10.0370.041\u00b10.0380.028\u00b10.02\n13 0.378\u00b10.1880.107\u00b10.0180.049\u00b10.0300.074\u00b10.034 0.019\u00b10.0210.049\u00b10.030\n9 0.301\u00b10.115 0.083\u00b10.0530.046\u00b10.0230.077\u00b10.0270.054\u00b10.0330.046\u00b10.023\n5 0.377\u00b10.1850.079\u00b10.0170.032\u00b10.020 0.068\u00b10.0550.038\u00b10.032 0.032\u00b10.020\n0 0.831\u00b10.0910.662\u00b10.1260.424\u00b10.0740.813\u00b10.1390.617\u00b10.1450.424\u00b10.074\nTable 5: AUC\u2191of a ResNet-18 and DenseNet-121 trained\nand evaluated on shortcut-corrupted data. We compare\nstudent models trained following our knowledge distillation\nprotocol using a low-capacity teacher network (Ours) to\nmodels trained following standard cross-entropy optimiza-\ntion (ERM). The best-performing model is inbold.\nCheXpert\nNoise Square (C) Square (R)\nResNet-18Ours0.68\u00b10.010.74\u00b10.020.75\u00b10.02\nERM 0.49\u00b10.010.53\u00b10.010.55\u00b10.01\nDenseNet-121Ours0.63\u00b10.020.69\u00b10.010.70\u00b10.02\nERM 0.50\u00b10.010.52\u00b10.010.53\u00b10.01\nISIC\nNoise Square (C) Square (R)\nResNet-18Ours0.76\u00b10.020.77\u00b10.030.77\u00b10.03\nERM 0.52\u00b10.010.60\u00b10.010.61\u00b10.01\nDenseNet-121Ours0.72\u00b10.010.72\u00b10.030.74\u00b10.03\nERM 0.51\u00b10.010.63\u00b10.010.62\u00b10.01\n2.Confidence Regularization:The teacher model is re-\nmoved entirely in favor of a form of self-regularization.\nRather than distilling knowledge from a teacher, we en-\ncourage the student network to maintain low confidence\nin its intermediate layer predictions by minimizing the KL\ndivergence between each layer\u2019s predictions and a uniform\nclass probability distribution. This forces the model to\navoid becoming overconfident in any particular features\ntoo early in the network, potentially discouraging reliance\non simple shortcut features. By comparing against this\napproach, we can determine whether the specific guid-\nance from a teacher model provides advantages beyond\nsimply preventing early layer overconfidence.Both alternatives represent reasonable approaches towards\nmitigating shortcut learning: the ImageNet teacher by trans-\nferring robust general visual representations, and confidence\nregularization by directly discouraging overconfidence in\nfeatures at any particular layer. Our fine-tuned specialist\nteacher consistently outperforms both alternatives across\nmost datasets and shortcut types, as shown in Table 6. This\nperformance is achieved without sacrificing fairness. This\nhighlights that a teacher with task-specific knowledge is\nbetter equipped to guide the student away from simplistic\nshortcut features and toward more robust, task-relevant\nfeatures. This is further supported by our findings in Figure\n7, which shows that our student trained with a fine-tuned\nteacher network achieves consistently higher AUC in the in-\ntermediate layers, and that the AUC of our student reaches\nhigher levels earlier in the model.\n4.3 Practical considerations for teacher model training\nFinally, we address critical practical questions about teacher\nmodel requirements that determine the real-world viability\nof our approach.\n4.3.1 Teacher effectiveness scales with training data\nvolume\nA critical question for practical implementation is the volume\nof unbiased data required to train an effective teacher model.\nIn our previous experiments, we evaluated our approach\nusing a teacher network trained on 20% of the full training\ndata from each dataset. To better understand the amount\nof required data, we now assess the efficacy of our approach\nwhen the teacher network is trained on as little as 5% and\n10% of the total training data. In each case, the teacher\ntraining data is excluded from the student\u2019s training. As a\nresult, each student network is trained on 95%, 90%, and\n460\nPreventing Shortcut Learning through Intermediate Layer Knowledge Distillation\n1 3 5 7 9 11 13 15 17\nLayer0.450.500.550.600.650.700.75AUCNoise\nModel\nOursf\nOursp\nOursc\n0.450.500.550.600.650.700.75Constant Square\n1 3 5 7 9 11 13 15 17\nLayer0.450.500.550.600.650.700.75Random SquareAUC per layer across shortcut types\nFigure 7: Per-layer AUC of ResNet-18 students trained on CheXpert data featuring various synthetic shortcuts. Ours f\nis our fine-tuned teacher model, Ours puses an ImageNet-pretrained ResNet-18 as a teacher, and Ours capplies pure\nconfidence regularization in the intermediate layers.\nTable 6: Performance of student models with different knowledge distillation approaches. Ours fis our fine-tuned teacher\nmodel, Ours puses an ImageNet-pretrained ResNet-18 as a teacher, and Ours capplies pure confidence regularization in\nthe intermediate layers. Results are presented as Mean \u00b1Std over 5-fold cross-validation. Models are marked asbestand\nsecond-best . When the difference between first and second best is statistically significant, the best-performing model is\nhighlighted with *.\nModelCheXpert ISIC\nNoise Square (C) Square (R) Noise Square (C) Square (R)\nAUC\u2191Ours f 0.689\u00b10.0440.747\u00b10.008*0.763\u00b10.010*0.775\u00b10.0230.777\u00b10.024*0.807\u00b10.016\nOurs p0.633\u00b10.0250.660\u00b10.0390.684\u00b10.025 0.727\u00b10.0340.680\u00b10.0290.782\u00b10.016\nOurs c0.635\u00b10.037 0.677\u00b10.029 0.673\u00b10.0070.753\u00b10.019 0.731\u00b10.023 0.696\u00b10.071\n\u2206TPR\u2193Ours f 0.377\u00b10.1850.079\u00b10.0170.034\u00b10.0160.068\u00b10.055 0.038\u00b10.0320.070\u00b10.043\nOurs p0.285\u00b10.0820.239\u00b10.1960.115\u00b10.094 0.218\u00b10.1230.318\u00b10.1620.084\u00b10.057\nOurs c0.351\u00b10.073 0.109\u00b10.089 0.140\u00b10.0620.036\u00b10.0330.070\u00b10.067 0.197\u00b10.172\n80% of the full training data, depending on the amount of\ndata used to train the teacher.\nAs illustrated in Figure 8, we observe a clear relationship\nbetween teacher training data volume and student perfor-\nmance. Bias metrics and overall model performance both\nimprove consistently as the amount of training data used\nfor the teacher increases. Notably, even when our teacher\nnetwork is trained on as little as 5% of our original training\ndata (56 images for ISIC), we still observe a substantial\nreduction in bias compared to ERM training.\nThe consistent performance advantage observed with\nminimal unbiased data has significant implications for real-\nworld applications. In clinical settings, where it is oftenchallenging to obtain large amounts of bias-free data, our re-\nsults indicate that even a small, carefully curated dataset can\neffectively guide the mitigation of shortcut learning. This\nfinding greatly enhances the practical applicability of our\napproach, making it more feasible in resource-constrained\nenvironments where extensive manual annotation or bias\nidentification could be prohibitively expensive. Although\nthe need to curate an unbiased training set is not completely\neliminated, the amount of teacher training data required\nmay be modest enough to be achievable in many practical\nscenarios.\n461\nBoland, Tsaftaris, and Dahdouh, 2025\n0.500.550.600.650.700.750.800.85AUC\nNoise\n Constant square\n Random square\n5 10 200.00.20.40.60.81.0 TPR\n5 10 20\n 5 10 20\nEffect of varying amount of teacher training data on performance and disparity\nT eacher training data amount (% of all data)Ours\nERM\nClean baseline\nFigure 8: AUC\u2191(top) and\u2206TPR\u2193(bottom) of ResNet-18 students trained on ISIC data featuring various synthetic\nshortcuts. We vary the proportion of the original training data used to train the teacher network, using subsets\nconsisting of 5%, 10%, and 20% of the original training data. In each case, teacher training data is excluded from the\nstudent\u2019s training. All shortcuts have a 100% prevalence in student training data. As shortcut reliance increases, overall\nperformance (AUC) declines and performance disparity (\u2206TPR) increases.\n4.3.2 Leveraging OOD data for teacher training maintains\neffectiveness\nIn practice, obtaining curated teacher training data from\nthe same distribution as the student\u2019s may not always be\nfeasible. We investigate whether teacher models whose\ntraining data is OOD from the student\u2019s can still effectively\nguide bias mitigation. For this, we focus on the task of\npneumothorax detection, training the teacher on MIMIC\nwhile the student is trained on CheXpert: both chest X-ray\ndatasets, but from different institutions.\nFigure 9 demonstrates that our approach remains ef-\nfective when teacher training data is OOD relative to the\nstudent. Performance improvements scale with teacher data\nvolume, though OOD teachers require substantially more\ntraining data than in-distribution teachers. For example,\nour teacher trained on 10% of the MIMIC training split\nis trained on approximately 400 images. By comparison,\nwe see superior performance in a student trained with an\nin-distribution CheXpert teacher trained on 10% of the\nCheXpert train split (approximately 140 images). This\nincreased data requirement likely reflects the underlying\ndistribution shift between the datasets and the requirement\nfor the teacher network to have learned robust features that\ntransfer across institutional differences in imaging protocols\nand patient populations. We also observe that ResNet-18\nteachers struggle on OOD test sets when they have beentrained on very little data, while ResNet-34 models per-\nform better under the same circumstances. This suggests\nthat the increased model capacity facilitates learning more\ngeneralizable features, particularly on smaller training sets.\nThese findings enhance practical applicability by demon-\nstrating that OOD training data can be used to train the\nteacher model where it is not possible to curate bias-free\nin-distribution data. However, when using OOD data to\ntrain the teacher it is important to consider to larger data\nrequirements required to achieve comparable performance.\n4.3.3 Robustness to shortcut features in teacher training\ndata\nA fundamental assumption of our work up until this point\nis the availability of perfectly clean training data for our\nteacher, free of all shortcuts present in the student\u2019s train-\ning data. However, this assumption may be unrealistic in\npractice, where subtle biases, such as demographic features\nor complex acquisition artifacts, can interact in unexpected\nways that make the identification and removal of all shortcut\nfeatures extremely challenging or impossible. To address\nthis limitation, we investigate the robustness of our ap-\nproach when the teacher\u2019s training data contains residual\nshortcut features at low prevalence rates.\nWe evaluate scenarios where shortcut features appear\nin 5%, 10%, and 15% of positive-class samples (and in\n462\nPreventing Shortcut Learning through Intermediate Layer Knowledge Distillation\n0.5500.5750.6000.6250.6500.6750.7000.7250.7500.775AUC\n10 20 30\nT eacher training data amount (% of all data)0.00.10.20.30.40.5 TPR\nStudent performance with a teacher trained on OOD data\nNoise Constant Square Random Square\nFigure 9:\u2206TPR and AUC of a ResNet-34 student trained\non CheXpert with various synthetic shortcuts. Teacher\nmodel is trained on MIMIC at various subset sizes (between\n10% - 30%). The student is trained on the full CheXpert\ntraining split with a shortcut prevalence of 95%.\nno negative-class samples) in the teacher\u2019s training data,\nwhile maintaining much higher prevalence in the student\u2019s\ntraining data. This simulates realistic conditions where\nshortcut learning mitigation efforts may not be able to\nguarantee, even with smaller, manually curated training\nsets, that the teacher\u2019s training data is entirely bias-free.\nWe focus our analysis on the CheXpert dataset, training\nResNet-18 models using the same protocol as in Section\n4.1.2.\nThe visual complexity of the shortcut has a material\nimpact on the prevalence at which we begin to observe\ndisparities in performance. Figure 10 illustrates both over-\nall performance (AUC) and disparity (\u2206TPR) of a student\nmodel as the prevalence of shortcut features in the teacher\u2019s\ntraining data increases. For complex shortcuts like the ran-\ndom square pattern, even with 15% prevalence in teacher\ndata, both AUC and\u2206TPR of the student remain compa-\nrable with the clean baseline. In contrast, simpler shortcuts\n(noise and constant square) show greater sensitivity to\nteacher data contamination, with noticeable degradation\neven at a prevalence of 5%. Such findings illustrate thatvery simple shortcut features significantly influence model\nlearning even at very low prevalence in the training data.\nOur findings align with the concept of \u201cavailability\u201d\nintroduced by Hermann et al. (2023), who demonstrate\nthat deep learning model\u2019s preferentially utilize the most\navailablefeatures of their training data (i.e., those which\nare most easily identifiable), even if they are lesspredictive\nthan more challenging features. The greater availability of\nour low-level, simpler shortcut features (noise and constant\nsquare) compared to the random square shortcut or any\ndisease feature leads the network to rely more heavily on\nthese features, even if they are present in as little as 5% of\npositive-class samples.\nWhile the curation of bias-free teacher training data\nremains ideal, where the identification and removal of all\npossible shortcuts may be impossible or prohibitively time-\nconsuming and costly, teacher dataset curation should focus\non identifying and removing the most easily identifiable\nshortcut features (e.g., treatment devices, hospital logos,\nobvious markings). Prioritizing these most available features\nprovides the greatest benefit for teacher effectiveness.\nDiscussion\nThis paper addresses the critical challenge of shortcut learn-\ning in medical image analysis, proposing a novel knowledge\ndistillation method leveraging teacher models fine-tuned on\na small amount of unbiased, task-relevant data to guide\nstudent models towards robust features of their training\ndata and away from bias features. Our findings highlight\nseveral key insights and practical advancements:\nFirst, we demonstrate that shortcut learning manifests\nas distinct patterns of overconfidence at intermediate net-\nwork layers, dependent on the type of shortcut involved.\nDiffuse shortcuts, such as noise patterns, tend to emerge in\nearlier network layers, suggesting that they do not require\nsignificant disambiguation to identify. In contrast, localized\nshortcuts like geometric shapes manifest in later layers, in-\ndicating they require more complex feature disambiguation\n(Figure 4) .\nThis layer-specific manifestation has important impli-\ncations for both shortcut detection and mitigation. The\nearly appearance of diffuse shortcuts suggests that initial\nnetwork layers are particularly susceptible to learning sim-\nple, texture-related spurious correlations. This aligns with\nprevious findings about the hierarchical nature of neural\nnetwork learning, where early layers typically learn basic\nfeatures while deeper layers capture more complex patterns\n(Baldock et al., 2021; Chen et al., 2020). The observation\nthat different shortcuts manifest at different depths sug-\ngests that effective mitigation strategies should consider the\nnetwork\u2019s entire processing pipeline rather than focusing\nsolely on the final classification layer.\n463\nBoland, Tsaftaris, and Dahdouh, 2025\n0.500.550.600.650.700.750.800.85AUC\nNoise\n Constant square\n Random square\n5 10 150.00.20.40.60.81.0 TPR\n5 10 15\n 5 10 15\nEffect of varying the shortcut prevalence in the teacher training data on student performance and disparity\nT eacher shortcut prevalence (% of positive-class samples)Ours\nERM\nClean baseline\nFigure 10: AUC\u2191(top) and\u2206TPR \u2193(bottom) of ResNet-18 students trained on CheXpert data featuring various\nsynthetic shortcuts. We vary the prevalence of the shortcut in the data used to train the teacher network. In each case,\nthe teacher is trained on a subset of 20% of the full training split, and the student is trained on the remaining 80%.\nThe shortcut feature has a prevalence of 85% in the student\u2019s training data across all experiments. As shortcut reliance\nincreases, overall performance (AUC) declines and performance disparity (\u2206TPR) increases.\nThis is supported by our finding that distillation from\nan unbiased teacher to the intermediate layers of a student\nmore effectively mitigates shortcut learning than distillation\nbased solely on the final output (Table 4).\nThis finding offers a more nuanced understanding of\nhow unwanted correlations manifest within the network\u2019s\ninternal representations, and we believe that these insights\nare valuable beyond the specific method we propose here.\nFor example, such an observation may serve as an effective\ntool to monitor the learning and performance of deep neural\nnetworks to identify when they may be relying on easy\nspurious features.\nA key contribution of our work is demonstrating that\nknowledge distillation from a teacher network trained on a\nsmall curated dataset significantly outperforms traditional\nde-biasing approaches (Tables 2 & 3). Our method ef-\nfectively prevents the student network from learning to\nrely on bias features present in their training data, sur-\npassing traditional empirical risk minimization and alterna-\ntive approaches such as confidence regularization or using\nImageNet-pretrained teachers (Table 6). The approach con-\nsistently improves generalization and robustness, evidenced\nby substantial performance gains on both in-distribution\nand out-of-distribution test sets for the CheXpert and ISIC\ndatasets (Table 3).\nOur results demonstrate that selective intermediate-layer\ndistillation can be as effective as comprehensive distillationacross all network layers. As shown in Table 4, distilling\nknowledge at only 5-9 layers consistently achieved compara-\nble or superior performance to full 17-layer distillation, both\nin terms of AUC and\u2206TPR. This finding suggests that com-\nprehensive distillation across all layers may be unnecessary\nin most cases and could even add excessive regularization\nto the student network\u2019s learning. While our random layer\nsampling approach proved effective, it represents a naive\nstrategy that does not consider layer-specific contributions\nto shortcut learning. Future work should explore princi-\npled methods for identifying the layers where distillation\nwould be most impactful. A more targeted distillation ap-\nproach could further improve the effectiveness of mitigating\nshortcut learning.\nImportantly, we demonstrate that compact architectures,\nsuch as AlexNet, can effectively guide larger, more sophisti-\ncated networks (ResNet-18 and DenseNet-121), addressing\npractical constraints related to training high-capacity mod-\nels on small, unbiased datasets (Table 5). This finding is\ncritical for practical deployment in clinical contexts, where\nlimited availability of unbiased data and computational\nconstraints can limit the use of larger, resource-intensive\nmodels.\nWhile our experiments are restricted to CNN-based ar-\nchitectures, transformer architectures are increasingly preva-\nlent in medical image analysis literature. Many KD methods\ndesigned for CNNs that leverage the feature-space represen-\n464\nPreventing Shortcut Learning through Intermediate Layer Knowledge Distillation\ntations are not directly applicable to transformer networks\ndue to the architectural differences. We suggest that since\nwe do not leverage feature vectors directly, our method\ncould translate to transformer architectures. Recent litera-\nture has demonstrated the efficacy of similar KD approaches\nin transformer architectures, suggesting that it would be\npossible to apply our framework to transformer architec-\ntures (Liu et al., 2024; Wang et al., 2022). However, we\nsuggest that establishing if the distinctive intermediate-layer\nconfidence trajectories that we see in CNN models (Figure\n4) is also mirrored in transformer architectures.\nThe requirement for a clean, curated dataset to train\nthe teacher model presents a potential limitation, though\nour approach only necessitates a small amount of training\ndata for the teacher network. While such an approach\nstill imposes limitations and necessitates some degree of\nmanual data curation and knowledge of possible sources of\nbias, the burden of doing so for this much smaller subset is\nsignificantly reduced compared to the full training dataset.\nOne interesting avenue for possible future work would\nbe the use of generative models to create clean, synthetic\ntraining data for the teacher model. Additionally, self-\nsupervised or unsupervised techniques for student training\nmay provide a route to remove the need for an unbiased\nteacher model, addressing one of the primary limitations of\nthis work.\nWhile our synthetic bias features provide a controlled\nexperimental environment, a critical next step is the in-\nvestigation of the effectiveness of our approach against a\nbroader range of real-world medical image shortcuts, such as\nthose related to patient demographics. This would further\nvalidate the practical utility of our method across diverse\nclinical contexts.\nOur work advances both the theoretical understanding\nand practical mitigation of shortcut learning in medical\nimage analysis. The demonstrated effectiveness of small\nspecialist teachers and selective layer distillation provides\na promising direction for developing robust medical AI sys-\ntems that can generalize across healthcare environments.\nAs these systems become increasingly prevalent in clinical\nsettings, approaches like ours that can effectively prevent\nshortcut learning while maintaining high performance be-\ncome crucial for ensuring safe and equitable healthcare\ndelivery.\nAcknowledgments\nThis work was supported by the UKRI EPSRC Centre for\nDoctoral Training in Applied Photonics [EP/S022821/1].\nEthical StandardsThe work follows appropriate ethical standards in conducting\nresearch and writing the manuscript, following all applicable\nlaws and regulations regarding treatment of animals or\nhuman subjects.\nConflicts of Interest\nWe declare we don\u2019t have conflicts of interest.\nData availability\nAll datasets used in this study are publicly available at the\nfollowing repositories:\nCheXpert: https://stanfordmlgroup.github.io/\ncompetitions/chexpert/\nISIC: https://challenge.isic-archive.com/dat\na/#2017\nSimBA: https://borealisdata.ca/dataset.xhtm\nl?persistentId=doi:10.5683/SP3/A9SOBZ\nMIMIC: https://www.physionet.org/content/mi\nmic-cxr-jpg/2.1.0/\nFitzpatrick17k: https://github.com/mattgroh/fi\ntzpatrick17k\nThe class-balanced subsets used for training the teacher\nmodels can be reproduced following the methodology de-\nscribed in Section 3.\nReferences\nKaoutar Ben Ahmed, Lawrence O Hall, Dmitry B Goldgof,\nand Ryan Fogarty. Achieving multisite generalization\nfor cnn-based disease diagnosis models by mitigating\nshortcut learning.IEEE Access, 10:78726\u201378738, 2022.\nShuang Ao, Stefan Rueger, and Advaith Siddharthan.\nConfidence-aware calibration and scoring functions for\ncurriculum learning. InFifteenth International Conference\non Machine Vision (ICMV 2022), volume 12701, pages\n558\u2013567. SPIE, 2023.\nRobert Baldock, Hartmut Maennel, and Behnam Neyshabur.\nDeep learning through the lens of example difficulty.Ad-\nvances in Neural Information Processing Systems, 34:\n10876\u201310889, 2021.\nImon Banerjee, Kamanasish Bhattacharjee, John L Burns,\nHari Trivedi, Saptarshi Purkayastha, Laleh Seyyed-\nKalantari, Bhavik N Patel, Rakesh Shiradkar, and Judy\nGichoya. \u201cshortcuts\u201d causing bias in radiology artificial\nintelligence: causes, evaluation, and mitigation.Journal\nof the American College of Radiology, 20(9):842\u2013851,\n2023.\n465\nBoland, Tsaftaris, and Dahdouh, 2025\nPedro RAS Bassi, Andrea Cavalli, and Sergio Decherchi.\nExplanation is all you need in distillation: Mitigating bias\nand shortcut learning.arXiv preprint arXiv:2407.09788,\n2024.\nNourhan Bayasi, Jamil Fayyad, Alceu Bissoto, Ghassan\nHamarneh, and Rafeef Garbi. Biaspruner: Debiased\ncontinual learning for medical image classification. InIn-\nternational Conference on Medical Image Computing and\nComputer-Assisted Intervention, pages 90\u2013101. Springer,\n2024.\nSara Beery, Grant Van Horn, and Pietro Perona. Recogni-\ntion in terra incognita. InProceedings of the European\nConference on Computer Vision (ECCV), pages 456\u2013473,\n2018.\nChristopher Boland, Owen Anderson, Keith A Goatman,\nJohn Hipwell, Sotirios A Tsaftaris, and Sonia Dahdouh.\nAll you need is a guiding hand: Mitigating shortcut bias\nin deep learning models for medical imaging. InMICCAI\nWorkshop on Fairness of AI in Medical Imaging, pages\n67\u201377. Springer, 2024a.\nChristopher Boland, Keith A Goatman, Sotirios A Tsaftaris,\nand Sonia Dahdouh. There are no shortcuts to anywhere\nworth going: Identifying shortcuts in deep learning models\nfor medical image analysis. InMedical Imaging with Deep\nLearning, 2024b.\nCristian Bucilu \u02c7a, Rich Caruana, and Alexandru Niculescu-\nMizil. Model compression. InProceedings of the 12th\nACM SIGKDD international conference on Knowledge\ndiscovery and data mining, pages 535\u2013541, 2006.\nJunbum Cha, Kyungjae Lee, Sungrae Park, and Sanghyuk\nChun. Domain generalization by mutual-information\nregularization with pre-trained models. InEuropean con-\nference on computer vision, pages 440\u2013457. Springer,\n2022.\nJunyi Chai, Taeuk Jang, and Xiaoqian Wang. Fairness with-\nout demographics through knowledge distillation.Ad-\nvances in Neural Information Processing Systems, 35:\n19152\u201319164, 2022.\nMinshuo Chen, Yu Bai, Jason D Lee, Tuo Zhao, Huan\nWang, Caiming Xiong, and Richard Socher. Towards\nunderstanding hierarchical learning: Benefits of neural\nrepresentations.Advances in Neural Information Process-\ning Systems, 33:22134\u201322145, 2020.\nNoel C.F. Codella, David Gutman, M. Emre Celebi, Brian\nHelba, Michael A. Marchetti, Stephen W. Dusza, Aadi\nKalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kit-\ntler, and Allan Halpern. Skin lesion analysis towardmelanoma detection: A challenge at the 2017 interna-\ntional symposium on biomedical imaging (isbi), hosted\nby the international skin imaging collaboration (isic). In\nProceedings - International Symposium on Biomedical\nImaging, volume 2018-April, pages 168\u2013172. IEEE Com-\nputer Society, 2018.\nRamon Correa, Khushbu Pahwa, Bhavik Patel, Celine M\nVachon, Judy W Gichoya, and Imon Banerjee. Effi-\ncient adversarial debiasing with concept activation vec-\ntor\u2014medical image case-studies.Journal of biomedical\ninformatics, 149:104548, 2024.\nNikolay Dagaev, Brett D Roads, Xiaoliang Luo, Daniel N\nBarry, Kaustubh R Patil, and Bradley C Love. A too-\ngood-to-be-true prior to reduce shortcut reliance.Pattern\nRecognition Letters, 166:164\u2013171, 2023.\nUS FDA et al. Marketing submission recommendations\nfor a predetermined change control plan for artificial\nintelligence.Machine Learning (AI/ML)-Enabled Device\nSoftware Functions, 2023.\nRobert Geirhos, Patricia Rubisch, Claudio Michaelis,\nMatthias Bethge, Felix A Wichmann, and Wieland Bren-\ndel. Imagenet-trained cnns are biased towards texture;\nincreasing shape bias improves accuracy and robustness.\nInInternational conference on learning representations,\n2018.\nRobert Geirhos, J \u00a8orn-Henrik Jacobsen, Claudio Michaelis,\nRichard Zemel, Wieland Brendel, Matthias Bethge, and\nFelix A Wichmann. Shortcut learning in deep neural\nnetworks.Nature Machine Intelligence, 2(11):665\u2013673,\n2020.\nAli Ghadiri, Maurice Pagnucco, and Yang Song. Xtran-\nprune: explainability-aware transformer pruning for bias\nmitigation in dermatological disease classification. InIn-\nternational Conference on Medical Image Computing and\nComputer-Assisted Intervention, pages 749\u2013758. Springer,\n2024.\nBen Glocker, Charles Jones, M \u00b4elanie Roschewitz, and Stefan\nWinzeck. Risk of bias in chest radiography deep learning\nfoundation models.Radiology: Artificial Intelligence, 5\n(6):e230060, 2023.\nAry L Goldberger, Luis AN Amaral, Leon Glass, Jeffrey M\nHausdorff, Plamen Ch Ivanov, Roger G Mark, Joseph E\nMietus, George B Moody, Chung-Kang Peng, and H Eu-\ngene Stanley. Physiobank, physiotoolkit, and physionet:\ncomponents of a new research resource for complex phys-\niologic signals.circulation, 101(23):e215\u2013e220, 2000.\n466\nPreventing Shortcut Learning through Intermediate Layer Knowledge Distillation\nMatthew Groh, Caleb Harris, Luis Soenksen, Felix Lau,\nRachel Han, Aerin Kim, Arash Koochek, and Omar Badri.\nEvaluating deep neural networks trained on clinical im-\nages in dermatology with the fitzpatrick 17k dataset. In\nProceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 1820\u20131828, 2021.\nMatthew Groh, Caleb Harris, Roxana Daneshjou, Omar\nBadri, and Arash Koochek. Towards transparency in\ndermatology image datasets with skin tone annotations\nby experts, crowds, and an algorithm.Proceedings of the\nACM on Human-Computer Interaction, 6(CSCW2):1\u201326,\n2022.\nMd Akmal Haidar, Nithin Anchuri, Mehdi Rezagholizadeh,\nAbbas Ghaddar, Philippe Langlais, and Pascal Poupart.\nRail-kd: Random intermediate layer mapping for knowl-\nedge distillation.arXiv preprint arXiv:2109.10164, 2021.\nKatherine L Hermann, Hossein Mobahi, Thomas Fel, and\nMichael C Mozer. On the foundations of shortcut learning.\narXiv preprint arXiv:2310.16228, 2023.\nGeoffrey Hinton. Distilling the knowledge in a neural net-\nwork.arXiv preprint arXiv:1503.02531, 2015.\nJeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Sil-\nviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad\nHaghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert:\nA large chest radiograph dataset with uncertainty labels\nand expert comparison. InProceedings of the AAAI\nconference on artificial intelligence, volume 33, pages\n590\u2013597, 2019.\nAlistair Johnson, Tom Pollard, Roger Mark, Seth Berkowitz,\nand Steven Horng. Mimic-cxr database.PhysioNet10,\n13026:C2JT1Q, 2024.\nAlistair EW Johnson, Tom J Pollard, Seth J Berkowitz,\nNathaniel R Greenbaum, Matthew P Lungren, Chih-ying\nDeng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-\nidentified publicly available database of chest radiographs\nwith free-text reports.Scientific data, 6(1):317, 2019.\nPatrik Kenfack, Ulrich A \u00a8\u0131vodji, and Samira Ebrahimi Kahou.\nAdaptive group robust ensemble knowledge distillation.\narXiv preprint arXiv:2411.14984, 2024.\nNicholas Konz and Maciej A Mazurowski. Reverse engi-\nneering breast mris: Predicting acquisition parameters\ndirectly from images. InMedical Imaging with Deep\nLearning, pages 829\u2013845. PMLR, 2024.\nYi Li and Nuno Vasconcelos. Repair: Removing representa-\ntion bias by dataset resampling. InProceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, pages 9572\u20139581, 2019.Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghu-\nnathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and\nChelsea Finn. Just train twice: Improving group ro-\nbustness without training group information. InInterna-\ntional Conference on Machine Learning, pages 6781\u20136792.\nPMLR, 2021.\nRuiping Liu, Kailun Yang, Alina Roitberg, Jiaming Zhang,\nKunyu Peng, Huayao Liu, Yaonan Wang, and Rainer\nStiefelhagen. Transkd: Transformer knowledge distillation\nfor efficient semantic segmentation.IEEE Transactions\non Intelligent Transportation Systems, 2024.\nNicolas M M \u00a8uller, Jochen Jacobs, Jennifer Williams, and\nKonstantin B \u00a8ottinger. Localized shortcut removal. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 3721\u20133725, 2023.\nNihal Murali, Aahlad Manas Puli, Ke Yu, Rajesh Ranganath,\net al. Shortcut learning through the lens of early training\ndynamics. 2022.\nVincent Olesen, Nina Weng, Aasa Feragen, and Eike Pe-\ntersen. Slicing through bias: Explaining performance\ngaps in medical image analysis using slice discovery meth-\nods. InMICCAI Workshop on Fairness of AI in Medical\nImaging, pages 3\u201313. Springer, 2024.\nCathy Ong Ly, Balagopal Unnikrishnan, Tony Tadic, Tirth\nPatel, Joe Duhamel, Sonja Kandel, Yasbanoo Moayedi,\nMichael Brudno, Andrew Hope, Heather Ross, et al.\nShortcut learning in medical ai hinders generalization:\nmethod for estimating ai model generalization without\nexternal data.NPJ Digital Medicine, 7(1):124, 2024.\nNicholas Petrick, Weijie Chen, Jana G Delfino, Brandon D\nGallas, Yanna Kang, Daniel Krainak, Berkman Sahiner,\nand Ravi K Samala. Regulatory considerations for medical\nimaging ai/ml devices in the united states: concepts and\nchallenges.Journal of Medical Imaging, 10(5):051804\u2013\n051804, 2023.\nShiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto,\nand Percy Liang. Distributionally robust neural net-\nworks for group shifts: On the importance of regu-\nlarization for worst-case generalization.arXiv preprint\narXiv:1911.08731, 2019.\nLaleh Seyyed-Kalantari, Haoran Zhang, Matthew BA Mc-\nDermott, Irene Y Chen, and Marzyeh Ghassemi. Under-\ndiagnosis bias of artificial intelligence algorithms applied\nto chest radiographs in under-served patient populations.\nNature medicine, 27(12):2176\u20132182, 2021.\nHarshay Shah, Kaustav Tamuly, Aditi Raghunathan, Pra-\nteek Jain, and Praneeth Netrapalli. The pitfalls of simplic-\nity bias in neural networks. In H. Larochelle, M. Ranzato,\n467\nBoland, Tsaftaris, and Dahdouh, 2025\nR. Hadsell, M.F. Balcan, and H. Lin, editors,Advances\nin Neural Information Processing Systems, volume 33,\npages 9573\u20139585. Curran Associates, Inc., 2020.\nRaissa Souza, Anthony Winder, Emma AM Stanley, Vibu-\njithan Vigneshwaran, Milton Camacho, Richard Camicioli,\nOury Monchi, Matthias Wilms, and Nils D Forkert. Identi-\nfying biases in a multicenter mri database for parkinson\u2019s\ndisease classification: Is the disease classifier a secret\nsite classifier?IEEE Journal of Biomedical and Health\nInformatics, 2024.\nEmma AM Stanley, Matthias Wilms, and Nils D Forkert. A\nflexible framework for simulating and evaluating biases\nin deep learning-based medical image analysis. InInter-\nnational Conference on Medical Image Computing and\nComputer-Assisted Intervention, pages 489\u2013499. Springer,\n2023.\nEmma AM Stanley, Raissa Souza, Anthony J Winder,\nVedant Gulve, Kimberly Amador, Matthias Wilms, and\nNils D Forkert. Towards objective and systematic evalua-\ntion of bias in artificial intelligence for medical imaging.\nJournal of the American Medical Informatics Association,\n31(11):2613\u20132621, 2024.\nEmma AM Stanley, Raissa Souza, Matthias Wilms, and\nNils D Forkert. Where, why, and how is bias learned in\nmedical image analysis models? a study of bias encod-\ning within convolutional networks using synthetic data.\nEBioMedicine, 111, 2025.\nAbdel Aziz Taha, Leonhard Hennig, and Petr Knoth. Con-\nfidence estimation of classification based on the distribu-\ntion of the neural network output layer.arXiv preprint\narXiv:2210.07745, 2022.\nHuan Tian, Bo Liu, Tianqing Zhu, Wanlei Zhou, and S Yu\nPhilip. Distilling fair representations from fair teachers.\nIEEE Transactions on Big Data, 2024.\nPrasetya Ajie Utama, Nafise Sadat Moosavi, and Iryna\nGurevych. Mind the trade-off: Debiasing nlu models\nwithout degrading the in-distribution performance.arXiv\npreprint arXiv:2005.00315, 2020.\nJiahao Wang, Mingdeng Cao, Shuwei Shi, Baoyuan Wu,\nand Yujiu Yang. Attention probe: Vision transformer\ndistillation in the wild. InICASSP 2022-2022 IEEE In-\nternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 2220\u20132224. IEEE, 2022.\nRyan Wang, Po-Chih Kuo, Li-Ching Chen, Kenneth Patrick\nSeastedt, Judy Wawira Gichoya, and Leo Anthony Celi.Drop the shortcuts: image augmentation improves fair-\nness and decreases ai detection of race and other de-\nmographics from medical images.EBioMedicine, 102,\n2024.\nZeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle\nGenova, Prem Nair, Kenji Hata, and Olga Russakovsky.\nTowards fairness in visual recognition: Effective strategies\nfor bias mitigation. InProceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 8919\u20138928, 2020.\nShirley Wu, Mert Yuksekgonul, Linjun Zhang, and James\nZou. Discover and cure: Concept-aware mitigation of spu-\nrious correlation. InInternational Conference on Machine\nLearning, pages 37765\u201337786. PMLR, 2023.\nYawen Wu, Dewen Zeng, Xiaowei Xu, Yiyu Shi, and Jing-\ntong Hu. Fairprune: Achieving fairness through pruning\nfor dermatological disease diagnosis. InInternational\nConference on Medical Image Computing and Computer-\nAssisted Intervention, pages 743\u2013753. Springer, 2022.\nYuyang Xue, Junyu Yan, Raman Dutt, Fasih Haider, Jing-\nshuai Liu, Steven McDonagh, and Sotirios A Tsaftaris.\nBmft: Achieving fairness via bias-based weight masking\nfine-tuning. InMICCAI Workshop on Fairness of AI in\nMedical Imaging, pages 98\u2013108. Springer, 2024.\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classifiers with localiz-\nable features. InProceedings of the IEEE/CVF interna-\ntional conference on computer vision, pages 6023\u20136032,\n2019.\nBrian Hu Zhang, Blake Lemoine, and Margaret Mitchell.\nMitigating unwanted biases with adversarial learning. In\nProceedings of the 2018 AAAI/ACM Conference on AI,\nEthics, and Society, pages 335\u2013340, 2018.\nHaoran Zhang, Natalie Dullerud, Karsten Roth, Lauren\nOakden-Rayner, Stephen Pfohl, and Marzyeh Ghassemi.\nImproving the fairness of chest x-ray classifiers. In Gerardo\nFlores, George H Chen, Tom Pollard, Joyce C Ho, and\nTristan Naumann, editors,Proceedings of the Conference\non Health, Inference, and Learning, volume 174 ofPro-\nceedings of Machine Learning Research, pages 204\u2013233.\nPMLR, 07\u201308 Apr 2022.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk mini-\nmization.arXiv preprint arXiv:1710.09412, 2017.\nZhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. InPro-\n468\nPreventing Shortcut Learning through Intermediate Layer Knowledge Distillation\nceedings of the AAAI conference on artificial intelligence,\nvolume 34, pages 13001\u201313008, 2020.\n469\nBoland, Tsaftaris, and Dahdouh, 2025\nAppendix A. Shortcut reliance results in\nintermediate-layer overconfidence\nThis section provides comprehensive layer-wise confidence\nanalysis extending our main findings from Section 4.1.1\nto additional architectures and datasets, demonstrating\nthe generalizability of our core observation that shortcut\nlearning manifests distinctly across network layers across\narchitectures and datasets.\nFigures 11 and 12 present intermediate layer confidence\nfor ResNet-18 and DenseNet-121 architectures. In our main\nexperiments, we use validation sets where shortcuts are\nbalanced across classes (present equally in both positive\nand negative samples). Under these conditions, we observe\nthat the overconfidence signal in DenseNet-121 is notice-\nably weaker than in ResNet-18. We hypothesize that the\nsubstantially larger capacity of DenseNet-121, combined\nwith early stopping on validation data where shortcuts no\nlonger correlate with class labels, prevents the network from\nfully developing shortcut dependencies.\nTo examine how validation set composition affects these\npatterns, Figures 13 and 14 show the same architectures\ntrained with validation sets that maintain the same shortcut-\nclass correlations as the training data. Under these condi-\ntions, the overconfidence signal becomes much more pro-\nnounced even in high-capacity models like DenseNet-121,\nas the validation setup no longer provides feedback that\ndiscourages shortcut reliance during training.\nAppendix B. Overall performance\nThis section provides detailed performance breakdowns, in-\ncluding AUC values and True Positive Rate (TPR) analysis\nfor bias-aligned and bias-contrasting samples, supplement-\ning the\u2206TPR analysis presented in Section 4.1.2 of the\nmain text.\nTable 7 presents AUC values across all experimental\nconditions, showing that our method consistently achieves\nperformance competitive with the clean baseline even when\ntrained on heavily biased data. Notably, our approach main-\ntains stable performance across varying bias prevalence rates,\nwhile competing methods show significant degradation at\nhigher bias levels.\nTables 8 and 9 provide absolute TPR values for CheX-\npert and ISIC datasets respectively, breaking down perfor-\nmance for bias-aligned samples (where shortcut presence\nmatches training correlation with the class-label) and bias-\ncontrasting samples (where shortcuts oppose training cor-\nrelation with the class-label). We demonstrate that our\nmethod achieves more balanced performance across both\nsample types, indicating reduced reliance on shortcut fea-\ntures for prediction. We note that our method demonstrates\nlower TPR for bias-aligned samples compared to other meth-ods, and that the better TPR seen in other models is likely\na result of shortcut reliance, causing a TPR much higher\nthan the clean baseline. We consistently observe that our\nmethod achieves a TPR for biased-aligned samples that is\nclosest to the clean baseline.\nAppendix C. Teacher sensitivity to shortcut\nfeatures\nHere, we highlight the sensitivity of our teacher networks to\ncorruption from shortcut features. Figure 15 supplements\nour findings in Section 4.3.3 and supports our argument that\npractitioners should prioritize removing the most available\n(easily identifiable) shortcuts from the teacher training data\nas these cause the most significant harm.\n470\nPreventing Shortcut Learning through Intermediate Layer Knowledge Distillation\n1 3 5 7 9 11 13 15 17\nLayer0.00.10.20.30.40.5ConfidenceNoise\nModel\nClean training data\nBiased training data0.00.10.20.30.40.5Constant Square\n1 3 5 7 9 11 13 15 17\nLayer0.00.10.20.30.40.5Random SquareConfidence per layer across shortcut types\nFigure 11: Per-layer confidence of ResNet-18 students trained on ISIC data featuring various synthetic shortcuts.\n1 5 9 13 17 21 25 29 33 37 41 45 49 53 57\nLayer0.00.10.20.30.40.5ConfidenceNoise\nModel\nClean training data\nBiased training data\n0.00.10.20.30.40.5Constant Square\n1 5 9 13 17 21 25 29 33 37 41 45 49 53 57\nLayer0.00.10.20.30.40.5Random SquareConfidence per layer across shortcut types\nFigure 12: Per-layer confidence of DenseNet-121 students trained on CheXpert data featuring various synthetic shortcuts.\nShortcut prevalence in the train split is 100%. In the validation and test split, the shortcut feature is balanced between\nclasses.\n471\nBoland, Tsaftaris, and Dahdouh, 2025\n1 5 9 13 17 21 25 29 33 37 41 45 49 53 57\nLayer0.00.10.20.30.40.5ConfidenceNoise\nModel\nClean training data\nBiased training data0.00.10.20.30.40.5Constant Square\n1 5 9 13 17 21 25 29 33 37 41 45 49 53 57\nLayer0.00.10.20.30.40.5Random SquareConfidence per layer across shortcut types\nFigure 13: Per-layer confidence of DenseNet-121 students trained on CheXpert data featuring various synthetic shortcuts.\nShortcut prevalence in the train and validation splits is 100%. In the test split, the shortcut feature is balanced between\nclasses.\n1 5 9 13 17 21 25 29 33 37 41 45 49 53 57\nLayer0.00.10.20.30.40.5ConfidenceNoise\nModel\nClean training data\nBiased training data0.00.10.20.30.40.5Constant Square\n1 5 9 13 17 21 25 29 33 37 41 45 49 53 57\nLayer0.00.10.20.30.40.5Random SquareConfidence per layer across shortcut types\nFigure 14: Per-layer confidence of DenseNet-121 students trained on ISIC data featuring various synthetic shortcuts.\nShortcut prevalence in the train and validation splits is 100%. In the test split, the shortcut feature is balanced between\nclasses.\n472\nPreventing Shortcut Learning through Intermediate Layer Knowledge Distillation\nTable 7:AUC\u2191 of a ResNet-18 trained on data with various bias prevalence rates. Results are presented as Mean \u00b1Std\nover 5-fold cross-validation. Models are marked asbestand second-best .\nPrev.ModelCheXpert ISIC\n(%) Noise Square (C) Square (R) Noise Square (C) Square (R)\n0 Baseline 0.709\u00b10.0240.755\u00b10.0130.752\u00b10.0130.749\u00b10.0240.809\u00b10.0190.808\u00b10.019\n100ERM 0.489\u00b10.0120.533\u00b10.0070.554\u00b10.0050.521\u00b10.010 0.600\u00b10.0110.612\u00b10.007\nMixUp 0.509\u00b10.0070.539\u00b10.0080.549\u00b10.0050.489\u00b10.0120.555\u00b10.0280.591\u00b10.014\nCutOut 0.498\u00b10.0290.548\u00b10.0100.583\u00b10.0110.521\u00b10.0230.627\u00b10.0130.639\u00b10.014\nCutMix 0.529\u00b10.007 0.680\u00b10.025 0.759\u00b10.011 0.516\u00b10.0150.753\u00b10.038 0.784\u00b10.017\nAug 0.483\u00b10.0090.585\u00b10.0170.555\u00b10.0100.518\u00b10.0060.731\u00b10.0170.626\u00b10.014\nOurs0.689\u00b10.0440.747\u00b10.0080.761\u00b10.0100.775\u00b10.0230.777\u00b10.0240.806\u00b10.016\n95ERM 0.579\u00b10.0090.597\u00b10.0070.595\u00b10.0150.602\u00b10.0320.645\u00b10.0240.651\u00b10.028\nMixUp 0.559\u00b10.0080.578\u00b10.0080.603\u00b10.0240.542\u00b10.0430.609\u00b10.0210.603\u00b10.024\nCutOut 0.568\u00b10.0130.599\u00b10.0130.611\u00b10.0400.608\u00b10.024 0.641\u00b10.0110.611\u00b10.040\nCutMix 0.567\u00b10.0200.739\u00b10.024 0.760\u00b10.0200.587\u00b10.0290.768\u00b10.031 0.760\u00b10.020\nAug 0.578\u00b10.014 0.628\u00b10.0280.597\u00b10.0150.608\u00b10.0260.745\u00b10.0100.633\u00b10.013\nGDRO 0.566\u00b10.0080.585\u00b10.0100.613\u00b10.0140.595\u00b10.0120.631\u00b10.0080.673\u00b10.015\nJTT 0.565\u00b10.0070.588\u00b10.0040.602\u00b10.0180.591\u00b10.0260.630\u00b10.0150.672\u00b10.016\nOurs0.693\u00b10.0290.756\u00b10.0050.756\u00b10.011 0.778\u00b10.0200.797\u00b10.0150.803\u00b10.009\n85ERM 0.621\u00b10.0180.625\u00b10.0180.661\u00b10.0130.694\u00b10.018 0.751\u00b10.0160.748\u00b10.030\nMixUp 0.598\u00b10.0170.630\u00b10.0300.697\u00b10.0210.664\u00b10.0340.687\u00b10.0190.706\u00b10.052\nCutOut 0.596\u00b10.0180.639\u00b10.0260.683\u00b10.0180.663\u00b10.0350.731\u00b10.0400.739\u00b10.029\nCutMix 0.604\u00b10.0150.742\u00b10.049 0.766\u00b10.0160.634\u00b10.0150.766\u00b10.0390.796\u00b10.019\nAug 0.636\u00b10.0340.677\u00b10.0400.661\u00b10.0350.667\u00b10.0390.768\u00b10.018 0.694\u00b10.041\nGDRO 0.616\u00b10.0120.649\u00b10.0150.725\u00b10.0150.655\u00b10.0270.732\u00b10.0220.780\u00b10.018\nJTT0.780\u00b10.0180.659\u00b10.0080.699\u00b10.0100.656\u00b10.0250.721\u00b10.0220.771\u00b10.018\nOurs 0.708\u00b10.037 0.756\u00b10.0080.760\u00b10.015 0.798\u00b10.0240.782\u00b10.0310.787\u00b10.026\n75ERM 0.681\u00b10.0230.686\u00b10.0380.718\u00b10.0150.711\u00b10.0280.763\u00b10.0130.784\u00b10.013\nMixUp 0.677\u00b10.0250.705\u00b10.0300.746\u00b10.0260.673\u00b10.0180.720\u00b10.0210.757\u00b10.016\nCutOut 0.675\u00b10.0270.707\u00b10.0260.744\u00b10.0110.715\u00b10.0340.726\u00b10.0200.757\u00b10.026\nCutMix 0.662\u00b10.0290.748\u00b10.027 0.759\u00b10.0310.696\u00b10.0250.768\u00b10.0300.781\u00b10.029\nAug 0.676\u00b10.0260.712\u00b10.0410.701\u00b10.0410.722\u00b10.029 0.766\u00b10.0220.731\u00b10.035\nGDRO 0.692\u00b10.010 0.731\u00b10.0120.768\u00b10.0100.696\u00b10.0130.770\u00b10.008 0.796\u00b10.008\nJTT 0.687\u00b10.0200.730\u00b10.0170.751\u00b10.0260.694\u00b10.0090.769\u00b10.0110.794\u00b10.012\nOurs0.709\u00b10.0390.760\u00b10.0150.762\u00b10.017 0.792\u00b10.0230.773\u00b10.0130.781\u00b10.029\n473\nBoland, Tsaftaris, and Dahdouh, 2025\nTable 8:TPR\u2191 of bias-aligned and bias-contrasting samples for a ResNet-18 trained on CheXpert data with various\nbias prevalence rates. Results are presented as Mean \u00b1Std over 5-fold cross-validation. Models are marked asbestand\nsecond-best . When multiple models achieve identical performance, all are highlighted.\nPrev.ModelNoise Square (C) Square (R)\n(%) Bias-Aligned Bias-Contrasting Bias-Aligned Bias-Contrasting Bias-Aligned Bias-Contrasting\n0 Baseline 0.942\u00b10.0360.811\u00b10.0690.804\u00b10.0800.811\u00b10.0690.822\u00b10.0690.811\u00b10.069\n100ERM1.000\u00b10.0000.000\u00b10.0001.000\u00b10.0000.000\u00b10.0000.995\u00b10.0050.004\u00b10.007\nMixUp1.000\u00b10.0000.013\u00b10.026 0.998\u00b10.003 0.000\u00b10.0000.974\u00b10.0180.004\u00b10.007\nCutOut1.000\u00b10.0000.001\u00b10.0011.000\u00b10.0000.000\u00b10.0000.982\u00b10.011 0.004\u00b10.009\nCutMix1.000\u00b10.0000.007\u00b10.0050.929\u00b10.0450.427\u00b10.114 0.894\u00b10.0140.765\u00b10.015\nAug 0.957\u00b10.092 0.000\u00b10.0000.996\u00b10.0040.017\u00b10.0150.982\u00b10.011 0.004\u00b10.009\nOurs 0.942\u00b10.0330.565\u00b10.1610.862\u00b10.0050.783\u00b10.0170.839\u00b10.0630.805\u00b10.050\n95ERM 0.995\u00b10.0100.055\u00b10.0220.993\u00b10.0110.081\u00b10.0880.973\u00b10.0140.182\u00b10.105\nMixUp 0.997\u00b10.0060.070\u00b10.1070.992\u00b10.0070.005\u00b10.0100.948\u00b10.0150.253\u00b10.100\nCutOut 0.997\u00b10.0040.047\u00b10.0510.987\u00b10.0130.074\u00b10.0610.959\u00b10.0070.330\u00b10.127\nCutMix 0.999\u00b10.001 0.025\u00b10.0290.908\u00b10.0220.583\u00b10.047 0.845\u00b10.0290.709\u00b10.070\nAug 0.997\u00b10.0030.098\u00b10.075 0.962\u00b10.0390.183\u00b10.1320.978\u00b10.014 0.159\u00b10.079\nJTT1.000\u00b10.0000.018\u00b10.0110.999\u00b10.0010.054\u00b10.0310.956\u00b10.0800.283\u00b10.041\nGDRO1.000\u00b10.0000.014\u00b10.0100.997\u00b10.002 0.019\u00b10.0140.981\u00b10.0110.279\u00b10.096\nOurs 0.935\u00b10.0370.563\u00b10.0920.862\u00b10.0310.773\u00b10.0190.810\u00b10.0510.775\u00b10.021\n85ERM 0.960\u00b10.0390.215\u00b10.1670.974\u00b10.0130.239\u00b10.1150.926\u00b10.0470.503\u00b10.106\nMixUp 0.985\u00b10.0110.286\u00b10.1190.962\u00b10.0350.284\u00b10.1170.903\u00b10.0220.566\u00b10.031\nCutOut0.988\u00b10.0090.178\u00b10.0720.977\u00b10.014 0.320\u00b10.2040.938\u00b10.0290.557\u00b10.127\nCutMix 0.979\u00b10.0210.246\u00b10.1060.886\u00b10.0390.675\u00b10.050 0.835\u00b10.0360.737\u00b10.037\nAug 0.957\u00b10.0390.292\u00b10.208 0.954\u00b10.0070.437\u00b10.1970.927\u00b10.0300.397\u00b10.144\nJTT 0.987\u00b10.005 0.268\u00b10.0340.968\u00b10.0080.435\u00b10.0300.933\u00b10.029 0.555\u00b10.144\nGDRO 0.983\u00b10.0060.224\u00b10.0330.977\u00b10.0110.353\u00b10.0570.922\u00b10.0120.654\u00b10.071\nOurs 0.929\u00b10.0420.581\u00b10.1190.861\u00b10.0600.755\u00b10.0300.860\u00b10.0260.801\u00b10.044\n75ERM 0.879\u00b10.0990.434\u00b10.1930.903\u00b10.1250.516\u00b10.0770.910\u00b10.0250.708\u00b10.078\nMixUp 0.944\u00b10.0170.564\u00b10.1300.888\u00b10.0660.557\u00b10.0880.849\u00b10.0440.692\u00b10.032\nCutOut 0.941\u00b10.0430.481\u00b10.0870.855\u00b10.1190.494\u00b10.1870.897\u00b10.021 0.729\u00b10.063\nCutMix 0.926\u00b10.0450.399\u00b10.0850.852\u00b10.0720.714\u00b10.091 0.819\u00b10.0280.763\u00b10.012\nAug 0.909\u00b10.0510.463\u00b10.1710.878\u00b10.0590.470\u00b10.2050.849\u00b10.0320.505\u00b10.071\nJTT 0.949\u00b10.023 0.497\u00b10.0520.928\u00b10.0250.667\u00b10.0360.871\u00b10.0280.725\u00b10.060\nGDRO0.951\u00b10.0040.444\u00b10.0150.904\u00b10.023 0.575\u00b10.0220.866\u00b10.0220.752\u00b10.019\nOurs 0.926\u00b10.0400.561\u00b10.162 0.878\u00b10.0290.751\u00b10.0420.854\u00b10.0440.793\u00b10.056\n474\nPreventing Shortcut Learning through Intermediate Layer Knowledge Distillation\nTable 9:TPR\u2191 of bias-aligned and bias-contrasting samples for a ResNet-18 trained on ISIC data with various bias\nprevalence rates. Results are presented as Mean \u00b1Std over 5-fold cross-validation. Models are marked asbestand\nsecond-best . When multiple models achieve identical performance, all are highlighted.\nPrev.ModelNoise Square (C) Square (R)\n(%) Bias-Aligned Bias-Contrasting Bias-Aligned Bias-Contrasting Bias-Aligned Bias-Contrasting\n0 Baseline 0.449\u00b10.1130.858\u00b10.0270.802\u00b10.0300.858\u00b10.0270.808\u00b10.0360.858\u00b10.027\n100ERM1.000\u00b10.0000.000\u00b10.000 1.000\u00b10.0000.223\u00b10.0931.000\u00b10.0000.156\u00b10.168\nMixUp 0.998\u00b10.004 0.000\u00b10.000 1.000\u00b10.0000.127\u00b10.1500.990\u00b10.012 0.238\u00b10.146\nCutOut1.000\u00b10.0000.000\u00b10.000 1.000\u00b10.0000.552\u00b10.0650.986\u00b10.0160.712\u00b10.066\nCutMix1.000\u00b10.0000.000\u00b10.000 0.905\u00b10.0320.546\u00b10.0830.856\u00b10.0210.729\u00b10.050\nAug1.000\u00b10.0000.000\u00b10.000 0.961\u00b10.019 0.800\u00b10.062 0.971\u00b10.0230.467\u00b10.136\nOurs 0.792\u00b10.0660.771\u00b10.090.829\u00b10.0440.852\u00b10.0280.829\u00b10.0420.900\u00b10.016\n95ERM1.000\u00b10.0000.041\u00b10.0190.993\u00b10.010 0.132\u00b10.0260.987\u00b10.020 0.285\u00b10.097\nMixUp 0.973\u00b10.0470.021\u00b10.0190.998\u00b10.0040.062\u00b10.0350.984\u00b10.0200.229\u00b10.081\nCutOut1.000\u00b10.0000.054\u00b10.0470.985\u00b10.0230.406\u00b10.2630.973\u00b10.0270.544\u00b10.144\nCutMix 0.998\u00b10.004 0.076\u00b10.079 0.909\u00b10.0170.598\u00b10.0980.880\u00b10.0240.757\u00b10.050\nAug 0.996\u00b10.0080.062\u00b10.0590.965\u00b10.0150.761\u00b10.079 0.985\u00b10.0140.429\u00b10.126\nGDRO 0.996\u00b10.0050.029\u00b10.0170.987\u00b10.0050.188\u00b10.0350.980\u00b10.0100.503\u00b10.024\nJTT1.000\u00b10.0000.054\u00b10.0310.991\u00b10.0110.198\u00b10.0490.987\u00b10.0140.482\u00b10.070\nOurs 0.858\u00b10.0420.781\u00b10.0620.898\u00b10.0240.798\u00b10.0350.873\u00b10.0310.821\u00b10.036\n85ERM 0.958\u00b10.0080.685\u00b10.034 0.931\u00b10.0200.555\u00b10.1040.954\u00b10.0110.660\u00b10.083\nMixUp 0.952\u00b10.0160.398\u00b10.1590.939\u00b10.0310.449\u00b10.1000.905\u00b10.0430.511\u00b10.084\nCutOut 0.952\u00b10.0240.442\u00b10.2150.954\u00b10.0220.483\u00b10.1550.933\u00b10.015 0.683\u00b10.130\nCutMix 0.970\u00b10.0250.317\u00b10.1130.895\u00b10.0330.681\u00b10.0740.871\u00b10.0250.785\u00b10.040\nAug 0.950\u00b10.0290.274\u00b10.1850.883\u00b10.0560.768\u00b10.066 0.913\u00b10.0510.594\u00b10.123\nGDRO0.980\u00b10.0100.283\u00b10.0830.945\u00b10.0340.609\u00b10.0520.903\u00b10.0390.806\u00b10.017\nJTT 0.978\u00b10.011 0.275\u00b10.1190.950\u00b10.019 0.600\u00b10.0580.911\u00b10.0290.753\u00b10.037\nOurs 0.855\u00b10.0330.792\u00b10.1000.863\u00b10.0470.817\u00b10.0320.861\u00b10.0310.811\u00b10.086\n75ERM 0.923\u00b10.0220.670\u00b10.048 0.829\u00b10.0410.616\u00b10.0510.894\u00b10.0190.721\u00b10.083\nMixUp0.948\u00b10.0190.477\u00b10.1680.927\u00b10.035 0.645\u00b10.1260.902\u00b10.0170.732\u00b10.040\nCutOut 0.933\u00b10.0120.668\u00b10.1890.956\u00b10.0140.816\u00b10.0470.896\u00b10.041 0.724\u00b10.136\nCutMix 0.910\u00b10.0400.580\u00b10.0980.867\u00b10.0290.714\u00b10.0560.863\u00b10.0260.793\u00b10.056\nAug 0.915\u00b10.0340.591\u00b10.1550.863\u00b10.0470.714\u00b10.0780.894\u00b10.0480.636\u00b10.118\nGDRO 0.942\u00b10.019 0.497\u00b10.0510.912\u00b10.0120.730\u00b10.0210.892\u00b10.0090.834\u00b10.018\nJTT 0.940\u00b10.0200.481\u00b10.0650.923\u00b10.0120.719\u00b10.0270.875\u00b10.0160.789\u00b10.036\nOurs 0.829\u00b10.0410.840\u00b10.0600.875\u00b10.0350.730\u00b10.099 0.869\u00b10.0410.805\u00b10.032\n475\nBoland, Tsaftaris, and Dahdouh, 2025\n0.5750.6000.6250.6500.6750.7000.7250.750AUC\nT eacher AUC vs Shortcut Prevalence\n5 10 15\nShortcut Prevalence0.10.20.30.40.50.60.70.8TPR\nT eacher TPR Disparity vs Shortcut PrevalenceNoise Constant Square Random Square\nFigure 15: AUC and\u2206TPR of ResNet-18 teacher networks\ntrained on 20% subsets of the CheXpert, corrupted with\nshortcut features at various prevalence. All test sets feature\nthe same shortcut feature as is present in the train split,\nevenly distributed between samples belonging to each class,\nand therefore is no longer a useful predictive feature.\n476\n",
    "title": "Preventing Shortcut Learning in Medical Image Analysis through Intermediate Layer Knowledge Distillation from Specialist Teachers",
    "arxiv_id": "2511.17421v1",
    "category": "recent",
    "authors": [
      "Christopher Boland",
      "Sotirios Tsaftaris",
      "Sonia Dahdouh"
    ],
    "published": "2025-11-21",
    "num_pages": 30,
    "num_chars": 112043
  },
  {
    "text": "DS-Span: Single-Phase Discriminative Subgraph\nMining for Efficient Graph Embeddings\nYeamin Kaiser1, Muhammed Tasnim Bin Anwar1, Bholanath Das1,\nChowdhury Farhan Ahmed1, and Md. Tanvir Alam1\nUniversity of Dhaka\nAbstract.Graph representation learning seeks to transform complex,\nhigh-dimensional graph structures into compact vector spaces that pre-\nservebothtopologyandsemantics.Amongthevariousstrategies,subgraph-\nbased methods provide an interpretable bridge between symbolic pat-\ntern discovery and continuous embedding learning. Yet, existing frequent\nor discriminative subgraph mining approaches often suffer from redun-\ndant multi-phase pipelines, high computational cost, and weak coupling\nbetween mined structures and their discriminative relevance. We pro-\nposeDS-Span, a single-phase discriminative subgraph mining frame-\nworkthatunifiespatterngrowth,pruning,andsupervision-drivenscoring\nwithin one traversal of the search space. DS-Span introduces acoverage-\ncapped eligibility mechanismthat dynamically limits exploration once a\ngraph is sufficiently represented, and aninformation-gain-guided selec-\ntionthat promotes subgraphs with strong class-separating ability while\nminimizing redundancy. The resulting subgraph set serves as an effi-\ncient, interpretable basis for downstream graph embedding and clas-\nsification. Extensive experiments across benchmarks demonstrate that\nDS-Span generates more compact and discriminative subgraph features\nthanpriormulti-stagemethods,achievinghigherorcomparableaccuracy\nwith significantly reduced runtime. These results highlight the potential\nof unified, single-phase discriminative mining as a foundation for scalable\nand interpretable graph representation learning.\nKeywords:Graph Mining\u00b7Discriminative Subgraph\u00b7Graph Embed-\nding\u00b7Frequent Pattern Mining\u00b7Representation Learning\n1 Introduction\nGraphs provide a natural and powerful formalism for representing relational\ndata in diverse domains such as chemistry, bioinformatics, transportation, and\nsocial networks. In these settings, the ability to learn compact yet expressive\nrepresentations of graphs is crucial for tasks such as classification, similarity\nsearch, and clustering. Graph representation learning aims to transform discrete\ngraph structures into continuous vector spaces while preserving structural and\nsemantic information.\nAmong the various approaches to graph representation,subgraph-based meth-\nodsoccupy a unique position by bridging symbolic and continuous perspectives.arXiv:2511.17419v1  [cs.LG]  21 Nov 2025\n2 Kaiser et al.\nInstead of directly parameterizing the graph through neural architectures, these\nmethods first discoverfrequentordiscriminativesubstructures that recur across\nthe dataset and then use their presence or embedding to form interpretable\ngraph-level features. This paradigm provides both transparency and efficiency,\nespecially when domain interpretability and explainability are essential.\nClassical algorithms such asgSpan[9] introduced depth-first search (DFS)-\nbased canonical coding and pattern growth for efficient enumeration of frequent\nsubgraphs.Latermethodsextendedthisideatowarddiscriminativemining,iden-\ntifying subgraphs that maximize class-separating power rather than sheer fre-\nquency. Despite their success, most existing discriminative mining frameworks\nshare three key limitations:\n1.Redundant multi-phase mining:Frequent and discriminative phases are\ntypically decoupled, leading to repeated subgraph enumeration and redun-\ndant computation.\n2.Weak coupling between supervision and mining:Class-label informa-\ntion is often used only after enumeration, rather than guiding the mining\nprocess itself.\n3.Limited scalability:Exhaustive enumeration on large or dense graphs pro-\nducessubstantialoverlapamongcandidatesubgraphs,causingcombinatorial\nexplosion and long runtimes.\nToaddressthesechallenges,weproposeDS-Span,asingle-phase discrimina-\ntive subgraph miningframework that integrates subgraph enumeration, pruning,\nand discriminative scoring into a unified search process. DS-Span maintains the\ncanonical DFS-based traversal of gSpan but introduces several novel mechanisms\nto enhance both efficiency and discriminative relevance:\n\u2013 Coverage-capped eligibility:graphs are dynamically excluded from fur-\nther exploration once their feature coverage exceeds a user-defined threshold,\npreventing redundant pattern growth.\n\u2013 Information-gain-guided search:candidate subgraphs are scored using\ninformation gain to prioritize those that best separate graph classes while\nminimizing overlap among features.\n\u2013 Single-phase pruning:all constraints on support, coverage, and discrimi-\nnative power are applied concurrently within one traversal, eliminating the\nneed for iterative re-mining or post-hoc filtering.\nBy embedding these mechanisms directly into the mining process, DS-Span\ndiscovers a compact and highly discriminative subset of subgraphs that can\nserve as efficient inputs for downstream embedding and classification models.\nThis unified framework drastically reduces redundancy and computational over-\nhead, leading to substantial runtime savings without sacrificing predictive per-\nformance.\nOur experimental study across diverse datasets demonstrates that DS-Span\nproduces more informative and less redundant subgraph features than existing\nDS-Span3\nmulti-phase methods. The resulting embeddings exhibit stronger class separa-\ntion and lower variance across folds, underscoring the efficacy of single-phase,\ndiscriminatively guided subgraph mining as a foundation for scalable and inter-\npretable graph representation learning.\n2 Proposed Approach\nWe presentDS-Span, a single-phase discriminative subgraph mining frame-\nwork that produces a compact, high-utility feature set for downstream graph\nembedding and classification. DS-Span preserves the DFS-code canonicalization\nand rightmost-path extensions ofgSpan[9], but differs in three design choices:\n(i) acoverage-capped eligibilitymechanism that prunes graphs from further ex-\nploration once sufficiently represented, (ii) asingle-phasemining schedule (no\nmulti-phase restarts), and (iii) adiscriminative, coverage-constrainedfeature se-\nlector. These choices eliminate redundant re-mining, guide the search toward\nclass-separating patterns, and improve time/memory efficiency.\n2.1 Preliminaries and Notation\nLetD={G i}N\ni=1be a collection of labeled graphs with (training) class labels\nyi\u2208 C. A graphG= (V, E, \u2113 V, \u2113E)has node labels\u2113 V:V\u2192\u03a3 Vand edge labels\n\u2113E:E\u2192\u03a3 E. For two graphsSandG, we writeS\u2286 isoGifSis subgraph-\nisomorphic toG. Thesupportof a patternSinDis\nsupp(S) :=\f\f{i\u2208[N] :S\u2286 isoGi}\f\f.\nWe use DFS-code tuples(u, v, \u2113 u, \u2113v, \u2113e)and rightmost-path extensions as in [9].\nA DFS code iscanonicalif it is minimal in the gSpan lexicographic order; the\ncanonical minimality test prevents duplicate patterns.\nCoverage.For a set of mined subgraphsFand a graphG i, define the coverage\ncov(i;F) :=\f\f{S\u2208 F:S\u2286 isoGi}\f\f. Two thresholds govern coverage: a per-graph\nminimum coverage min_cov\u2208N, and a per-graphcap cap :=\u03b3\u00b7min_covwith\n\u03b3\u22651. Coverage influenceseligibility: graphs that reach the cap stop contributing\nto further extensions.\nDiscriminative score.For a candidateS, letI(S) ={i:S\u2286 isoGi}and I(S)\nits complement. LetH(\u00b7)denote Shannon entropy over class labels. We use\ninformation gain\nIG(S) :=H(y)\u2212\u0010\n|I(S)|\nNH(y|i\u2208I(S)) +|I(S)|\nNH(y|i\u2208 I(S))\u0011\n,\nand equivalently refer toweighted entropyWE(S) :=H(y)\u2212IG(S)when min-\nimizing. A dataset-levelcoverage constraintrequires the selected feature setF \u22c6\nto cover at least a fraction\u03c4of graphs:\f\fS\nS\u2208F\u22c6I(S)\f\f\u2265\u03c4N.\n4 Kaiser et al.\nAlgorithm 1:Single-Phase Mining with Coverage-Capped Eligibility\nInput :D: graphs;\u03b4: support;min_cov;\u03b3; DFS-code canonical order\nOutput:C: candidate subgraph set;cov: per-graph coverage\n1InitializeC \u2190 \u2205;\n2Initializecov[i]\u21900and eligibility setE \u2190[N];\n3FunctionMine(code,E)\n4X\u2190RightMostExtensions(code,{G i:i\u2208 E});\n5foreachx\u2208Xdo\n6code\u2032\u2190code\u222a {x};\n7ifIsCanonical(code\u2032)andsupp(code\u2032)\u2265 \u2308\u03b4N\u2309then\n8C \u2190 C \u222a {code\u2032};\n9foreachi\u2208 I(code\u2032)do\n10cov[i]\u2190cov[i] + 1;\n11ifcov[i]\u2265\u03b3\u00b7min_covthen\n12E \u2190 E \\ {i}\n13end\n14end\n15Mine(code\u2032,E)\n16end\n17end\n18Mine(\u2205,E);\n2.2 Single-Phase Prospective Mining with Coverage-Capped\nEligibility\nClassical discriminative pipelines often alternate between (i) frequent enumera-\ntion under a support threshold and (ii) post-hoc discriminative filtering, some-\ntimes across multiple miningphases. In contrast, DS-Span performs asingle\nDFS traversal that (a) enforces a global support threshold and (b) dynamically\nshrinks the set of graphs that are eligible to produce further extensions as their\ncoverage grows.\nLet\u03b4\u2208(0,1]be the relative support threshold; we requiresupp(S)\u2265 \u2308\u03b4\u00b7N\u2309.\nWe maintain a per-graph coverage counter and aneligibility maskE \u2286[N]of\ngraphs whose coverage is<cap. Rightmost-path extensions are collectedonly\nfrom graphs inE. Whenever a new frequent, canonical extensionS\u2032is accepted,\nwe increment coverage for graphs inI(S\u2032)and remove any graphiwithcov(i)\u2265\ncapfromE, preventing redundant growth from already well-represented graphs.\nCoverage completion (fairness top-up).A single traversal can still leave some\ngraphs withcov(i)<min_cov. To avoid under-representing such graphs in\ndownstream learning, wecompletecoverage by enumerating the smallest canon-\nical codeswithin each under-covered graphand adding them toCuntilmin_cov\nis met for that graph. Thesecoverage fillershave low support by construction\nand are subsequently down-weighted or removed by the discriminative selector\n(2.3).\nDS-Span5\nAlgorithm 2:Coverage-Constrained Discriminative Selection\nInput :C: candidates;{y i}: labels;K;\u03c4\nOutput:F \u22c6: selected features\n1ComputeIG(S)for allS\u2208 C;\n2SortCbyIG(S)descending;\n3F\u22c6\u2190 \u2205,U \u2190 \u2205//U: covered graph indices\n4foreachS\u2208 Cdo\n5if|F \u22c6|< Kand\f\fU \u222a I(S)\f\f>|U|then\n6F \u22c6\u2190 F \u22c6\u222a {S};\n7U \u2190 U \u222a I(S);\n8if|U| \u2265\u03c4Nthen\n9break\n10end\n11end\n12end\nUnlike multi-phase methods that restart mining with relaxed thresholds, DS-\nSpan grows patterns once and prunesgraphs(eligibility) as soon as they are suf-\nficientlyrepresented.Thisreducesextensiongeneration,avoidsrevisitingalready\nsaturated regions, and cuts redundant isomorphism checks, all while maintain-\ning the anti-monotonicity guarantees of support and the duplicate-avoidance\nguarantees of DFS-code minimality.\n2.3 Discriminative Selection under a Coverage Constraint\nThe mined candidate poolCis distilled into a compact feature setF \u22c6by optimiz-\ning for discriminative utility subject to dataset coverage. We formulate selection\nas a coverage-constrained set function optimization:\nmax\nF\u2286C,|F|\u2264KX\nS\u2208FIG(S)s.t.\f\f\f[\nS\u2208FI(S)\f\f\f\u2265\u03c4N,(1)\nwhereKisanoptionalfeaturebudgetand\u03c4\u2208(0,1]controlsdatasetcoverage\n(e.g.,\u03c4= 0.95). We use a simple greedy strategy that is effective and easy to\nreproduce: sort candidates byIG(S)descending, add a candidate if itimproves\nthe coverage of under-covered graphs and respects the budget, and stop when\nthe coverage constraint is met.\nPost-hoc filters that ignore coverage tend to pick many near-duplicates of the\nsamemotif,hurtinggeneralizationandwastingcapacity.Ourcoverageconstraint\nexplicitly promotesrepresentativefeatures, reducing redundancy. Empirically\n(Sec. 3), this increases accuracy withfewerfeatures and reduces variance across\nfolds.\n6 Kaiser et al.\n2.4 Feature Embedding and Classification\nFrom the selected discriminative subgraphsF \u22c6={S k}K\u2032\nk=1, each graphG iis\nrepresented by a normalized binary incidence vector\n(xi)k=(1\n|{j:S j\u2286isoGi}|,ifS k\u2286isoGi,\n0,otherwise.\nThis normalization divides by the number of subgraphs present inG i, ensuring\nthat larger graphs do not produce disproportionately high feature magnitudes.\nThe resulting feature matrixX= [x 1, x2, . . . , x N]\u22a4is sparse and interpretable,\ndirectly encoding the presence of mined substructures.\nEmbedding model.Following the architecture ofDisFPGC[1], we employ\na shallow CBOW-style embedding network that learns two parameter matrices\nW\u2208RK\u2032\u00d7EandW\u2032\u2208RE\u00d7N, whereEdenotes the embedding dimension and\nNthe number of graphs. Each forward pass computes hidden activations\nhi=W\u22a4xi, u i=W\u2032\u22a4hi,\nand outputs\u02c6y i= softmax(u i). The model is trained to minimize cross-entropy\nloss between\u02c6y iand the true one-hot label vectort i:\nL=\u2212NX\ni=1t\u22a4\nilog \u02c6y i.\nThe same optimization procedure and criteria described in DisFPGC are used\nhere. Consequently, any difference in classification performance arises solely from\nDS-Span\u2019s feature quality.\n2.5 Design Contrasts with Prior Literature\nVersus gSpan [9].Both use DFS-code canonicalization and rightmost expan-\nsion. DS-Span addscoverage-capped eligibilityand adiscriminative, coverage-\nconstrainedselection after a single traversal, prioritizing class-separating, non-\nredundant patterns.\nVersus multi-phase discriminative miners (DisFPGC [1]).Multi-phase\nschedules repeatedly enumerate with varying thresholds and only later integrate\nsupervision, which causes redundant work and weak coupling. DS-Span mines\nonce at a fixed\u03b4, prunes exploration early via eligibility caps, and selects fea-\ntures with an explicit coverage constraint, producing leaner and more predictive\nfeature sets with lower runtime.\nVersus embedding-first methods.Our pipeline isfeature-first: it yields in-\nterpretable substructures with quantified discriminative value. Embeddings are\na thin, optional layer on top of these features, making improvementsattributable\nto the mined patterns rather than model capacity.\nDS-Span7\n3 Implementation & Experimental Results\nThis section distils the evaluation of DS-Span into four viewpoints: (i) accuracy\nversus existing graph-classification methods, (ii) how many subgraphs are mined\n(iii) how long mining takes and (iv) how alternative embedding choices affect\nclass separability.\n3.1 Datasets, Protocol, and Hardware\nWe evaluate on the TU Dataset benchmarks [6]:D&D[5],ENZYMES[2],\nProteins[3],MUTAG[4],PTC[7],NCI1andNCI109[8], andReddit-M-\n5k[10]. For each dataset we regenerate depth-first search frequency caches, run\nten repetitions of stratified 10-fold cross-validation with the supervised coverage\ncap in Section 2.3, and log the number of retained subgraphs per fold. The\nCBOW-style embedding model uses five epochs, learning rate\u03b1= 1.0, and\n64-dimensional representations. All experiments, including the reproduction of\nDisFPGC[1], run on an AMD Ryzen 5 5600G CPU with 32 GB RAM and an\nRTX 1060 GPU (Python 3.12.3).\nTable 3 already consolidates the requested accuracy comparison: DS-Span\nleads on D&D, ENZYMES, MUTAG, NCI109, Proteins, and PTC, ties Reddit-\nM-5k, and yields only NCI1 to DisFPGC. To complement those numbers we\ncompare feature budgets and mining costs, then describe how different embed-\nding choices behave.\n3.2 Feature Budget and Mining Cost\nTable 1: Average feature counts for DS-Span and DisFPGC.\nDataset Avg. DS-Span features Avg. DisFPGC features\nD&D 9.14 231.00\nEnzymes 3.57 179.50\nMutag 20.02 220.60\nNCI1 12.92 283.60\nNCI109 11.55 262.60\nProteins 18.99 607.60\nPTC 15.07 180.10\nTables 1 and 2 show that DS-Span typically retains fewer than twenty sub-\ngraphs and completes mining in seconds, whereas DisFPGC produces hundreds\nof features and still spends7\u2013265\u00d7longer because it repeatedly re-enumerates\ncandidates before applying supervision. Filtering and embedding remain sub-\nsecond on all datasets and are omitted for clarity.\n8 Kaiser et al.\nTable 2: Average mining times (seconds) for DS-Span and DisFPGC.\nDataset DS-Span mining DisFPGC mining\nD&D 54.61 709.80\nEnzymes 807.23 1144.80\nMutag 201.24 4178.00\nNCI1 110.01 422.30\nNCI109 96.37 362.10\nProteins 1455.33 7448.20\nPTC 11.20 148.40\n3.3 Embedding Quality and Visual Evidence\nTo compare embedding choices we visualise DS-Span, a Gaussian random base-\nline,random-featureembeddings(binaryindicatorsforarandomsubsetofmined\nsubgraphs), their trained counterpart, and the reproduced DisFPGC embed-\ndings. Figure 1 illustrates the D&D case: DS-Span produces clearly separated\nclusters despite using only nine patterns, DisFPGC forms worse clusters and\nrequires roughly twenty-five times as many subgraphs, and the random variants\ncollapse into an indistinguishable cloud. Other datasets exhibit the same quali-\ntative behaviour, underscoring that DS-Span\u2019s discriminative mining drives its\naccuracy advantage.\n(a) DS-Span\n (b) Random baseline\n (c) DisFPGC reproduction\nFig.1: t-SNE visualisations for the D&D dataset comparing DS-Span embed-\ndings, Gaussian noise, and the reproduced DisFPGC embeddings.\nDS-Span9\nTable 3: Comparison of Methods on Datasets\nMethod D&D Enzymes Reddit-\nM-5kMutag NCI1 NCI109 Proteins PTC\nWL-1 78.50\n(0.44)50.03\n(2.04)\u2013\n\u201375.35\n(2.44)84.19\n(0.41)84.33\n(0.26)73.03\n(0.33)60.72\n(1.80)\nWL-OA 78.64\n(0.48)57.02\n(1.24)\u2013\n\u201382.71\n(2.34)85.06\n(0.31)85.06\n(0.42)73.10\n(0.81)60.74\n(1.72)\nSP 78.47\n(0.61)39.87\n(2.01)\u2013\n\u201382.24\n(1.80)73.97\n(0.41)73.00\n(0.28)75.53\n(0.59)59.36\n(1.79)\nrLap BGRL74.80\n(10.17)81.50\n(5.39)\u2013\n\u2013\u2013 84.34\n(4.03)\u2013 \u2013 \u2013\nrLap GCL70.90\n(5.01)87.50\n(9.86)\u2013\n\u2013\u2013 75.27\n(3.34)\u2013 \u2013 \u2013\nnode2vec \u2013\n\u201372.63\n(10.20)\u2013\n\u201352.68\n(1.56)57.49\n(3.57)58.85\n(8.00)\u2013\n\u2013\u2013\n\u2013\nsub2vec \u2013\n\u201361.05\n(15.79)\u2013\n\u201350.67\n(1.50)53.03\n(5.55)59.99\n(6.38)\u2013\n\u2013\u2013\n\u2013\ngraph2vec 58.64\n(0.01)44.33\n(0.09)\u2013\n\u201383.15\n(9.25)73.22\n(1.81)74.26\n(1.47)73.30\n(2.05)60.17\n(6.86)\nHGP-SL 80.96\n(1.26)68.79\n(2.11)\u2013\n\u2013\u2013\n\u201378.45\n(0.77)80.67\n(1.16)84.91\n(1.62)\u2013\n\u2013\nGraphSage 72.90\n(2.00)58.20\n(6.00)50.0\n(1.3)79.80\n(13.90)76.00\n(1.80)\u2013\n\u201373.00\n(4.50)\u2013\n\u2013\nSAGPool g76.19\n(0.94)\u2013\n\u2013\u2013\n\u201390.42\n(7.78)74.18\n(1.20)74.06\n(0.78)70.04\n(1.47)\u2013\n\u2013\nDiffPool 75.00\n(3.50)59.50\n(5.60)53.8\n(1.4)77.60\n2.7076.90\n(1.90)\u2013\n\u201372.70\n3.8073.70\n(3.50)\nGIN \u03f5\u2212JK \u2013\n\u201339.30\n(1.60)57.0\n(1.7)\u2013\n\u201378.30\n(0.30)\u2013\n\u201372.20\n(0.70)\u2013\n\u2013\nGCN \u2013\n\u2013\u2013\n\u2013\u2013\n\u201385.60\n(5.80)80.20\n(2.00)\u2013\n\u201376.00\n(3.20)64.20\n(4.30)\nGCKN \u2013\n\u2013\u2013\n\u2013\u2013\n\u201397.20\n(2.80)83.90\n(1.20)\u2013\n\u201375.90\n(3.20)69.40\n(3.50)\nU2GNN 95.67\n(1.89)\u2013\n\u2013\u2013\n\u201388.47\n(7.13)\u2013\n\u2013\u2013\n\u201380.01\n(3.21)91.81\n(6.61)\nGIU-Net \u2013\n\u201370.00\n\u2013\u2013\n\u201395.70\n\u201380.20\n\u201377.00\n\u201377.60\n\u201385.70\n\u2013\nGE-FSG 91.69\n(0.02)49.33\n(0.07)\u2013\n\u201384.74\n(0.07)84.36\n(0.02)85.59\n(0.01)81.79\n(0.04)62.57\n(0.09)\nDisFPGC 93.46\n(0.54)59.82\n(0.16)93.10\n(1.52)97.00\n(0.53)94.92\n(0.21)97.40\n(0.12)83.44\n(0.67)84.94\n(0.50)\nGE-ODS 96.12\n(0.29)93.80\n(0.59)93.19\n(0.25)99.67\n(0.27)89.44\n(0.31)99.43\n(0.08)93.89\n(0.59)95.03\n(0.83)\n4 Conclusion\nIn this work, we proposed DS-Span, a computationally efficient approach for\ngenerating whole-graph embeddings by leveraging a single-phase discriminative\n10 Kaiser et al.\nsubgraphminingtechnique.Ourmethodaddressesthechallengesposedbytradi-\ntional multi-phase approaches, such as high computational cost and redundancy,\nbydynamicallyshrinkingthesearchspaceandemployingarobustfilteringmech-\nanism to identify highly discriminative subgraphs. These subgraphs form the ba-\nsis for embeddings that preserve critical structural and class-separating features.\nThe contributions of our work are multifaceted. First, we developed a single-\nphase subgraph mining algorithm that eliminates iterative overhead and signifi-\ncantlyenhancesscalability.Second,weintroducedasupervisedgraphembedding\nmethodology that outperforms existing state-of-the-art methods in classification\ntasks, as demonstrated across diverse benchmark datasets. Lastly, we showed the\npractical applicability of our approach in critical domains, such as drug discov-\nery, by enabling precise and efficient classification of molecular graphs, thereby\nreducing resource-intensive experimental validation. The results indicate that\nour method attains greater accuracy than baseline methods and state-of-the-art\ntechniques in most datasets, with a very low standard deviation. This reflects\nthe robustness and stability of our work. Specifically, it indicates our approach\neffectively discriminates between classes and generates high-quality whole-graph\nembeddings useful for downstream graph analytics. As our approach is computa-\ntionally efficient and highly optimized, it requires fewer resources and processing\ntime than its predecessors with similar techniques. These findings underline the\neffectiveness of our method in graph classification and suggest its potential for\npractical applications. In future work, we aim to explore unsupervised filter-\ning mechanisms to enhance the generalizability of our method and investigate\ndataset-specific hyperparameter tuning to optimize performance across diverse\ngraph datasets even further.\nReferences\n1. Alam, M.T., Ahmed, C.F., Samiullah, M., Leung, C.K.: Discriminating frequent\npattern based supervised graph embedding for classification. In: PAKDD (2021)\n2. Borgwardt, K.M., Ong, C.S., Schonauer, S., Vishwanathan, S.V.N., Smola, A.J.,\nKriegel, H.p.: Protein function prediction via graph kernels. Bioinformatics\n21(Suppl 1), i47\u2013i56 (6 2005). https://doi.org/10.1093/bioinformatics/bti1007,\nhttps://doi.org/10.1093/bioinformatics/bti1007\n3. Borgwardt, K.M., O.C.S.S.V.S.S.A.K.H.: Protein function prediction via graph\nkernels3(10 2005)\n4. Debnath, A.K., Lopez de Compadre, R.L., Debnath, G., Shusterman, A.J.,\nHansch, C.: Structure-activity relationship of mutagenic aromatic and het-\neroaromatic nitro compounds. correlation with molecular orbital energies\nand hydrophobicity. Journal of Medicinal Chemistry34(2), 786\u2013797 (1991).\nhttps://doi.org/10.1021/jm00106a046, https://doi.org/10.1021/jm00106a046\n5. Dobson, P.D., Doig, A.J.: Distinguishing Enzyme Structures from Non-enzymes\nWithout Alignments. Journal of Molecular Biology330(4), 771\u2013783 (7 2003).\nhttps://doi.org/10.1016/s0022-2836(03)00628-4, https://doi.org/10.1016/s0022-\n2836(03)00628-4\n6. Morris, C., Kriege, N.M., Bause, F., Kersting, K., Mutzel, P., Neumann, M.: Tu-\ndataset: A collection of benchmark datasets for learning with graphs. In: ICML\n2020 Workshop on Graph Representation Learning and Beyond (2020)\nDS-Span11\n7. Toivonen, H., Srinivasan, A., King, R.D., Kramer, S., Helma, C.: Statisti-\ncal evaluation of the Predictive Toxicology Challenge 2000\u20132001. Bioinformat-\nics19(10), 1183\u20131193 (07 2003). https://doi.org/10.1093/bioinformatics/btg130,\nhttps://doi.org/10.1093/bioinformatics/btg130\n8. Wale, N., Watson, I., Karypis, G.: Comparison of descriptor spaces for chemical\ncompound retrieval and classification. Knowledge and Information Systems14(3),\n347\u2013375 (Mar 2008). https://doi.org/10.1007/s10115-007-0103-5\n9. Yan, X., Han, J.: gspan: graph-based substructure pattern mining. In: 2002 IEEE\nInternational Conference on Data Mining, 2002. Proceedings. pp. 721\u2013724 (2002).\nhttps://doi.org/10.1109/ICDM.2002.1184038\n10. Yanardag, P., Vishwanathan, S.: Deep Graph Kernels. KDD \u201915: Proceedings of\nthe 21th ACM SIGKDD International Conference on Knowledge Discovery and\nData Mining pp. 1365\u20131374 (8 2015). https://doi.org/10.1145/2783258.2783417,\nhttps://doi.org/10.1145/2783258.2783417\n",
    "title": "DS-Span: Single-Phase Discriminative Subgraph Mining for Efficient Graph Embeddings",
    "arxiv_id": "2511.17419v1",
    "category": "recent",
    "authors": [
      "Yeamin Kaiser",
      "Muhammed Tasnim Bin Anwar",
      "Bholanath Das",
      "Chowdhury Farhan Ahmed",
      "Md. Tanvir Alam"
    ],
    "published": "2025-11-21",
    "num_pages": 11,
    "num_chars": 22636
  },
  {
    "text": "CREST: Improving Interpretability and Effectiveness\nof Troubleshooting at Ericsson through\nCriterion-Specific Trouble Report Retrieval\nSoroush Javdan\nCarleton University\u201e soroushjavdan@cmail.carleton.ca Ottawa, Canada\nEricsson Canada Inc., soroush.javdan@ericsson.com Ottawa, Canada\nPragash Krishnamoorthy\nEricsson Canada Inc., pragash.krishnamoorthy@ericsson.com Ottawa, Canada\nOlga Baysal\nCarleton University, olga.baysal@carleton.ca Ottawa, Canada\nAbstract\nThe rapid evolution of the telecommunication industry necessitates ef-\nficient troubleshooting processes to maintain network reliability, software\nmaintainability, andservicequality. TroubleReports(TRs), whichdocument\nissues in Ericsson\u2019s production system, play a critical role in facilitating the\ntimely resolution of software faults. However, the complexity and volume of\nTR data, along with the presence of diverse criteria that reflect different as-\npects of each fault, present challenges for retrieval systems. Building on prior\nwork at Ericsson, which utilized a two-stage workflow, comprising Initial Re-\ntrieval(IR)andRe-Ranking(RR)stages, thisstudyinvestigatesdifferentTR\nobservation criteria and their impact on the performance of retrieval models.\nWeproposeCREST(Criteria-specificRetrievalviaEnsembleofSpecialized\nTR models), a criterion-driven retrieval approach that leverages specialized\nmodels for different TR fields to improve both effectiveness and interpretabil-\nity, thereby enabling quicker faultresolution and supporting softwaremainte-\nnance. CREST utilizes specialized models trained on specific TR criteria and\naggregates their outputs to capture diverse and complementary signals. This\napproach leads to enhanced retrieval accuracy, better calibration of predicted\nscores, and improved interpretability by providing relevance scores for eacharXiv:2511.17417v1  [cs.SE]  21 Nov 2025\ncriterion, helping users understand why specific TRs were retrieved. Using a\nsubset of Ericsson\u2019s internal TRs, this research demonstrates that criterion-\nspecific models significantly outperform a single model approach across key\nevaluation metrics. This highlights the importance of all targeted criteria\nused in this study for optimizing the performance of retrieval systems.\nKeywords:\nTrouble report, software maintenance, bug reports, information retrieval,\nneural ranking, natural language processing, telecommunications\n1. Introduction\nThe modern telecommunication industry is a dynamic, rapidly evolving\nfield that relies heavily on efficient troubleshooting processes to preserve net-\nwork reliability while ensuring customers receive high-quality service.\nAt Ericsson, a trouble report (TR) is a critical tool for tracking infor-\nmation regarding the detection, characteristics, and eventual resolution of\nproblems. TRs document issues and incidents that arise during the develop-\nment and maintenance phases of the software products at Ericsson.\nEachTRconsistsofmultiplesectionsthatdefineitscharacteristics. ATR\ntypically records aheadlinewhich serves as a short summary of the trouble,\naprioritytag, theresponsible team, theproduct under test, and thefault\ncategorywhich indicate the defect\u2019s type to aid TR triage, among others.\nHowever, the most crucial section isthe observation section, which serves as\na detailed explanation of the fault written by the creator of the TR. This\nsection provides essential information for the designated team to promptly\nresolve the issue. The observation section is a free-text format, allowing\nthe reporter to provide a comprehensive description of the problem. The\nTR observationincludes detailed information, called criteria, describing\nthe fault by providing all the necessary information for the fast resolution\nof the fault. Each of these criteria focuses on a different aspect of the fault.\nCommon criteria includegeneral descriptionof the fault,conditionsunder\nwhich the fault occurred, itsimpacton the system, thefrequencyof the\nfault and Steps toreproducethe fault. These criteria are considered the\nmost crucial based on feedback from domain experts. According to their\ninput, these criteria most often anchor discussions between reporters and\nresponsible teams during triage and resolution. They also appear with high\nfrequency in recent TR templates.\n2\nTo ensure the quality of TRs, they must follow a standard template and\nmeet defined quality standards before being published for resolution. The\nresolution of a fault (called the answer) is only accepted after it has been\nvalidated. Previously resolved TRs (historical data) play a critical role as the\ninformation they contain can be used by quality assurance (QA) testers to\nimprove the quality of new TRs. By quickly identifying similar past TRs and\nproviding relevant information to designated teams, historical data enables\nefficient troubleshooting. Moreover, the design team can use these relevant\nresolved TRs during the problem analysis and correction phases, which can\nfurther reduce the time spent on debugging and testing. As a result, imple-\nmenting a high-performance TR retrieval system to extract similar, previ-\nously resolved TRs can speed up software development and maintenance at\nEricsson.\nIn recent years, there has been a notable increase in the efforts of apply-\ning machine learning (ML) and natural language processing (NLP) methods\nacross various domains, including the telecommunication industry. Large\nlanguage models (LLMs) such as bidirectional encoder representations from\ntransformers (BERT) (Devlin et al., 2019) have shown great potential in\nleveraging textual data to address various tasks. Ericsson researchers pro-\nposed BERTicsson (Grimalt et al., 2022) as a BERT-based TR recommender\nsystem. It combines two stages: initial retrieval (IR) and re-ranking (RR).\nBERTicsson also uses the Sentence-BERT (Reimers and Gurevych, 2019)\nand monoBERT (Nogueira and Cho, 2019) architecture to extract and rank\nasubsetofhistoricalTRsbasedontheirrelevancetonewTRs. Priorworkby\nBosch et al. (2022) uses a similar architecture to extract and identify dupli-\ncate TRs. Prior works have also combined the headline with the observation\nas inputs. However, the observation was treated as one unstructured block,\nwithout parsing or modeling at the level of individual criteria, which meant\nits internal signals were neither isolated nor weighted explicitly. Building on\nthefoundationalwork(Grimaltetal.,2022;Boschetal.,2022)thatpioneered\nTR retrieval studies at Ericsson, we leverage an internal LLM, specifically\nRoBERTa, that was trained on the internal and external telecommunication\ndata within Ericsson.\nWhile existing models have advanced TR retrieval, the inherent complex-\nity and diversity of TR observations suggest there is still room for enhance-\nment. EachTRiscomposedofmultiplesections, withtheobservation section\nstanding out as a particularly information-rich component. This section en-\ncompasses various criteria, such as thetrouble description,impact on system,\n3\nandother criteria, each contributing differently to the quality of retrieval.\nUnderstanding how these individual components influence retrieval outcomes\nis essential for identifying areas of improvement and guiding model develop-\nment. Inspired by frameworks like Branch Train Merge (BTM) (Li et al.,\n2022)andDEMIXlayers(Gururanganetal.,2022), andDORIS-MAE(Wang\netal.,2023), weintroducetheCriteria-specificRetrievalviaEnsembleofSpe-\ncialized TR models (CREST). Similar to how DORIS-MAE demonstrates\nthe benefit of decomposing complex queries into aspects, and how BTM and\nDEMIX leverageexpertmodels formodular learning, CREST trains separate\nmodels for each TR criterion. These specialized models are then combined\nthrough a weighted ensemble to improve retrieval performance.\nBeyond aiming to improve retrieval performance, CREST also tackles\none of the key limitations of many existing methods, which is the lack of\ntransparency in how recommendations are generated. CREST tackles this\nissue by calculating separate relevance scores for each TR criterion and then\ncombining them to determine the final ranking. This approach is designed to\nsupport interpretability1by making the influence of each TR component ex-\nplicit, enabling users to better understand how recommendations are formed.\nIn essence, CREST is designed with interpretability at its core, making the\ncontribution of each criterion both clear and measurable.\nTo further explore CREST\u2019s interpretability, we investigate how well its\ngenerated detailed relevance scores align with true relevance judgments by\nanalyzing confidence calibration. Poorly calibrated scores can undermine\ntrust in retrieval systems, even when accuracy is high. Therefore, we assess\nthe degree to which CREST\u2019s predicted relevance scores accurately reflect\nthe likelihood of a correct retrieval. This calibration analysis is especially\nimportant in real-world practice, such as Ericsson, where confidence scores\ncan guide decisions.\nFinally, to ensure that improvements extend beyond offline evaluation, we\ncomplement our experiments with a pilot user study at Ericsson. The study\nexamineswhetherCREST\u2019scriterion-wisescoresenhancethetransparencyof\ntherankingsandwhethertherecommendedTRsareperceivedascredibleand\npractically useful during triage, rather than merely appearing more accurate\n1In this paper, we use the terms interpretability, explainability, and transparency inter-\nchangeably to refer to the extent to which a model\u2019s retrieval behavior can be understood\nand justified by human users.\n4\nin offline metrics.\nIn summary, our analysis shows that observation criteria contribute un-\nevenly across different retrieval stages. Building on this, our criterion-specific\nensemble (CREST) consistently outperforms single-model approaches in the\ntwo-stage workflow and remains superior even when evaluated in isolation.\nMoreover, CREST improves confidence calibration and provides criterion-\nwise relevance scores, making recommendations both more reliable and more\ninterpretable for engineering decision-making.\nUsing Ericsson\u2019s internal trouble-reporting dataset, we answer the follow-\ning research questions:\n1. RQ1:What impact does each TR criterion have on retrieval model\nperformance?\n2. RQ2:To what extent does CREST improve the retrieval performance\ncompared to a single model?\n3. RQ3:How does the calibration of relevance scores produced by CREST\ncompare to those generated by the criterion-agnostic model?\n4. RQ4:What are the users\u2019 perceptions and rankings of CREST in terms\nof transparency, usefulness, credibility, and accuracy?\nThe remainder of this paper is organized as follows: Section 2 highlights\nbackground relevant to this study and summarizes related work. Section 3\npresents the details of the study approach and includes an overview of re-\ntrieval systems, data related to TRs, the initial retrieval (IR) stage, the\nre-ranking (RR) stage, training, and inference. Section 4 presents the eval-\nuation procedure, including evaluation strategy, metrics, and datasets. Sec-\ntion 5 reports the results of our empirical study. Finally, Section 7 concludes\nthe paper by discussing possible future research directions.\n2. Background and Related Work\nSince we leverage Large Language Models (LLMs) in this work, this sec-\ntion provides an overview of LLMs, highlighting their strengths and limita-\ntions, as well as their application within the telecommunications industry.\nAdditionally, it discusses recent research on the use of LLM-based models for\ntext retrieval problems.\n5\n2.1. Large Language Models\nNatural Language Processing (NLP) has been reshaped by the rise of\nlarge language models (LLMs), which excel at tasks such as text classifica-\ntion, summarization, and semantic understanding. Cutting-edge models like\nthe GPT series (Achiam et al., 2023), LLaMA3 (Grattafiori et al., 2024),\nDeepSeek (Liu et al., 2024), Mistral (Jiang et al., 2023), and Qwen (Yang\net al., 2024) are now increasingly integrated into retrieval pipelines, serv-\ning roles as encoders, re-rankers, or generative reasoners. In parallel, classic\ncompact transformers including BERT (Holm, 2021), RoBERTa (Liu et al.,\n2019), T5(Raffeletal.,2020), andERNIE(Zhangetal.,2019)remainwidely\ndeployed in production due to strong efficiency\u2013accuracy trade-offs and ma-\nture tooling (Puthenputhussery et al., 2025).\nIn this study, we focus on Ericsson\u2019s domain-trained RoBERTa (TeleR-\noBERTa) for two main reasons. TeleRoBERTa, trained on proprietary tele-\ncom corpora, matches the performance of much larger foundation LLMs on\ntelecom standards QA benchmarks while using an order of magnitude fewer\nparameters. This makes it particularly well aligned with the distributions\nfound in telecom-specific tasks (Karapantelakis et al., 2024). In addition, our\nobjective is criterion-aware retrieval that is both efficient and interpretable,\nand that can be immediately integrated into Ericsson\u2019s existing two-stage\npipeline. Fine-tuning a compact, domain-adapted encoder not only reduces\ncompute and latency overhead but also enables drop-in improvements within\nthe existing production stack.\n2.2. Neural Retrieval Models\nNeural retrieval models (NRMs) for document search largely fall into two\nfamilies:cross-encodersandbi-encoders. Cross-encoders(e.g.,monoBERT(Nogueira\nand Cho, 2019)) score a query\u2013document pair jointly, yielding strong rele-\nvance estimates but high computational cost; multi-stage pipelines therefore\nretrieve with BM25 and re-rank with monoBERT or duoBERT\u2019s pairwise\ncomparator (Nogueira et al., 2019). To reduce cost, bi-encoders encode\nqueries and documents independently (e.g., TwinBERT (Lu et al., 2020)\nwith a Siamese setup (Chicco, 2021)), enabling offline document embeddings\nfor fast initial retrieval; late-interaction models like ColBERT (Khattab and\nZaharia, 2020) bridge the two by combining separate encodings with token-\nlevel matching. Within Ericsson, BERTicsson (Grimalt et al., 2022) applied\nthis multi-stage recipe to recommend solutions for new TRs, outperforming\n6\nBM25 (Robertson and Zaragoza, 2009). Related work on duplicate TR de-\ntection (Bosch et al., 2022) explored domain adaptation (Ruder et al., 2019),\nsequence learning (Ruder et al., 2019), multi-stage fine-tuning, and elastic\nweight consolidation (Kirkpatrick et al., 2017) to mitigate catastrophic for-\ngetting.\nAll prior systems treat a TR observation as a single block of text. This\nmonolithic view obscures criterion-specific signals and limits interpretability\nof relevance scores. These gaps motivate our approach which explicitly mod-\nels observation criteria and aggregates their signals to improve both retrieval\nquality and transparency.\n2.3. Transparency and Explainability in Neural Retrieval Models\nWhileneuralretrievalmodelshaveachievedimpressiveperformanceacross\na variety of information retrieval tasks, they often operate as black boxes, of-\nfering limited insights into how specific relevance decisions are made (Rudin,\n2019). This lack of transparency poses significant challenges in domains such\nas software maintenance, where practitioners require justifiable and inter-\npretable recommendations to support critical decision-making.\nBroadly, approaches to explainability in neural retrieval can be grouped\ninto two categories: post-hoc and design-time strategies (Anand et al., 2022).\nPost-hoc methods attempt to interpret trained models by analyzing model\nbehaviour or internal representations after training is complete. These in-\nclude techniques such as feature attribution (Wang et al., 2024; Zhang et al.,\n2020), attention mechanism (Lucchese et al., 2023), and representation prob-\ning (Wallat et al., 2023), each aimed at uncovering which input features or\nstructures most influence the model\u2019s outputs. While insightful, these ap-\nproaches often operate heuristically and may not yield stable or actionable\nexplanations across instances or domains.\nOn the other hand, models that explicitly integrate explainability into\ntheir architecture offer a more systematic path toward interpretability. These\nmodelsoftenutilizemodularstructuresorenforcearchitecturalconstraintsto\nensure that each part of the decision process can be traced and understood.\nIn the context of neural retrieval, such approaches may involve segmenting\nthe relevance estimation process into interpretable components aligned with\ndomain-specific dimensions of the input (Yu et al., 2022; Leonhardt et al.,\n2023).\nIn the context of TR retrieval, we adopt a similar strategy through the\nproposed CREST model, which generates criterion-specific relevance scores\n7\nthat are combined into a final ranking. This design improves both retrieval\naccuracy and interpretability, allowing users to trace each recommendation\nback to distinct TR criteria. In this way, CREST promotes explainability\nby design, ensuring that each component\u2019s contribution to the final output\nis both meaningful and measurable.\n2.4. Confidence Calibration of Neural Ranking Models\nNeural rankers often output relevance scores that are not well-aligned\nwith the actual likelihood of relevance (Penha and Hauff, 2021a). While\nthese scores can be effective for sorting documents, the lack of calibration\ncan limit their interpretability and downstream utility, especially in settings\nwhere scores are aggregated, thresholded, or presented to users. Calibra-\ntion analysis allows us to assess how well the predicted scores reflect true\nprobabilities, providing a clearer picture of a model\u2019s reliability.\nIn retrieval systems that rely on multiple independent signals, such as\nthose derived from different aspects or criteria, calibrated outputs are par-\nticularly valuable. When models produce confidence scores that are bet-\nter aligned with observed relevance, combining their outputs becomes more\nmeaningfulandconsistent. Calibratedscoresalsosupportmorerobustdecision-\nmaking when setting thresholds or ranking candidates across criteria. Thus,\neven in systems that do not explicitly optimize for calibration, understanding\nand measuring this property is key to ensuring the reliability of the produced\nrelevance scores.\n2.5. Related Work\nThetextretrievaltaskhasseensubstantialadvancementswiththeadvent\nof large language models (LLMs). Recently, researchers have applied differ-\nent techniques to improve LLM-based text retrieval systems. RocketQAv2\n(Ren et al., 2021) and AR2 (Zhang et al., 2021a) use a joint training strategy\nfor both the retriever and re-ranker parts. RocketQAv2 jointly trains the re-\ntrieverandthere-rankerwithafocusonreducingthedifferencebetweentheir\nrelevance prediction distributions. AR2 formulates the text-ranking problem\nusing a Generative Adversarial Network (GAN) (Aggarwal et al., 2021). It\njointly optimizes a retriever as the generative model and a ranker as the dis-\ncriminative model, with the retriever generating hard negatives to improve\nranking performance. Poly-encoder (Humeau et al., 2020), HLATR (Zhang\net al., 2022), and Uni-encoder (Song et al., 2023) aim to balance the efficiency\n8\nof bi-encoders with the rich interaction capabilities of cross-encoders. Ran-\nkLLaMA (Ma et al., 2024) was introduced as a fine-tuned LLaMA model for\nmulti-stage text retrieval, demonstrating that more advanced LLMs can out-\nperform smaller models. DORIS-MAE (Wang et al., 2023) tackles the limita-\ntions of NIR models trained on simple queries, which often fail on complex,\nmultifaceted inputs. It introduced a benchmark dataset with hierarchical\naspect-based queries in the scientific domain and demonstrated that aspect\ndecomposition can improve retrieval performance. Parameter-efficient fine-\ntuning (PEFT) techniques have also gained traction in text retrieval. Tam\net al. (2023b) employed prompt tuning techniques, demonstrating that by\nupdating only a small portion of the model parameters, performance compa-\nrable to conventional fine-tuning can be achieved. This method significantly\nenhances out-of-domain generalization and improves confidence calibration.\nJung et al. (2022) introduced a Semi-Siamese Bi-encoder Neural Ranking\nModel utilizing PEFT techniques, which has shown significant improvement\nby updating only a small portion of the model parameters.\nIn the realm of transparency and explainability, several approaches have\nbeenproposedtomakeneuralretrievalmodelsmoreinterpretable. Leonhardt\net al. (2023) introduced the Select-and-Rank paradigm, wherein the model\nselectsasubsetofsentencesfromadocumentasanexplanationandthenuses\nthis selection exclusively for prediction. This approach treats explanations\nas integral components of the ranking process, enhancing interpretability by\ndesign. Zhang et al. (2021b) proposed ExPred, a model employing multi-task\nlearning during the explanation generation phase. It balances explanation\nand prediction losses and subsequently utilizes a separate prediction network\ntrained solely on the extracted explanations to optimize task performance.\nWallat et al. (2023) conducted an in-depth analysis of BERT\u2019s ranking capa-\nbilities by probing its internal representations. Their study reveals insights\ninto BERT\u2019s effectiveness in ranking tasks and highlights areas for improve-\nment in aligning its internal mechanisms with established information re-\ntrieval principles.\nRecent studies have also emphasized the importance of calibration in\nneural rankers. Penha and Hauff (2021b) investigated the calibration and\nuncertainty of neural retrieval models in conversational search, highlight-\ning that neural rankers often produce overconfident predictions. Tam et al.\n(2023a) proposed prompt tuning methods that improve generalization and\ncalibration of dense retrievers with minimal parameter overhead. Yu et al.\n(2024) used LLMs to generate natural language explanations and applied\n9\nMonte Carlo sampling to achieve better scale calibration, while maintaining\nor improving ranking performance.\nThese advancements highlight the continuous evolution and optimization\nof text retrieval systems using LLMs and related techniques. CREST takes a\ndifferent approach compared to earlier interpretable or LLM-based retrieval\nsystemslikeExPred(Zhangetal.,2021b)andRankLLaMA(Maetal.,2024),\nwhich mainly aim to improve explanation generation or scale single end-to-\nend rankers. Instead, CREST introduces a modular, multi-aspect retrieval\nframework. Rather than depending on one model to capture all query seman-\ntics, it breaks down each query into predefined criteria and combines the out-\nputs of specialized models trained for those aspects. This structure enables\nCREST to deliver stronger retrieval performance and clearer interpretability,\nsince each result\u2019s relevance can be directly tied to specific criteria.\n3. Methodology\nIn this study, we conduct a comprehensive investigation to understand\nhow different Trouble Report (TR) observation criteria affect the perfor-\nmance of TR retrieval models such as BERTicsson (Grimalt et al., 2022).\nThisstudyalsoaimstotraincriterion-specificmodelsandaggregatethem\nto optimize the retrieval process, enhancing the system\u2019s ability to attend to\ninformation from different criteria. The TR observations are preprocessed\nand parsed using a standardized Ericsson TR template to extract various\ninformative criteria. The impact of each criterion is assessed on the overall\nretrieval system\u2019s performance. Moreover, this approach provides trans-\nparencyinthedecision-makingprocessbyprovidingseparaterelevancescores\nto each observation criterion, enabling users to trace retrieval outcomes back\nto meaningful components of the input.\n3.1. Two-stage TR Retrieval Model\nThis methodology employs a two-stage ranking architecture similar to\nBERTicsson where the models used are adapted with RoBERTa, resulting in\nTwinRoBERTa or ColRoBERTa for initial retrieval and monoRoBERTa for\nre-ranking stage. The first stage utilizes a bi-encoder architecture, which is\nless computationally intensive and allows faster processing. This efficiency\narises from the ability to compare pre-calculated document embeddings with\nthe embeddings of new queries. The processed text from the preprocessing\nstep serves as the input for the IR stage, which retrieves a top-K list of\n10\nFigure 1: Overview of the utilized TR recommendation system.\ncandidate TRs ranked by the relevance of their accepted answers to the\ntarget TR observation.\nThe second stage involves re-ranking (RR) the candidates provided by\nthe IR stage. Here, each candidate from the top-K list is paired with the\nquery and processed through the monoRoBERTa model, a cross-encoder that\nprovides a more detailed and computationally intensive comparison. This re-\nranking stage benefits from the cross-encoder\u2019s ability to assess finer details\nwithintheinteractionsbetweenthequeryanddocument, resultinginahighly\naccurate, final ranked list of relevant TRs.\nFigure 1 provides an overview of the TR retrieval system utilized in this\nstudy, illustrating the data flow through the multi-stage architecture. The\nworkflow begins with the preprocessing of TRs and the calculation of em-\nbeddings. Document embeddings for the corpus are computed once and used\nthroughout the IR step. The query embeddings are similarly generated and\nused to select the top-K most similar TR documents. These selected docu-\nments are then paired with the query for the re-ranking step, executed by\nmonoRoBERTa.\nThis two-stage strategy efficiently integrates the speed and lower com-\nputational demands of a bi-encoder in the IR phase, with the accuracy and\ndepth of analysis provided by a cross-encoder in the RR phase.\nCriteria-specificRetrievalviaEnsembleofSpecializedTRmodels(CREST)\nadopts a similar two-stage approach but distinguishes itself by utilizing an\naggregation of TR retrieval models, each trained with a different TR observa-\ntion criterion and specializing in retrieving documents based on that specific\n11\naspect. This ensemble setup enables the retrieval system to attend to vari-\nous facets of the queries and retrieve documents relevant to each criterion.\nA document\u2019s final relevance score is then computed through a weighted ag-\ngregation of its individual criterion-specific scores, ensuring that the distinct\ncontributions of each criterion are accurately captured and utilized.\nThis design not only enhances the overall effectiveness of the TR retrieval\nsystem by leveraging the strengths of specialized models but also provides in-\ndividual relevance scores per criterion, offering transparency and traceability\nin the decision-making process for end-users.\n3.2. Trouble Report Data\nIn this study, we utilize historically resolved trouble reports (TRs) as the\ntraining data for both the IR and RR models. We primarily focus on the\nheadline and observation sections of the TRs, which can describe the prob-\nlem presented and are used as queries to retrieve relevant documents. The\naccepted answers, which detail the resolutions to these problems, serve as the\ndocuments in our retrieval system. This setup allows us to rank previously\nresolved TRs based on their relevance to new queries that are formulated\nfrom the headline and observation sections of new TRs. Notably, while our\napproach is centered around retrieving TRs based on the relevancy of their\nanswerstothenewTRsobservation, itspotentialapplicationsextendbeyond\nthis scope. For instance, our methodology can be adapted for identifying TR\nduplicates (Bosch et al., 2022), where the emphasis lies on finding similar\nTRs based on the similarity of their observations.\nA typical TR at Ericsson contains the following sections:\n1.Headline: A sentence summarizing the problem, often containing crit-\nical information about the issue.\n2.Observation: Detailed text that describes the problem comprehen-\nsively. This free-format text includes vital details for the responsible\nteam, such as the general description of the tester\u2019s observation,impact\non the system,conditionscausing the issue,frequencyof the problem,\nandreproducibility.\n3.Answer: Extensive text that explains the root cause and resolution.\n4.Faulty Product Detail: Information about the fault, the affected soft-\nware product, and additional relevant details.\nParsing is applied to the TR observation to extract multiple criteria and\nassess their contributions to retrieval. We use a lightweight Python regex\n12\nparser that targets the standardized observation template, looking for head-\ners explaining different criteria. Although the observation is free text, testers\ncomplete a consistent, organization-wide template. Fields are optional, so\nsome entries can be missing or not be complete. Ericsson\u2019s internal quality-\ncontrol review takes place before TR is being published, which limits format\ndriftandtypographicalvariation. Inpractice, theparserisreliable, withonly\noccasional edge cases such as merged or empty fields We select the following\nspecific criteria for this study:\n1.Trouble Description: This is the most informative part of the observa-\ntion, as it explains the problem in detail.\n2.Impact: A short text explaining the impact of a new problem on the\nsystem.\n3.Condition: The condition in which the problem occurred.\n4.Frequency: The frequency of observing the problem.\n5.Steps to reproduce: Explain if the problem is reproducible and how it\ncan be reproduced.\nFigure 2 illustrates an example of the TR observation. Although it is of\nfree format, the observation follows a template that assists writers in struc-\nturing and positioning its various criteria. Figure 3 shows the token length\ndistribution of observation section criteria before and after parsing. While\nunparsedobservationsoftencontainlong, unstructured text, theparsedfields\nare typically more concise and structured. This reinforces the importance of\nevaluating each field\u2019s individual contribution to retrieval effectiveness, offer-\ning a more targeted alternative to treating the observation as a monolithic\ninput.\nWith this in mind, we generate multiple datasets that pair the TR head-\nline with different parsed observation criteria. This setup allows us to inves-\ntigate the retrieval value of each field and gain a better understanding of its\nrole in the overall retrieval task. Moreover, these insights not only support\nthe design of our criterion-specific modeling approach but also offer practical\nguidance: if certain criteria are shown to have a stronger impact on retrieval\nquality, TR creators can be encouraged to elaborate more on those fields,\nultimately improving the effectiveness of the retrieval system.\n3.3. Initial Retrieval\nIn the initial retrieval (IR) stage, pre-processed queries and documents\nextracted from TRs are utilized to generate a top-K candidate list. The ini-\n13\nFigure 2: An example of the TR observation field with different criteria.\nFigure 3: Distribution of TR observation criteria based on the token length.\ntial retrieval of candidates within the top-K list is not essential, as a later\nre-ranking stage will adjust their order. However, it is crucial that the IR\n14\nstageincludesasmanyrelevantdocumentsaspossiblewithinthetop-Klistto\nachieveahighoverallsystemperformance. Fortheretrievalofthetop-Kcan-\ndidates,abi-encoderarchitectureisemployed,specificallytheTwinRoBERTa\nand ColRoBERTa models. These models are frequently used in various infor-\nmation retrieval and similarity comparison tasks (e.g., BERTicsson (Grimalt\net al., 2022)). Their main objective is to determine the similarity between\ntwo inputs. In our context, these inputs are a TR observation (including\nheadlines and various criteria) and a TR answer. They effectively separate\nthe query and document processing by employing distinct encoders for each,\nfollowed by a mean pooling layer to generate fixed-length representations\nfor both inputs. To further enhance domain-specific performance, TeleR-\noBERTa (Holm, 2021) is incorporated, a version of the RoBERTa model\nfurther trained in telecommunications data. The domain-specific knowledge\nof this model can benefit the TR retrieval system performance (Nimara et al.,\n2024).\nThe similarity score between query and document embeddings is calcu-\nlated using a fully connected layer. The resulting score is then used to rank\nand select the top-K candidates. For training, the model is exposed to rele-\nvant and irrelevant query and document pairs in order to adjust its weights,\nensuring higher scores for relevant pairs and lower scores for irrelevant ones.\nTo minimize latency during inference, the embeddings of all documents\nin the corpus are pre-calculated by leveraging the decoupled nature of Twin-\nRoBERTa and ColRoBERTa, which independently process queries and doc-\numents. This allows for only computing the query representation during\nthe inference and comparing it against all pre-stored document embeddings.\nThis possibility makes models that follow bi-encoder architectures signifi-\ncantly faster than cross-encoder architectures like monoRoBERTa, in which\nboth query and document are processed simultaneously.\n3.4. Re-Ranking Stage\nIn the re-ranking (RR) stage, the IR output, which is the top-K can-\ndidates, is further processed for a more precise ranking. In practice, we\ngenerate this top-K with the strongest IR configuration on our validation\nset, and use that retriever\u2019s output as input to RR, so the cross-encoder\noperates on the best available candidate set. Following the approach used\nin BERTicsson (Grimalt et al., 2022), this study uses a cross-encoder ar-\nchitecture, specifically adopting the monoRoBERTa framework for the RR\n15\nstage. Similar to the IR stage, TeleRoBERTa is employed here to leverage\nits telecommunications domain knowledge.\nTheinputofthemonoRoBERTamodelisaconcatenatedstringofaquery\nanddocumenttokensseparatedbyaspecialtokenasdefinedinlargelanguage\nmodels, e.g., \u201c[CLS], query tokens, [SEP], document tokens, [SEP]\u201d. Since\nboth the query and document can contain important contextual information,\nwe allocate tokens equally between them to preserve balanced representation.\nUnlike bi-encoder models, which use a decoupled approach to represent\nqueries and documents, the cross-encoder architecture integrates the pro-\ncessing of query and document representations. Since cross-encoder models\ncompute query and document representations together, pre-computing em-\nbeddings is not feasible, resulting in higher retrieval latency. However, the\nincreased latency remains minimal, as the model only applies to the top-K\ncandidates identified in the IR stage, which is a subset considerably smaller\nthan the entire TR corpus.\nDuring training, the cross-encoder receives pairs of relevant and irrelevant\nqueries and documents, adjusting its weights to enhance its performance. In\nthe inference phase, the model only processes receiving query pairing with\nthe top-K candidates extracted during the IR stage. By limiting the number\nof query-document pairs processing, the computational latency inherent to\ncross-encoder models is effectively mitigated.\nSimilar to the IR stage, applying CREST in the RR stage can improve\nperformance. However, unlike the IR stage, using CREST in the RR stage\nwill introduce additional latency, as the top-K candidates must be processed\nby each criterion-specific model. This increases latency by a factor equal to\nthe total number of criterion-specific models. This added latency is accept-\nable, as the ensemble size is small, with CREST utilizing only four criterion-\nspecific models.\n3.5. Criteria-specific Retrieval via Ensemble of Specialized TR models(CREST)\nIn this study, we introduce the Criteria-specific Retrieval via Ensemble of\nSpecializedTRmodels(CREST),anensemble-basedframeworkdesignednot\nonly to enhance retrieval performance but also to improve transparency (aka\ninterpretability) in the retrieval process. By leveraging the structured nature\nof TR observations, each model in the ensemble is trained using a distinct\ncriterion extracted from the observation section, enabling it to specialize\nin handling a specific type of information. This setup allows CREST to\n16\nFigure 4: Overview of CREST in a two-stage pipeline: bi-encoders (Twin/ColRoBERTa)\nretrieve top-K candidates and a cross-encoder (monoRoBERTa) re-ranks them. Unlike the\nbaseline criteria-agnostic two-stage workflow shown in Figure 1, CREST adds criterion-\nspecific models whose per-criterion scores are aggregated into the final relevance score.\ncapture diverse aspects of the TR content and aggregate them to form a\nmore comprehensive and interpretable relevance signal.\nCREST supports both bi-encoder (TwinRoBERTa, ColRoBERta) and\ncross-encoder (monoRoBERTa) architectures. For each criterion, a separate\nquery is generated, and its corresponding model is used to evaluate document\nrelevance. These individual scores are then combined through a learned,\ncriterion-specific weighted aggregation, implemented as a linear combination\nof criterion-specific scores with non-negative weights in range of [0,1] to pro-\nduce the final relevance score for each TR document. These aggregation\nweights are optimized in a separate training stage for each cross-encoder and\nbi-encoder models, while the models parameters are kept frozen. The opti-\nmization uses a hinge loss applied over the aggregated scores, and the final\naggregation model is selected based on the Mean Reciprocal Rank (MRR)\nachieved on the validation set. This mechanism ensures that the specific\ncontribution of each criterion is reflected in the final ranking, offering greater\ntransparency and traceability in the retrieval process. Figure 4 provides an\noverview of the CREST setup.\nFigure 5 illustrates what CREST may look like in practice. Users can\ninteractively select which criteria to activate, such as system impact, condi-\ntion, frequency, or how to reproduce, depending on the context or diagnostic\ngoal. Once a new trouble description is entered, CREST generates criterion-\nspecific queries, scores each candidate TR using its corresponding model,\nand visualizes both the aggregated and disaggregated scores. The coloured\nbar indicators enable users to assess the relative contribution of each crite-\n17\nFigure 5: Mockup of the CREST interface showing selectable criteria and both disag-\ngregated (per-criterion) and aggregated relevance scores, enabling configurable focus and\nclearer rationale for retrieved results.\nrion to the final score, providing transparency and control over the retrieval\nprocess. The framework is adaptive to missing inputs; if certain criteria are\nunavailable during inference, only the relevant specialized models are trig-\ngered. Likewise, users can configure the system to focus solely on a particular\ncriterion when diagnosing a specific issue or activate all available criteria to\nmaximize overall performance. This flexibility makes CREST suitable for a\nrange of retrieval scenarios and user needs.\nCREST is not positioned as a replacement for existing retrieval models\nbut rather as a modular enhancement that can be integrated into various\nTR retrieval pipelines. Its criterion-specific decomposition and aggregation\nstrategy make it a flexible solution that can extend to systems based on other\nLLMs.\n3.6. Training\nThe training process of the TR retrieval system involves training both the\nbi-encoder and cross-encoder models in a supervised manner. For training,\nqueries, positive documents, and negative documents are created from the\n18\nextracted TRs. The query is the description of the issue mentioned in the TR\n(e.g., headline, and various criteria). The positive document is the answer\nsection paired with the same TR, while the negative document is the same\nquery from the TR, paired with the answer from a different TR. The result\nis two pairs:<query,positive document>(called a relevant pair) and<\nquery,negative document>(called an irrelevant pair). The final training\ndatasets maintain a 1:1 ratio for positive and negative pairs, meaning that\nfor each collected TR, there is one positive pair and one negative pair.\nThe goal of this study is to explore the effect of various TR observa-\ntion criteria on retrieval performance and train criterion-specific models. To\nsupport this, we construct separate datasets for each observation criterion,\nallowing us to both train specialized retrieval models and assess the individ-\nual contribution of each field to the overall system. This setup enables a\ndetailed evaluation of criterion-level impact while also serving as the founda-\ntion for the proposed ensemble framework, where models trained on different\ncriteria are later aggregated.\nIn total, two types of models were trained in this study: TwinRoBERTa,\nas a bi-encoder, and monoRoBERTa, as a cross-encoder. All models use\nthe same triplet hinge loss function (Jung et al., 2022), which leverages the\nrelevance scores calculated for both positive and negative pairs to optimize\nthe ranking performance. We used a batch size of 64 and the Adam optimizer\nwith a learning rate of10\u22125for all encoders.\n3.7. Inference\nDuring the inference phase, the TR retrieval system begins the ranking\nprocessoncethetesterinitiatesanewTRwithbothheadlineandobservation\ndetails. The query is formed by combining the headline and available criteria\nfrom the observation section.\nThe IR stage starts by computing the query that is then used to calculate\nrelevance scores for different documents, extracting the top-K candidates.\nDuring the RR stage, the system re-orders the extracted candidates from the\nIR stage. Each candidate is paired with the query, and the pair is processed\nthrough a cross-encoder model to compute a new relevance score, which is\nused to refine the ranking of top-K candidates.\nAdjusting the value of K affects both performance and latency. A higher\nK increases the likelihood of capturing relevant documents but also increases\nthe input for the RR stage, thus increasing latency.\n19\nStage Single Model CREST\nIR and RR(Training) Single training 4 separate trainings\nIR(Inference) 1 LLM pass 4 LLM passes\nRR(Inference)kLLM passes4\u2217kLLM passes\nTable 1: Comparison of computation and latency costs between the single model and the\nCREST model, withkrepresenting the number of candidates returned in the IR stage.\nFor both IR and RR stages, the number of criterion-specific models used\nin CREST is determined by the availability of the observation criteria after\nparsing the observation entered by the tester. If the TR observation contains\nall criteria, then all models in CREST are active. This helps to ensure that\nthe system uses all available information to optimize retrieval performance.\nAs the document embeddings are pre-calculated for the IR stage, only\nquery embeddings need to be generated at runtime. In CREST, this involves\nfour query embeddings instead of one, but they are computed in parallel\nso the latency remains close to that of a single model. At a large scale,\nthis keeps first-stage retrieval feasible on very large TR corpora, since each\nquery\u2019s four embeddings are matched against pre-computed indexes to select\na small candidate set for reranking (RR).\nThe RR stage is more expensive because CREST scores every candidate\nwith all criterion-specific models. We manage this by keeping the candidate\nset relatively small and by batching candidates so each model processes them\nin one pass. Running models across two GPUs further reduces wall-clock\ntime and keeps end-to-end latency within acceptable service windows for\nproduction. The number of criteria can also be reduced for tasks that require\nvery low latency, which creates a trade-off between latency and performance.\nTable 1 presents training and inference costs.\nAs presented in Table 1, the training latency for CREST is four times\nhigher than that of a single model when using a single GPU, due to the need\nto train four separate criterion-specific models. However, we leveraged four\nGPUs to run these trainings concurrently, which kept the overall training\ntime comparable to that of a single model, while increasing GPU resource\nusage accordingly.\n20\n4. Evaluation\nIn this study, we create distinct datasets incorporating diverse informa-\ntion for both training and evaluation. We also determine the impact of\nvarious TR observation criteria on retrieval system performance and the en-\nhancement that the CREST can bring to the retrieval system.\n4.1. Datasets\nTo construct datasets, trouble reports were organized into groups based\non the presence of specific criteria within their observation sections, as illus-\ntrated in Figure 2. Due to the heterogeneous nature of TRs, not all contain\nidentical observation fields, which affects the dataset\u2019s composition. We con-\nsistently include the headline section and trouble description criteria in all\ndatasets to preserve essential information. Approximately60%of the parsed\nTRs used for this study contain all listed criteria, and this coverage is sig-\nnificantly higher among more recent TRs than among older reports partially\nincluded in this study.\nTo prepare data for the criterion-specific models and analyze the signif-\nicance of each of the individual criteria, the following approach was applied\nfor each criterion:\n1. Filtering TRs to isolate those containing the criterion under study.\n2. Forming queries and documents from this refined subset to produce\ndatasetsfocusedonthatspecificcriterion. Documentsarepre-processed\nversions of TR answers.\n3. Combining the \u201cheadline\u201d and \u201ctrouble description\" with the criteria\nunder evaluation to formulate queries, ensuring each dataset is tailored\nto our research focus.\nA baseline dataset is created for each experimental set, consisting of TRs\nwith queries derived solely from the \u201cheadline\u201d and \u201ctrouble description\u201d.\nThis baseline allows for direct performance comparison between criterion-\nspecific datasets and their corresponding baseline, measuring the impact of\neach criterion.\nFrom a subset of the TR corpus that includes all criteria shown in Fig-\nure2, werandomlyextracttwonon-overlappingsetsof1,000TRs. Thesesets\nform the basis for the validation and test datasets used in each experiment.\nIt is important to note that the TRs in the validation and test datasets re-\nmain constant across all experiments to ensure consistent evaluation. Table 2\nreports the different datasets along with their metrics.\n21\nDataset Included Fields Number of TRs for Training\nHTI H + T + Impact 8,641\nHTF H + T + Frequency 11,864\nHTC H + T + Condition 10,175\nHTR H + T + Reproducibility 8,097\nSingle Model - 14,504\nTable 2: Criterion-specific training datasets and sample sizes (number of TRs used for\ntraining). Included fields indicate query construction. Abbreviations: H =headline, T =\ntrouble description, I =impact, F =frequency, C =condition, R =reproducibility.\n4.2. Evaluation Strategy\nTo assess the effectiveness of each experiment, we undertake a compre-\nhensive comparison between the performance of each criterion-specific model\nand its corresponding baseline. This comparative approach enables us to\nmeasure the impact of the examined TR observation criteria on the overall\nperformance of the retrieval system, which is calculated for each criterion\n(impact on system, condition, and others) as follows:\nIC=P C\u2212PCbaseline (1)\nWhereP Crepresents the performance of a criterion-specific model for the\ncriterion under study.P Cbaselinedenotes the performance of the corresponding\nbaseline model, which includes headline and trouble description, but not the\ncriterion under study.I Cis the impact score, indicating the performance\ndifference caused by introducing the specific criterion being evaluated.\nThe impact score for each criterion is calculated for both bi-encoder and\ncross-encoder architectures within the two-stage workflow, as shown in Fig-\nure 1. In addition to this, we compare the overall retrieval performance\nacross criterion-specific models, the CREST ensemble, and the single model\nbaseline. This comparison is conducted under both the full two-stage setup\nand the cross-encoder (monoTeleRoBERTa) in isolation. The goal of this\nevaluation is to determine the effectiveness of incorporating criterion-specific\nsignals into the retrieval process and to assess whether the ensemble strat-\negy in CREST leads to consistent performance improvements over individual\nmodels and baselines across different configurations.\nTo assess confidence calibration, we adopt the methodology proposed by\nPenha and Hauff (2021b), which transforms the ranking task into a multi-\n22\nclass classification problem. For each query, we select the top five documents\nbased on their retrieval scores and apply a softmax function to normalize\nthe scores into probabilities. This reformulation enables the computation of\ncalibration metrics, allowing us to evaluate how well the predicted relevance\nscores reflect actual relevance likelihoods. We follow the same evaluation\nsetting presented in their study to ensure consistency and comparability in\nour calibration analysis.\n4.3. Evaluation Metrics\nIn our study, we employ three key metrics to quantify the performance of\nmodelsondatasetsintroducedinSection4.1: MeanReciprocalRank(MRR),\nRecall@K, and nDCG.\nMean Reciprocal Rank (MRR):For each query, the Reciprocal Rank is\nthe inverse of the rank of the first relevant document. For instance, if the\nfirst relevant document appears at position 2 in the retrieval system output,\nthe RR is1\n2.MRRis calculated by obtaining the mean over the RR of all\nqueries.\nRecall@K:Evaluates the model by its effectiveness in retrieving relevant\ndocuments within the top-K results without considering their actual rank.\nNormalized Discounted Cumulative Gain (nDCG):Measures the rank-\ning quality of retrieval results, giving a higher score for relevant documents\nranked higher in the list.\nExpected Calibration Error (ECE):To evaluate how well the predicted\nrelevance scores reflect the true likelihood of relevance, we employ the Ex-\npected Calibration Error. This metric quantifies the difference between pre-\ndicted confidence and empirical accuracy. The predictions are grouped into\nMequally spaced bins based on their confidence scores. For each binB m,\nthe absolute difference is computed between the average predicted confidence\n\u02c6piand the observed accuracy (i.e., the fraction of correct predictions). The\nfinal ECE is the weighted average of these differences across all bins, defined\nas:\nECE =X\nm= 1M|Bm|\nn\f\f\f\f\f1\nBmX\ni\u2208B m[I(\u02c6y i=yi)\u2212\u02c6p i]\f\f\f\f\f(2)\nThis metric was originally proposed by Naeini et al. (2015) and has been\nadapted for neural ranking settings in recent works (Penha and Hauff, 2021b;\n23\nTam et al., 2023a). A lower ECE indicates better calibration, meaning that\nthe model\u2019s predicted relevance scores are more trustworthy.\nCalibration Diagrams:Theseplotsvisualizetherelationshipbetweencon-\nfidence and accuracy. A perfectly calibrated model aligns with the diagonal\nline, where confidence matches accuracy. Deviations from this line indicate\nmiscalibration, with underconfidence or overconfidence depending on the di-\nrection of the shift.\n5. Results\nIn this section, we present the evaluation results of the CREST compared\nto a single criterion-agnostic model TR retrieval model. We also discuss the\nimpact score of each criterion on retrieval performance. The performance is\nassessed by several key metrics: Recall@K (R@5,R@10,R@15), Mean Re-\nciprocal Rank (MRR), andnDCG@15. We present the evaluation results for\nall bi-encoder and cross-encoder models, including their performance within\na two-stage workflow and the cross-encoder in isolation.\n5.1. Impact of Each Criterion (RQ1)\nTable 3 and Table 4 present the criterion-specific models alongside their\nbaselines, using TwinRoBERTa and ColRoBERTa for the IR stage, respec-\ntively. Table5presentsthecriterion-specificmodelswiththeirbaselinesusing\nthe monoRoBERTa model for the RR stage. The findings from the evalua-\ntiondemonstratethatcriterion-specificmodelsenhanceretrievalperformance\ncompared to their respective baselines. In the IR stage using TwinRoBERTa\nmodel, HTI, HTF, and HTC consistently outperform their corresponding\nbaselines across all metrics. Compared to the baseline single model that uses\nall available information, these models also show improved performance, with\nthe exception of HTI inR@5, where it slightly underperforms. On the other\nhand, the HTR model performs worse than both its baseline and the single\nmodel across all metrics, indicating that this criterion may not contribute as\neffectively to improving retrieval quality in the IR setting.\nIn the IR stage with ColRoBERTa, the criterion-specific models once\nagain outperform their baselines for HTI and HTC. The same holds for HTF,\nwith the exception of theR@15metric, where the baseline performed slightly\nbetter. Similar to the TwinRoBERTa results, these models consistently sur-\npass the single baseline model that uses all available information, except in\nthe case of HTF atR@5. HTR follows a comparable trend to TwinRoBERTa\n24\nInitial Retrieval TwinRoBERTa-base\n(Bi-Encoder) HTI HTI baseline HTF HTF baseline HTC HTC baseline HTR HTR baseline Single Model BM25\nR@5 49.85% 45.95% 52.95% 49.25% 51.35% 51.05% 50.15%51.45% 49.95% 46.85%\nR@10 58.56% 53.55%61.36% 57.56% 59.79% 57.96% 57.66% 59.46% 57.96% 51.55%\nR@15 64.36% 59.16%65.77% 62.26% 65.17% 62.96% 61.96% 64.96% 62.66% 54.95%\nMRR 42.19% 38.59%43.89% 40.75% 43.14% 41.71% 41.92% 42.95% 42.04% 30.58%\nnDCG@15 52.87% 49.68% 48.39% 45.08% 47.58% 46.00% 45.95% 47.40% 46.19% 42.73%\nTable 3: Performance of criterion-specific models, their baselines, and a single criterion-\nagnostic model for TwinRoBERTa-base encoder in the IR stage.\nInitial Retrieval ColRoBERTa-base\n(Bi-Encoder) HTI HTI baseline HTF HTF baseline HTC HTC baseline HTR HTR baseline Single Model BM25\nR@5 58.16% 56.96% 55.06% 54.95% 56.86% 53.85% 52.95% 53.75% 55.56% 46.85%\nR@10 65.76% 63.66% 63.86% 63.36%65.77% 62.26% 61.86% 61.27% 62.66% 54.95%\nR@15 69.37% 68.57% 67.97% 68.27%70.97% 66.97% 67.17% 66.27% 67.17% 51.55%\nMRR 49.07% 47.42% 47.22% 46.93%49.13% 47.09% 45.61% 46.79% 45.98% 30.58%\nnDCG@15 53.25% 52% 51.45% 51.28%53.63% 51.06% 50.01% 50.70% 50.29% 42.73%\nTable 4: Performance of criterion-specific models, their baselines, and a single criterion-\nagnostic model for ColRoBERTa-base encoder in the IR stage.\nby performing worse than both its respective baseline and the single baseline\nmodel. However, in the ColRoBERTa setting, HTR performs considerably\nbetter, and its negative effect is relatively minor. Overall, these outcomes\nconfirm the recurring pattern that HTR can reduce performance when used\nalone in IR stage, while the other criteria contribute positively in most cases.\nRR results are reported using the top-K candidates produced by theCol-\nRoBERTaIR configuration, which demonstrated consistently superior per-\nformance and thus provides the most reliable candidate pool for re-ranking.\nIn the RR stage, all criterion-specific models outperform their baselines and\nthe single model across most metrics. A notable reversal occurs with HTR,\nwhich underperforms its baseline in IR under both TwinRoBERTa and Col-\nRoBERTa, yet in RR achieves the highest recall at all cutoffs and ties for\nthe topnDCG@15, with onlyMRRslightly higher for HTF. This suggests\nthat the richer contextual information available during re-ranking enables\nthe model to make better use of HTR-specific signals that were less effective\nin the retrieval-only setting. HTI and HTC remain strong in RR, but their\nadvantage is smaller than in IR. This pattern suggests that criterion-specific\nmodeling continues to add value, with impact and condition cues driving\nretrieval performance, while the other criteria help refine the final ranking.\nTable 6 presents the performance of criterion-specific models with their\nbaselines using only the isolated monoRoBERTa cross-encoder, without ben-\nefiting from the IR stage. The results reveal that the single model, which\nutilizes all available criteria, consistently achieves better performance across\n25\nRe-Ranking monoRoBERTa-base\n(Cross-Encoder) HTI HTI baseline HTF HTF baseline HTC HTC baseline HTR HTR baseline Single Model\nR@5 65.87% 60.16% 65.47% 62.06% 66.27% 61.56%66.67% 60.36% 63.66%\nR@10 74.17% 68.77% 74.17% 70.97% 74.67% 70.17%75.88% 68.77% 70.27%\nR@15 77.78% 72.37% 78.48% 73.67% 79.28% 73.87%79.58% 72.27% 74.17%\nMRR 52.02% 49.52%52.88% 51.79% 50.58% 51.19% 52.42% 48.16% 51.77%\nnDCG@15 57.89% 54.72%58.66% 56.78% 57.17% 56.40%58.66% 53.72% 56.93%\nTable 5: Performance of criterion-specific models, their baselines, and a single model for\nthe monoTeleBERTa-base encoder in the RR stage.\nIsolated monoRoBERTa-base\n(Cross-Encoder) HTI HTI baseline HTF HTF baseline HTC HTC baseline HTR HTR baseline Single Model\nR@5 56.06% 55.36% 55.66% 55.86% 54.35% 54.75% 58.46% 50.65%60.96%\nR@10 67.47% 65.27% 65.57% 65.47% 66.47% 65.47% 67.07% 61.56%68.57%\nR@15 73.67% 70.97% 70.57% 70.07% 72.37% 70.57% 72.97% 67.87% 73.27%\nMRR 43.76% 42.53% 43.14% 42.69% 40.40% 42.21% 44.68% 37.55%47.24%\nnDCG@15 50.15% 48.60% 48.98% 48.51% 47.29% 48.28% 50.78% 43.94%52.90%\nTable 6: Performance of criterion-specific models, their baselines, and a single model for\nthe isolated monoTeleBERTa-base encoder.\nnearly all metrics and settings. This stands in contrast to the RR stage re-\nsults, where criterion-specific models surpassed both their baselines and the\nsingle model. The lack of improvement here suggests that the re-ranking\nbenefits observed earlier rely heavily on the synergy between the IR and RR\nstages rather than on the cross-encoder alone. Without the support of an\nIR stage to pre-select relevant candidates, the cross-encoder operates over a\nbroader, noisier set of inputs, which can dilute the effectiveness of specialized\nrepresentations.\nOverall, these results emphasize that criterion specialization is most ef-\nfective when integrated into a pipeline where the IR stage helps isolate more\nrelevant candidates, creating a setting where the RR model can more effec-\ntively leverage targeted representations.\nTable 7 presents the impact scores of each criterion across both the IR\nandRRstages. WithTwinRoBERTainIR,\u201csystemimpact\u201d (HTI)showsthe\nstrongest positive influence across all metrics, with \u201cfrequency\u201d (HTF) and\n\u201ccondition\u201d (HTC) following, and \u201creproducibility\u201d (HTR) reducing perfor-\nmance. With ColRoBERTa in IR, \u201ccondition\u201d emerges as the leading signal\nwhile \u201csystem impact\u201d remains beneficial, \u201cfrequency\u201d offers smaller gains,\nand \u201creproducibility\u201d is mixed, hurting early precision but providing slight\nimprovements at deeper recall. In the RR stage using monoRoBERTa over\nColRoBERTa candidates, \u201creproducibility\u201d delivers the largest gains on all\nmetrics, while impact and \u201cfrequency\u201d remain positive and \u201ccondition\u201d con-\ntributes less to MRR.\n26\nTwo Stage IR stage - TwinRoBERTa-base IR stage - ColRoBERTa IR RR Stage - monoRoBERTa-base\nRetrieval HTI HTF HTC HTR HTI HTF HTC HTR HTI HTF HTC HTR\nR@5 3.93.7 0.3 -1.3 1.2 0.13.01-0.8 5.71 3.41 4.716.31\nR@10 5.013.8 1.83 -1.8 2.1 0.503.510.59 5.40 3.20 4.507.10\nR@15 5.23.51 2.21 -3.0 0.8 -0.34.00.9 5.41 4.81 5.417.31\nMRR 3.63.14 1.43 -1.03 1.65 0.292.04-1.18 2.50 1.09 -0.614.26\nnDCG@15 3.193.311.58 -1.45 1.25 0.172.57-0.69 3.17 1.88 0.774.94\nTable 7: Impact of each criterion on the retrieval model performance. RR-stage result is\ncomputed using monoRoBERTa withColRoBERTaas the preceding IR stage.\nModel Variant MRR R@5 R@10 R@15 nDCG@15\nIR - ColRoBERTaCREST (All) 52.50% 61.36% 69.27% 75.18% 57.19%\nCREST w/o I 52.20%(\u21930.30) 60.62%(\u21930.74) 67.94%(\u21931.33) 72.14%(\u21933.04) 56.26%(\u21930.93)\nCREST w/o F 52.46%(\u21930.04) 61.66%(\u21910.30) 68.87%(\u21930.40) 74.47%(\u21930.71) 57.00%(\u21930.19)\nCREST w/o C 51.21%(\u21931.29) 59.96%(\u21931.40) 69.27%(0.00) 73.27%(\u21931.91) 55.77%(\u21931.42)\nCREST w/o R 51.87%(\u21930.63) 60.92%(\u21930.44) 68.54%(\u21930.73) 74.05%(\u21931.13) 56.46%(\u21930.73)\nIR - TwinRoBERTaCREST (All) 50.64% 60.02% 67.74% 72.04% 55.07%\nCREST w/o I 49.53%(\u21931.11) 59.02%(\u21931.00) 66.63%(\u21931.11) 71.14%(\u21930.90) 53.99%(\u21931.08)\nCREST w/o F 49.25%(\u21931.39) 58.02%(\u21932.00) 64.93%(\u21932.81) 71.24%(\u21930.80) 53.75%(\u21931.32)\nCREST w/o C 49.63%(\u21931.01) 58.86%(\u21931.16) 66.47%(\u21931.27) 71.47%(\u21930.57) 54.16%(\u21930.91)\nCREST w/o R 50.14%(\u21930.50) 58.92%(\u21931.10) 67.23%(\u21930.51) 71.44%(\u21930.60) 54.54%(\u21930.53)\nRR - monoRoBERTaCREST (All) 57.69% 70.07% 77.73% 81% 63.08%\nCREST w/o I 56.62%(\u21931.06) 69.68%(\u21930.39) 77.62%(\u21930.11) 80.87%(\u21930.13) 62.26%(\u21930.82)\nCREST w/o F 56.28%(\u21931.41) 69.99%(\u21930.08) 77.92%(\u21910.19) 80.87%(\u21930.13) 62%(\u21931.08)\nCREST w/o C 57.24%(\u21930.45) 69.55%(\u21930.52) 77.49%(\u21930.24) 80.75%(\u21930.25) 62.68%(\u21930.4)\nCREST w/o R 55.91%(\u21931.78) 68.43%(\u21931.64) 76.88%(\u21930.85) 80.86%(\u21930.14) 61.66%(\u21931.42)\nTable 8: Ablation of criterion-specific models in the CREST ensemble (percent). Differ-\nences in parentheses indicate percentage-point change vs. the model\u2019sCREST (All). I =\nImpact (HTI), R = Reproducibility (HTR), F = Frequency (HTF), C = Condition (HTC).\n\u201cw/o X\u201d excludes criterion X from the ensemble. Differences are percentage points relative\ntoCREST (All)for the same backbone.\nOne possible reason why HTR performs worse in the IR stage is that the\nbi-encoder compares short query embeddings with pre-computed document\nembeddings, which tends to favour concise descriptors. HTR text often in-\ncludes procedural steps, boilerplate phrases, and local details such as paths\nor versions, which may dilute the main semantic signal. Because answers\ndo not always mirror these stepwise details, the similarity match is often\nweaker than for HTI, HTF, or HTC, which contain clearer fault descriptors\nand affected components. In contrast, the RR stage may benefit more from\nHTR since the cross-encoder can capture word interactions, ordering, and\nnegation, making it easier to align procedural steps with the answer section\nresolution explanations.\nTo further analyze the impact of each criterion on ensemble performance,\n27\nTable 8 presents results in which CREST is compared with all criteria ac-\ntive and with one removed. The comparison shows that across all three\nbackbones,CREST (All)consistently delivers the most reliable performance.\nRemoving any single criterion degrades every evaluation metric, highlighting\ntheir strong complementarity. A notable case arises with reproducibility (R),\nwhich on its own has a negative effect in IR tasks as shown in Table 7. Yet\nTable 8 reveals that excludingRfrom the ensemble reduces effectiveness,\nindicating that CREST is able to exploit its value in combination with other\ncriteria. This suggests thatRsupplies a complementary signal that is not\nfully captured elsewhere. For both IR models the effect of removingRis less\nsevere than removing other criteria, whereas for RR it has a greater negative\nimpact. These patterns suggest that the ensemble leveragesRas a high-\nprecision discriminator in RR while down-weighting its noisier effect in IR,\nwhich explains why its exclusion harms final performance.\nOverall, the ablation study demonstrates that each criterion contributes\npositively when aggregated. By providing distinct forms of evidence, they\nenhance ranking quality once richer interactions are integrated, supporting\nthe use of the complete criterion set in CREST. The results also suggest that\nthe contribution of each criterion is not uniform and varies across the IR\nand RR stages. HTI, HTF, and HTC consistently show a positive impact,\nhighlighting their critical role in both stages of retrieval. Interestingly, HTR,\nwhich negatively affects performance in the IR stage, shows notable improve-\nments in the RR stage. This indicates that certain criteria, like HTR, may\nrequire a richer interaction or additional context to become useful, which the\nRR stage is better able to provide. Moreover, it underscores the need to\nrevisit the current TR template structure to ensure that the most influential\ninformation is surfacing early, making it easier for readers to quickly identify\nthe most important information.\n5.2. Performance improvement by CREST (RQ2)\nTable 9 demonstrates the performance improvement of CREST over the\nsingle criterion-agnostic retrieval model in the two-stage workflow. This com-\nparison includes results for the IR stage (TwinRoBERTa and ColRoBERTa),\nfollowed by the RR stage (monoRoBERTa) over ColRoBERTa candidates.\nThe performance evaluation of CREST demonstrates significant gains\nacross all metrics, compared to a single criterion-agnostic model approach in\nboth the IR and RR stages. In the IR stage, by aggregating relevance scores\nfrom all criterion-specific models (HTI, HTC, HTF, HTR) through weighted\n28\nIR (TwinRoBERTa) IR (ColRoBERTa) RR (monoRoBERTa)\nCRESTSingle criterion-agnostic\nretrieval modelCRESTSingle criterion-agnostic\nretrieval modelCRESTSingle criterion-agnostic\nretrieval model\nR@5 60.16% 49.95% 61.36% 55.56% 70.07% 63.66%\nR@10 67.67% 57.96% 69.27% 62.66% 77.73% 70.27%\nR@15 72.67% 62.66% 75.18% 67.17% 81% 74.17%\nMRR 50.21% 42.04% 52.5% 45.98% 57.69% 51.77%\nnDCG@15 54.87% 46.19% 57.19% 50.29% 63.08% 56.93%\nTable 9: CREST performance improvement over the single criterion-agnostic model in\nthe two-stage (IR-RR) workflow. RR-stage result is computed using monoRoBERTa with\nColRoBERTaas the preceding IR stag\nensembling, CREST achieves over10.2%improvement forR@5compared to\nthe single model. This indicates a substantial improvement in retrieving rel-\nevant documents within the top 5 ranked results. Similarly, CREST achieves\n9.7%and10.0%improvement forR@10andR@15, respectively, compared to\nthe single model. CREST also shows8.1%improvement inMRRand8.6%\nimprovement innDCG@15. Using ColRoBERTa for IR, CREST remains\nsuperior to the single model with gains of5.8%forR@5,6.6%forR@10,\n8.0%forR@15,6.5%forMRR, and6.9%fornDCG@15.\nIn the RR stage, CREST continues to outperform the single model across\nall metrics with a6.4%improvement forR@5,7.5%forR@10, and6.8%\nforR@15. Compared to the criterion-agnostic single model,MRRand\nnDCG@15also improve with CREST by5.9%and6.1%, respectively. These\nresults show that the benefits of CREST persist across both ranking stages,\nconsistently improving retrieval effectiveness.\nTable10presentstheperformanceofCRESTcomparedtoasinglecriterion-\nagnostic model, focusing solely on the cross-encoder (monoRoBERTa) with-\nout relying on the IR stage. While Table 6 showed limited gains for criterion-\nspecific models in isolation, the ensemble approach in CREST yields clear\nimprovements across all metrics. Notably,R@5improves by5.47%,R@10\nby7.08%,R@15by8.29%, MRR by5.44%, andnDCG@15by6.09%. This\nsuggests that breaking down the input based on TR criteria and assigning it\nto expert models is more effective than applying a single model to process\nthe entire TR. Furthermore, it shows that results demonstrate that CREST\neffectively leverages the strengths of individual criterion-specific models and\nhighlights the benefit of integrating multiple perspectives, even without the\naid of the IR stage.\nThese findings indicate that CREST consistently outperforms the single\nmodelapproachacrossallmetricsinboththeIRandRRstages, underscoring\n29\nIsolated monoRoBERTa\nCREST Single criterion-agnostic retrieval model\nR@5 66.43% 60.96%\nR@10 75.65% 68.57%\nR@15 81.56% 73.27%\nMRR 52.68% 47.24%\nnDCG@15 58.99% 52.90%\nTable 10: Performance of CREST and a single retrieval model.\nthe effectiveness of leveraging criterion-specific models for TR retrieval. By\nmodeling each criterion independently and then aggregating their outputs,\nCREST can capture diverse and complementary signals that a single model\nmight overlook. This leads to more accurate retrieval of relevant TRs, which\ncan directly benefit the troubleshooting workflow.\nA more accurate TR retrieval system helps engineers find previously re-\nsolved TRs that are more closely aligned with the current issue, improving\nthe relevance and reliability of the suggested solutions. The improved per-\nformance of CREST is directly reflected in the utility of the CREST tool.\nAlongside retrieving more relevant TRs, the tool provides a breakdown of\nrelevance scores across criteria, offering insight into why each result was se-\nlected. This makes the retrieval process more interpretable and actionable.\nFor instance, if a match is primarily driven by \u201csystem impact\u201d and \u201ccondi-\ntion\u201d, users can quickly assess its relevance to the new issue. This integration\nnot only supports faster resolution but also helps teams focus on the most\ncritical aspects of a problem, reducing manual effort and improving workflow\nefficiency.\nIn practice, this means that CREST doesn\u2019t just retrieve better matches,\nit also supports decision-making by highlighting why those matches were\nselected. This capability helps engineers respond more efficiently, reduces\ntrial-and-error in solution discovery, and enables more consistent handling of\nrecurring issues.\nThat said, CREST does not always outperform a single model. Fail-\nures typically occur when the IR candidate set excludes the true match, a\nsituation more likely with short or ambiguous queries or with sparse obser-\nvations. In such cases, a single model may be preferable, as it processes the\nTR observation holistically, increasing query length and potentially reducing\nambiguity by incorporating information overlooked by criterion-specific mod-\n30\nInitial Retrieval Re-Ranking\nHTI 0.0287 0.0186\nHTF 0.0308 0.0197\nHTC 0.0304 0.0213\nHTR 0.0344 0.0199\nCREST 0.0249 0.0175\nSingle retrieval model 0.0345 0.0254\nTable 11: Expected calibration error (ECE) for all models.\nels. Despite quality assurance measures prior to TR publication, instances\nstill arise where TR creators comply with all requirements yet provide de-\nscriptions that lack sufficient detail or contain ambiguity, negatively affecting\nboth CREST and the single model. Another scenario in which a single model\nis advantageous arises when only a single criterion (for example, \u201csystem im-\npact\u201d) is present for a TR. Criterion-specific models become vulnerable to\nnoise from that single criterion, with no others available to mitigate it. In\nsuch situations, a reasonable fallback is to adopt a criterion-agnostic single\nmodel.\n5.3. Relevance Score Calibration Analysis (RQ3)\nTo better understand the quality of the predicted relevance scores and\ntheir alignment with actual outcomes, we analyze the calibration of each\nmodel using the Expected Calibration Error (ECE). A lower ECE indicates\nthat the predicted probabilities better reflect the true likelihood of relevance,\ncontributing to more trustworthy and reliable relevance scores for each cri-\nterion. This is particularly important in retrieval systems where confidence\nestimates play a role in guiding downstream decisions. Table 11 reports the\nECE values for all models across both the initial retrieval and re-ranking\nstages.\nAsshowninTable11,thecriterion-specializedmodelsconsistentlyachieve\nlower ECE scores compared to the single retrieval model, with the difference\nbeing more notable in the re-ranking stage. This suggests that modeling rel-\nevance per criterion results in more reliable confidence estimation, without\nthe need for additional calibration methods. Even though calibration was\nnot explicitly targeted during training, the specialized models yield better\nalignment between predicted and actual relevance probabilities.\n31\nFigure 6: Calibration diagrams for initial retrieval (left) and re-ranking (right). The left\nplot shows the calibration performance of the initial retrieval stage, while the right plot\nillustrates the calibration diagram obtained after re-ranking.\nFigure 6 shows the calibration diagrams for both stages, comparing the\nCREST model with the single criterion-agnostic retrieval model. In both\ncases, the ensemble model tracks more closely to the ideal calibration line,\nwhile the single model shows larger deviations. These trends are consistent\nwith the ECE results and indicate that combining criterion-specific predic-\ntions leads to a better confidence calibration of relevance scores.\nThis makes the confidence scores easier to interpret, since they can be\nread as reliable probabilities rather than opaque values. Engineers can then\nset risk-aware thresholds, decide when to auto-suggest or defer to manual\nreview, andidentifywhichcriteriondrivesamatchwithdependablecertainty.\nIt also lowers the risk of over-confidence and reduces the chance of irrelevant\nevidencebeingpassedtodownstreamcomponentssuchasre-rankersorRAG-\nstyle assistants.\nTogether, thesefindingshighlightthatleveragingcriterion-specificmodels\nimproves not only retrieval performance but also the calibration and reliabil-\nity of the predicted scores. This is particularly important when transparency\nin the decision-making process is a key requirement.\n5.4. Pilot User Study (RQ4)\nWecomplementedtheofflineevaluationwithasmallpilottocheckwhether\nthe gains we observe translate into practice. The goal was to understand if\nthe criterion-wise scores make the ranking more transparent, whether the\n32\nTR1 TR2 TR3 TR4 TR5\nParticipants P1 P2 P3 P1 P2 P3 P1 P2 P3 P1 P2 P3 P1 P2 P3\nExplainability of Scoring Good Good Good Good Fair Fair Good Fair Very Poor Good Fair Fair Good Fair Good\nHelpfulness of Ranking Good Good Very Poor Good Fair Fair Good Fair Good Good Good Good Excellent Fair Good\nTrustworthiness of Ranking Good Fair Very Poor Good Fair Fair Good Good Fair Very Poor Good Good Good Fair Good\nAccuracy of Ranking Excellent Fair Very Poor Excellent Good Good Excellent Good Fair Very Poor Good Good Good Fair Good\nTable 12: Participants\u2019 (P1\u2013P3) ratings for five TRs (TR1\u2013TR5).\nrecommendations are useful during triage, and how credible and accurate\nthe top results appear to possible end users.\nThis study was conducted at Ericsson with three practitioners and five\nreal-lifeTRs. AllparticipantshadQAtestingbackgrounds: twowereactively\ninvolved in QA testing at the time of the study, and one was not currently\ntesting but had prior QA experience. One participant had less than five years\nof experience at Ericsson, and the other two had more than five years. For\neach TR, participants first read its description and then reviewed the top five\ncandidateTRsretrievedbyCREST.Eachcandidatewaspresentedwithboth\nthe aggregated CREST score and the detailed criterion-wise scores, allowing\nparticipants to examine how individual criteria contributed to the final rel-\nevance score. They then rated (i) explainability of scoring, (ii) helpfulness\nfor triage tasks (such as root-cause clues, mitigation, symptom matching,\nand TR authoring), (iii) trustworthiness, and (iv) perceived accuracy, using\na six-point scale ranging from \u201cNot at all\u201d to \u201cExcellent\u201d. Participants were\ngiven one week to complete the evaluation at their own pace and on average\nspent about 15 minutes evaluating each TR. At the end of the evaluation,\nthey were also asked to share their overall impressions of CREST. Given\nthe small number of participants and TRs, this pilot should be viewed as\nexploratory and intended to provide initial qualitative insights.\nTable 12 reports the results of the pilot user study. With 15 ratings\nper evaluation metric,explainability of scoringwas ranked as mostlyGood\n(8/15, 53%) orFair(6/15, 40%), indicating that criterion-wise scores im-\nproved transparency, yet leaving room for clearer presentation.Helpfulness\nof rankingwasGood/Excellentin 10/15 (67%), suggesting the provided TR\nlists often surfaced actionable cues.Trustworthinesswas perceived asGood\nin 8/15 (53%) andFairin 5/15 (33%). Lastly, perceivedaccuracywas\nGood/Excellentin 10/15 (67%), with a minority ofVery Poorjudgments\n(2/15). We did not observe any \u201cNot at all\u201d ratings for any metric.\nDifferences across TRs were mainly explained by input quality, with\nsparse or unclear criteria affecting CREST\u2019s performance. Findings con-\nsistent with both performance and calibration gains observed in offline ex-\n33\nperiments. Moreover, explainability benefited from the criterion-wise scores,\nwhich participants used to interpret why a given TR surfaced and to justify\nkeeping or discarding specific candidates.\nParticipants feedback on negative cases reinforces our earlier findings,\nhighlighting the strength of CREST when the criteria are informative and\nits weaker performance when they are brief or ambiguous. Moreover, partic-\nipant P3 noted that the scoring explanations allowed them to justify select-\ning the most relevant TR from the candidate list provided, illustrating how\ntransparency in the rankings can support more informed decision-making.\n6. Discussion\nWe now discuss the implications of our work (Section 6.1) and address\nthe threats to validity and how we mitigated them (Section 6.2).\n6.1. Implications\nThe proposed CREST model has practical implications for improving\nthe efficiency and reliability of quality assurance (QA) and design teams at\nEricsson. By offering a more accurate and interpretable retrieval of relevant\ntroublereports(TRs), CRESTcanassistengineersinidentifyingrelatedpast\nissues more effectively, thereby accelerating the fault resolution process. This\nis particularly valuable in complex systems where understanding the context\nand history of software faults plays a crucial role in diagnosing and resolving\nnew incidents.\nOne of the key benefits of CREST is its ability to surface criterion-specific\nrelevance scores, helping users understand which aspects of a TR (e.g., func-\ntional area, fault type, or impacted component) contributed most to the\nretrieval outcome. This transparency can guide engineers in validating the\nretrieved TRs, increase their trust in the system, and potentially uncover\nnew resolution strategies based on previously overlooked criteria.\nFrom a practical standpoint, CREST can be integrated into existing TR\nretrieval workflows within Ericsson with minimal disruption. Depending on\nthe requirements of the task, CREST can be used in the Initial Retrieval\nstage, especially for latency-sensitive applications, or extended in both stages\nof a two-stage pipeline to fully leverage its benefits. In addition to its role\nin TR retrieval, CREST can also serve as a retrieval engine for Retrieval-\nAugmented Generation (RAG) systems tailored to TR-related tasks. In this\n34\ncontext, CREST\u2019s interpretable retrieval helps explain why specific infor-\nmation sources were selected and how they contributed to the final output,\nenhancing both the transparency and reliability of the generated responses.\nOverall, CREST offers a practical and effective enhancement to existing\nTR retrieval systems, enabling Ericsson teams to reduce resolution times, im-\nprove traceability, and support more informed decision-making during soft-\nware maintenance and troubleshooting.\n6.2. Threats to Validity\nSeveral factors may threaten the validity of our findings. First, the inter-\nnal data from Ericsson is proprietary and cannot be publicly shared due to\nnon-disclosure obligations. This data may not generalize to other telecom-\nmunication environments, limiting external validity due to different TR char-\nacteristics and structures.\nSecond, the quality of TRs and parsing tools is critical and can impact\nmodel performance, especially during deployment. Inconsistencies in these\ntools or data can influence the effectiveness of the TR retrieval system.\nThird, criterion-specific datasets may not contain TRs of similar quality,\nresulting in performance variations. We introduced a baseline to isolate the\neffect of each criterion, but differences may still affect results.\nFourth, CREST is designed to enhance explainability by exposing per-\ncriterion relevance scores. To assess its practical value, a pilot user study\nwas conducted. The results suggest that per-criterion scores support sense-\nmaking and traceability of rankings, highlighting CREST\u2019s potential for in-\nterpretability and transparency. However, the study was limited in scale and\nscope, and a larger, more comprehensive evaluation involving broader user\ngroups is needed and left as a possible future direction.\nMoreover, due to Ericsson\u2019s policy and resource limitation challenge,\nwe have only evaluated our approach with an internally trained RoBERTa\nmodel, and as a result, the impact of the CREST approach may vary across\ndifferent large language models (LLMs). Additionally, the effectiveness of\nthe CREST approach may not be consistent across various TR retrieval ap-\nplications, which limits the generalizability of our findings.\nWhile we acknowledge these limitations, we hope to address them in\nfuture research to validate and generalize our findings.\n35\n7. Conclusion\nThisstudyinvestigatestheimpactofvarioustroublereportcriteriaonthe\nperformance of the Initial Retrieval (IR) and Re-Ranking (RR) stages within\ntheTRretrievalsystem. Byutilizingabi-encoderintheIRstageandacross-\nencoder in the RR stage, we were able to evaluate each criterion\u2019s influence\nin a comprehensive two-stage workflow. Notably, criteria such as \u201csystem\nimpact\" (HTI) significantly improved recall and ranking metrics during the\nIRstage, whereas\u201creproducibility\"(HTR) negativelyinfluencedtheIRstage\nbut showed positive effects in the RR stage, highlighting that different stages\nbenefit from different types of information. This illustrates the importance\nof selectively parsing and utilizing specific information to enhance retrieval\nperformance, as opposed to leveraging all available data indiscriminately.\nThe standalone evaluation of each criterion\u2019s impact also highlighted the\nbenefits of criterion-specific modeling, especially in re-ranking, where the\ndetailed reasoning capabilities of cross-encoders are better suited to exploit\ncriterion-specific signals.\nThe proposed Criteria-specific Retrieval via Ensemble of Specialized TR\nmodels (CREST) demonstrates a significant advancement in TR retrieval\napproaches. By training each model within the ensemble to focus on a\nunique criterion, we cultivated specialization in handling specific types of\ninformation. This specialization allowed CREST to effectively combine the\ndiverse strengths of each model, leading to improved overall performance.\nThe findings of this study demonstrate that CREST consistently outper-\nforms single-model approaches across key metrics in both IR and RR stages\nwhen applied in a two-stage workflow. In addition, even when evaluated in\nisolation (i.e., without the IR stage), CREST continued to outperform the\nsingle-modelapproachacrossallmetrics, reinforcingthestrengthofcriterion-\nspecific modeling in constrained input settings. Moreover, CREST improves\nthe calibration of predicted confidence scores, resulting in outputs that are\nnot only more precise but also more reliable, which is an important factor\nwhen transparency and interpretability are critical in decision-making pro-\ncesses. Finally, the pilot user study demonstrates that CREST\u2019s criterion-\nwise scores can improve perceived transparency and that the recommenda-\ntions were often judged useful, credible, and accurate for triage, with some\nresponses also suggesting areas for further refinement.\nIn summary, CREST substantially enhances the accuracy and efficiency\nof TR retrieval systems on our industrial dataset. Beyond improved retrieval\n36\naccuracy, the CREST tool can also support practical decision-making by\nproviding criterion-wise relevance scores for each retrieved TR, making the\nretrieval process interpretable and traceable. This capability enables engi-\nneers to better understand the match rationale, prioritize investigation based\non critical factors, and ultimately accelerate issue resolution. Future research\nshould aim to refine criteria aggregation methods and explore the integra-\ntion of non-TR sources and non-textual information to further elevate TR\nretrieval system performance. These advancements hold the potential to sig-\nnificantly improve the capabilities and effectiveness of TR retrieval systems\nin industrial applications.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L.,\nAlmeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al., 2023. Gpt-\n4 technical report. arXiv preprint arXiv:2303.08774 .\nAggarwal, A., Mittal, M., Battineni, G., 2021. Generative adver-\nsarial network: An overview of theory and applications. Interna-\ntional Journal of Information Management Data Insights 1, 100004.\ndoi:10.1016/j.jjimei.2020.100004.\nAnand, A., Lyu, L., Idahl, M., Wang, Y., Wallat, J., Zhang, Z., 2022. Ex-\nplainable information retrieval: A survey. arXiv preprint arXiv:2211.02405\n.\nBosch, N., Shalmashi, S., Yaghoubi, F., Holm, H., Gaim, F., Payberah, A.H.,\n2022. Fine-tuning bert-based language models for duplicate trouble report\nretrieval, in: 2022 IEEE International Conference on Big Data (Big Data),\npp. 4737\u20134745.\nChicco, D., 2021. Siamese Neural Networks: An Overview. US, New York,\nNY. doi:10.1007/978-1-0716-0826-5_3.\nDevlin, J., Chang, M.W., Lee, K., Toutanova, K., 2019. Bert: Pre-training of\ndeep bidirectional transformers for language understanding, in: Proceed-\nings of the 2019 conference of the North American chapter of the associ-\nation for computational linguistics: human language technologies, volume\n1 (long and short papers), pp. 4171\u20134186.\n37\nGrattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al., 2024. The\nllama 3 herd of models. arXiv preprint arXiv:2407.21783 .\nGrimalt, N.M.I., Shalmashi, S., Yaghoubi, F., Jonsson, L., Payberah, A.H.,\n2022. Berticsson: A recommender system for troubleshooting. SDU@\nAAAI .\nGururangan, S., Lewis, M., Holtzman, A., Smith, N.A., Zettlemoyer, L.,\n2022. Demix layers: Disentangling domains for modular language model-\ning, in: Proceedings of the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics: Human Language\nTechnologies, pp. 5557\u20135576.\nHolm, H., 2021. Bidirectional encoder representations from transformers\n(bert)forquestionansweringinthetelecomdomain. Master\u2019sthesis.KTH,\nSchool of Electrical Engineering and Computer Science (EECS.\nHumeau, S., Shuster, K., Lachaux, M.A., Weston, J., 2020. Poly-encoders:\nArchitectures and pre-training strategies for fast and accurate multi-\nsentence scoring, in: Proceeding of 8th International Conference on Learn-\ning Representations, 2020.\nJiang, A.Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.S.,\nde las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier,\nL., Lavaud, L.R., Lachaux, M.A., Stock, P., Scao, T.L., Lavril, T.,\nWang, T., Lacroix, T., Sayed, W.E., 2023. Mistral 7b. URL:\nhttps://arxiv.org/abs/2310.06825,arXiv:2310.06825.\nJung, E., Choi, J., Rhee, W., 2022. Semi-siamese bi-encoder neural ranking\nmodel using lightweight fine-tuning, in: Proceedings of the ACM Web\nConference 2022, pp. 502\u2013511. doi:10.1145/3485447.3511978.\nKarapantelakis, A., Thakur, M., Nikou, A., Moradi, F., Olrog, C., Gaim, F.,\nHolm, H., Nimara, D.D., Huang, V., 2024. Using large language models to\nunderstand telecom standards, in: 2024 IEEE International Conference on\nMachine Learning for Communication and Networking (ICMLCN), IEEE.\npp. 440\u2013446.\nKhattab, O., Zaharia, M., 2020. Colbert: Efficient and effective passage\nsearch via contextualized late interaction over bert, in: Proceedings of the\n38\n43rd International ACM SIGIR Conference on Research and Development\nin Information Retrieval, 2020, pp. 39\u201348. doi:10.1145/3397271.3401075.\nKirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G.,\nRusu, A.A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A.,\net al., 2017. Overcoming catastrophic forgetting in neural networks. Pro-\nceedings of the national academy of sciences 114, 3521\u20133526.\nLeonhardt, J., Rudra, K., Anand, A., 2023. Extractive explanations for\ninterpretable text ranking. ACM Transactions on Information Systems 41,\n1\u201331.\nLi, M., Gururangan, S., Dettmers, T., Lewis, M., Althoff, T., Smith, N.A.,\nZettlemoyer, L., 2022. Branch-train-merge: Embarrassingly parallel train-\ning of expert language models, in: First Workshop on Interpolation Regu-\nlarizers and Beyond at NeurIPS 2022.\nLiu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C.,\nZhang, C., Ruan, C., et al., 2024. Deepseek-v3 technical report. arXiv\npreprint arXiv:2412.19437 .\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis,\nM., Zettlemoyer, L., Stoyanov, V., 2019. Roberta: A robustly optimized\nbert pretraining approach.arXiv:1907.11692. arXiv preprint.\nLu, W., Jiao, J., Zhang, R., 2020. Twinbert: Distilling knowledge to twin-\nstructuredcompressedbertmodelsforlarge-scaleretrieval,in: Proceedings\nof the 29th ACM International Conference on Information & Knowledge\nManagement, 2020, pp. 2645\u20132652.\nLucchese, C., Minello, G., Nardini, F.M., Orlando, S., Perego, R., Veneri, A.,\n2023. Can embeddings analysis explain large language model ranking?, in:\nProceedings of the 32nd ACM International Conference on Information\nand Knowledge Management, pp. 4150\u20134154.\nMa, X., Wang, L., Yang, N., Wei, F., Lin, J., 2024. Fine-tuning llama\nfor multi-stage text retrieval, in: Proceedings Of The 47th International\nACM SIGIR Conference On Research And Development In Information\nRetrieval, pp. 2421\u20132425. doi:10.1145/3626772.3657951.\n39\nNaeini, M.P., Cooper, G., Hauskrecht, M., 2015. Obtaining well calibrated\nprobabilities using bayesian binning, in: Proceedings of the AAAI confer-\nence on artificial intelligence.\nNimara, D.D., Gebre, F.G., Huang, V., 2024. Entity recognition in telecom-\nmunications using domain-adapted language models, in: 2024 IEEE In-\nternational Conference on Machine Learning for Communication and Net-\nworking (ICMLCN), IEEE. pp. 240\u2013245.\nNogueira, R., Cho, K., 2019. Passage re-ranking with bert.\narXiv:1901.04085. arXiv preprint.\nNogueira, R., Yang, W., Cho, K., Lin, J., 2019. Multi-stage document rank-\ning with bert.arXiv:1910.14424. arXiv preprint.\nPenha, G., Hauff, C., 2021a. On the calibration and uncertainty of neu-\nral learning to rank models for conversational search, in: Merlo, P.,\nTiedemann, J., Tsarfaty, R. (Eds.), Proceedings of the 16th Conference\nof the European Chapter of the Association for Computational Linguis-\ntics: Main Volume, Association for Computational Linguistics, Online.\npp. 160\u2013170. URL:https://aclanthology.org/2021.eacl-main.12/,\ndoi:10.18653/v1/2021.eacl-main.12.\nPenha, G., Hauff, C., 2021b. On the calibration and uncertainty of neural\nlearning to rank models for conversational search, in: Proceedings of the\n16th Conference of the European Chapter of the Association for Compu-\ntational Linguistics: Main Volume, pp. 160\u2013170.\nPuthenputhussery, A., Kang, C., Magnani, A., Zhang, T., Shang, H., Yadav,\nN., Chandran, P., Madhani, B., Fu, Y.T., Wang, H., et al., 2025. Large\nscale deployment of bert based cross encoder model for re-ranking in wal-\nmart search engine, in: Proceedings of the 48th International ACM SIGIR\nConference on Research and Development in Information Retrieval, pp.\n4365\u20134369.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,\nY., Li, W., Liu, P.J., 2020. Exploring the limits of transfer learning with\na unified text-to-text transformer. Journal of Machine Learning Research\n21, 1\u201367.\n40\nReimers, N., Gurevych, I., 2019. Sentence-bert: Sentence embeddings using\nsiamese bert-networks, in: Proceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), pp.\n3982\u20133992. doi:10.18653/v1/D19-1410.\nRen, R., Qu, Y., Liu, J., Zhao, W.X., She, Q., Wu, H., Wang, H., Wen,\nJ.R., 2021. Rocketqav2: A joint training method for dense passage re-\ntrieval and passage re-ranking, in: Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Processing, pp. 2825\u20132835.\ndoi:10.18653/v1/2021.emnlp-main.224.\nRobertson, S., Zaragoza, H., 2009. The probabilistic relevance frame-\nwork: Bm25 and beyond. Found. Trends Inf. Retr. 3, 333\u2013389.\ndoi:10.1561/1500000019.\nRuder,S.,Peters,M.E.,Swayamdipta,S.,Wolf,T.,2019. Transferlearningin\nnatural language processing, in: Proceedings of the 2019 Conference of the\nNorthAmericanChapteroftheAssociationforComputationalLinguistics:\nTutorials, pp. 15\u201318. doi:10.18653/v1/N19-5004.\nRudin, C., 2019. Stop explaining black box machine learning models for high\nstakes decisions and use interpretable models instead. Nature machine\nintelligence 1, 206\u2013215.\nSong, C., He, H., Yu, H., Fang, P., Cui, L., Lan, Z., 2023. Uni-encoder: Afast\nand accurate response selection paradigm for generation-based dialogue\nsystems. Findings of the Association for Computational Linguistics: ACL\n2023, 6231\u20136244. doi:10.18653/v1/2023.findings-acl.388.\nTam, W., Liu, X., Ji, K., Xue, L., Liu, J., Li, T., Dong, Y., Tang, J.,\n2023a. Parameter-efficientprompttuningmakesgeneralizedandcalibrated\nneural text retrievers, in: Findings of the Association for Computational\nLinguistics: EMNLP 2023, pp. 13117\u201313130.\nTam, W.L., Liu, X., Ji, K., Xue, L., Zhang, X., Dong, Y., Liu, J.,\nHu, M., Tang, J., 2023b. Parameter-efficient prompt tuning makes\ngeneralized and calibrated neural text retrievers. Findings of the As-\nsociation for Computational Linguistics: EMNLP 2023, 13117\u201313130.\ndoi:10.18653/v1/2023.findings-emnlp.87.\n41\nWallat, J., Beringer, F., Anand, A., Anand, A., 2023. Probing bert for rank-\ning abilities, in: European Conference on Information Retrieval, Springer.\npp. 255\u2013273.\nWang, J.A., Wang, K., Wang, X., Naidu, P., Bergen, L., Paturi, R., 2023.\nScientific document retrieval using multi-level aspect-based queries. Ad-\nvances in Neural Information Processing Systems 36, 38404\u201338419.\nWang, Y., Chen, X., Verberne, S., 2024. Quids: Query intent generation via\ndual space modeling. arXiv preprint arXiv:2410.12400 .\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D.,\nHuang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang,\nJ., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M.,\nXue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Xia, T., Ren, X.,\nRen, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z.,\nQiu, Z., 2024. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115\n.\nYu, P., Cohen, D., Lamba, H., Tetreault, J., Jaimes, A., 2024. Explain then\nrank: Scale calibration of neural rankers using natural language explana-\ntions from large language models. arXiv e-prints , arXiv\u20132402.\nYu, W., Sun, Z., Xu, J., Dong, Z., Chen, X., Xu, H., Wen, J.R., 2022. Ex-\nplainable legal case matching via inverse optimal transport-based rationale\nextraction, in: Proceedings of the 45th international ACM SIGIR confer-\nence on research and development in information retrieval, pp. 657\u2013668.\nZhang, H., Gong, Y., Shen, Y., Lv, J., Duan, N., Chen, W., 2021a. Adversar-\nial retriever-ranker for dense text retrieval.arXiv:vol. abs/2110.03611.\nZhang, R., Guo, J., Fan, Y., Lan, Y., Cheng, X., 2020. Query understand-\ning via intent description generation, in: Proceedings of the 29th ACM\nInternational Conference on Information & Knowledge Management, pp.\n1823\u20131832.\nZhang, Y., Long, D., Xu, G., Xie, P., 2022. Hlatr: enhance multi-stage text\nretrieval with hybrid list aware transformer reranking.arXiv:2205.10569.\narXiv preprint.\n42\nZhang, Z., Han, X., Liu, Z., Jiang, X., Sun, M., Liu, Q., 2019. Ernie: En-\nhanced language representation with informative entities, in: Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguis-\ntics, Jul. 2019, pp. 1441\u20131451. doi:10.18653/v1/P19-1139.\nZhang, Z., Rudra, K., Anand, A., 2021b. Explain and predict, and then\npredict again, in: Proceedings of the 14th ACM international conference\non web search and data mining, pp. 418\u2013426.\n43\n",
    "title": "CREST: Improving Interpretability and Effectiveness of Troubleshooting at Ericsson through Criterion-Specific Trouble Report Retrieval",
    "arxiv_id": "2511.17417v1",
    "category": "recent",
    "authors": [
      "Soroush Javdan",
      "Pragash Krishnamoorthy",
      "Olga Baysal"
    ],
    "published": "2025-11-21",
    "num_pages": 43,
    "num_chars": 95486
  },
  {
    "text": "SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding\nNikolay Nikolov Giuliano Albanese Sombit Dey Aleksandar Yanev\nLuc Van Gool Jan-Nico Zaech Danda Pani Paudel\n{nikolay.nikolov, giuliano.albanese, sombit.dey, aleksandar.yanev\nluc.vangool, jan-nico.zaech, danda.paudel}@insait.ai\nINSAIT, Sofia University \u201dSt. Kliment Ohridski\u201d\nFranka WidowX SIMPLER (W idowX)\nRobot data (x1M frames)Avg. T ask Progress\n(b) (c) Zero-shot Franka (DROID)?(a)\u2705\n\u274c20x less data\nEvaluation results and sample environments\nFigure 1. (a) Most VLAs fail to show zero-shot performance on the challenging Franka (DROID) setup in unseen environments, without\ntask or environment-specific fine-tuning. (b) SPEAR-1 operates in this challenging setup, outperforms\u03c00-FAST [33] and matches\u03c0 0.5[6]\non Franka (DROID) embodiment zero-shot in unseen environments while using 20\u00d7less robot demonstrations data. It also shows strong\nperformance on WidowX (Bridge) (c) SPEAR-1 evaluation results on different embodiments and in different environments.\nAbstract\nRobotic Foundation Models (RFMs) hold great promise\nas generalist, end-to-end systems for robot control. Yet\ntheir ability to generalize across new environments, tasks,\nand embodiments remains limited. We argue that a major\nbottleneck lies in their foundations: most RFMs are built\nby fine-tuning internet-pretrained Vision-Language Models\n(VLMs). However, these VLMs are trained on 2D image-\nlanguage tasks and lack the 3D spatial reasoning inher-\nently required for embodied control in the 3D world. Bridg-\ning this gap directly with large-scale robotic data is costly\nand difficult to scale. Instead, we propose to enrich easy-\nto-collect non-robotic image data with 3D annotations and\nenhance a pretrained VLM with 3D understanding capa-\nbilities. Following this strategy, we train SPEAR-VLM, a\n3D-aware VLM that infers object coordinates in 3D space\nfrom a single 2D image. Building on SPEAR-VLM, we in-\ntroduce our main contribution,SPEAR-1: a robotic foun-dation model that integrates grounded 3D perception with\nlanguage-instructed embodied control. Trained on\u223c45M\nframes from 24 Open X-Embodiment datasets, SPEAR-1\noutperforms or matches state-of-the-art models such as\u03c0 0-\nFAST and\u03c0 0.5, while it uses 20\u00d7fewer robot demonstra-\ntions. This carefully-engineered training strategy unlocks\nnew VLM capabilities and as a consequence boosts the reli-\nability of embodied control beyond what is achievable with\nonly robotic data. We make our model weights and 3D-\nannotated datasets publicly available.\n1. Introduction\nVision-Language-Action (VLA) models have emerged as a\npromising paradigm for building generalist, end-to-end sys-\ntems for robot control. Their success relies on two fac-\ntors: (1) the strong visual-linguistic understanding inher-\nited from internet-scale pretraining of the underlying Vi-\nsion Language Model (VLM), which provides broad \u201ccom-\n1arXiv:2511.17411v1  [cs.RO]  21 Nov 2025\nmon sense\u201d knowledge, and (2) training on large, diverse\ndatasets of robot demonstrations.\nDespite this progress, the landscape of generalist VLA\npolicies remains fragmented in terms of generalization \u2013\nacross embodiments, environments, and tasks. This be-\ncomes especially prominent in zero-shot performance in\nunseenreal-world environments with variations in cam-\nera positions and out-of-distribution backgrounds, such as\nthe typical deployment scenarios of the Franka (DROID)\nsetup [2]. In contrast, as shown in Fig. 1 (a), most existing\nVLAs (e.g. OpenVLA [19], CogAct [21], SpatialVLA [35],\nMotoVLA [40]) achieve high zero-shot performance1in\n\u201ctoy\u201d environments with seen camera positions, but struggle\nwith zero-shot performance inunseenchallenging Franka\nscenarios and depend on task- or environment-specific fine-\ntuning. Recent efforts such as\u03c0 0and\u03c0 0.5[6] push to-\nward broader generalization, yet at the cost of closed large-\nscale robotic data. We introduceSPEAR-1, which ad-\nvances these desired generalization capabilities while be-\ning substantially more data-efficient. Quantitatively (see\nFig. 1 (b)),SPEAR-1outperforms\u03c0 0-FAST [33] and\nmatches\u03c0 0.5on multiple robot embodiments using20\u00d7\nfewer demonstrations, which is especially important given\nthe high cost and logistical difficulty of collecting real-\nworld robotic data.\nWe achieve this efficiency by introducing explicit 3D\nawareness into the vision-language backbone before any\nrobot training. The model incorporates a pretrained depth\nencoder and is optimized on 3D-aware vision-language\ntasks such as distance estimation and 3D bounding box pre-\ndiction, embedding control-relevant spatial reasoning di-\nrectly into its representations. Achieving such integration\nis non-trivial: aligning 3D geometric cues with high-level\nlinguistic and visual features requires detailed multimodal\ndataset annotations and precise cross-modal calibration, as\nnaive fusion often degrades both semantic understanding\nand spatial accuracy. In contrast, existing VLAs rely on 2D\nVLMs that excel at semantic perception, but lack geometric\nunderstanding, forcing them to learn 3D structure implicitly\nfrom large-scale robot demonstrations. This dependence on\ncostly and embodiment-specific data limits scalability and\ngeneralization across environments, underscoring the diffi-\nculty and significance of SPEAR-1\u2019s design.\nIn our progression from spatial understanding to em-\nbodied control, we introduce a staged training pipeline,\nas shown in Fig. 2. InStage 1, we develop a 3D-aware\nvision-language model,SPEAR-VLM, which extends a\npretrained VLM by learning spatial reasoning from non-\nrobotic 2D images annotated with 3D cues. This stage es-\ntablishes a perceptual backbone that encodes geometric re-\nlations while preserving the rich semantic priors of large-\n1For more details on the definition of \u201dzero-shot performance in unseen\nenvironments\u201d see Appendix A.4scale pretraining. InStage 2, we introduce anaction ex-\npertthat maps the grounded visual-language representa-\ntions to motor actions. This stage demands well-tuned\nvision-language-action modeling choices and a carefully-\ncrafted multi-embodiment data processing strategy to learn\nprecise low-level robot controls. Together, these stages\nbridge the gap between internet-scale 2D perception and\nembodied 3D interaction, progressively transforming pas-\nsive spatial understanding into actionable behavior.\nUnlike previous works that address the challenge of 3D\nknowledge for robot control [35, 40], SPEAR-1 demon-\nstrates improvement on a foundation model level, with an\nend-to-end policy across multiple different environments\nand robots. It is capable of achieving state-of-the-art robot\ncontrol on multiple robot embodiments without requiring\ntarget evaluation environment fine-tuning. Furthermore,\nSPEAR-1 demonstrates how significant amounts of robot\ndemonstration data can be \u201creplaced\u201d by non-robotic 3D-\nannotated image data. In summary, our work makes the\nfollowing contributions:\n\u2022SPEAR-VLM: a VLM withcontrol-inspired 3D ca-\npabilities(e.g. localizing objects in 3D), trained on\ncarefully-crafted VQA tasks and enriched 2D-image non-\nrobotic data. Importantly, SPEAR-VLM directly boosts\ndownstream VLA performance.\n\u2022SPEAR-1: an open-weightrobotics foundation model\nwith 3D understanding, which significantly outperforms\nor matches the strongest state-of-the-art baselines, trained\nwith 20\u00d7more robot demonstration data\n\u2022Extensive experimental validation: we demonstrate\nstrong generalization across diverse settings with a sub-\nstantial reduction in reliance on hard-to-collect robotic\ndata. Notably, using only200k non-robotic 2D images,\nSPEAR-1 surpasses models trained with more than900M\nadditional frames of robotic demonstrations.\n2. Related Work\nSpatial Understanding for VLMs. The majority of exist-\ning VLMs trained on large-scale datasets have been limited\nto flat 2D image understanding [4, 17, 26, 41, 43, 46]. Our\nwork extends the PaliGemma VLM [4] by integrating the\nMoGe monocular depth estimator [47] as a supplementary\nvision backbone and by training on manipulation-relevant\n3D tasks to enhance the VLM\u2019s 3D understanding. Pre-\nviously, Chen et al. [8] used a similar data annotation ap-\nproach for training a 3D-aware VLM, but they do not inte-\ngrate a pretrained depth estimator and neither their model\nnor their dataset is publicly accessible. Additionally, unlike\nSpatialVLM [8] or RoboSpatial [39], trained on high-level\nspatial relationships, our SPEAR-VLM focuses on explicit\n3D coordinate prediction, a pretraining task much closer\nto embodied control. SpatialBot [7] also previously pro-\nposed a spatially-aware VLM targeting robot control, but\n2\nSPEAR-1SPEAR-VLM Action Expert\nThe 3D bouding box of the carrot is\n[0.3, 0.7, 0.24], [-0.05, 0.1, 0.24], ...\n[continuous actions]\u00a0\n0.05, 0.12, -0.34, 0.0, 0.07, 0.99, 0, ...\nRobotics Knowledge Robotics Knowledge Robotics Knowledge\nData Cost Data Cost Data Cost\nThe 2D bounding box\u00a0\nof the bear is [127, 64, 340, 123]What are the coordinates of the\n3D\u00a0bounding box of the carrot ?What is the 2D bounding box\u00a0\nbounding box of the bear?Pick up the orangeStage 0\u00a0 (PaliGemma)\nWeb dataStage 1 (SPEAR-VLM)\nNon-robotic 2D image dataStage 2 (SPEAR-1)\nRobot demonstrations dataFigure 2.SPEAR-1 stages of training. Stage 0: General VLM\npretraining on web scale data,e.g. PaliGemma.Stage 1: Integrate\na mono depth vision encoder to buildSPEAR-VLMand train it\non embodied-inspired VQA tasks,e.g. 3D bounding box or object-\nto-object distance estimation. We use 2D images from non-robotic\ndata, enriched with 3D annotations.Stage 2: Add anaction expert\nto trainSPEAR-1on robot demonstration data,e.g. OpenX [32].\nEach stage boosts the model\u2019s robotics-relevant knowledge and ca-\npabilities, but the abundance and diversity of data decreases.\ntheir method involves a multi-step VLM inference process\nand was never shown to integrate into a VLA for generalist\nrobotic control.\nVision-Language-Action Models. Recently, multiple\nworks have developed generalist robot policies [5, 6, 10,\n19, 32, 33, 53] trained on multiple robot embodiments.\nSPEAR-1 builds on top of the\u03c0 0architecture, which com-\nbines a pretrained PaliGemma VLM and an action ex-\npert module, but we initialize the underlying VLM from\nour SPEAR-VLM to integrate pretrained 3D understand-\ning. Previously, SpatialVLA [35] proposed integrating a\nmonocular depth encoder [48] in the VLA, but without any\nVLM alignment or pretraining and therefore learning 3D\ncapabilities entirely from hard-to-collect robotic data. Mol-\nmoAct [20] recently proposed a spatially-aware VLA, but\nthe approach involves \u2019reasoning\u2019 at inference time, render-\ning the method too slow for real-time control. Most closely\nrelated, Gemini Robotics 1.0 [44] follows a similar 3D pre-\ntraining method to fine-tune the significantly larger Gemini\n2.0 [34] and distill into a smaller VLA with reasoning capa-\nbilities. With most of the method\u2019s details undisclosed, our\nwork still differs in several important aspects: (1) we inves-\ntigate the benefits of 3D pretraining in isolation, (2) train\nmuch smaller open-access model on limited, less diverse\nopen data from OpenX [32], and, most importantly, (3) we\ndemonstrate the ability to reduce the need for robotic data\nwith non-robotic 2D images.3. Method\nIn this section, we describe SPEAR-1 and its training recipe\nin detail. In section 3.1 we describe the architecture, data\ngeneration pipeline, and training procedure of our 3D-aware\nSPEAR-VLM. This stage aims to enhance the 3D spatial\nunderstanding capabilities of an off-the-shelf VLM through\nfine-tuning on 3D spatial perception tasks. We then pro-\nceed, in section 3.2 to detail the architecture and training\nprocedure of SPEAR-1, which comprises a pre-training and\npost-training stage.\n3.1. SPEAR-VLM\nOur approach considers the architecture of recent robotics\nfoundational models that are based on VLMs, pretrained on\nlarge corpora of internet-scale text-image data The architec-\nture of those models usually consists of a vision encoder, a\nvision-to-text-embedding feature projector, and a LLM. The\nmajority of the tasks on which VLMs are usually trained are\nlimited to 2D [4, 17, 25]. To extend the capabilities of a pre-\ntrained VLM to 3D understanding, we propose (1) extend-\ning the model architecture by adding a monocular depth en-\ncoder and (2) training the VLM on VQA tasks that require\nexplicit 3D reasoning.\nSPEAR-VLM Architecture. Our model builds on\nPaliGemma [4] as backbone, but the same method can be\nused with any late-fusion VLM [1, 11, 27]. PaliGemma\nconsists of three main components: (1) a SigLIPvisual en-\ncoder[50], (2) a linearprojectorthat maps the visual to-\nkens predicted by the visual encoder to the language model\ninput space and (3) a Gemmalanguage model[42]. To en-\nable the model to perceive accurate depth, we integrate the\nMoGe [47] depth encoder as an additional vision encoder.\nWe choose MoGe due to its affine-invariant modeling ap-\nproach, capable of fitting cameras with different intrinsics.\nOur intuition is that affine-invariant depth should general-\nize better across environments thus being better suited for\nlearning generalist robot control. Similar to the MoGe de-\ncoder inputs, we concatenate the intermediate features from\nthe last 4 layers of the MoGe ViT encoder along the feature\ndimension and project them to the LLM embedding space\nvia a randomly-initialized linear projector. The visual input\nto the LLM consists of the averaged outputs of the SigLIP\nand MoGe projectors. To encode 3D information into text\nwe extend the PaliGemma tokenizer withN= 10243D\ntokens (see Fig. 3 and Appendix A.1.3).\n3D pretraining tasks. Given the above architecture, we\npropose a pre-training scheme to enable the model to lever-\nage the depth information in MoGe\u2019s encoder features and\nacquire 3D spatial understanding capabilities. To embed as\nmuch control-relevant 3D knowledge in the SPEAR-VLM\nas possible, we design VQA tasks inspired by the embodied\ntasks a VLA needs to learn,e.g.\u2019Output the vertices of the\n3D bounding box of object X\u2019or\u2019Output thexyzcompo-\n3\nSPEAR-VLM data mixture\nData AnnotationsSPEAR-VLM Architecture\nImagesText\nTokens\nHand towel: [-0.6, 0.7, 1.2], [-0.3, ...];\nBowl: [0.4, 0.5, 0.9], [0.3, ...];\nDistances: 0.23, 0.56, ...\nBlue jar 3D bounding box pixel coordinates:\n[120, 60],\u00a0[130, 67],\u00a0[125, 62],\u00a0...3D Object\u00a0\n\u00a0Detection\nThe frying pan's bounding box has\nvertices [0.5, 0.8, 0.24], [-0.03, 0.2, 0.7], ...What are the coordinates of the 3D bounding\nbox\u00a0of\u00a0the frying pan?\nObject-\nObject\u00a0\n\u00a0Relations\nCamera\nto\u00a0ObjectObject\nKeypointsPrompt Category\nOutput the 3D bounding boxes of the hand\ntowel and the bowl, and the distances\nbetween their verticesSpatial Labels\nOutput the pixel locations of the 3D bounding\nbox vertices of\u00a0the blue jar?Output the coordinates of the closest, furthest,\nand center points of the egg in 3D\nClosest point: [0.6, 0.8, 2.3]; \nFurthest point: [0.64, 0.82, 2.5]; Center: ...Prompt\u00a0\nGemma 2BConv\nDecoder\nMoGe\nEncoder\nAffine Invariant\nPoint Map P\nViT\u00a0\u00a0SigLIP\nSemantic Labels Segmentation Masks\n 3D point cloud\nSpatial Labels\nOutput\nText Tokens\nPixel\u00a0Tokens\n3D\u00a0Tokenspaper roll\nchopping board\nbaking paper box\nstove\nvinegar bottle\nblack peeler\n...Figure 3.SPEAR-VLM overview.Left: Training data mixture, auto computed spatial annotations and example question-answer pairs\nfrom each category. Right: High-level architecture with fusion between SigLIP and MoGe encoders and PaliGemma embeddings expansion\nwith 3D tokens. This design equips SPEAR-VLM with explicit 3D understanding that serves as a strong foundation for SPEAR-VLA.\nnents of the distance between object X and object Y\u2019. These\nVLM pre-training tasks ensure learning semantic 3D local-\nization, object-to-object spatial relations, and 3D coordinate\nsystem geometry (Fig. 3). For a full list of question-answer\npairs, see Appendix A.1.1.\n3D Vision-Question-Answering Data. As few open\ndatasets contain the annotations needed for the proposed\ntraining scheme, we devise the following semi-automatic\nannotation pipeline to enrich existing datasets with the nec-\nessary annotations:object-level segmentation masks, se-\nmantic labels and projected 3D point cloud. Importantly,\nour pipeline requires only 2D images as input and off-the-\nshelf vision foundation models:\n1. Use Gemini [9] to detect 2D bounding boxes and seman-\ntic labels for the objects in the image.\n2. Prompt SAM2 [36, 37] with the detected bounding\nboxes to produce instance-level segmentation masks.\n3. Obtain 3D point cloud annotations for the entire image\nvia MoGe direct point cloud predictions [47].\nTo construct a training example, we randomly sample\na templated text prompt and a set of objects from the im-\nage. We then filter the annotated MoGe 3D point cloud with\nthe object mask to obtain the object 3D point cloud. Based\non the segmented point cloud, we compute the oriented 3D\nbounding box and construct the question-answer pair.\nWe focus on indoor environments and annotate the\n\u201ccooking\u201d and \u201dbike repair\u201d parts of EgoExo4D [13] that\nalready have segmentation masks, resulting in 200k images.\nFor visual diversity, we further annotate 30k frames of the\nBridge-V2 [45] robot demonstration dataset, downsampled\nto 10% in the VLM training data mixture.\nTraining process. Similar to LLaVa [25], we train\nSPEAR-VLM in two stages. In the first stage, we initialize\nfrom PaliGemma and MoGe weights, with the MoGe pro-jector and the LLM 3D token embeddings initialized ran-\ndomly. We train only the randomly initialized weights and\nSigLIP projector, keeping everything else frozen. In the\nsecond and longer stage, we keep only SigLIP and MoGe\nencoders frozen and we scale the next-token-prediction loss\nfor 3D tokens by a factor\u03bb= 2.\n3.2. SPEAR-1\nSPEAR-1 follows a similar overall architecture as\u03c0 0[5],\nhowever, we build on SPEAR-VLM, use a rotation formu-\nlation in flow matching on theS3manifold of unit quater-\nnions, and several data & engineering improvements. De-\nsign decisions were ablated on small-scale experiments on\nBridgeData V2 [45] due to the cost of training on the entire\nOpenX mixture. We summarize these key decisions and\nlearning in the following.\nPreliminaries.Formally, we aim to learn a function\u03c0(\u00b7)\nmapping an observationo tto a sequence of robot actions\nAt= [a t,at+1, . . .a t+H\u22121 ]over a horizonH. The obser-\nvation is defined aso t= [I1\nt, . . . ,In\nt,pt,lt], whereIi\ntis the\ni-th image observation from an uncalibrated camera,p tis\na vector containing the robot state comprising of the end-\neffector pose and gripper state,l tis a vector of language\ntokens representing the language instruction.\nArchitecture.We follow the broadly accepted architecture\nintroduced in\u03c0 0: a Flow Matching action expert that pro-\ncesses proprioception observations and predicts the robot\nactions by attending to the VLM\u2019s intermediate key-value\npairs. For full details, see Appendix A.2.2 and\u03c0 0[5].\nFlow Matching Formulation.The action sequence pre-\ndiction is supervised via conditional flow matching [23, 24,\n28]. The model takes as input the observationo t, the flow-\nmatching step\u03c4\u2208[0,1]and a sequence of noisy actions\nA\u03c4\nt= [a\u03c4\nt, . . . ,a\u03c4\nt+H\u22121 ]and predicts a denoising vector\n4\nv\u03b8(A\u03c4\nt,ot). We denote the decomposed action of transla-\ntion, rotation and gripper components asa t= [x t,qt,gt].\nWe use the square brackets operator[\u00b7]on the predicted de-\nnoising vectorv \u03b8and the denoising vector fielduto denote\na specific component,e.g.u[x t]corresponds to the transla-\ntion component of the denoising vector field.\nWe follow a flow matching formulation in linear space\nfor translation and on theS3manifold of unit quaternions\nfor rotation. For simplicity, we omit the gripper component\nas it follows the same linear formulation as translation.\nDuring training, we sample a random timestep\u03c4\u223c\nB(\u03b1, \u03b2)and random noisex \u03f5\u223c N(0,I),q \u03f5\u223c U(S3).\n\u201cNoisy actions\u201d are computed by linear interpolation for\ntranslationx\u03c4\nt=\u03c4x t+ (1\u2212\u03c4)x \u03f5and spherical linear in-\nterpolation on theS3manifold for quaternion rotation\nq\u03c4\nt=sin\u0000\n(1\u2212\u03c4)\u03b8\u0001\nsin\u03b8q\u03f5+sin(\u03c4\u03b8)\nsin\u03b8qt,(1)\nwith\u03b8= cos\u22121(q\u03f5\u00b7qt). The \u201cnoisy action sequence\u201dA\u03c4\nt\nis passed as input to the model and trained to output the de-\nnoising vector fieldu(A\u03c4\nt|At) =dA\u03c4\nt\nd\u03c4. Training is super-\nvised with the conditional flow-matching loss, equivalent to\nthe MSE loss for translation\nLR3(\u03b8) =\f\f\f\fv\u03b8(A\u03c4\nt,ot)[Xt]\u2212u(A\u03c4\nt|At)[Xt]\f\f\f\f2.(2)\nFor rotations, we combine a cosine loss between the veloc-\nity predictionsv \u03b8(A\u03c4\nt,ot)[q]\u2208R4and the denoising vec-\ntor fieldu(A\u03c4\nt|At)[q]. and a geodesic loss [12, 14] between\na target quaternionq\u03c4+\u03b4\nt\u2208S3computed from Eq. (1) at\ntimet+\u03b4, and a quaternion predictionq\u03c4+\u03b4\n\u03b8,t=q\u03c4\nt\u2297q\u03b4\n\u03b8,t\u2208\nS3, withq\u03b4\n\u03b8,t\u2208S4computed by integratingv \u03b8[qt]\u2208R4\nover a small integration step\u03b4\u223c U(0.01,1\u2212\u03c4). The total\nloss is the sum of the translation and rotation loss\nL(\u03b8) =E p(At|ot),q(A\u03c4\nt|At)[LR3(\u03b8) +L S3(\u03b8)].(3)\nDuring inference, we generate actions by integrating the\nlearned vector field from\u03c4= 0to\u03c4= 1, starting with\nrandom noisex 0\u223c N(0,I),q 0\u223c U(S3)and using Euler\nintegration in linear space for translations\nx\u03c4+\u03b4\nt=x\u03c4\nt+\u03b4vx\n\u03b8(A\u03c4\nt,ot),(4)\nand on theS3manifold for rotations\nq\u03c4+\u03b4\nt=q\u03c4\nt\u2297q\u03b4\nt(vq\n\u03b8(A\u03c4\nt,ot)).(5)\nSee Appendix A.2.3 for more details on flow matching.\nImage Resolution.We select a resolution of280\u00d7210for\nthe main external camera and112\u00d7112for the wrist cam-\nera, and resize images either by a central crop or padding.\nImportantly, unlike prior work [19], we do not distort the\naspect ratio of the images by naive resizing as this alsodistorts camera intrinsics and negatively affects depth and\npoint cloud estimates. As wrist cameras contain less infor-\nmation than the external camera, we use a lower resolution,\nwithout losing important details, and reduce training and in-\nference compute.\nFine-tuning vision encoders.As previously observed by\nReVLA [10], robotics training can degrade the represen-\ntations of the pre-trained vision encoders. We experiment\nwith various configurations of vision encoder training and\nfind the optimal setting to keep both SigLIP and MoGe\nvision encoders trainable during VLM training, but freeze\nMoGe in the VLA training stage.\nControl frequency & Data normalization.We use an\naction chunk of sizeH= 5and frequency of 5Hz. For\ndatasets not providing observations at 5Hz we resample the\naction targets via linear interpolation. We design data nor-\nmalization to encourage learning motion across datasets, in-\nstead of \u201cmemorizing\u201d each dataset separately. For target\ncontrol normalization, we use global quantile normalization\nwith statistics computed across the entire training mixture.\nRotations.We investigate various rotation representations\nincluding Euler angles, rotation matrices and unit quater-\nnions. This is run in combination of using different rotation\nlosses, including MSE or cosine for velocity predictions\nand geodesic and/or chordal loss [14] for integrated rotation\npredictions, as well as end-effector or robot base reference\nframes. We use Gram-Schmidt orthonormalization [12] to\nensure valid rotation matrix predictions, but we find half-\nspace unit quaternions to produce better results overall. We\nalso find our proposed formulation on the manifold of unit\nquaternionsS3\u2192S3to be more stable and effective than\nlinear flow matchingR4\u2192S3.\nEvaluation and Checkpointing.We ablate all design\nchoices by evaluating on the SIMPLER WidowX environ-\nments [22]. We set the same seed and enable deterministic\nCUDA operations for all VLA ablations to reduce training\nvariance. We further resort to exponential moving average\n(EMA) checkpointing, which significantly stabilizes final\ncheckpoint performance. For further details and ablations,\nsee Appendix A.2.4.\n4. Experimental evaluation\nWe evaluate the performance of SPEAR-1 as a general-\nist policy for robot manipulation and compare it to open-\nweights state-of-the-art VLA models. Our experiments aim\nto answer the following research questions:\n1. Does 3D VLM pretraining improve the downstream\nVLA performance on robot control tasks?\n2. How well does SPEAR-1 compare against state-of-the-\nart VLA models?\nTo answer these questions, we evaluate SPEAR-1 on\na variety of manipulation tasks in simulation and multiple\nreal-world environments on several robot embodiments.\n5\nAverage Chess piece on board Eggplant in pot Lobster in pan Pink cup on blue plate Stack cups020406080100T ask progress (%)\n10621 2168\n5883\n54628379758883\n6088\nSpatialVLA\nOpenVLA\nSPEAR-1 (ours)Figure 4.Real world evaluation on WidowX.SPEAR-1 is able to achieve 10% higher average task progress across all tasks than\nOpenVLA, a strong baseline in this setting. Bottom images correspond to the real-world tasks, whose performances are reported above.\nAverage Apple in drawer and close Carrot on plate Carrot on plate \n(w/ elevation)Cover the pot Marker in cup020406080100T ask progress (%)14\n715 173064\n506288\n635873 75\n6783\n7267\n0-FAST\n0.5\nSPEAR-1 (ours)\nFigure 5.Real world evaluation on Franka.We find that without any fine-tuning on the target environment, SPEAR-1 noticeably\noutperforms\u03c0 0-FAST, and matches\u03c0 0.5, even though both baselines are trained on 20\u00d7more robotic data from significantly more diverse\nenvironments. The bottom row shows challenging, varied Franka environments where SPEAR-1 maintains strong zero-shot performance.\n4.1. Implementation details\nVLM training.We train SPEAR-VLM with a batch size\nof 512 for 2k steps during the first stage and 10k steps for\nthe second, for a total of 18hrs on 16 Nvidia H200 GPUs.\nVLA pre-training.For VLA training, we start from\nSPEAR-VLM and randomly initialized action expert. We\nprovide two camera views as inputs to the model: exter-\nnal, with resolution 280x210, and wrist, with resolution\n112x112. When the wrist camera is not available, we feed\na black image. We train on 32 H200 GPUs with batch size\n2048 for 300k steps (\u223c6 days) on a data mixture compris-\ning 24 datasets (see Appendix A.2.1) from the Open X-\nEmbodiment (OXE) collection [32].\nVLA post-training.For WidowX real-world and SIM-\nPLER simulation and Franka real-world experiments, weadditionally fine-tune our OXE pre-trained SPEAR-1 for\n50k steps on the Bridge V2 [45] and DROID [18] datasets\nrespectively. We refer to these versions asSPEAR-1\n(Bridge)andSPEAR-1 (DROID)respectively.\n4.2. 3D ablations: SPEAR-VLM vs PaliGemma\nWe first evaluate whether 3D VLM pretraining improves\nVLA performance on downstream tasks and what design\nchoices matter. Due to the cost of training on the entire\nOXE mixture (Tab. 4), we train only on specific subsets.\nSIMPLER WidowX experiments. We perform an ablation\nstudy by training on a subset of the Bridge V2 [45] dataset,\ncontaining demonstrations only from a single kitchen sink\nenvironment, and evaluate the models in the SIMPLER [22]\nWidowX environments (see Appendix A.2.5 for details).\nThis induces a distribution shift, which allows to demon-\n6\nExperimentVLM\nArchitecture 3D tasksVLM training\nSigLIP MoGeVLA training\nSigLIP MoGePut Carrot\non PlatePut Eggplant in\nYellow BasketPut Spoon\non TowelStack Green Block\non Yellow BlockAvg. Success\nRate\n1. no 3D PaliGemma None train \u2013 train \u2013 25% 0% 54% 4.1% 20.8%\n2. no OBJ SPEAR-VLM points frozen frozen train frozen 37.5% 0% 45.8% 4.1% 20.8%\n3. no VLM-T SPEAR-VLM objects frozen frozen train frozen 41.7% 0% 70.8% 0% 29.1%\n4. no VLA-MF SPEAR-VLM objects frozen frozen traintrain 29.1% 0% 41.7% 4.1% 18.8%\n5. SPEAR-VLM SPEAR-VLM objects train train train frozen 50% 0% 79.1% 12.5% 35.4%\nTable 1.SPEAR-VLM 3D ablations. Ablations on a single environment subset of Bridge V2 [45] to show the impact of 3D VLM\npretraining. Without object-level 3D tasks (OBJ), 3D VLM pretraining does not show improvement over PaliGemma (1 vs. 2). Training\nMoGe during VLA training (VLA-MF) significantly degrades performance (3 vs. 4). Our study shows that the best training configuration\nis obtained when both SigLIP and MoGe are trained during VLM pretraining (5), followed by the frozen MoGe during VLA training.\nMethodCarrot on\nPlate (Dist)Carrot on\nPlate (Elev.)Marker in\nCup (Dist)Avg. Task\nProgress\n\u03c00-PaliGemma (DROID) 0% 32%67% 34%\n\u03c00- SPEAR-VLM (DROID) 42% 52%43% 46%\nTable 2.SPEAR-VLM vs. PaliGemma for the downstream\nVLA tasks.The experiments were conducted on the Franka\n(DROID) platform and the models were from trained from scratch\non DROID. SPEAR-VLM achieves noticeable improvements.\n\u201dCarrot on Plate\u201d is not a part of DROID. This indicates SPEAR-\nVLM leads to better generalization.\nstrate the benefits of 3D pretraining when evaluating in un-\nseen environments. In contrast, training on entire Bridge\nV2 leads to nearly the same performance for all models due\nto the close match between training and evaluations.\nResults are reported in Tab. 1. First, we note that sim-\nply using SPEAR-VLM architecture and training without\nobject-level prompts, but only 3D coordinates of random\npixels (row 2), does not lead to any meaningful change\nin VLA performance over the baseline\u03c0 0model based on\nPaliGemma (row 1). However, training SPEAR-VLM on\nall 3D object-level tasks (Fig. 3), we observe a significant\nimprovement in performance (row 3 and 5 vs. row 1). We\nalso observe the importance of training SigLIP and MoGe\nencoders both during VLM and VLA training (row 3-5),\nwith the best performance achieved when both are fine-\ntuned during VLM training and frozen MoGe during VLA\ntraining (row 5). We hypothesize this is because SigLIP has\nbeen trained only for image level semantics, while MoGe\nhas been trained for dense and detailed depth prediction,\nwhich is much closer to the nature of robotic manipulation.\nReal-world Franka experiments. To further validate the\nbenefits of 3D VLM pretraining, we run comparisons by\ntraining on the DROID dataset [18]. Due to the higher\ncost, we train only 2 models: one initialized from the\nbase PaliGemma VLM and the other from our 3D-aware\nSPEAR-VLM. We refer to the resulting models as\u03c0 0-\nPaliGemma (DROID) and\u03c0 0- SPEAR-VLM (DROID) re-\nspectively. We compare the performance of both VLAs on\nthree of the four tasks from our Franka experiments (Sec-\ntion 4.4). The results are reported in Tab. 2. We can observethat\u03c0 0- SPEAR-VLM (DROID) is able to outperform the\nbaseline by more than 10% on average. We note that the\ntask \u201cCarrot on plate\u201d is not a part of the DROID training\ndataset, thus shows the improved generalization capabilities\nof SPEAR-VLM. The lower scores of both models on the\nvariations tabletop/elevations are likely due to workspace\n3D position being out-of-distribution. Even in this case,\u03c0 0-\nSPEAR-VLM (DROID) is able to successfully complete the\ntask in some cases while\u03c0 0-PaliGemma (DROID) fails ev-\nery time.\n4.3. Simulation experiments\nWe evaluate SPEAR-1 on the WidowX environments of the\nSIMPLER simulation benchmark [22], and compare it with\nOpenVLA [19] and SpatialVLA [35]. We report the results\nin Tab. 3. Our model is able to outperform the baselines\nby more than 10%. In our experience, we found SIMPLER\nsimulation results only to be indicative of relative perfor-\nmance of the models on the real WidowX robot, but not\nnecessarily of absolute performance. Therefore, we focus\non real-world evaluations.\nModelPut Carrot\non PlatePut Eggplant\nin Yellow BasketPut Spoon\non TowelStack Green Block\non Yellow BlockAvg. Success\nRate\nOpenVLA 0% 4.1% 0% 0% 1.0%\nSpatialVLA 25.0%100.0% 16.7% 29.2% 42.7%\nSPEAR-1 (ours) 58.3% 62.5%62.5%45.82% 57.3%\nTable 3.SIMPLER [22] simulation evaluations.SpatialVLA\nnumbers are from [35]. SPEAR-1 outperforms the considered\nbaselines by more than 10%.\n4.4. Real-world experiments\nWe conduct evaluations on a total of 10 manipulation tasks\nacross two robot platforms: WidowX and Franka Research\n3. The tasks are designed to assess the ability of the eval-\nuated models to generalize to unseen environments and ob-\njects. We design the tasks to be challenging for all models.\nFor more details about the selected tasks see Appendix A.3.\nEvaluation protocol. For each task we define M initial con-\nditions by varying the starting position of the objects in the\nscene. We execute N trials for each initial condition, for a\ntotal of N\u00d7M trials per task. For each model, we evaluate\n7\nand report the average task progress across all tasks, config-\nurations, and trials. To that end, following previous works\n[3, 32], we define a scoring rubric with partial scoring for\neach task (see Appendix A.3 for scoring rubrics details).\nWidowX experiments. Our hardware setup for this set of\nexperiments closely matches the Bridge V2 setup [45], with\na single external camera positioned on the side of the robot\narm, pointing toward the workspace. For this set of exper-\niments, 5 tasks are evaluated, with M = 4, N = 3, for a to-\ntal of 60 trials per model. We compare the performance of\nSPEAR-1 with OpenVLA [19], using the publicly released\nimplementation and model weights. In this setting, we do\nnot compare against\u03c0 0[5],\u03c0 0-FAST [33] and\u03c0 0.5[6] due\nto the lack of publicly accessible weights for the WidowX\nplatform. The results are reported in Fig. 4. SPEAR-1 is\nable to achieve 10% higher average task progress across all\ntasks than OpenVLA, a very strong baseline in this setting.\nFranka experiments.Our hardware setup for this set of\nexperiments is similar to that of DROID [18]. We design\n5 tasks, with M = 5 and N = 3, for a total of 75 trials per\nmodel. We found that the wrist camera view is crucial for\ntraining and deployment on DROID. To ensure a fair com-\nparison, we compare against open-weights models that use\nboth the external and wrist camera. Specifically, we com-\npare SPEAR-1 with the DROID-finetuned variants of\u03c0 0-\nFAST [5, 33], a strong autoregressive baseline, and\u03c0 0.5[6],\none of the latest state-of-the-art robotic foundation model\noptimized for open-world generalization.\nThe results of our real-world experiments are reported in\nFig. 5. Without any fine-tuning on the target environment,\nSPEAR-1 is able to significantly outperform\u03c0 0-FAST, and\nmatch\u03c0 0.5. We note that both baselines do not integrate any\nsort of specialized 3D-aware training and are trained on at\nleast 900M more robot demonstration frames collected in\ndiverse environments. In contrast, SPEAR-1 is trained on\n\u223c45M frames, approximately20\u00d7less robotics data. These\nresults indicate the importance of 3D-based knowledge and\npretraining for VLA\u2019s generalization. As an architecturally\nclose comparison,\u03c0 0-FAST integrates a specialized action\ntokenization compared to\u03c0 0and was the first generalist pol-\nicy trained on the DROID [18] to be successfully evaluated\nzero-shot in unseen environments, without fine-tuning. In\ncomparison, SPEAR-1, which also follows the\u03c0 0archi-\ntecture, can reach\u223c5\u00d7higher performance than\u03c0 0-FAST\nwithout fine-tuning and without the large-scale robotic data\nused by\u03c0 0-FAST.\nApart from architectural enhancements and co-training\non top of\u03c0 0-FAST,\u03c0 0.5integrates high-level semantic sub-\ntask prediction and robotic data mixture explicitly focused\non environment diversity and generalization. Qualitatively\nand quantitatively, we find\u03c0 0.5to perform better at environ-\nment generalization than\u03c0 0-FAST and match SPEAR-1\u2019s\nperformance on our set of evaluation tasks. This suggeststhat 3D VLM pretraining on non-robotic data from diverse\nenvironments is a more scalable way to boost robotic mod-\nels\u2019 generalization capabilities without the need for large-\nscale robotic data collection in diverse environments.\n5. Discussion and Limitations\nAs highlighted by our experimental evaluation, SPEAR-1\nachieves state-of-the-art performance in multiple zero-shot\nrobot control scenario, both in simulation and in the real-\nworld. Nevertheless, our approach still presents several lim-\nitations. The proposed 3D VLM pre-training strategy is not\nwell suited for deformable objects or objects with complex\nshapes.\nFuture work could explore the use of different 3D pri-\nors to better capture the geometry of such objects. Addi-\ntionally, the coordinates of the 3D bounding boxes labels\ncomputed using MoGe\u2019s affine-invariant depth predictions\nare not in metric space. Further investigation is required\nto analyze the implications of this design choice on down-\nstream performance, as well as to explore how ground truth\npoint cloud labels or state-of-the-art metric-depth estima-\ntors could be integrated to help resolve this limitation.\nWhile we have showed the benefits of 3D VLM pre-\ntraining on downstream robot control tasks, the scaling laws\nrelating the latter to the quantity and quality of 3D pre-\ntraining data are still not well understood. Due to resource\nand time constraints, we leave this investigation for future\nwork. Another limitation of SPEAR-1 is the need to fine-\ntune on the target embodiment to achieve satisfactory re-\nsults. We plan to explore how to alleviate this requirement\nin future work. It also remains to be seen how well SPEAR-\n1 generalizes to orders of magnitude more tasks and en-\nvironments against models such as\u03c0 0.5trained on signif-\nicantly more diverse robot data.\n6. Conclusion\nIn this work we introduced SPEAR-1 and SPEAR-VLM\nthat demonstrate a path towards building generalist robot\npolicies from data beyond robot teleoperation only.\nOur method targets the VLM backbone withSPEAR-\nVLM, a 3D-aware VLM trained on 2D images from non-\nrobotic datasets enriched with 3D annotations. To embed\ncontrol-relevant 3D knowledge in SPEAR-VLM, we train\nit on VQA questions, inspired by embodied tasks. Stepping\non this foundation, we builtSPEAR-1, a robotic foundation\nmodel that can be deployed robustly across multiple robot\nplatforms and environments, and matches or outperforms\nstate-of-the-art foundation models which have been trained\non20\u00d7more robot demonstrations.\nOur work supports the hypothesis that enhancing VLM\ncapabilities with non-robotic embodied knowledge is a scal-\nable way toreduce dependence on hard-to-collect robot\n8\ndemonstrations and build future robotic foundation models.\nAcknowledgments\nProject Lead: Nikolay Nikolov, Project Manager: Jan-Nico\nZaech, PI: Danda Pani Paudel, Luc Van Gool\nWe thank Alexander-Marc Spiridonov, Anna-Maria Ha-\nlacheva and Yutong Hu for feedback and helpful technical\ndiscussions. We also thank Hristo Venev for engineering\nsupport and Kamen Pavlov for help with figures and visu-\nals.\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katie Millican, Malcolm Reynolds, Roman Ring,\nEliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,\nSina Samangooei, Marianne Monteiro, Jacob Menick, Se-\nbastian Borgeaud, Andy Brock, Aida Nematzadeh, Sa-\nhand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,\nOriol Vinyals, Andrew Zisserman, and Karen Simonyan.\nFlamingo: a visual language model for few-shot learning.\narXiv preprint arXiv:2204.14198, 2022. 3\n[2] Pranav Atreya, Karl Pertsch, Tony Lee, Moo Jin Kim, Arhan\nJain, Artur Kuramshin, Clemens Eppner, Cyrus Neary, Ed-\nward Hu, Fabio Ramos, et al. Roboarena: Distributed real-\nworld evaluation of generalist robot policies. InProceedings\nof the Conference on Robot Learning (CoRL 2025), 2025. 2,\n16\n[3] Jose Barreiros, Andrew Beaulieu, Aditya Bhat, Rick Cory,\nEric Cousineau, Hongkai Dai, Ching-Hsin Fang, Kunimatsu\nHashimoto, Muhammad Zubair Irshad, Masha Itkina, et al.\nA careful examination of large behavior models for multitask\ndexterous manipulation.arXiv preprint arXiv:2507.05331,\n2025. 8\n[4] Lucas Beyer, Andreas Steiner, Andr \u00b4e Susano Pinto, Alexan-\nder Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann,\nIbrahim Alabdulmohsin, Michael Tschannen, Emanuele\nBugliarello, Thomas Unterthiner, Daniel Keysers, Skanda\nKoppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko,\nNeil Houlsby, Manoj Kumar, Keran Rong, Julian Eisensch-\nlos, Rishabh Kabra, Matthias Bauer, Matko Bo \u02c7snjak, Xi\nChen, Matthias Minderer, Paul V oigtlaender, Ioana Bica,\nIvana Balazevic, Joan Puigcerver, Pinelopi Papalampidi,\nOlivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen,\nand Xiaohua Zhai. PaliGemma: A versatile 3B VLM for\ntransfer.arXiv preprint arXiv:2407.07726, 2024. 2, 3\n[5] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail,\nMichael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom,\nKarol Hausman, Brian Ichter, et al.pi 0: A vision-language-\naction flow model for general robot control.arXiv preprint\narXiv:2410.24164, 2024. 3, 4, 8, 14, 15\n[6] Kevin Black, Noah Brown, James Darpinian, Karan Dha-\nbalia, Danny Driess, Adnan Esmail, Michael Robert Equi,\nChelsea Finn, Niccolo Fusai, Manuel Y . Galliker, Dibya\nGhosh, Lachy Groom, Karol Hausman, brian ichter, Szymon\nJakubczak, Tim Jones, Liyiming Ke, Devin LeBlanc, SergeyLevine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl\nPertsch, Allen Z. Ren, Lucy Xiaoyang Shi, Laura Smith,\nJost Tobias Springenberg, Kyle Stachowicz, James Tanner,\nQuan Vuong, Homer Walke, Anna Walling, Haohuan Wang,\nLili Yu, and Ury Zhilinsky.\u03c0 0.5: a vision-language-action\nmodel with open-world generalization. In9th Annual Con-\nference on Robot Learning, 2025. 1, 2, 3, 8, 15, 16\n[7] Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan, Xiaoqi\nLi, Wankou Yang, Hao Dong, and Bo Zhao. Spatialbot:\nPrecise spatial understanding with vision language models.\narXiv preprint arXiv:2406.13642, 2024. 2\n[8] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa\nSadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endow-\ning vision-language models with spatial reasoning capabili-\nties. InProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 14455\u201314465,\n2024. 2\n[9] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice\nPasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blis-\ntein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5:\nPushing the frontier with advanced reasoning, multimodality,\nlong context, and next generation agentic capabilities.arXiv\npreprint arXiv:2507.06261, 2025. 4\n[10] Sombit Dey, Jan-Nico Zaech, Nikolay Nikolov, Luc\nVan Gool, and Danda Pani Paudel. Revla: Reverting vi-\nsual domain limitation of robotic foundation models.arXiv\npreprint arXiv:2409.15250, 2024. 3, 5\n[11] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey\nLynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong\nHuang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-\nworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,\nMarc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,\nand Pete Florence. Palm-e: An embodied multimodal lan-\nguage model.arXiv preprint arXiv:2303.03378, 2023. 3\n[12] Andreas Ren \u00b4e Geist, Jonas Frey, Mikel Zhobro, Anna Lev-\nina, and Georg Martius. Learning with 3d rotations, a hitch-\nhiker\u2019s guide to SO(3). InForty-first International Confer-\nence on Machine Learning, 2024. 5\n[13] Kristen Grauman, Andrew Westbury, Lorenzo Torresani,\nKris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar\nAshutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote,\net al. Ego-exo4d: Understanding skilled human activity\nfrom first-and third-person perspectives. InProceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 19383\u201319400, 2024. 4, 13\n[14] Richard Hartley, Jochen Trumpf, Yuchao Dai, and Hongdong\nLi. Rotation averaging.International journal of computer\nvision, 103(3):267\u2013305, 2013. 5\n[15] John Hewitt. Initializing new word embeddings for\npretrained language models.https : / / www . cs .\ncolumbia.edu/ \u02dcjohnhew/vocab- expansion.\nhtml. 13\n[16] HuggingFace. Huggingface transformers documen-\ntation.https : / / huggingface . co / docs /\ntransformers/en/main_classes/model. 13\n[17] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna,\nPercy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic\n9\nvlms: Investigating the design space of visually-conditioned\nlanguage models.arXiv preprint arXiv:2402.07865, 2024.\n2, 3\n[18] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Bal-\nakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush\nNasiriany, Mohan Kumar Srirama, Lawrence Yunliang\nChen, Kirsty Ellis, et al. Droid: A large-scale in-the-wild\nrobot manipulation dataset. InRobotics: Science and Sys-\ntems, 2024. 6, 7, 8, 15, 16\n[19] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao,\nAshwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan\nFoster, Grace Lam, Pannag Sanketi, et al. Openvla: An\nopen-source vision-language-action model.arXiv preprint\narXiv:2406.09246, 2024. 2, 3, 5, 7, 8, 15, 16\n[20] Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo\nLiu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang,\nSangho Lee, et al. Molmoact: Action reasoning models that\ncan reason in space.arXiv preprint arXiv:2508.07917, 2025.\n3\n[21] Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen,\nMozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu,\nYizhong Zhang, et al. Cogact: A foundational vision-\nlanguage-action model for synergizing cognition and action\nin robotic manipulation.arXiv preprint arXiv:2411.19650,\n2024. 2, 15, 16\n[22] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees,\nHomer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel\nSieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea\nFinn, Hao Su, Quan Vuong, and Ted Xiao. Evaluating\nreal-world robot manipulation policies in simulation.arXiv\npreprint arXiv:2405.05941, 2024. 5, 6, 7, 15\n[23] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maxim-\nilian Nickel, and Matthew Le. Flow matching for genera-\ntive modeling. InThe Eleventh International Conference on\nLearning Representations, 2023. 4\n[24] Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta\nShaul, Matt Le, Brian Karrer, Ricky TQ Chen, David Lopez-\nPaz, Heli Ben-Hamu, and Itai Gat. Flow matching guide and\ncode.arXiv preprint arXiv:2412.06264, 2024. 4\n[25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. InAdvances in Neural Information\nProcessing Systems, pages 34892\u201334916. Curran Associates,\nInc., 2023. 3, 4\n[26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning.Advances in neural information\nprocessing systems, 36, 2024. 2\n[27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning.Advances in neural information\nprocessing systems, 36, 2024. 3\n[28] Qiang Liu. Rectified flow: A marginal preserving approach\nto optimal transport.arXiv preprint arXiv:2209.14577, 2022.\n4\n[29] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection.arXiv preprint\narXiv:2303.05499, 2023. 13[30] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis.Communications of the ACM, 65(1):99\u2013106, 2021.\n12\n[31] Octo Model Team, Dibya Ghosh, Homer Walke, Karl\nPertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey\nHejna, Charles Xu, Jianlan Luo, Tobias Kreiman, You\nLiang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan\nVuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey\nLevine. Octo: An open-source generalist robot policy.\nInProceedings of Robotics: Science and Systems, Delft,\nNetherlands, 2024. 15\n[32] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bew-\nley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant\nRai, Anikait Singh, Anthony Brohan, et al. Open x-\nembodiment: Robotic learning datasets and rt-x models.\narXiv preprint arXiv:2310.08864, 2023. 3, 6, 8, 15\n[33] Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess,\nSuraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and\nSergey Levine. Fast: Efficient action tokenization for vision-\nlanguage-action models.arXiv preprint arXiv:2501.09747,\n2025. 1, 2, 3, 8, 15, 16\n[34] Sundar Pichai, Demis Hassabis, and Koray Kavukcuoglu.\nIntroducing gemini 2.0: our new ai model for the agentic era.\nhttps://blog.google/technology/google-\ndeepmind / google - gemini - ai - update -\ndecember-2024/. 3\n[35] Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi\nYe, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao,\nDong Wang, et al. Spatialvla: Exploring spatial represen-\ntations for visual-language-action model.arXiv preprint\narXiv:2501.15830, 2025. 2, 3, 7, 12, 15, 16\n[36] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang\nHu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman\nR\u00a8adle, Chloe Rolland, Laura Gustafson, et al. Sam 2:\nSegment anything in images and videos.arXiv preprint\narXiv:2408.00714, 2024. 4, 13\n[37] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kun-\nchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen,\nFeng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang,\nHongyang Li, Qing Jiang, and Lei Zhang. Grounded sam:\nAssembling open-world models for diverse visual tasks,\n2024. 4\n[38] Moritz Reuss, Hongyi Zhou, Marcel R \u00a8uhle, \u00a8Omer Erdinc \u00b8\nYa\u02d8gmurlu, Fabian Otto, and Rudolf Lioutikov. FLOWER:\nDemocratizing generalist robot policies with efficient vision-\nlanguage-action flow policies. In7th Robot Learning Work-\nshop: Towards Robots with Human-Level Abilities, 2025. 15,\n16\n[39] Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen\nTyree, Yu Su, and Stan Birchfield. Robospatial: Teaching\nspatial understanding to 2d and 3d vision-language models\nfor robotics. InProceedings of the Computer Vision and Pat-\ntern Recognition Conference, pages 15768\u201315780, 2025. 2\n[40] Alexander Spiridonov, Jan-Nico Zaech, Nikolay Nikolov,\nLuc Van Gool, and Danda Pani Paudel. Generalist robot ma-\n10\nnipulation beyond action labeled data. In9th Annual Con-\nference on Robot Learning, 2025. 2, 15\n[41] Andreas Steiner, Andr \u00b4e Susano Pinto, Michael Tschannen,\nDaniel Keysers, Xiao Wang, Yonatan Bitton, Alexey Grit-\nsenko, Matthias Minderer, Anthony Sherbondy, Shangbang\nLong, et al. Paligemma 2: A family of versatile vlms for\ntransfer.arXiv preprint arXiv:2412.03555, 2024. 2\n[42] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert\nDadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,\nMorgane Rivi `ere, Mihir Sanjay Kale, Juliette Love, et al.\nGemma: Open models based on gemini research and tech-\nnology.arXiv preprint arXiv:2403.08295, 2024. 3, 14\n[43] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya\nPathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Ta-\ntiana Matejovicova, Alexandre Ram \u00b4e, Morgane Rivi `ere,\net al. Gemma 3 technical report.arXiv preprint\narXiv:2503.19786, 2025. 2\n[44] Gemini Robotics Team, Saminda Abeyruwan, Joshua\nAinslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Are-\nnas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch,\nMaria Bauza, Michiel Blokzijl, et al. Gemini robotics:\nBringing ai into the physical world.arXiv preprint\narXiv:2503.20020, 2025. 3\n[45] Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim,\nMax Du, Chongyi Zheng, Tony Zhao, Philippe Hansen-\nEstruch, Quan Vuong, Andre He, Vivek Myers, Kuan Fang,\nChelsea Finn, and Sergey Levine. Bridgedata v2: A dataset\nfor robot learning at scale. InProceedings of the Conference\non Robot Learning (CoRL), 2023. 4, 6, 7, 8, 13, 14, 15\n[46] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan,\nJinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, et al. Qwen2-vl: Enhancing vision-language model\u2019s\nperception of the world at any resolution.arXiv preprint\narXiv:2409.12191, 2024. 2\n[47] Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang,\nYu Deng, Xin Tong, and Jiaolong Yang. Moge: Unlocking\naccurate monocular geometry estimation for open-domain\nimages with optimal training supervision.arXiv preprint\narXiv:2410.19115, 2024. 2, 3, 4, 13\n[48] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiao-\ngang Xu, Jiashi Feng, and Hengshuang Zhao. Depth any-\nthing v2. InThe Thirty-eighth Annual Conference on Neural\nInformation Processing Systems, 2024. 3\n[49] Micha\u0142 Zawalski, William Chen, Karl Pertsch, Oier Mees,\nChelsea Finn, and Sergey Levine. Robotic control via em-\nbodied chain-of-thought reasoning. In8th Annual Confer-\nence on Robot Learning, 2024. 15\n[50] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and\nLucas Beyer. Sigmoid loss for language image pre-training.\nInProceedings of the IEEE/CVF International Conference\non Computer Vision, pages 11975\u201311986, 2023. 3\n[51] Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang\nZhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han,\nChelsea Finn, et al. Cot-vla: Visual chain-of-thought rea-\nsoning for vision-language-action models. InProceedings\nof the Computer Vision and Pattern Recognition Conference,\npages 1702\u20131713, 2025. 15, 16[52] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3D: A\nmodern library for 3D data processing.arXiv:1801.09847,\n2018. 13\n[53] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted\nXiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker,\nAyzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong\nTran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre\nSermanet, Pannag R. Sanketi, Grecia Salazar, Michael S.\nRyoo, Krista Reymann, Kanishka Rao, Karl Pertsch, Igor\nMordatch, Henryk Michalewski, Yao Lu, Sergey Levine,\nLisa Lee, Tsang-Wei Edward Lee, Isabel Leal, Yuheng\nKuang, Dmitry Kalashnikov, Ryan Julian, Nikhil J. Joshi,\nAlex Irpan, Brian Ichter, Jasmine Hsu, Alexander Herzog,\nKarol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu,\nPete Florence, Chelsea Finn, Kumar Avinava Dubey, Danny\nDriess, Tianli Ding, Krzysztof Marcin Choromanski, Xi\nChen, Yevgen Chebotar, Justice Carbajal, Noah Brown, An-\nthony Brohan, Montserrat Gonzalez Arenas, and Kehang\nHan. Rt-2: Vision-language-action models transfer web\nknowledge to robotic control. InProceedings of The 7th\nConference on Robot Learning, pages 2165\u20132183. PMLR,\n2023. 3\n11\nA. Appendix\nThe appendix is organized as follows:\n\u2022 In Sec. A.1 we provide more details on the VLM pre-\ntraining including VQA tasks, encoder fusion strategies,\n3D tokenization and data annotation pipeline.\n\u2022 In Sec. A.2 we provide more details on the VLA training\nincluding data mixture, architecture, flow matching and\ndesign decision ablation results.\n\u2022 In Sec. A.3 we provide the scoring rubrics for real-world\nevaluation tasks\n\u2022 In Sec. A.4 we discuss the differences between Bridge V2\nand DROID datasets for zero-shot control evaluations in\nunseen environments.\nDataset Weight\naustin buds dataset 0.5\naustin sailor dataset 2.0\naustin sirius dataset 0.5\nberkeley autolab ur5 1.0\nberkeley cable routing 0.1\nberkeley fanuc manipulation 1.0\nbridge 18.0\ndlredan shared control 0.1\ndroid 35.0\nfmb 1.5\nfractal20220817 data 12.0\nfurniture bench dataset 1.5\niamlab cmu pickup insert 0.3\nkuka 4.0\nlanguage table 1.5\nnyufranka play dataset 0.3\nroboset (kinesthetic) 2.0\nroboset (teleoperation) 5.0\nroboturk 3.0\nstanford hydra dataset 3.0\ntaco play 2.0\ntoto 1.5\nucsd kitchen dataset 0.2\nutaustin mutex 3.0\nviola 1.0\nTable 4. Open X-Embodiment data mixture for SPEAR-1 pre-\ntraining\nA.1. VLM training\nA.1.1. VQA tasks for VLM pre-training\nThe Visual Question Answering (VQA) tasks used during\nVLM pre-training are inspired by VLA embodied tasks and\naim to embed as much control-relevant 3D information into\nthe VLM as possible. We use templated question-answer\npairs grouped in the following categories:\n\u20223D keypoints prediction: Output the 3D coordinates of\nthe closest, furthest and center points of an object withrespect to the camera frame.\n\u20223D bounding prediction: Output the vertices of the 3D\nbounding box of an object.\n\u2022Object-to-object distance prediction: Output the direct\ndistance between object X and object Y in 3D space as\nwell as itsxyzcomponents.\n\u2022Object-to-object bounding box prediction: Output the\ndistance between the bounding box vertices and the cen-\nters of object X and object Y .\n\u2022Backprojection: Locate the vertices of the 3D bounding\nbox of an object on the 2D image.\n\u2022Chain-of-thought comparisons: What is the distance\nfrom the camera to object X? What is the distance from\nthe camera to object Y? Which object is closer to the cam-\nera?\nTo further encourage the model to \u2019reason\u2019 over the in-\nformation provided and attend to the right objects, in a sin-\ngle training example we use a random number (between 1\nand 4) of question-answer pairs corresponding to different\nprompts and objects in the scene. To resolve ambiguities, if\ntwo instances of the same type of object appear in the im-\nage, we filter them out and never ask questions about them.\nA.1.2. VLM encoder fusion strategies\nWe experimented with 2 different strategies to combine the\noutputs of the SigLIP and MoGe encoders:\n1. Concatenating the visual features predicted by both en-\ncoders and projecting them via a linear layer to the LLM\nembedding space. In particular, for SigLIP we take only\nthe tokens at the last layer of the vision encoder, while\nfor MoGe we take the tokens at the last 4 layers of the\nencoder, following the approach used by MoGe architec-\nture to decode the features to a 3D point cloud.\n2. Using MoGe\u2019s predicted 3D point cloudPin the\ncamera ego pose (in an affine-invariant space) and\nadding them to the SigLIP encoder features, similar to\nSpatialVLA [35]. In particular, MoGe\u2019s 3D point cloud\noutputP\u2208RH\u00d7W\u00d73is embedded toP\u2032\u2208Rh\u00d7w\u00d7d\nthrough a projector\u03c8(\u00b7), composed of normaliza-\ntion, convolution, sinusoidal embedding\u03b3(x) =\n(x,sin(20\u03c0x),cos(20\u03c0x), . . . ,sin(2L\u22121\u03c0x),cos(2L\u22121\u03c0x))\n[30] and an MLP. Finally, the featuresF\u2032=F+P\u2032\nare fed to PaliGemma\u2019s SigLIP linear projector, where\nF\u2208Rh\u00d7w\u00d7ddenotes the features at the SigLIP encoder\noutput.\nDuring our preliminary VLM evaluations we found the\nfirst strategy to demonstrate qualitatively better perfor-\nmance on bounding box prediction tasks. In particular,\nmodels trained with the second approach struggled to con-\nsistently output \u201dgrammatically\u201d correct bounding boxes,\ne.g. they would output 22 or 23 3D tokens instead of the\nrequired 24. We therefore used the first approach for all\nVLM pre-training experiments in the main paper.\n12\nDataset Domain / Subset # Annotated Images Segmentation Masks\nEgoExo4D [13] Cooking & Bike Repair\u223c200k GT\nBridge [45] Robot Demonstrations\u223c30k SAM2 Generated\nTotal\u223c230k\nTable 5. Annotated image counts for training dataset construction, with segmentation mask availability.\nTask 0.25 0.50 0.75 1.00\nCarrot on Plate\n(w/ distractors & elevations) Reach carrot Pick up carrot Drop on/near plate Correctly place on plate\nMarker in Cup\n(w/ distractors) Reach marker Pick up marker Drop on/near cup Place inside cup\nCover the Pot \u2013 Pick up lid Drop lid on pot Correctly cover pot\nApple in drawer and close Pick up the apple Put the apple in the drawer Half-close the drawer Fully close the drawer\nTable 6. Scoring rubric for Franka evaluation tasks.\nTask 0.25 0.50 0.75 1.00\nEggplant in pot Reach the eggplant Pick up the eggplant Drop the eggplant near the pot Drop the eggplant on the pot\nPink cup on blue plate Reach the pink cup Pick up the pink cup Drop the pink cup near the plate Place the pink cup correctly\nChess piece on board - Go to the brown chess piece Pick up the brown chess piece Drop it on the board\nLobster in the pan - Pick up the lobster Drop the lobster near the pan Place the lobster inside the pan\nCorn between cups - Pick up the corn - Place the corn between the cups\nTable 7. Scoring rubric for WidowX evaluation tasks.\nA.1.3. 3D tokenization\nTo encode 3D information into text we extend the\nPaliGemma tokenizer withN= 10243D tokens, as 3D co-\nordinates are conceptually different from the existing visual\nand language tokens. This is in line with PaliGemma\u2019s ap-\nproach of extending Gemma\u2019s tokenizer to pixel locations.\nEach 3D token corresponds to a quantizeddistance valuein\nthe range[z min, zmax], wherez minandz maxare computed\nas the 1st and 99th quantiles of the 3D point cloud distribu-\ntion along any of thexyzcoordinates.\nWe found thedistance valuesin the data to approxi-\nmately follow a Normal distribution. Therefore, to allow\nfor more accurate tokenization, we compute non-uniform\nbins with fine-grained discretization around the mean and\nspread out widths near the tails such that the distribution of\n3D tokens is approximately uniform.\nWe initialize the new token embedding weights from a\nmultivariate normal distribution that has the mean and co-\nvariance of the pretrained embeddings [15, 16].\nA.1.4. VQA data annotation pipeline\nWe follow the method described in Section 3.1 in order\nto enrich 2D images with semantics, segmentation masks\nand 3D point clouds. We also experimented with Ground-\ningDINO [29] instead of Gemini, but we found the semanticlabels produced by GroudingDINO to be a lot less accurate\nand consistent. We found that prompting SAM2 [36] with\n2D bounding boxes near the target objects, leads to segmen-\ntation masks of high quality.\nWe also found that MoGe [47] outputs depths at differ-\nent scales depending on the input image size. Therefore,\nwe resize all our images to 840x630 for MoGe point cloud\nannotations.\nFor 3D bounding box estimation, after filtering the 3D\npoint cloud with a segmentation mask, we run statistical\noutlier removal and esitmate an oriented 3D bounding box\naround the remaining points using Open3D [52]. To facili-\ntate learning, we order all 8 bounding box vertices in a con-\nsistent way, starting based on their spatial coordinates with\nrespect to the camera frame.\nA.2. VLA training\nA.2.1. Data mixture\nWe report the VLA training data mixture in Tab. 4. The\nsampling weights are chosen manually based on dataset\nsize, visual and task diversity, and quality of language an-\nnotations.\n13\n(a) Training environment:\nT oykitchen 2\n(b) SIMPLER:\nPut Carrot on Plate\n(c) SIMPLER:\nPut Eggplant in Yellow Basket\n(d) SIMPLER:\nPut Spoon on T owel\n(e) SIMPLER:\nStack Green Block on Yellow BlockFigure 6.3D ablation environments on WidowX.(a) Training data subset from Bridge V2 [45]. (b) - (e) SIMPLER evaluation environ-\nments.\nA.2.2. VLA training details\nReference Frames.In this work we focus on learning po-\nsition control of fixed-base single-arm manipulators. Each\ncontrol in the sequenceA t= [a t,at+1, . . .a t+H\u22121 ]is de-\nfined as a delta with respect to the current end-effector carte-\nsian pose\u2206 EE= [\u2206 T,\u2206R]. The translation component,\n\u2206Tis in robot base frame and the rotation component,\u2206 R,\nis in end-effector frame. The gripper action is binary.\nAction Chunking.During VLA training we use an ac-\ntion chunk of sizeH= 5and frequency of 5Hz. As not\nall datasets in Open X-Embodiment provide action labels at\n5Hz, we downsample or upsample the actions accordingly\nvia linear interpolation. This is done with the goal to en-\ncourage the model to share knowledge across datasets with\ndifferent control frequencies and embodiments instead of\n\u2019memorizing\u2019 each dataset separately.\nArchitecture.Similar to\u03c0 0[5], SPEAR-1 combines a\nVLM, which processes the image-language inputs, with an\naction expertmodule, which processes robot proprioception\nobservations and predicts the robot action sequence con-\nditioned on the VLM transformer\u2019s intermediate key-value\npairs. The action expert has the same architecture and num-\nber of layers as the Gemma [42] transformer and configu-\nration downsized totoken size= 4096, hidden size=\n4096, for a total of\u223c300M parameters, which is ex-\nactly the same as\u03c0 0[5]. Corresponding layers in the\nVLM and the action expert have a shared attention op-\neration with block-wise causal attention over the blocks\n[It,lt],[pt],[at+1, . . . ,a t+H\u22121 ]. Within each block, there\nis full bidirectional attention and the tokens in each block\ncan attend to tokens in previous blocks, but cannot attend to\nthe tokens in future blocks. During training, only the action\nsequence prediction is supervised and gradient updates are\npropagated back to the VLM parameters through the shared\nattention layers.\nA.2.3. Flow matching details\nTo address the inherent double coverage of 3D rotations by\nthe unit quaternion groupS3, we ensure that all quaternions\nused during training and inference lie in the same half-space\ndefined byR(q) =q w>0.\nQuaternion integration. Given a unit quaternionq t\u2208S3and its time derivative \u02d9qt\u2208R4, we can compute the an-\ngular velocity of rotation via\u03c9 t= 2.0\u00b7Im(q\u2217\nt\u2297\u02d9qt)\u2208R3.\nFor a small time step\u2206t, the corresponsing delta rotation is\ngiven by a rotation vector around the unit axis\u03c9=\u03c9/||\u03c9||\nover an angle\u2206\u03d5=||\u03c9||\u2206t. The corresponding delta\nquaternion is given by\n\u2206q=\u0014\ncos\u0012\u2206\u03d5\n2\u0013\n,\u03c9sin\u0012\u2206\u03d5\n2\u0013\u0015\n.(6)\nThe integrated unit quaternion is then given byq t+\u2206t =\nqt\u2297\u2206q\u2208S3,\nRotation losses. The denoising vector field for quater-\nnionsu t(q\u03c4\nt|qt)\u2208R4is computed as:\nut(q\u03c4\nt|qt) =dq\u03c4\nt\nd\u03c4=\n=\u03b8\nsin\u03b8\u0002\n\u2212cos\u0000\n(1\u2212\u03c4)\u03b8\u0001\nq\u03f5+ cos\u0000\n\u03c4\u03b8\u0001\nqt\u0003\n.(7)\nThe cosine loss is applied directly on the velocity predic-\ntions and has the form:\nLcos\nt(\u03b8) = 1\u2212v \u03b8(A\u03c4\nt,ot)[q]\u00b7u(A\u03c4\nt|At)[q].(8)\nThe geodesic loss is applied on an integrated rotation pre-\ndictionq\u03c4+\u03b4\n\u03b8,t\u2208S3, derived by integrating the noised\ninput quaternionq\u03c4\ntwith the predicted rotation velocity\nv\u03b8(A\u03c4\nt,ot)[q]over a small time step\u03b4. We follow the inte-\ngration method described above. The target is given by the\nground truth interpolated quaternion at timet+\u03b4, denoted\nasq\u03c4+\u03b4\nt\u2208S3. The closed form geodesic loss is given by:\nLgeo\nt(\u03b8) = min|q\u03c4+\u03b4\nt\u00b1q\u03c4+\u03b4\n\u03b8,t|.(9)\nThe complete rotation loss is given by:\nLS3(\u03b8) =t+HX\nk=t\u0002\nLcos\nk(\u03b8) +Lgeo\nk(\u03b8)\u0003\n.(10)\nA.2.4. VLA design decisions details\nWe present more details and results on the design choices\nwe explored for VLA training.\n14\nExperimentPut Carrot\non PlatePut Eggplant\nin Yellow BasketPut Spoon\non TowelStack Green Block\non Yellow BlockAvg. Success\nRate\n224\u00d7224 70.8% 70.8% 79.1% 8.3% 57.25%\n280\u00d7210 62.5%75.0%83.3%12.5% 58.3%\nTable 8.Image resultion ablations.Different resolutions lead to\ncomparable results on SIMPLER WidowX tasks.\nExperimentPut Carrot\non PlatePut Eggplant\nin Yellow BasketPut Spoon\non TowelStack Green Block\non Yellow BlockAvg. Success\nRate\ntrainable SigLIP 75.0%100.0% 79.1%37.5% 72.9%\nfrozen SigLIP 62.5% 54.1% 83.3% 25% 56.3%\nfrozen-trainable SigLIP 66.6% 83.3%100.0% 33.3% 70.8%\nlower lr SigLIP 58.3% 58.3% 79.1% 29.1% 56.3%\nTable 9.Vision encoder training.Trainable SigLIP outperforms\nother strategies on SIMPLER WidowX tasks. Frozen SigLIP fol-\nlowed by switching on gradients is comparable.\nExperimentPut Carrot\non PlatePut Eggplant\nin Yellow BasketPut Spoon\non TowelStack Green Block\non Yellow BlockAvg. Success\nRate\n99-th quantile 54.1% 79.1% 79.1%33.3% 61.5%\nmin-max const 66.6%87.5%87.5% 20.8% 65.6%\nmean-std 45.8% 79.1% 45.8% 25.0% 49.0%\nTable 10.Translation controls normalization.Normalizing\ntranslation controls with min-max constants outperforms other\nstrategies on SIMPLER WidowX tasks.\nFlow\nmatchingVelocity\nLossRotation\nlossPut Carrot\non PlatePut Eggplant\nin Yellow BasketPut Spoon\non TowelStack Green Block\non Yellow BlockAvg. Success\nRate\nlinear MSE geodesic 41.6%100.0% 41.6% 16.6% 50.0%\nlinear cos geodesic 41.6% 87.5% 50.0% 29.1% 52.1%\nS3MSE geodesic 45.8% 62.5%75.0%45.8% 57.3%\nS3cos geodesic 45.8% 79.1% 66.6%45.8% 59.4%\nTable 11.Linear vsS3Flow Matching for rotations.S3flow\nmatching consistently outperforms linear flow matching on SIM-\nPLER WidowX tasks.\nImage Resolution.Image resolution ablations are pre-\nsented in Tab. 8. We observe that square vs 4:3 aspect ratio\ndoes not significantly affect performance.\nFine-tuning vision encoders.Ablations on fine-tuning vi-\nsion encoders are presented in Tab. 9. Trainable SigLIP\nstrongly outperforms a frozen SigLIP as well as SigLIP with\na lower learning rate compared to the rest of the weights.\nFreezing SigLIP and fine-tuning for additional 2k steps\n(frozen-trainable SigLIP) leads to comparable performance\nto trainable SigLIP, but requires an additional hyperparam-\neter tuning.\nControls normalization.Ablations on translation controls\nnormalization are presented in Tab. 10. Mean-std normal-\nization is significantly worse than other forms of normaliza-\ntion. Min-max normalization with const values is slightly\nbetter than per-dataset min-max normalization with 1st and\n99th quantiles.\nRotations.Partial ablations on rotation representations are\npresented in Tab. 11.S3flow matching consistently out-\nperforms linear flow matching. Cosine distance leads to\nslightly better performance than MSE for rotation velocity\nprediction.A.2.5. 3D ablation details\nSIMPLER WidowX experiments. For SIMPLER [22] 3D\nablations on WidowX, we train on a subset of the Bridge\nV2 [45] dataset, containing demonstrations only from a sin-\ngle kitchen sink environment. The resulting subset com-\nprises\u223c41%of the original Bridge V2 dataset. We train\neach VLA for 30k steps with batch size 512. Example\nimages from the training and evaluation environments are\nshown in Fig. 6.\nFranka experiments. For the 3D ablations on a real-\nworld Franka robot, we train on the entire DROID [18]\ndataset for 100k steps with batch size 2048. Both models\ntake as input both side and wrist cameras.\nA.3. Real-world robot task description and scoring\nWe provide the detailed task progression scoring for all real-\nworld evaluations on the WidowX and Franka in Tab. 6 and\nTab. 7 respectively.\nA.4. Zero-shot control: Bridge V2 vs. DROID\nModelZero-shot control embodiments\nin real-worldunseenenvironment\nRT-1-X[32] WidowX\nRT-2-X[32] WidowX, Google Robot\nOcto[31] WidowX, Google Robot?\nOpenVLA[19] WidowX\nSpatialVLA[35] WidowX\nCogACT[21] WidowX\nFLOWER[38] WidowX\nMotoVLA[40] WidowX\nCoT-VLA[51] WidowX\n\u03c00[5] Franka, Others?\n\u03c00-FAST[33] Franka, Others?\n\u03c00.5[6]Franka, Mobile Fibocom,\nMobile Galaxea, Others?\nGemini Robotics 1.5Bimanual Franka,\nAloha, Apollo humanoid\nRDT1 Bimanual UR5, Aloha\nRDT2 Bimanual UR5, Bimanual Franka\nSmolVLA S0101\nGROOT-N1.5 None\nSPEAR-1 (ours) WidowX,Franka\nTable 12. Most works on generalist models for robot manipula-\ntion evaluate zero-shot control on Bridge V2 + WidowX using in-\ndistribution environments. Only few do so on DROID + Franka in\nunseenenvironments.\nMost works on generalist models for robot manipulation\n[19, 21, 32, 49, 51] evaluate the zero-shot control capabili-\nties of their policies by pretraining on the Bridge V2 dataset\n[45] and deploying on the WidowX robot in environments\nclose to the training distribution. Bridge V2, however, is\nnot very diverse in the number of environments, objects,\n15\nand camera viewpoints. As a result, we observe that models\npre-trained on Bridge V2 only perform well on WidowX\nenvironments when the deployment scenario is similar to\nwhat is seen in the dataset (e.g. in the blue toy sink environ-\nment), but are usually very sensitive to variations in camera\nposition and OOD backgrounds and objects. In addition, the\nWidowX arm has a very low payload and short reach, which\nmakes it unable to manipulate objects beyond the items in\na toy kitchen set. The DROID dataset [18], on the other\nhand, is significantly more diverse, and the Franka arm used\nfor data collection is more capable. Furthermore, DROID\ndemonstrations are collected primarily in real-world envi-\nronments instead of toy environments, the number of unique\nscenes is20\u00d7higher, and the camera viewpoints vary sig-\nnificantly. Therefore, we posit that pretraining on DROID\nand deploying on Franka is a superior experimental setup to\nshowcase generalization to more realistic real-world scenar-\nios, as shown by [2]. The diversity and richness of DROID,\nhowever, is at the same time a challenge. Training general-\nist control policies on DROID that perform well zero-shot\non a Franka robot in unseen environment, is a task that, to\nthe best of our knowledge, has been tackled successfully\nonly by a handful of works so far [6, 33]. In contrast, mul-\ntiple other works that pre-train on DROID, resort to mixing\nor fine-tuning on demonstrations collected from the spe-\ncific target environment in order to achieve good perfor-\nmance [19, 21, 35, 38, 51]. Therefore, as suggested also by\n[33], we argue that achieving state-of-the-art performance\non zero-shot control on the DROID setup by pre-training on\nDROID is a significantly stronger result than pre-training\non Bridge V2 and deploying on WidowX.\n16\n",
    "title": "SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding",
    "arxiv_id": "2511.17411v1",
    "category": "recent",
    "authors": [
      "Nikolay Nikolov",
      "Giuliano Albanese",
      "Sombit Dey",
      "Aleksandar Yanev",
      "Luc Van Gool",
      "Jan-Nico Zaech",
      "Danda Pani Paudel"
    ],
    "published": "2025-11-21",
    "num_pages": 16,
    "num_chars": 71778
  }
]