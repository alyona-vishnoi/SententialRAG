[
  {
    "text": "Scaling Spatial Intelligence with Multimodal Foundation Models\nZhongang Cai\u2217,1, Ruisi Wang\u2217,1, Chenyang Gu\u2217,1, Fanyi Pu\u2217,1,2, Junxiang Xu\u2217,1, Yubo Wang\u2217,1,\nWanqi Yin\u2217,1, Zhitao Yang\u2217,1, Chen Wei\u2217,1, Qingping Sun\u2217,1, Tongxi Zhou\u2217,1, Jiaqi Li\u2217,1,\nHui En Pang\u2217,2, Oscar Qian\u2217,1,2, Yukun Wei1, Zhiqian Lin1, Xuanke Shi1, Kewang Deng1,\nXiaoyang Han1, Zukai Chen1, Xiangyu Fan1, Hanming Deng1, Lewei Lu1, Liang Pan1,\nBo Li2, Ziwei Liu\u0000,2, Quan Wang\u0000,1, Dahua Lin\u0000,1, Lei Yang\u2217,\u0000,1\n\u2217Core Contributors,\u0000Corresponding Authors,\n1SenseTime Research,2Nanyang Technological University\nAbstract\nDespite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial\nintelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial\nintelligence within theSenseNova-SIfamily, built upon established multimodal foundations including\nvisual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation\nmodels (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial\nintelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a\nrigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across\na broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on\nMindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal\nunderstanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling,\ndiscuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk\nof overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning,\nand validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will\nbe updated continuously. All newly trained multimodal foundation models are publicly released to facilitate\nfurther research in this direction.\nDate:November 18, 2025\nVersion:1.0\nModel:https://huggingface.co/collections/sensenova/sensenova-si\nCode:https://github.com/OpenSenseNova/SenseNova-SI\n1 Introduction\nIn recent years, multimodal foundation models [ 12,14,60] have achieved groundbreaking progress across a wide\nspectrum of tasks. However, it has become evident that even the most advanced models still struggle with spatial\nintelligence: the ability to understand, reason about, and act within three-dimensional space, which is fundamental\nto embodied AGI that can perceive, adapt to, and interact with the physical world. Interestingly, such tasks are often\nconsidered trivial for humans [ 6]. One of the key limitations lies in the scarcity and imbalance of spatially grounded\ndata. While recent efforts have introduced a surge of large-scale datasets targeting various facets of spatial reasoning,\nthese resources remain fragmented and heterogeneous in scope and quality. Consequently, the community is still in the\nearly stages of understanding how multimodal foundation models acquire and develop spatial intelligence, and what\n1arXiv:2511.13719v1  [cs.CV]  17 Nov 2025\nSpatial Relations Comprehensive ReasoningPerspective -taking Metric Measurement\nGPT-5GPT-5\nGPT-5\nGPT-5SenseNova -SIInternVL3 \u22128B SenseNova -SIInternVL3\u22128B\nSenseNova -SIInternVL3 \u22122B\nSenseNova -SIInternVL3\u22128B\nSenseNova -SIInternVL3 \u22122BSenseNova -SIInternVL3\u22128B\nSenseNova -SIInternVL3 \u22122BVST -7B\nSpaceR -7B\nSpatialLadder -3BSpaceR -7BSpatialLadder -3BVST -7BVST -7B\nVST -7B\nSpaceR -7B\nSpatialLadder -3BSenseNova -SIInternVL3 \u22122BSpatialLadder -3B\nMindCube : 85.6VSI: 68.7\nSITE:\n   47.7\nViewSpatial :\n54.6 MMBench -EN:\n84.9 MMSI:\n 43.3\nSpaceR -7B\nCambrian -S-7B Cambrian -S-7B Cambrian -S-7B \nCambrian -S-7B ViLaSR -7B\nViLaSR -7BViLaSR -7B\nViLaSR -7B\nFigure 1Guided by taxonomy of spatial intelligence [ 6], we scaled spatial data to constructSenseNova-SI-8M, which we leverage to\ninvestigate the impact of data scaling on cultivating spatial capabilities in various MLLMs. The four subfigures at the corners elaborate\nSenseNova-SI\u2019s performance on four core spatial capabilities (i.e., Perspective-taking, Spatial Relations, Metric Measurement, and\nComprehensive Reasoning). Through data scaling, SenseNova-SI surpassing open-source models and even outperforms GPT-5 in\nspecific spatial abilities, such as Perspective-taking. The lines denote the average performance across benchmark subtasks within\neach capability, while the shaded regions (confidence bands) represent \u00b10.5 standard deviation. At center, we showSenseNova-SI\nachieves state-of-the-art (SoTA) results on five recent spatial intelligence benchmarks (VSI, MMSI, MindCube, ViewSpatial, and\nSITE) while maintaining strong performance on a general multimodal benchmark (MMBench-En).\nstrategies are effective in fostering this capability.\nIn this work, we aim to provide timely insights into cultivating spatial intelligence within state-of-the-art multimodal\nfoundation models by leveraging their powerful generalist backbones and scaling up diverse data collections. Our study\ninvestigates the data scaling laws of spatial intelligence through extensive experiments on the widely adopted InternVL3\nmultimodal foundation model family [60], and further extends the analysis to Qwen3-VL [12] as well as Bagel [14], a\nunified understanding and generation model. We envision the resulting models, denoted with theSenseNova-SIprefix,\nas open research platforms to advance studies in spatial intelligence. To preserve compatibility with existing research\npipelines, we deliberately avoid altering the original architectures of the base models. Instead, we adopt a data-centric\napproach, emphasizing the role of data scaling and training strategies as the primary drivers of spatial understanding\ncapability. Our systematic collection and synthesis of spatial data are guided by a principled taxonomy of fundamental\nspatial intelligence capabilities [ 6], leading toeight millionsamples (named SenseNova-SI-8M) spanning five key\ndomains: Metric Measurement (MM), Spatial Relations (SR), Mental Reconstruction (MR), Perspective-taking (PT),\nand Comprehensive Reasoning (CR). We analyze a diverse collection of public datasets for spatial intelligence, followed\nby strategic further scaling that place a special focus on perspective-taking, an underrepresented capability that is critical\nto spatial intelligence, while isolated from general multimodal capabilities [27].\nWe evaluate the SenseNova-SI foundation models across a broad suite of benchmarks, including VSI-Bench [ 51],\nMMSI [ 55], MindCube [ 57], ViewSpatial [ 25], and SITE [ 45], following continued training on our comprehensive\nspatial intelligence data collection. The models achieve state-of-the-art performance among open-source models of\ncomparable sizes, with the best performance achieving 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube,\n54.6% on ViewSpatial, and 50.1% on SITE, while retaining their original strengths on general multimodal understanding\nbenchmarks such as MMBench-En (84.9%). Our analysis reveals several key findings: (1) Scaling law of spatial\nintelligence. We systematically investigate how spatial intelligence scales under mixed data regimes. Our analysis\nreveals distinct scaling behaviors across spatial capabilities and model sizes, and suggests that the observed saturation\ntrends may signal that future advances require paradigm shifts built upon and beyond SenseNova-SI. (2) Emergent\ngeneralization through diverse data. We report surprising findings that point to early signs of emergent spatial intelligence:\nmodels trained on one set of spatial tasks exhibit nontrivial transfer to seemingly unrelated tasks, and demonstrate\nextrapolation to longer spatial contexts beyond the training distribution. (3) Robustness against overfitting and shortcuts.\n2\nThrough controlled experiments and circular test designs, we rigorously validate that SenseNova-SI genuinely acquires\nspatial capabilities rather than exploiting memorization, annotation biases, or unintended shortcuts in the training\ndata. (4) Preliminary study of spatial chain-of-thought (CoT). We construct and evaluate three representative text CoT\nschemes, but find that none of them reliably improve spatial reasoning beyond what is achieved through simple QA-style\ndata scaling. These results suggest that extending text-based CoT paradigms to spatial intelligence is non-trivial and may\nrequire fundamentally different reasoning mechanisms. (5) Downstream task validation. To assess the practical utility\nof SenseNova-SI, we apply SenseNova-SI to robotic manipulation tasks without any finetuning, and achieve notable\nperformance improvements on EmbodiedBench [ 52], demonstrating the potential of SenseNova-SI as a foundation for\nembodied AI.\nIn summary, we introduce the SenseNova-SI series of multimodal foundation models, which achieve new state-of-the-art\nperformance across major spatial intelligence benchmarks. Our study further validates that data scaling governs the\nprogression of spatial intelligence. We envision SenseNova-SI as a strong, robust baseline that future research can build\nupon to drive deeper advances in this critical field.\n2 Related Works\n2.1 Multimodal Foudational Models\nRecent studies [ 6,27,58] reveal that while models like GPT-5 demonstrate strong planar reasoning capabilities, they\nstill lag significantly behind humans in Spatial Intelligence (SI). Furthermore, EASI [ 6] shows that the performance\ngap between open-source and closed-source models on SI tasks is relatively small. These findings motivate us to\nenhance the spatial intelligence of widely used open-source models (e.g., QwenVL series [ 2,3,12,42] and InternVL\nseries [ 10,44,60]). This not only enables fairer comparisons among models of similar scale but also facilitates the\ncommunity\u2019s direct use of our models for downstream tasks, (e.g., VLA [22, 52, 61]), with minimal substitution costs.\n2.2 Multimodal Models for Spatial Intelligence\nEfforts to enhance spatial intelligence in multimodal models primarily follow two approaches:leveraging 3D expertsor\ncurating spatial-specific datasets. As spatial intelligence is inherently linked to 3D vision, an intuition is to employ 3D\nexpert encoders that infer key 3D attributes from images [ 9,39,47]. Spatial-MLLM [ 47] incorporates VGGT [ 40] as an\ninput-level encoder to capture 3D information, while VLM-3R [ 15] integrates 3D information using combined geometry\nand camera-view tokens. Recently, 3DThinker [ 9] aligns model-generated 3D features with VGGT-derived supervision\nat the output level. Conversely, some studies [ 8,11,48,50] inject visual-spatial knowledge through dataset curation and\ntraining paradigm. SpatialVLM [ 8] pioneered this direction by synthesizing 2B VQA samples focused on two-object\nspatial relationships. SpaceR [ 33] uses RL for spatial reasoning, while MindCube [ 57] explores SFT and RL using QA\nand two types of cognitive maps. SpatialLadder [ 26] constructs a dataset with 26K samples and introduces a three-stage\nprogressive training strategy. Concurrently, VST [ 53] adopts a two-phase training approach, using 4.1M samples for\nSFT on spatial perception and 135K samples for RL on spatial reasoning. Cambrian-S [ 54] develops VSI-590K dataset\nand employs a four-stage training framework to progressively enhance spatial video understanding. In this work, we\nsystematically scale datasets targeting core spatial capabilities [ 6], addressing key gaps in existing datasets, particularly\nthe previously overlooked perspective-taking tasks.\n3 Data\nThe limitations in spatial intelligence mainly stem from high-quality, diverse data scarcity. In this work, we strategically\nscale data to expand coverage toward holistic spatial intelligence, rather than merely increasing data volume.\n3.1 Task Taxonomy\nWe adopt a principled approach, following the EASI [ 6] protocol to decompose spatial intelligence into key fundamental\ncapabilities. We focus on five capabilities that are closely aligned with real-world scenarios. For each, we analyze the\ncore cognitive operation and derive tasks to ensure comprehensive coverage. Fig. 2 illustrates the dataset constructed\nunder this taxonomy.\n3\nVSI-590K\n[Route Planning] These are frames of a video. You are a robot  beginning at the ottoman facing the storage organizer. You want to navigate to the radiator. You will perform the following actions (Note: for each [please fill in], choose either 'turn back,' 'turn left,' or 'turn right.'): 1. [please fill in] 2. Go forward until the dresser. You have reached the final destination. \u2013turn back.                                 \n[Appearance Order] How do the categories\u2014tv, sink, oven, table\u2014first appear in the video? \u2013C. table, tv, sink, oven.                                \n[Object Counting] These are frames of a video. How many lamp(s) are in this room? \u20132.                                 \nMRPT Camera MotionPT CorrespondenceSRMM\nCR\n[Size Obj]Measure the maximum edge length of `toaster`.-0.34 m.\n[Size Scene]Determine the total area of this room in square meters.-18.64.\n[Front-Back]Which of the two points is farthest from the camera?-Point B.                                 \n[Front-Back]In terms of proximity to the camera, which is closer: a table or a sofa? -Table.                                 \n[Large-Small]Which object's longest edge is shorter, the dishwasher or the fireplace? -dishwasher.                                 \n[Corr Point]Match the point from image 1 with the correct point in image 2. -A.                                 \n[Corr Object]The object at [0.42, 0.40, 0.56, 0.62] in image 1 is at which bboxin image 2? -D. [0.52, 0.10, 0.66, 0.32].                                 \n[Corr Scene]Select the image that shows the same scene as the reference scene.-B.                                 \n[Cam Trans. Dir.]While capturing image 1, where do I find the other camera (image 2)? -Left-Back.                                 \n[Cam Trans. Dist.]What's the measurement of the camera's movement vector's length? -`1131` mm.                                 \n[Cam Rotation Dir.]From image 1 to 2, what is the correct camera rotation? -Rotate to right, look up. \n[Cam Rotation Dir.]From image 1 to 2. What is the rotation direction? -Rotated to the Left.                                 \n[Cam Rotation Deg.]What is the total vertical rotation angle from one shot to another? \u201315 degrees.                                 \n[Cam View]Looking at image 2, where can you find the sink?-Right-Back.                                 \n[Obj-Target View]If I am standing by stove and facing electrical outlet,  where is faucet? -Right-Back.                                 \n[Obj-Orient View]When I stand at stove facing the same direction as it, where is fridge relative to me? -Right.                                 \n[Obj-Orient View]Which egocentric view image correctly matches the exocentric view? -C.                                 \n[Hypothetical View] A person is facing north when plugging in phone charger. Where is the pet bed relative to the microwave?  \u2013Southwest.                                 \n[Obj Reconstruction]Suppose Image 1 shows the front side of the cereal box.Which side of it is shown in image 2? -Front-Right.\n[Obj Reconstruction]Suppose Image 1 shows the front side of the bottle.Which side of it is shown in image 2? -Back-Right.PT Allo. Trans.\n[Corr Object]What object exists in both images? -Bag.                                 \n[Dist. Cam-Obj]Calculate the distance from the nearest point of `chair` to the camera in meters. -1.77 m.[Dist. Cam-Obj]How far is the annotated point from the camera in millimeters? -`1926` mm.                                 \n[Dist. Obj-Obj]How far apart are `monitor` and `telephone` based on their center points? -0.50 m. [Above-Below]Is the centerpointof dining table higher than the countertop? -No.                                 Non-SI2D GroundingMMSRPT CorrespondencePT Camera Motion PT Allocentric TransformationMRCRGeneral QAMultiSpaViCAMindCubeVLM-3RFurther Scaling\n[Near-Far]Which object is nearest to `lamp`: `closet`, `bed`, `desk`, or `laundry hamper`? -bed.                                 \nOpen3D-VQACLEVR(series)REL3DSATGRiD-3D500K1M0Scale\n1 2 3 4 5 6 7 8 9 10 11 12 14 1516 \n13 1 2 3 4 5 6 7 8 9 10 11 12 13 14 1516 1 2 1 2 \n1 \n2 \n1 2 1 2 1 \n2 1 \n2 \n1 \n2 \n1 \n2 1 \nA\nB\nC 1 \n2 \n1 \n3 2 \n4 \n1 \nB A \nC \n1 \n2 1 2 \n1 2 \n1 2 3 64 597 81210 111513 141 2 3 64 597 81210 111513 14Figure 2 SenseNova-SI-8Mreorganizes 4M open-source data and scales 4.5M additional data, according to fundamental spatial\ncapbilities [ 6]. It covers general visual understanding (Non-SI), 2D grounding, and five core spatial abilities: Metric Measurement\n(MM), Spatial Relationship (SR), Perspective-Taking (PT), Mental Reconstruction (MR), and Comprehensive Reasoning (CR).\nNotably, SenseNova-SI-8M addresses the previously overlooked PT tasks. How data from each source is mapped to the core spatial\ncapabilities is illustrated at the top (with a scale in the upper-right corner indicating the number of QA pairs), while representative\ndata samples (organized hierarchically by core capability) are shown below.\n4\nMetric Measurement (MM).MM involves a basic understanding of the physical scale and typical object sizes. We\ninclude distances estimation between the camera and objects and pairs of objects, and size estimation across scales from\nindividual objects to entire scenes.\nSpatial Relations (SR).We define SR as the ability to impose and reason within a 3D coordinate system. In egocentric,\nlocal level of view, it unfolds into front\u2013back, left\u2013right, and up\u2013down relations between subjects. In global, scene level,\nthese relations extend to near\u2013far and relative scale (large\u2013small) comparisons.\nMental Reconstruction (MR).MR focuses on inferring 3D object structure from limited 2D observations. We adopt a\ndiagnostic task, which identifies which side of an object is visible. This requires the integration of sparse 2D cues to\ninfer 3D geometry and align views in a canonical object-centric frame.\nPerspective-taking (PT).PT addresses reasoning with changing camera viewpoints. We construct PT tasks in a\nprogressively more challenging hierarchy:\n\u2022View Correspondence.Establish correspondences of points or objects across views, recognizing entities under\nchanges in viewpoint, scale, and occlusion.\n\u2022Camera Motion Reasoning.Infer relative camera motion between views, linking appearance changes to 3D\ntransformations.\n\u2022Allocentric Transformation.Simulate viewpoint shifts and express spatial relations across coordinate systems,\nincluding camera, object-target, and self-oriented views.\nThis layered design ensures that PT goes beyond pattern matching across images, encouraging the model to build\ninternal representations of how observations transform with viewpoint changes.\nComprehensive Reasoning (CR).CR tasks involve coordinating multiple spatial capabilities with extended memory\nand multi-step reasoning. Such data is scarce and often limited to simple scenarios. As these tasks lie beyond our main\ngoal of scaling spatial QA and core spatial capabilities, we reuse existing datasets as a lightweight complement.\n3.2 Data Sources.\nGeneral QA.We collect a set of open-source general-purpose QA datasets for 2D image understanding. Specifically,\nwe use VSR [28], SPEC [34], GQA [19], VQA [1], and IconQA [31], resulting in about 0.6M QA pairs.\nCommunity Datasets on Spatial Intelligence.Among existing open-source resources, we identify several datasets that\nfocus on spatial reasoning, including Open3D-VQA [ 59], CLEVR-series [ 21], REL3D [ 17], SAT [ 35], GRiD-3D [ 24],\nMultiSpa [ 50], MindCube [ 57], ViCA [ 16], VLM-3R [ 15], and VSI-590K [ 54]. We incorporate all of these datasets,\nyielding in total about 3.3M QA pairs.\nFurther Scaling on Spatial Intelligence.Building on these open-source data, we find gaps in task coverage and data\nimbalance. MM and SR dominate the data, while PT and MR remain underrepresented. For point, object, scene level\ncorrespondence, only MultiSpa provides point level QAs. Camera motion is also mostly limited to MultiSpa. Allocentric\nviewpoint transformation, especially object-centric and hypothetical views, is largely unexplored, as real-world QA\nlabels are scarce. Tasks such as object reconstruction remain unaddressed.\nTo address these gaps, we leverage richly annotated, scene-diverse 3D datasets, including MessyTable [ 5], ScanNet [ 13],\nScanNet++ [ 56], SUN RGB-D [ 36], CA-1M [ 23], Ego-Exo4D [ 18], and Matterport3D [ 7], to generate large-scale,\naccurate and task-balanced QA pairs. This scaling process contributes 4.5M data, increasing the overall corpus size to\n8.5M QA pairs.\n4 Training\nWe adopt three multimodal foundation models in this study.\nQwen3-VL [ 12]is the most capable multimodal model in the Qwen series to date. It adopts a strategy to scale from\nlanguage foundation, that expand a strong LLM foundation to handle vision or audio modalities.\nInternVL-3[ 60] is natively multimodal, training vision and language jointly from scratch, thus enables stronger\ncross-modal alignment, more efficient scaling, and improved visual\u2013language reasoning.\n5\nModels VSI-Bench [51] MMSI-Bench [55] MindCube* [57] ViewSpatial [25] SITE [45] MMB-EN [29]\nMetric MRA, Acc Acc Acc Acc CAA Acc\nHuman 79.2 97.2 94.5-67.5-\nRandom Choice34.0 25.0 33.0 26.3 0.0 25.0\nProprietary Models\nSeed-1.6-2025-06-15 [37] 49.9 38.3 48.7 43.8 54.6 87.5\nGemini-2.5-pro-2025-06 [38] 53.5 38.0 57.6 46.0 57.0 90.1\nGrok-4-2025-07-09 [49] 47.9 37.8 63.543.2 47.0 86.3\nGPT-5-2025-08-07 [32] 55.0 41.856.3 45.5 61.885.2\nOpen-source General Models\nBagel-7B-MoT [14] 31.4 31.0 34.7 41.3 37.0 82.8\nQwen2.5-VL-3B-Instruct [3] 27.0 28.6 37.6 31.9 33.1 77.4\nQwen2.5-VL-7B-Instruct [3] 32.3 26.8 36.0 36.8 37.6 82.6\nQwen3-VL-2B-Instruct [12] 50.3 28.9 34.5 36.9 35.6 75.1\nQwen3-VL-8B-Instruct [12] 57.9 31.1 29.4 42.2 45.8 84.6\nInternVL3-2B [60] 32.9 26.5 37.5 32.5 30.0 79.7\nInternVL3-8B [60] 42.1 28.0 41.5 38.6 41.1 81.7\nOpen-source Spatial Intelligence Models\nMindCube-3B-RawQA-SFT [57] 17.2 1.7 51.724.1 6.3 32.3\nSpatialLadder-3B [26] 44.8 27.4 43.4 39.8 27.9 72.5\nSpatial-MLLM-4B [47] 46.3 26.1 33.4 34.6 18.0 64.5\nSpaceR-7B [33] 41.5 27.4 37.9 35.8 34.2 75.4\nViLaSR-7B [48] 44.6 30.2 35.1 35.7 38.7 81.1\nVST-3B-SFT [53] 57.9\u202030.2\u202035.9 52.835.8 80.9\u2020\nVST-7B-SFT [53] 60.6\u202032.0\u202039.7 50.5 39.6 83.3\u2020\nCambrian-S-3B [54] 57.3\u202025.2 32.5 39.0 28.3 76.0\u2020\nCambrian-S-7B [54] 67.5\u202025.8 39.6 40.9 33.0 80.4\u2020\nOurs\nSenseNova-SI Bagel-7B-MoT 41.6(+32.5%)36.2(+16.8%)50.8(+46.4%)50.3(+21.8%)41.6(+12.4%)83.4(+0.72%)\nSenseNova-SI Qwen3-VL-8B 62.9(+8.6%) 37.5(+20.6%) 74.8(+154.4%)48.4(+14.7%) 50.1(+9.3%) 83.5(-1.30%)\nSenseNova-SI InternVL3-2B 63.7(+93.6%)34.2(+29.1%)41.8(+11.5%) 52.6(+61.8%)36.7(+22.3%)78.9(-1.00%)\nSenseNova-SI InternVL3-8B 68.7(+63.2%) 43.3(+54.6%) 85.6(+106.3%) 54.6(+41.5%) 47.7(+16.1%) 84.9(+3.92%)\nTable 1 Evaluation on key spatial intelligence and general benchmarks.For concurrent works, VST [ 53] and Cambrian-S-7B [ 54],\n\u2020denotes benchmark results directly cited from their papers. All other results are verified on EASI-Benchmark [ 6], using the official\nprompts. MindCube\u2217denotes MindCube-Tiny. Dark purple highlights the best result and light purple indicates the second-best\nresult within Proprietary and Open-source models, respectively.\nBagel[ 14] represents a new paradigm of unified understanding and generation. We include it in our study to examine\nwhether such unified architectures can acquire strong spatial understanding capabilities.\nTraining Scheme.Each foundation model is trained for one epoch on the same dataset using 128 GPUs with batch size\n2048. Each training takes approximately three days. We employ AdamW [ 30] with a learning rate of 5 \u00d710\u22126for all\nmodel-training runs. Maximum 16 frames are sampled for video data.\n5 Experiments\n5.1 Evaluation Benchmarks.\nTo assess SenseNova-SI under a broad range of scenarios, we select five newly released benchmarks for a complementary\ncoverage of spatial intelligence.\nVSI-Bench[ 51] targetsvideo-basedvisual-spatial reasoning, evaluating a model\u2019s ability to perceive and understand\nthe 3D layout of real indoor scenes over an extended context. We uniformly sample 32 frames from each video during\ntesting.\nMMSI-Bench[ 55] extends spatial reasoning tomulti-imagesettings, requiring models to integrate spatial cues across\nmultiple views. MMSI is notably challenging: each question is manually crafted by researchers rather than mass-\ngenerated through templates.\n6\n[MMSI Attr-Appr]How many different drawers with a width greater than their height appear in total? \u2013D. 5.[MMSI Pos-Cam-Cam]Camera coordinate system +Y up, -Z forward, right-handed. How can the first image be obtained from the second image? \u2013A. Rotate a positive angle around the Y axis.                                 \n[MMSI Pos-Cam-Cam]Camera was facing the west side of the room when the first picture was taken, which direction is the camera facing in the room when the second picture is taken? \u2013C. North.\n[SITE Mov&Nav-Maze]How many right turns are there in the provided path (marked by Blue) from S (green block) to E (red block)? \u2013C. 2.\n[Cam Rotation (MessyTable)]From image 1 to 2. What is the rotation direction?-Rotated to the Left.                                 [Obj-Orient View (Ego-Exo4D)]Which egocentric view image correctly matches the exocentric view?-B.                                 \n90.4%\n23.8%\n25.6%\n66.7%\nFigure 3Observations ongeneralization abilityfrom a single data source and single task. The upper example demonstrates how\ntraining on ego-exo association task enhance performance on task required imagined first-person perspectives. The lower example\ndemonstrates how a camera rotation task, based on cross-view visual correspondence, generalizes to tasks with distinct questions and\nvisual appearances. These findings suggest the potential existence ofmeta-tasksin PT, which may enable related spatial capabilities.\nMindCube[ 57] targetsmental modelingof scenes from limited observations, probing the ability to reconstruct\noccluded spaces and simulate viewpoints. Following the official setup, we train and evaluate on the non-overlapping\nMindCube-10K and MindCube-Tiny respectively.\nViewSpatial-Bench[ 25] isolatesmulti-perspectivelocalization, evaluating a model\u2019s perspective-taking ability to reason\nacross egocentric (camera) and allocentric (human or object) viewpoints.\nSITE[ 45] provides abroad cognitive coverage, unifying over thirty datasets that span diverse facets of spatial\nintelligence. We adopt SITE to assess the generalization ability of SenseNova-SI, as it consists of highly abstract test\ncases.\n5.2 Main Results\n5.2.1 Spatial Intelligence Benchmarks.\nWe compare SenseNova-SI against leading open-source and proprietary multimodal models. As shown in Tab. 1, we\nobserve three key findings: (1) SenseNova-SI outperforms all general open-source models by clear margins, and even\nsurpasses strong proprietary ones such as GPT-5 [ 32], revealing persistent knowledge gaps in existing foundation models.\n(2) SenseNova-SI also achieves superior performance over all dedicated spatial-intelligence models, suggesting that\nalgorithmic innovation alone may be premature when the benefits of large-scale spatial data have not yet been fully\nrealized. Notably, SenseNova-SI surpasses two recent strong baselines (VST [ 53] and Cambrian-S [ 54]) even when using\ncomparable amounts of training data (Fig. 1). We attribute these gains to the inclusion of extensive perspective-taking\ndata, which is central to spatial intelligence. (3) While InternVL3 [ 60], Qwen3-VL [ 12], and Bagel [ 14] exhibit slightly\ndifferent behaviors, SenseNova-SI consistently improves upon all three families. This further validates the effectiveness\nof our scaling strategy across diverse architecture designs and pretraining paradigms.\n5.2.2 Retention of General Multimodal Capabilities.\nAs shown in Tab. 1, which includes MMBench-En [ 29] as a representative general multimodal benchmark, large-scale\ntraining aimed at cultivating spatial intelligence does not compromise the performance of SenseNova-SI on more general\nmultimodal tasks. Empirically, we find that data diversity is crucial: incorporating a wide coverage of multimodal data\nand varied general knowledge sources effectively mitigates catastrophic forgetting and preserves overall multimodal\ncompetence.\n7\n5.3 Scaling\n5.3.1 Effectiveness.\nAs shown in Fig. 1, scaling spatial intelligence data leads to steady improvements across all key capability dimensions.\nWe highlight three observations: (1) Data mixing is highly effective. By aggregating a wide collection of public datasets\nand further enlarging the spatial intelligence corpus, SenseNova-SI surpasses existing 7B spatial-intelligence baselines\nwith models one size smaller (2B) under comparable data budgets. (2) Model size impacts capability trends. While\nInternVL3 2B and 8B variants exhibit similar performance trajectories on MM, SR, and CR tasks, their behaviors\ndiverge sharply on PT tasks. We hypothesize that the 2B model lacks sufficient capacity to robustly learn viewpoint\ntransformations: a challenging but essential component of spatial intelligence. (3) Capability-wise differences reveal\ndata-driven gains. Proprietary models such as GPT-5 [ 32] are notably strong on SR, yet show clear deficiencies in\nPT. In contrast, SenseNova-SI-InternVL3-8B convincingly outperforms GPT-5 on PT, benefiting from the large-scale,\ncomprehensive perspective-taking data introduced during continued scaling. Interestingly, even though we include very\nlimited CR data during training, SenseNova-SI still gradually surpasses GPT-5 in CR performance. This suggests the\npresence of capability synergy, where advances in fundamental spatial tasks (e.g., PT and SR) transfer to more complex\nreasoning skills. We discuss this further in Sec. 5.4.\n5.3.2 Saturation.\nAs shown in Fig. 1, the performance gains gradually diminish as the amount of training data increases. While it remains\nunclear whether continued scaling will eventually reach a tipping point that triggers stronger emergent capabilities\n(though we note some early signs discussed in Sec. 5.4), we concur with the broader community that data scaling alone\nis unlikely to achieve human-level spatial intelligence [ 54]. Motivated by this, we commit to fully open-sourcing the\nweights of SenseNova-SI, allowing the community to bypass the costly scaling stage and instead focus on advancing\nalgorithmic innovation on top of a strong, spatially capable foundation.\n5.4 Capability Emergence\nWe present several interesting cases observed during scaling that may suggest early signs of emerging spatial intelligence.\n5.4.1 Spill-Over.\nModel Benchmark# Frames\n16 32 64 128\nCambrian-S-7B [54]VSI [51] 58.6 63.6 66.467.5\nVSI-Debiased [4] 49.7 55.6 59.159.9\nSenseNova-SI InternVL3-8BVSI [51] 64.6 68.768.866.3\nVSI-Debiased [4] 58.962.862.4 59.7\nTable 2Ablation on inference frames. Our model is trained\non maximum 16 frames per sample, while Cambrian-S-\n7B [54] is trained on 64/128 frames. SenseNova-SI demon-\nstrates strong extrapolation capabilities beyond the training\nnumber of frames. Interestingly, SenseNova-SI shows a\nclear lead over Cambrian-S-7B [ 54] on two benchmarks,\neven with fewer frames at inference.Large-scale mixed-domain training inevitably exposes mod-\nels to a broad distribution of scenarios, making it increasingly\ndifficult to determine whether downstream improvements\nstem from genuine, generalizable spatial reasoning or from\nincidental overlap with training data. To more rigorously ex-\namine spatial capability spill-over, we therefore conduct con-\ntrolled experiments in which models are trained on a single\ndataset and evaluated on tasks drawn from entirely different\ndomains. As shown in Fig. 3, we observe clear emergence\nand transfer of spatial udnerstanding across tasks. The view-\ntransformation dataset, constructed from Ego-Exo4D [ 18],\nrequires models to translate between egocentric and exocen-\ntric viewpoints\u2014forcing them to infer cross-view geometric\nrelationships. This ability transfers strongly to downstream\ntasks such as Maze Pathfinding and MMSI [ 55] Pos-Cam-\nCam, both of which depend on sequential viewpoint simulation and aggregating information across views. Similarly,\nthe dataset built from MessyTable [ 5] images requires models to identify shared objects and infer spatial relationships\nbetween two viewpoints. This yields notable gains on benchmark sub-categories such as MMSI [ 55] Pos-Cam-Cam and\nAttr-Appr, both of which rely on robust spatial correspondence identification between paired images.\n8\n5.4.2 Extrapolation.\nA surprising observation is that although SenseNova-SI is trained with at most 16 frames per sample, it generalizes\neffectively to sequences of 32 frames or more at inference time, as shown in Tab. 2. This suggests that SenseNova-SI\nlearns to construct coherent spatial structure rather than merely repeating patterns confined to the supervised training\nwindow. Interestingly, while SenseNova-SI does not continue to extrapolate beyond 64 frames, unlike Cambrian-\nS [54], which is explicitly trained with much longer context windows of 64 or 128 frames, SenseNova-SI nevertheless\nachieves performance comparable to Cambrian-S while using substantially fewer frames at inference. This indicates\nthat SenseNova-SI possesses a stronger spatial understanding capability that enables it to form meaningful connections\nacross larger temporal gaps, without relying on densely sampled frame sequences.\n5.5 Overfit and Shortcut Analysis\nModels Standard Soft cir. Hard cir. w/o. Vis.\nMindCube-SFT-RawQA [54] 51.7 45.8 23.1 50.7\nSenseNova-SI InternVL3-8B 85.6 84.0 75.6 52.5\nTable 3Analysis on MindCube [ 57].Soft cir.andHard cir.\nstands for Soft circular and Hard circular following [ 6].w/o.\nVis.indicates testing without visual as input, following [ 41].Recent studies [ 4,41] have shown that multimodal models\ncan exploit language shortcuts to answer questions without\ngenuine visual reasoning. To ensure that the improvements\nof SenseNova-SI are not due to overfitting to QA text, we\nconduct targeted analyses on VSI [51] and MindCube [57].\nThe recently proposed VSI-Debiased [ 4] is a specifically de-\nsigned variant of VSI to eliminate text-only shortcuts by re-\nmoving questions that can be answered correctly without\nvisual understanding. As shown in Tab. 2, when evaluated on VSI-Debiased, SenseNova-SI exhibits a substantially\nsmaller performance drop compared to Cambrian-S-7B [ 54], indicating that SenseNova-SI relies less on textual heuristics\nand more on spatially grounded understanding.\nFor MindCube, we follow the protocol in [ 41] and evaluate modelswithout visual inputs. Surprisingly, as shown in\nTab. 3, the previous open-source SoTA on MindCube,MindCube-RawQA-SFT[ 57] achieves a score of 50.7 without any\nimages, which is almost identical to its performance with full visual inputs, revealing a heavy dependence on language\npriors rather than visual reasoning.\nIn contrast, SenseNova-SI drops from 85.6 to 52.5 in the no-vision setting, validating that it genuinely uses visual\ninformation rather than relying on language shortcuts. Notably, both models converge to a score around 50 in the\nabsence of images, underscoring the importance of debiasing benchmarks, as argued in [4].\nTo further verify that SenseNova-SI does not overfit to text option ordering, we conduct circular tests [ 6,27,29],\nwhich reorders the choices in the questions to eliminate dependency on certain text patterns. As reported in Tab. 3,\nSenseNova-SI exhibits minimal degradation under the Soft circular test. Even in the Hard circular test, which requires\nrobust handling of all rotations of answer choices, SenseNova-SI drops only 10 points, whereas MindCube-RawQA-SFT\ndrops nearly 30 points. This demonstrates that SenseNova-SI is far less sensitive to superficial text patterns and possesses\nmore stable, input-grounded reasoning.\n5.6 Spatial Chain-of-Thought\nCoT StyleAverage #\nOutput TokenVSI-Bench: Obj. Rel. Direction\nOverall Easy Medium Hard\nInternVL3-8B 1 39.3 48.8 47.0 21.9\nNo CoT 3.4 54.9 62.2 55.8 46.6\nCoT-GPT-5 1070.7 40.0 41.4 43.1 36.1\nCoT-MindCube-Aug-CGMap 1490.6 39.9 45.9 42.7 33.7\nCoT-SenseNova-SI-CGMap 2262.8 47.9 53.9 51.3 41.0\nTable 4Impact of Chain-of-Thought (CoT) formats on the\nObject Relative Direction subset of VSI-Bench.Chain-of-Thought (CoT) [ 46] has become thede factoap-\nproach for complex reasoning tasks. However, despite nu-\nmerous recent attempts [ 20,43,55,57], incorporating CoT\nvariants typically yields only marginal gains (often \u223c2%),\nwhich are consistently overshadowed by improvements de-\nrived from large-scale curated spatial datasets.\nIn Tab. 4, we present a preliminary evaluation of different\nCoT styles. We examine three paradigms: (1) CoT-GPT-5,\nwhich directly uses a large language model (GPT-5 [ 32]) to\nannotate CoT given the question and ground-truth answer; (2)\nCoT-MindCube-Aug-CGMap, which follows MindCube [ 57] and constructs a JSON-style cognition map (CogMap)\nwithin the CoT; (3) CoT-SenseNova-SI-CGMap, our extended CogMap that provides step-by-step tracking of objects\n9\nacross frames, maps them to a world coordinate system with precise (rather than coarse-grid) coordinates, and reasons\nabout relative spatial relationships more explicitly.\nWe train each variant on roughly 100K examples, reasonably large compared to typical CoT studies, and evaluate on\nVSI\u2019s Object Relative Direction task, a challenging subset known to impede strong baselines such as InternVL3. We find\nthat (1) our elaborated CoT achieves the highest improvement among the three, but (2) all CoT variants yield limited\nabsolute gains, insufficient to justify their computational overhead, especially given the extra tokens required during\nboth training and inference.\nWe acknowledge that these results are preliminary, and it is too early to draw definitive conclusions. Nevertheless, our\nfindings suggest that while carefully engineered CoT can offer modest benefits and should not be dismissed outright,\ntext-based reasoning alone may be neither the most efficient nor the most effective paradigm for spatial intelligence.\nThis may signal the need for a broader paradigm shift beyond conventional CoT.\n5.7 Downstream Task\nGPT-4o InternVL3-8B SenseNova-SI InternVL3-8B\nOP SIP OP SIP OP SIP\n37.5 45.8 10.4 20.816.6(+59.6%)33.3(+60.0%)\nTable 5Success rate onSpatialsubset of Embodied-\nBench [ 52].OP: Official Prompt;SIP: Spatial-Intelligence-\noriented Prompt.To evaluate the practical utility of SenseNova-SI \u2019s enhanced\nspatial intelligence, we conduct downstream robot manipu-\nlation experiments on EmbodiedBench [ 52], focusing specif-\nically on its spatial subset. In this setting, SenseNova-SI\nis instantiated as an embodied agent that controls a virtual\nFranka Panda robot to execute user instructions containing\nrich spatial language such as \"left\", \"on top of\", \"rear\", and\n\"horizontal\". Importantly,no finetuningof SenseNova-SI is\nperformed for this evaluation. Quantitative results for the\nspatial subset are shown in Tab. 5.\nStack the left triangular prism on top of the right cylinder.\nPut the left star into the shape sorter.\nFigure 4Visualization of the manipulation task rollout in\nEmbodiedBench [ 52], performed by the embodied agent\npowered by SenseNova-SI.We report success rates under two prompting settings: the of-\nficial prompt (OP) and a spatial-intelligence-oriented prompt\n(SIP). OP supplies bounding-box coordinates extracted from\nthe input image, whereas SIP enriches OP with additional\nobject-grounding cues to reduce ambiguity from object recog-\nnition and better isolate spatial-reasoning performance.\nAcross both OP and SIP, SenseNova-SI delivers substantial\nimprovements, demonstrating that enhanced spatial intelli-\ngence directly benefits embodied manipulation. We observe\nthat SenseNova-SI more reliably identifies key spatial cues,\nenabling more accurate reasoning and more consistent action\nplanning.\nRepresentative rollouts are visualized in Fig. 4. These exam-\nples illustrate that SenseNova-SI effectively integrates spatial\ninformation from both language instructions and visual obser-\nvations, plans coherent motion trajectories, and generates action sequences that allow the robot to successfully complete\nthe tasks.\n6 Conclusion\nIn this work, we validate the effectiveness of scaling spatial intelligence across multiple multimodal foundation models,\nand achieve significant performance gains across the board. We further validate that the enhanced foundation models\nretain their general capabilities, and start to develop generalization capabilities that were not possible without training on\nlarge-scale, diverse data. We hope our study lays a solid foundation for future research on developing spatial intelligence\nin multimodal foundation models.\n10\nReferences\n[1]Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. Vqa:\nVisual question answering. In Proceedings oftheIEEE/CVF International Conference onComputer Vision, December 2015.\n[2]Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.\nQwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint\narXiv:2308.12966, 2023.\n[3]Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang,\nHumen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu,\nJiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical\nreport. arXiv preprint arXiv:2502.13923, 2025.\n[4]Ellis Brown, Jihan Yang, Shusheng Yang, Rob Fergus, and Saining Xie. Benchmark designers should\" train on the test set\" to\nexpose exploitable non-visual shortcuts. arXiv preprint arXiv:2511.04655, 2025.\n[5]Zhongang Cai, Junzhe Zhang, Daxuan Ren, Cunjun Yu, Haiyu Zhao, Shuai Yi, Chai Kiat Yeo, and Chen Change Loy.\nMessytable: Instance association in multiple camera views. In Proceedings oftheEuropean Conference onComputer Vision ,\npages 1\u201316. Springer, 2020.\n[6]Zhongang Cai, Yubo Wang, Qingping Sun, Ruisi Wang, Chenyang Gu, Wanqi Yin, Zhiqian Lin, Zhitao Yang, Chen Wei,\nXuanke Shi, et al. Has gpt-5 achieved spatial intelligence? an empirical study. arXiv preprint arXiv:2508.13142, 2025.\n[7]Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng,\nand Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017.\n[8]Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing\nvision-language models with spatial reasoning capabilities. In Proceedings oftheIEEE/CVF Conference onComputer Vision\nandPattern Recognition, pages 14455\u201314465, 2024.\n[9]Zhangquan Chen, Manyuan Zhang, Xinlei Yu, Xufang Luo, Mingze Sun, Zihao Pan, Yan Feng, Peng Pei, Xunliang Cai,\nand Ruqi Huang. Think with 3d: Geometric imagination grounded spatial reasoning from limited views. arXiv preprint\narXiv:2510.18632, 2025.\n[10] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei\nLu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings ofthe\nIEEE/CVF Conference onComputer Vision andPattern Recognition,, pages 24185\u201324198, 2024.\n[11] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt:\nGrounded spatial reasoning in vision-language models. Advances inNeural Information Processing Systems , 37:135062\u2013\n135093, 2024.\n[12] QwenLM Team (Alibaba Cloud). Qwen3-vl: Multimodal large language model series. https://github.com/QwenLM/\nQwen3-VL, 2025. GitHub repository; accessed: 2025-11-14.\n[13] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-\nannotated 3d reconstructions of indoor scenes. In Proceedings oftheIEEE/CVF International Conference onComputer Vision ,\npages 5828\u20135839, 2017.\n[14] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang\nSong, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683 ,\n2025.\n[15] Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan,\net al. Vlm-3r: Vision-language models augmented with instruction-aligned 3d reconstruction. arXiv preprint arXiv:2505.20279 ,\n2025.\n[16] Qi Feng. Towards visuospatial cognition via hierarchical fusion of visual experts. arXiv preprint arXiv:2505.12363, 2025.\n[17] Ankit Goyal, Kaiyu Yang, Dawei Yang, and Jia Deng. Rel3d: A minimally contrastive benchmark for grounding spatial relations\nin 3d. Advances inNeural Information Processing Systems, 33:10514\u201310525, 2020.\n[18] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh,\nVijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and\n11\nthird-person perspectives. In Proceedings oftheIEEE/CVF Conference onComputer Vision andPattern Recognition , pages\n19383\u201319400, 2024.\n[19] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question\nanswering. In Proceedings oftheIEEE/CVF Conference onComputer Vision andPattern Recognition , pages 6700\u20136709,\n2019.\n[20] Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, and Li Yi. Omnispatial: Towards\ncomprehensive spatial reasoning benchmark for vision language models. arXiv preprint arXiv:2506.03135, 2025.\n[21] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A\ndiagnostic dataset for compositional language and elementary visual reasoning. In Proceedings oftheIEEE/CVF Conference\nonComputer Vision andPattern Recognition,, pages 2901\u20132910, 2017.\n[22] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan P Foster,\nPannag R Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy\nLiang, and Chelsea Finn. Openvla: An open-source vision-language-action model. In Pulkit Agrawal, Oliver Kroemer, and\nWolfram Burgard, editors, Proceedings oftheConference onRobot Learning , volume 270 of Proceedings ofMachine Learning\nResearch, pages 2679\u20132713. PMLR, 06\u201309 Nov 2025.\n[23] Justin Lazarow, David Griffiths, Gefen Kohavi, Francisco Crespo, and Afshin Dehghan. Cubify anything: Scaling indoor\n3d object detection. In Proceedings oftheIEEE/CVF Conference onComputer Vision andPattern Recognition , pages 22225\u2013\n22233, 2025.\n[24] Jae Hee Lee, Matthias Kerzel, Kyra Ahrens, Cornelius Weber, and Stefan Wermter. What is right for me is not yet right\nfor you: A dataset for grounding relative directions via multi-task learning. In Lud De Raedt, editor, Proceedings ofthe\nThirty-First International Joint Conference onArtificial Intelligence , pages 1039\u20131045. International Joint Conferences on\nArtificial Intelligence Organization, 7 2022.\n[25] Dingming Li, Hongxing Li, Zixuan Wang, Yuchen Yan, Hang Zhang, Siqi Chen, Guiyang Hou, Shengpei Jiang, Wenqi Zhang,\nYongliang Shen, et al. Viewspatial-bench: Evaluating multi-perspective spatial localization in vision-language models. arXiv\npreprint arXiv:2505.21500, 2025.\n[26] Hongxing Li, Dingming Li, Zixuan Wang, Yuchen Yan, Hang Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao,\nand Yueting Zhuang. Spatialladder: Progressive training for spatial reasoning in vision-language models. arXiv preprint\narXiv:2510.08531, 2025.\n[27] Yijiang Li, Qingying Gao, Tianwei Zhao, Bingyang Wang, Haoran Sun, Haiyun Lyu, Robert D Hawkins, Nuno Vasconcelos,\nTal Golan, Dezhi Luo, et al. Core knowledge deficits in multi-modal language models. arXiv preprint arXiv:2410.10855 , 2024.\n[28] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions oftheAssociation forComputational\nLinguistics, 11:635\u2013651, 2023.\n[29] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In Proceedings oftheEuropean Conference on\nComputer Vision, pages 216\u2013233. Springer, 2024.\n[30] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n[31] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: A\nnew benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021.\n[32] OpenAI. GPT-5 System Card. Technical report, OpenAI, August 2025. Accessed: 2025-08-10.\n[33] Kun Ouyang, Yuanxin Liu, Haoning Wu, Yi Liu, Hao Zhou, Jie Zhou, Fandong Meng, and Xu Sun. Spacer: Reinforcing mllms\nin video spatial reasoning. arXiv preprint arXiv:2504.01805, 2025.\n[34] Wujian Peng, Sicheng Xie, Zuyao You, Shiyi Lan, and Zuxuan Wu. Synthesize diagnose and optimize: Towards fine-grained\nvision-language understanding. In Proceedings oftheIEEE/CVF Conference onComputer Vision andPattern Recognition ,\npages 13279\u201313288, 2024.\n[35] Arijit Ray, Jiafei Duan, Ellis Brown, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi,\nBryan A Plummer, Ranjay Krishna, et al. Sat: Dynamic spatial aptitude training for multimodal language models. arXiv\npreprint arXiv:2412.07755, 2024.\n12\n[36] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao. Sun rgb-d: A rgb-d scene understanding benchmark suite. In\nProceedings oftheIEEE/CVF Conference onComputer Vision andPattern Recognition, pages 567\u2013576, 2015.\n[37] ByteDance Seed Team. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025.\n[38] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M\nDai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 ,\n2023.\n[39] Haochen Wang, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiangyu Zhang, and Zhaoxiang Zhang. Ross3d: Reconstructive\nvisual instruction tuning with 3d-awareness. Proceedings oftheIEEE/CVF International Conference onComputer Vision ,\n2025.\n[40] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. VGGT: Visual\ngeometry grounded transformer. In Proceedings oftheIEEE/CVF Conference onComputer Vision andPattern Recognition, ,\npages 5294\u20135306, 2025.\n[41] Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Sharon Li, and Neel Joshi. Is a picture worth a thousand\nwords? delving into spatial reasoning for vision language models. Advances inNeural Information Processing Systems , 37:\n75392\u201375421, 2024.\n[42] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge,\nYang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin.\nQwen2-vl: Enhancing vision-language model\u2019s perception of the world at any resolution. arXiv preprint arXiv:2409.12191 ,\n2024.\n[43] Siting Wang, Luoyang Sun, Cheng Deng, Kun Shao, Minnan Pei, Zheng Tian, Haifeng Zhang, and Jun Wang. Spatialviz-bench:\nAutomatically generated spatial visualization reasoning tasks for mllms. arXiv preprint arXiv:2507.07610, 2025.\n[44] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye,\nJie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint\narXiv:2508.18265, 2025.\n[45] Wenqi Wang, Reuben Tan, Pengyue Zhu, Jianwei Yang, Zhengyuan Yang, Lijuan Wang, Andrey Kolobov, Jianfeng Gao, and\nBoqing Gong. Site: towards spatial intelligence thorough evaluation. arXiv preprint arXiv:2505.05456, 2025.\n[46] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought\nprompting elicits reasoning in large language models. Advances inNeural Information Processing Systems , 35:24824\u201324837,\n2022.\n[47] Diankun Wu, Fangfu Liu, Yi-Hsin Hung, and Yueqi Duan. Spatial-mllm: Boosting mllm capabilities in visual-based spatial\nintelligence. arXiv preprint arXiv:2505.23747, 2025.\n[48] Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. Reinforcing spatial reasoning\nin vision-language models with interwoven thinking and visual drawing. arXiv preprint arXiv:2506.09965, 2025.\n[49] xAI. Grok 4, 7 2025. URLhttps://x.ai/news/grok-4. Model announcement.\n[50] Runsen Xu, Weiyao Wang, Hao Tang, Xingyu Chen, Xiaodong Wang, Fu-Jen Chu, Dahua Lin, Matt Feiszli, and Kevin J\nLiang. Multi-spatialmllm: Multi-frame spatial understanding with multi-modal large language models. arXiv preprint\narXiv:2505.17015, 2025.\n[51] Jihan Yang, Shusheng Yang, Anjali W Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal\nlarge language models see, remember, and recall spaces. In Proceedings oftheIEEE/CVF Conference onComputer Vision\nandPattern Recognition,, pages 10632\u201310643, 2025.\n[52] Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella,\nMarziyeh Movahedi, Manling Li, et al. Embodiedbench: Comprehensive benchmarking multi-modal large language models for\nvision-driven embodied agents. arXiv preprint arXiv:2502.09560, 2025.\n[53] Rui Yang, Ziyu Zhu, Yanwei Li, Jingjia Huang, Shen Yan, Siyuan Zhou, Zhe Liu, Xiangtai Li, Shuangye Li, Wenqian Wang,\nYi Lin, and Hengshuang Zhao. Visual spatial tuning. arXiv preprint arXiv:2511.05491, 2025.\n[54] Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan\nWang, et al. Cambrian-s: Towards spatial supersensing in video. arXiv preprint arXiv:2511.04670, 2025.\n13\n[55] Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu\nYue, et al. Mmsi-bench: A benchmark for multi-image spatial intelligence. arXiv preprint arXiv:2505.23764, 2025.\n[56] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nie\u00dfner, and Angela Dai. Scannet++: A high-fidelity dataset of 3d indoor\nscenes. In Proceedings oftheIEEE/CVF International Conference onComputer Vision, pages 12\u201322, 2023.\n[57] Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chan-\ndrasegaran, Han Liu, Ranjay Krishna, et al. Spatial mental modeling from limited views. In Proceedings oftheIEEE/CVF\nInternational Conference onComputer Vision Workshop, 2025.\n[58] Songsong Yu, Yuxin Chen, Hao Ju, Lianjie Jia, Fuxi Zhang, Shaofei Huang, Yuhan Wu, Rundi Cui, Binghao Ran, Zaibin Zhang,\net al. How far are vlms from visual spatial intelligence? a benchmark-driven perspective. arXiv preprint arXiv:2509.18905 ,\n2025.\n[59] Weichen Zhang, Zile Zhou, Zhiheng Zheng, Chen Gao, Jinqiang Cui, Yong Li, Xinlei Chen, and Xiao-Ping Zhang. Open3dvqa:\nA benchmark for comprehensive spatial reasoning with multimodal large language model in open space. arXiv preprint\narXiv:2503.11094, 2025.\n[60] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie\nShao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint\narXiv:2504.10479, 2025.\n[61] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid,\net al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Proceedings oftheConference on\nRobot Learning, pages 2165\u20132183. PMLR, 2023.\n14\n",
    "title": "Scaling Spatial Intelligence with Multimodal Foundation Models",
    "authors": [
      "Zhongang Cai",
      "Ruisi Wang",
      "Chenyang Gu",
      "Fanyi Pu",
      "Junxiang Xu",
      "Yubo Wang",
      "Wanqi Yin",
      "Zhitao Yang",
      "Chen Wei",
      "Qingping Sun",
      "Tongxi Zhou",
      "Jiaqi Li",
      "Hui En Pang",
      "Oscar Qian",
      "Yukun Wei",
      "Zhiqian Lin",
      "Xuanke Shi",
      "Kewang Deng",
      "Xiaoyang Han",
      "Zukai Chen",
      "Xiangyu Fan",
      "Hanming Deng",
      "Lewei Lu",
      "Liang Pan",
      "Bo Li",
      "Ziwei Liu",
      "Quan Wang",
      "Dahua Lin",
      "Lei Yang"
    ],
    "published": "2025-11-17",
    "arxiv_id": "2511.13719v1",
    "num_pages": 14,
    "num_chars": 56332
  },
  {
    "text": "UNSAMV2:\nSelf-Supervised Learning Enables Segment Anything at Any Granularity\nJunwei Yu Trevor Darrell XuDong Wang*\nUC Berkeley\nProject Page: https://yujunwei04.github.io/UnSAMv2-Project-Page/\n0.11.0Granularity = 0.10.11.0Granularity = 0.280.11.0Granularity = 0.660.11.0Granularity = 1.0a) Interactive Segmentation with Single Point and Granularity Valueb) Whole-Image Segmentation with Granularity Value\nc) Video Segmentation with Single Point and Granularity Value0.11.0Granularity = 0.35\n0.11.0Granularity = 0.75\nd) Unsupervised Granularity-Aware Divide and Conquer PipelineFine-grained, Granularity\u2208[0.1,0.3)Medium Size, Granularity\u2208[0.3,0.8)Whole Entity, Granularity\u2208[0.8,1.0]Bottom-upTop-down\n0.11.0Granularity = 0.200.11.0Granularity = 0.310.11.0Granularity = 0.640.11.0Granularity = 0.88\nFigure 1.What is an object? The notion has long been debated: should it follow a model\u2019s learned semantics or an annotator\u2019s subjective\njudgment? UNSAMV2 takes a third path, granting users full flexibility to define objectness through promptable segmentation with a single\npoint and a continuous, controllable granularity score. Built on SAM-2 [35], UNSAMV2 introduces a granularity-aware, self-supervised\ntraining pipeline based on divide-and-conquer pseudo-labels [47]. Trained on just 6000 unlabeled images, it segments anything from fine-\ngrained parts to holistic objects, achieving state-of-the-art performance across interactive, whole-image, and video segmentation tasks.\nAbstract\nThe Segment Anything Model (SAM) family has become\na widely adopted vision foundation model, but its abil-\nity to control segmentation granularity remains limited.\nUsers often need to refine results manually \u2014 by adding\nmore prompts or selecting from pre-generated masks \u2014 to\nachieve the desired level of detail. This process can be am-\nbiguous, as the same prompt may correspond to several\nplausible masks, and collecting dense annotations across\nall granularities is prohibitively expensive, making super-\n*Corresponding authorvised solutions infeasible. To address this limitation, we\nintroduceUNSAMV2which enables segment anything at\nany granularity without human annotations.UNSAMV2\nextends the divide-and-conquer strategy of UnSAM by dis-\ncovering abundant mask-granularity pairs and introducing\na novel granularity control embedding that enables pre-\ncise, continuous control over segmentation scale. Remark-\nably, with only 6Kunlabeled images and 0.02% additional\nparameters,UNSAMV2substantially enhances SAM-2,\nachieving segment anything at any granularity across inter-\nactive, whole-image, and video segmentation tasks. Evalu-\nated on over 11 benchmarks,UNSAMV2improves NoC 90\n(5.69\u21924.75), 1-IoU (58.0\u219273.1), and AR 1000 (49.6\u2192\n68.3), showing that small amounts of unlabeled data with\n1arXiv:2511.13714v1  [cs.CV]  17 Nov 2025\ngranularity-aware self-supervised learning method can un-\nlock the potential of vision foundation models.\n1. Introduction\nWhat is an object? This question has long been debated in\nboth vision and cognition. Should an object be defined by\na model\u2019s learned semantics or by an annotator\u2019s subjective\njudgment? We propose a third perspective: granting users\nfull flexibility to define objectness through a single point\nprompt and a continuous granularity score.\nImagine clicking on a single point in an image and\nsmoothly adjusting a slider that controls segmentation gran-\nularity. At low granularity values, the model reveals fine-\ngrained parts with precise boundaries; as the granularity in-\ncreases, it gradually merges regions into larger and more\nsemantically coherent entities.\nIn essence, this transforms SAM\u2019s [24] three discrete\nmask hypotheses into a continuous granularity axis capable\nof producing multiple masks per point, each corresponding\nto a user-defined granularity. Such controllable segmenta-\ntion enables users to flexibly define what constitutes an \u201cob-\nject\u201d for their specific task, whether it involves part-level\nanalysis, instance grouping, or large-scale region editing.\nThe emergence of the Segment Anything family [24, 35]\nhas positioned SAM as a vision foundation model, signif-\nicantly advancing diverse tasks such as video object track-\ning [51, 56], multi-scale perception [18, 48], and compo-\nsitional understanding [6, 9, 45]. However, SAM and its\nsuccessors rely heavily onsupervised learningfrom SA-\n1B dataset [24]. This pipeline directly ties the model\u2019s no-\ntion of an \u201cobject\u201d to human annotation bias and limits its\noutput to three discrete mask hypotheses per prompt. As\na result, segmentation in SAM isdefined by supervision\nrather thandiscovered from data. This design assumes a\nshallow hierarchy of objectness, while real-world scenes\nexhibit complex, nested part\u2013whole structures that cannot\nbe captured through fixed human labels. Although SA-1B\ncovers a broad range of object sizes, it lacks explicit corre-\nspondences between instance- and part-level masks, making\nit difficult for supervised models to learn how granularity\nshould vary continuously.\nWe argue that segmentation granularity should be\nlearned through unsupervised learning, which allows mod-\nels to infer object hierarchies directly from image statis-\ntics instead of depending on predefined labels.To this end,\nwe introduceUNSAMV2, a self-supervised framework that\nenables segmentation at any granularity without human su-\npervised labels. UNSAMV2 learns a continuous represen-\ntation of granularity that bridges the gap between parts and\nwholes, giving users full control over segmentation masks.\nUNSAMV2 builds upon UnSAM [47], which introduced\nan unsupervised divide-and-conquer strategy for discover-\nFigure 2. UNSAMV2 achieves state-of-the-art performance\nacross interactive segmentation benchmarks.Across multi-\nple datasets, UNSAMV2 consistently outperforms SAM-2 and\nprior methods, turning segmentation into a controllable and inter-\npretable process rather than a fixed prediction.\ning hierarchical masks. While UnSAM focused on unsu-\npervised hierarchy construction,UNSAMV2extends this\nidea togranularity-controllable segmentation. In thedivide\nstage, we use the normalized-cut method MaskCut [44] to\nextract instance-level masks. In theconquerstage, we re-\ncursively merge similar pixels within each instance to dis-\ncover finer parts, forming hierarchical pseudo-labels that\nencode relative scale. From these hierarchies, we compute\na continuous granularity scalar for each mask, representing\nits position along the part\u2013whole continuum.\nWe then augment SAM-2 [35] with a lightweight granu-\nlarity encoder and a granularity-aware mask token. Given a\npoint prompt and a granularity scalarg, UNSAMV2 pre-\ndicts the mask corresponding to the desired granularity,\nturning segmentation into a controllable function of scale.\nTraining only the lightweight SAM-2 decoder for four hours\nwith 2 A100 GPUs on 6,000 unlabeled images (0.02% ad-\nditional parameters) enables smooth interpolation between\ncoarse and fine structures and reveals the latent hierarchy\nwithin SAM\u2019s feature space.\nAcross interactive, whole-image, and video segmenta-\ntion benchmarks, UNSAMV2 consistently surpasses SAM-\n2 [35] and previous state-of-the-art promptable segmenta-\ntion methods. Evaluated on more than 11 widely used\ndatasets, including SA-1B, COCO, and PartImageNet, UN-\n2\nSAMV2 improves NoC 90from 5.69 to 4.75, 1-IoU from\n58.0 to 73.1, and AR 1000 from 49.6 to 68.3, all achieved\nusing only unlabeled data. The resulting model empowers\nusers to define their own notion of objectness and to explore\nsegmentation as a continuous, controllable process rather\nthan a static prediction.\nContributions.(i)We propose UNSAMV2, a granularity-\ncontrollable segmentation framework that enables continu-\nous control of mask granularity from a single point prompt\nand scalar input.(ii)We develop an unsupervised granu-\nlarity discovery pipeline that learns hierarchical instance\u2013\npart structures and assigns each mask a continuous scale,\napplicable to various promptable segmentation models.\n(iii)Trained on only 6,000 unlabeled images, UNSAMV2\nachieves state-of-the-art results across interactive, whole-\nimage, and video segmentation benchmarks.\n2. Related Work\nMulti-Granularity Segmentation.Segment Anything\nproject [24, 35] has greatly advanced segmentation perfor-\nmance by leveraging large-scale human-annotated data and\nextensive compute. Extensions such as Semantic-SAM [25]\nimprove fine-grained predictions through a multiple-choice\nlearning design [12, 26]. However, these approaches con-\nstrain point-prompt predictions to a fixed number of candi-\ndate masks, forcing users to manually select from limited\noutputs or give additional prompts. This restriction high-\nlights the need for explicit control over mask granularity.\nRecent work [28, 42] has begun to tackle such ambiguity.\nGARField [23] and SAMPart3D [54, 55] address scale am-\nbiguity in 3D scene decomposition via absolute scale condi-\ntioning, while GraCo [58] achieves granularity-controllable\ninteractive segmentation by extending SimpleClick [29]\nwith discrete granularity inputs.\nIn contrast, our UNSAMV2 tackles mask ambiguity in\na fully self-supervised manner by treating granularity as a\ncontinuous, relative concept. We enable granularity-aware\nsegmentation within the widely adopted SAM framework\nwithout requiring manual annotation.\nSelf-Supervised Learning and Unsupervised Segmen-\ntation.Self-supervised learning (SSL) methods such as\nMAE [16], JEPA [2], and DINO [5, 32, 40] demonstrate that\nlarge-scale pretraining can endow vision transformers with\nstrong semantics-aware representations, benefiting a wide\nrange of downstream tasks [10, 19, 20, 50, 52, 53, 57]. In\nparallel, unsupervised segmentation has gained lots of at-\ntention [1, 13, 14, 22, 31, 39, 43, 49, 60]. CutLER [44], as a\nrecent foundational work of unsupervised image segmenta-\ntion, greatly advanced unsupervised instance segmentation\nby introducing MaskCut, a normalized-cuts\u2013based strat-\negy [37] that iteratively extracts multiple objects from im-\nages. VideoCutLER [46] extended this framework to video\nthrough a cut\u2013synthesize\u2013learn pipeline. CutS3D [38] in-\nGranularity = 0.1Granularity = 0.14Granularity = 0.22Granularity = 0.39\nGranularity = 0.68Granularity = 0.75Granularity = 0.81Granularity = 1.0\nAny Granularity ScaleThree random masksNo Granularity ControlSAM-2UnSAMv2 (Ours)Figure 3. From ambiguity to control.Without granularity input,\nSAM-2 yields up to three masks per point, requiring users to man-\nually choose one. UNSAMV2 resolves this ambiguity by intro-\nducing a continuous granularity variable, allowing users to obtain\nthe intended object at any scale with a single prompt. This simple\naddition turns segmentation from a discrete guess into a continu-\nous, controllable reasoning process.\ntroduces the concept of projecting 2D image into 3D space\nvia ZoeDepth [3] to enhance unsupervised segmentation\nperformance on overlapping objects. SOHES [4] adopts\na bottom-up merging scheme, grouping pixels based on\ncosine similarity to progressively discover objects. More\nrecently, UnSAM [47] introduced a divide-and-conquer\nparadigm to generate hierarchical pseudo labels.\nBuilding on these efforts, UnSAMv2 leverages the hi-\nerarchical mask discovery perspective in the divide-and-\nconquer [47] pipeline and extends it to assign an explicit\ngranularity scale for each pseudo mask. This enables unsu-\npervised segmentation at arbitrary levels of detail, realizing\nthe goal of segment anything at any granularity.\n3. Method\nWe presentUNSAMV2, a self-supervised framework that\nenables segmentation at arbitrary levels of granularity with-\nout human annotations. Unlike the supervised SAM [24]\npipeline, which depends on human-labeled object masks,\nUNSAMV2 learns granularity directly from image statis-\ntics through a hierarchy-aware divide-and-conquer process.\nThis enables segmentation granularity to be continuously\ncontrolled by a single scalar input, rather than restricted to\na fixed number of discrete mask tokens.\nWe first review prior unsupervised segmentation meth-\nods (Sec. 3.1) and the limitations of supervised training\nparadigms (Sec. 3.2). We then present our granularity-\naware divide-and-conquer algorithm that automatically\nconstructs mask\u2013granularity pairs from unlabeled data\n(Sec. 3.3). Next, we describe the architectural design that\nempowers any promptable segmentation model to interpret\nand control segmentation granularity (Sec. 3.4). We then\nbriefly discuss the difference with prior supervised learn-\ning works (Sec. 3.5). Finally, we present UNSAMV2+, a\nlightly supervised variant that integrates SA-1B annotations\nto further refine the granularity learning process (Sec. 3.6).\n3\n3.1. Preliminaries\nUnSAM, MaskCut, and SOHES.UnSAM [47] intro-\nduced a divide-and-conquer strategy to generate pseudo\nmasks without supervision. In thedividestage, a cut-based\nsegmentation method MaskCut [44] is applied to obtain\ninstance/semantic-level masks. Then, inspired by bottom-\nup hierarchical segmentation,e.g., SOHES [4], theconquer\nstage iteratively merges similar pixels under a sequence\nof thresholds, constructing part\u2013whole hierarchies. For-\nmally, for a local image regionI local, patch-level features\nK=DINO(I local)are extracted using DINO [40]. Neigh-\nboring patches are merged according to the cosine similarity\nof their DINO features against thresholds\u03b8 1, . . . , \u03b8 l, yield-\ning part-level masks nested inside instances. This process\nproduces a discrete but rich hierarchy of mask granularity.\nSegment Anything family.Segment Anything models\n(SAM and SAM-2) [24, 35] have advanced promptable seg-\nmentation with a scalable encoder\u2013decoder design.(1) Im-\nage encoder:a ViT that maps an input image to multi-\nscale dense embeddings while preserving spatial structure.\n(2) Prompt encoder:embeddings for user inputs such as\npoints, boxes, or masks that condition the segmentation pro-\ncess and guide attention to regions of interest.(3) Mask\ndecoder:a lightweight transformer that fuses image and\nprompt features to predict segmentation masks.\nDespite these strengths, the training pipeline is fullysu-\npervisedon SA-1B [24], which ties the notion of objectness\nto human-labeled masks. Moreover, the decoder employs\nthree fixed mask tokens (small, medium, large), producing\nat most three hypotheses per prompt. This discretization\nlimits controllable granularity and discourages hierarchical\nreasoning about parts and wholes, motivating an unsuper-\nvised formulation that learns granularity from data rather\nthan from fixed labels.\n3.2. Limitations of SAM\u2019s Supervised Paradigm\nLack of granularity control.When a single point corre-\nsponds to multiple plausible objects (e.g., a part versus the\nwhole), SAM generates up to three discrete masks and re-\nquires manual selection by the user. Without an explicit\ngranularity variable, the model cannot traverse scales con-\ntinuously\u2014fine details and coarse structures remain discon-\nnected. This limitation not only reduces efficiency in inter-\nactive segmentation but also prevents smooth, interpretable\ncontrol over the level of detail.\nLack of hierarchical reasoning.Supervised training on\nhuman-labeled masks encourages SAM to learn a flat ob-\nject representation, where parts and instance/semantic-level\nsegments are treated as isolated entities rather than com-\nponents within a hierarchy. As a consequence, SAM lacks\nstructural awareness and fails to capture relationships across\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nGranularity102\n101\n100101102Mask Percentage - Log Scale59.8%\n23.4%\n7.8%\n3.1%\n1.6%\n1.0%\n0.69%\n0.40%2.2%Sparse\nRegion \nFigure 4. Granularity distribution of discovered masks.Our\ndivide-and-conquer pipeline produces a rich, left-tailed hierar-\nchy of pseudo-masks, dominated by fine-grained structures. De-\nspite this imbalance, UNSAMV2 learns stable semantics across all\nscales. Hierarchical perception can emerge from unlabeled data!\nscales. It struggles to segment scenes at intermediate lev-\nels of detail or to uncover the nested hierarchical structure\nof visual scenes (Fig. 3). This limitation underscores the\nnecessity ofunsupervised learning, which can recover hier-\narchical dependencies directly from image statistics rather\nthan relying on human annotations.\n3.3. Granularity-Aware Divide-and-Conquer\nWe extend UnSAM\u2019s divide-and-conquer framework to au-\ntomatically construct dense mask\u2013granularity pairs from\nunlabeled images. The process consists of four stages.\nStage 1: Instance discovery via N-Cuts.We employ the\nnormalized-cut-based method CutLER [44] to generate seg-\nmentation masksM={m 1, . . . , m n}at varying levels of\ngranularity. We filter out noisy CutLER mask outputs with\na confidence threshold\u03c4 conf,\nMhigh={m i|conf(m i)\u2265\u03c4 conf},\u2200i\u2208[n].(1)\nStage 2: Instance\u2013part relationship discovery.We iden-\ntify instance-level masksM inst\u2286 M highbased on two cri-\nteria: (i) their area-to-image-area ratio exceeds the thresh-\nold\u03c4 area, and (ii) they dominate overlapping masks, which\nmeans for anym i\u2208 M instandm j\u2208 M high, we have\nIoU(m i, mj)\u2265\u03c4 overlap\u21d2Area(m i)\u2265Area(m j).(2)\nThe remaining masks formM rest=M high\\ M inst. Each\nmr\u2208 M restis assigned as a part ofm iif IoU(m r, mi)>\n\u03c4overlap ; otherwise, it is discarded. Therefore, for every in-\nstance maskm i, we have a set of part-level masksM i,part.\nStage 3: Fine-grained mask discovery.To enrich granu-\nlarity, we further apply patch merging to eachm i\u2208 M inst,\ndecomposing it into finer structures. The resulting fine-\ngrained masksM i,conquer are merged withM i,part through\nNon-Maximum Suppression (NMS), denoted asM i,final.\n4\nGranularity =  1.00.11.0Figure 5. Granularity as a relative notion.At a fixed granu-\nlarity value, mask sizes vary widely across scenes, showing that\nUNSAMV2 learns granularity relationally, consistent with human\nperception of parts and wholes rather than simply associating it\nwith absolute size.\nThis process expands granularity richness in a hierarchical\nperspective while maintaining mask quality.\nStage 4: Continuous granularity assignment.After ob-\ntaining the instance\u2013part hierarchies, we assign each mask\nmi\u2208 M i,final a continuous granularity scoreg i\u2208[0.1,1.0]\naccording to its relative area within the hierarchy:\ngi=\u0012\u221aAi\u2212\u221aAmin\u221aAmax\u2212\u221aAmin\u0013\n\u00b70.9 + 0.1,(3)\nwhereA idenotes the area of maskm i. As illustrated in\nFig. 4, this generates a dense hierarchy of masks spanning\nfine to coarse scales. We observe that most masks have\ngi<0.4, indicating an abundance of part-level structures;\nyet UNSAMV2 learns stable semantics across all scales\n(Fig. 5), suggesting robust hierarchical understanding.\n3.4.UNSAMV2Architecture\nGranularity encoding.As shown in Fig. 6, we introduce\na lightweight granularity encoding module that converts a\nscalarginto a high-dimensional Fourier embedding [41],\ndenoted as\u03d5(g)\u2208RdFourier. Then, we project\u03d5(g)into\nthe decoder feature space using a three-layer MLP:E g=\nMLP(\u03d5(g))\u2208Rddecoder.This granularity embedding is con-\ncatenated with the point prompt featuresE p:E prompt =\nConcat(E p, Eg).This design allows the mask decoder to\njointly interpret spatial prompts and granularity cues.\nGranularity-aware mask decoding.We replace SAM\u2019s\nthree fixed-size mask tokens with a single learnable\ngranularity-aware mask token. This token attends to both\nprompt embeddings and image features through self- and\ncross-attention, producing a mask that corresponds to the\ntarget granularity. The design is model-agnostic and can be\nintegrated with any promptable segmentation framework.Parameter efficiency.The additional encoding and token\nmodules introduce less than 0.02% extra parameters while\nproviding continuous control over mask detail.\n3.5. Distinctions with prior methods.\nPrior methods such as GraCo [58] and GARField [23] treat\ngranularity as a discrete variable or directly associate it\nwith absolute mask size. However, discretizing granular-\nity imposes artificial boundaries between adjacent levels,\npreventing smooth transitions across scales, while absolute-\nsize definitions fail to account for contextual differences,\ne.g., a \u201csmall\u201d mask could correspond to an entire object\nin one image but only a part in another. In contrast, UN-\nSAMV2 formulates granularity as acontinuous, relative\nmeasure within the instance\u2013part hierarchy, aligning more\nclosely with human perception, where objects and parts are\ninterpreted in relation to one another rather than by their\nabsolute scales. This formulation enables UNSAMV2 to\ncapture fine-grained hierarchical structure and traverse seg-\nmentation levels seamlessly.\n3.6.UNSAMV2+: Light-Supervised Variant\nAlthough the unsupervised pipeline generates abundant\nmask\u2013granularity pairs, pseudo labels can be noisy. To fur-\nther stabilize learning, we introduceUNSAMV2+, a lightly\nsupervised variant that incorporates SA-1B ground-truth\nmasks into the divide stage:M UNSAMV2+ =M CutLER \u222a\nMSA-1B.The combined masks are then processed through\nthe same conquer and granularity-assignment stages as in\nUNSAMV2. This hybrid supervision effectively balances\nhuman-curated and self-discovered hierarchies, producing\ncleaner masks while preserving the scalability and flexibil-\nity of unsupervised granularity learning.\n4. Experiments\n4.1. Model Training Settings\nPseudo mask-granularity data generation.For the di-\nvide stage of our unsupervised data pipeline, we set\u03c4 conf\nto 0.3 and\u03c4 overlap to 0.8. In the conquer stage, we lever-\nage DINOv3 [40] vision transformer to extract feature em-\nbedding and set the iterative merging thresholds as\u03b8=\n[0.9,0.8,0.7,0.6,0.5]. We run the unsupervised pipeline to\ngenerate mask-granularity pairs on 6,000 images from SA-\n1B.UNSAMV2 training.We adopt SAM-2-small as the\nbase model, and with only 8 A-100 GPU hours, we finetune\nthe base model for 5 epochs with 6,000 unlabeled images.\nDuring this process, we freeze the vision encoder and train\nthe granularity encoding module, granularity mask token,\nand the two-way transformer block in the mask decoder\nwith LoRA [17]. See more experiment details in Sec. A2.\n5\n0.11.0Granularity = 0.290.11.0Granularity = 0.37\n0.11.0Granularity = 0.450.11.0Granularity = 1.0\nIMG-EMBEDPT-EMBEDGRA-EMBED\n[ Granularity Mask Token ]Point EncoderVision EncoderFourier  EncoderTwo-Way TransformerToken Decoder0.11.00.290.370.45Granularity SliderFigure 6. Architecture of UNSAMV2.Built on SAM-2, UNSAMV2 introduces a Fourier-based granularity encoder and a granularity-\naware mask token to enable segmentation at arbitrary granularity. A scalar granularity inputg\u2208[0.1,1]is mapped to a high-dimensional\nembedding via Fourier transformation and an MLP, then injected into the transformer alongside the sparse point prompt embedding and\ndense image embedding. The granularity-aware mask token attends to image, point, and granularity embeddings, and is finally decoded by\na token decoder into a mask at the requested granularity.\nPrev. SOTAUnSAMv20.11.0Granularity = 0.10.11.0Granularity = 1.00.11.0Granularity = 0.30.11.0Granularity = 0.7\n0.11.0Granularity = 0.30.11.0Granularity = 1.00.11.0Granularity = 0.60.11.0Granularity = 0.9\nFigure 7. Qualitative comparison with the previous state-of-the-art method[58]. Each scene shows results at different target granu-\nlarity values. Prior methods often break one object into parts at high granularity or include extra regions at low granularity. In contrast,\nUNSAMV2 produces clear and consistent masks with smooth transitions across scales.\n4.2. Evaluation Datasets and Metrics\nInteractive Image Segmentation.Following Sim-\npleClick [29] and GraCo [58], we evaluate UNSAMV2\u2019s\ncapability to segment objects at various granularity levels\nwith the number of clicks (NoC) required to reach a cer-\ntain Intersection over Union (IoU) threshold to assess the\nmodel\u2019s efficiency in segmenting the target object. Specifi-\ncally, we choose NoC 80and NoC 90which record the aver-\nage number of clicks needed to achieve 80% and 90% IoU\nbetween predicted mask and ground-truth mask. In addi-\ntion, we measure the IoU between the predicted mask and\nthe ground-truth mask with just one click, denoted as 1-\nIoU in the table. For object levels, we conduct evaluations\non 5 commonly used datasets [15, 24, 30, 33, 36], and for\npart level, we evaluate models on [7, 8]. Note that for the\nfairness of comparison, we only compare datasets across\nmodels that are not in-distribution with their training data,\ne.g., SAM-2 [35] is trained on SA-1B, SimpleClick [29] and\nGraCo [58] are trained with SBD and PascalPart. We pro-vide detailed descriptions of evaluation datasets in Sec. A1.\nWhole-Image Segmentation.We evaluate our model\u2019s\nperformance in identifying all possible masks for given im-\nages in a zero-shot setting across 5 datasets that span a\nbroad range of granularity [11, 24, 27, 34, 59]. Importantly,\neach benchmark annotates only specific semantic classes\nand typically emphasizes certain levels of the part\u2013instance\nhierarchy, whereas our method predicts masks at all levels\nand for any class. Consequently, COCO Average Precision\n(AP) does not faithfully capture open-world performance.\nFollowing prior work [4, 44, 47], we report Average Recall\n(AR 1000) for comparison across methods.\n4.3. Experimental Results\nInteractive Segmentation.Remarkably, UNSAMV2 sur-\npasses SAM-2 across all datasets in zero-shot evalua-\ntion settings as summarized in Table 1. Finetuned with\nonly 6,000 images with unsupervised pseudo-labels, UN-\nSAMV2 demonstrates superior performance in segment-\ning objects at various granularity levels. It indicates that\n6\nDatasets\u2192Averaged GrabCut Berkeley SBD PascalPart PartImageNet\nMethod NoC 80\u2193NoC 90\u21931-IoU NoC 80NoC 901-IoU NoC 80NoC 901-IoU NoC 80NoC 901-IoU NoC 80NoC 851-IoU NoC 80NoC 851-IoU\nSAM-2 [35] 4.02 5.69 58.0 1.48 1.54 76.8 1.29 1.72 80.0 2.85 6.73 64.6 9.37 11.80 29.0 5.09 6.64 39.5\nUnSAMv2\u22173.41 4.75 73.1 1.16 1.30 91.4 1.10 1.60 90.8 2.75 5.97 74.8 7.84 9.59 51.5 4.21 5.29 57.1\nvs. sup. SAM-2 -0.61 -0.94 +15.1 -0.32 -0.24 +14.6 -0.19 -0.12 +10.8 -0.10 -0.76 +10.2 -1.53 -2.21 +22.5 -0.88 -1.35 +17.6\nUnSAMv2+\u22173.30 4.56 74.7 1.22 1.26 92.6 1.12 1.37 91.1 2.55 5.72 77.3 7.87 9.67 52.0 3.75 4.77 60.7\nTable 1. Comparison with SAM-2 on interactive segmentation benchmarks.Trained on 6,000 images with purely unsupervised\npseudo-labels, UNSAMV2 significantly outperforms SAM-2. We report promptable segmentation performance in terms of NoC and 1-\nIoU across five benchmarks. * Following [58], we select the optimal granularity from 0.1 to 1.0 in steps of 0.1 and report averaged results.\nDatasets\u2192Averaged GrabCut Berkeley DA VIS SA-1B PartImageNet\nMethod NoC 80\u2193NoC 90\u21931-IoU NoC 80NoC 901-IoU NoC 80NoC 901-IoU NoC 80NoC 901-IoU NoC 80NoC 901-IoU NoC 80NoC 851-IoU\nSimpleClick [29] 3.32 4.87 60.2 1.32 1.48 89.6 1.26 2.44 84.6 2.88 5.38 75.6 4.80 7.27 21.9 6.34 7.76 29.2\nGraCo [58]\u22172.35 3.42 74.4 1.24 1.32 91.1 1.17 1.63 88.7 2.84 5.09 74.0 2.41 4.01 66.2 4.11 5.03 51.7\nSAM-2 [35] 2.44 3.63 69.0 1.48 1.54 76.8 1.29 1.72 80.0 2.51 4.50 71.71.823.76 77.1 5.09 6.64 39.5\nUnSAMv2\u22172.28 3.40 79.3 1.16 1.30 91.4 1.10 1.60 90.8 2.75 4.66 84.7 2.18 4.16 72.4 4.21 5.29 57.1\nUnSAMv2+\u22172.07 3.10 81.7 1.22 1.26 92.6 1.12 1.37 91.1 2.36 4.40 85.7 1.87 3.69 78.4 3.75 4.77 60.7\nvs. prev. SOTA -0.28 -0.32 +7.30 -0.02 -0.06 +1.50 -0.05 -0.26 +2.40 -0.48 -0.69 +11.70 -0.54 -0.32 +12.20 -0.36 -0.26 +9.00\nTable 2. State-of-the-art performance on interactive segmentation at various granularity levels.Trained with only 6,000 images\nusing a combination of supervised and unsupervised labels, UNSAMV2+ demonstrates superior segmentation quality and indicates how\neffectively unsupervised methods can complement supervised data. Results of [29, 58] are reproduced with official code and checkpoints.\n*: Following [58], we select optimal granularity from 0.1 to 1.0 with a step of 0.1 and report average results.\nMethods Avg. COCO LVIS ADE Entity SA-1B\nSAM [24] 49.6 49.6 46.1 45.8 45.9 60.8\nUnSAM [47] 39.2 40.5 37.7 35.7 39.6 41.9\nUnSAM+ [47] 52.6 52.2 50.8 45.3 49.8 64.8\nUnSAMv2 68.3 74.5 63.8 68.4 64.3 70.6\nUnSAMv2+ 74.1 79.1 70.3 72.7 70.3 77.9\nvs. prev. SOTA +21.5 +26.9 +19.5 +27.4 +20.5 +13.1\nTable 3. State-of-the-art performance on whole image segmen-\ntation.UNSAMV2 outperform baseline methods [24, 47] on eval-\nuation datasets that contain instances over a wide range of granu-\nlarity levels. The evaluation metric isAR 1000. We copy results\nof SAM and UnSAM from [47]. For UNSAMV2, we aggregate\nmasks generated at granularity levels ranging from 0.1 to 1.0 in\nincrements of 0.1 and filter out low-confidence masks.\nour model architecture and unsupervised data pipeline ef-\nfectively guide the base model to understand the meaning\nof granularity scaler. On average, UNSAMV2 surpasses\nSAM-2 by 15.2% in NoC 80, 16.5% in NoC 90, and 26.0%\nin 1-IoU. With UNSAMV2, user could segment their de-\nsired objects accurately at any granularity level with fewer\nprompt points, which greatly enhances the flexibility of ob-\nject segmentation and prepares for downstream tasks. In\naddition, as shown in Table 2, UNSAMV2+, trained with\na combination of supervised and unsupervised labels on\n6,000 images achieves state-of-the-art performance on all\nmetrics across multiple evaluation datasets. Qualitative\ncomparisons with previous SOTA [58] are shown in Fig. 7.\nWhole-Image Segmentation.Apart from SOTA results in\npoint-based interactive segmentation, UNSAMV2 achieves\nsuperior performance on whole-image segmentation across\ndatasets with instances of abundant granularity levels, out-Methods# PascalPart PartImageNet\nImages NoC 80\u2193NoC 85\u21931-IoU NoC 80\u2193NoC 85\u21931-IoU\nSAM-2 \u2013 9.37 11.8 29.0 5.09 6.64 39.5\nUnSAMv2 1K 7.82 9.41 49.1 4.48 5.50 55.5\nUnSAMv2 3K 7.85 9.55 51.0 4.27 5.32 55.7\nUnSAMv2 6K 7.84 9.59 51.5 4.21 5.29 57.1\nTable 4. Ablations for training data size. UNSAMV2 starts\nto demonstrate a decent understanding of granularity scale even\ntrained with only 1,000 images with unsupervised pseudo-labels.\nperforming SAM [24] by 37.7% and UnSAM [47] by 29.8%\nin AR 1000. The results are shown in Table 3. With a gran-\nularity scalar as input, UNSAMV2 can surface instances\nover a wide range of detail. As shown in Fig. 8, users sim-\nply set the desired granularity to obtain all candidate masks\nin images at that level, creating new possibilities on how to\nintegrate segmentation models into vision task pipelines.\nVideo Segmentation Results.Despite being trained solely\non images, UNSAMV2 demonstrates promising capabili-\nties on interactive video segmentation. Although we keep\nSAM-2\u2019s memory module frozen during training, UN-\nSAMV2 still achieves competitive results on video data,\ndemonstrating that the granularity embeddings and mask to-\nken propagate across frames effectively. This further shows\nthat our self-supervised pipeline enables granularity to be\nassimilated into pretrained model\u2019s reasoning flow. We\nshow qualitative video segmentation results in Fig. 9.\n5. Ablations\nEfficiency of UNSAMV2 training.In Table 4, we ablate\nthe training data size used for UNSAMV2. We find that\n7\nGranularity =  0.211.0\n0.1Granularity =  0.151.00.1Granularity =  0.121.00.1Granularity =  0.151.00.1Granularity =  0.181.00.1Granularity =  0.231.00.1\nGranularity =  0.391.00.1Granularity =  0.781.00.1Granularity =  0.911.00.1Granularity =  0.821.00.1Granularity =  0.451.00.1Granularity =  0.831.00.1UnSAMv2Figure 8. Visualizations for whole-image segmentation.Low granularity reveals fine parts, while higher values recover whole objects.\nUNSAMV2 offers controllable, scalable whole-image segmentation capability, even for scenes with many densely packed entities.\nGranularity =  0.540.11.0Granularity =  0.790.11.0Granularity =  0.630.11.0Granularity =  0.180.11.0Granularity =  0.370.11.0Granularity =  0.450.11.0\nGranularity =  0.710.11.0\nFigure 9. Granularity generalizes to video.We prompt UN-\nSAMV2 with a point and granularity value on the first frame, then\npropagate the mask to later frames. Even though trained only on\nimages, UNSAMV2 maintains consistent masks over time, show-\ning strong temporal coherence and transferability.\ngranularity training is highly sample-efficient: UNSAMV2\nalready shows a solid grasp of granularity with only 1,000\nimages. We attribute this efficiency to two factors. First, we\nderive the granularity scale hierarchically, mirroring how\nhumans perceive scale\u2014by relative size within a hierarchy\nrather than absolute size. Second, during training we update\nonly0.1%of the parameters and keep the rest frozen, which\npreserves pretrained model\u2019s segmentation ability. Over-\nall, our procedure effectively teaches pretrained model the\nGrabCut Berkeley SBD PascalPart PartImageNet1-IoU ( )\n76.880.0\n64.6\n29.039.588.285.2\n73.4\n50.755.591.4 90.8\n74.8\n51.557.1SAM-2 UnSAMv2 w/o Gra. token UnSAMv2 w/ Gra. tokenFigure 10. Ablation of granularity-aware mask token.Directly\nfinetuning the original SAM-2 mask tokens leads to limited gains,\nsuggesting they already encode strong mask priors. Adding our\ngranularity-aware token enables efficient learning of granularity.\nMethodsSup. Unsup. Berkeley DA VIS PascalPart PtIn\nData Data 1-IoU 1-IoU 1-IoU 1-IoU\nSAM-2 \u2013 \u2013 80.0 71.7 29.0 39.5\nUnSAMv2\u2713 \u271789.7 80.8 42.5 54.3\nUnSAMv2 \u2717\u2713 90.8 84.7 51.5 57.1\nUnSAMv2+ \u2713 \u2713 91.1 85.7 52.0 60.7\nTable 5. Ablation for purely supervised granularity training.\nOnly training with SA-1B ground-truth labels results in mediocre\nperformance compared to data from our unsupervised pipeline.\nThis indicates how we crucial our unsupervised pseudo-labels are\nto maximize UNSAMV2\u2019s capability in learning granularity.\nconcept of mask granularity: with just a few example im-\nages, UNSAMV2 learns a latent representation of granular-\nity rather than merely memorizing instances.\nGranularity-aware mask token.We observe that train-\ning UNSAMV2 directly with the original mask tokens and\ntoken decoding module in SAM-2 leads to unsatisfying per-\nformance (Fig. 10). This phenomenon has been noticed in\nHQ-SAM [21]. It indicates SAM-2\u2019s original mask tokens\nhave already shown strong-prior knowledge on what consti-\ntutes as an object. Teaching these pretrained tokens to un-\nderstand the meaning of granularity is difficult. Thus, we in-\ntroduce a new mask token and its MLP to do mask decoding\nthat is only trained with mask data accompanied with gran-\n8\nrank\u2192none 4 8 16 32\nNoC 80\u2193 4.47 4.41 4.21 4.40 4.38\n(a) LoRA rank.N\u2192 3 5 7\nNoC 80\u21934.21 4.31 4.23\n(b) # points per mask.N\u219220 30 50\nNoC 80\u21934.41 4.21 4.25\n(c) # masks per image.dfourier\u219232 64 128 256\nNoC 80\u2193 4.44 4.30 4.21 4.37\n(d) Fourier dimension for granularity.\nTable 6. Ablations for hyperparameter and design choicesused for training UNSAMV2. We report UNSAMV2\u2019s interactive segmen-\ntation performance on validation set of PartImageNet.(a)We vary the rank of LoRA weights in mask decoder.(b)We vary the number of\ncorrection points sampled per mask.(c)We vary the number of masks randomly sampled per iteration when training UNSAMV2 models.\n(d)We study different choices of dimension to encode granularity scaler in Fourier feature space. Default settings are highlighted in teal.\nDesired ObjectClick #1Click #2Click #3SAM-2UnSAMv2Granularity = 0.451.00.1\nFigure 11. Fewer prompts, more control.SAM-2 often needs\nmultiple clicks to isolate the target object. With a single granular-\nity value, UNSAMV2 finds the correct mask efficiently, and it can\nalso work with multi-point prompts for finer control.\nularity scales. By conducting self-attention with the gran-\nularity embedding encoded by Fourier module and cross-\nattention with image embeddings, the newly introduced to-\nken learn the representation of masks and their correspond-\ning granularity simultaneously. The newly introduced token\nis a crucial component of UNSAMV2 and a key component\nto achieve the goal \u2013 segment anything at any granularity.\nUnsupervised pseudo-labelsare pivotal for granularity\ntraining, as shown in Table 5. We observe that when\ntrained solely on human-labeled SA-1B annotations, UN-\nSAMV2 performs unsatisfactorily compared with training\nthat also incorporates pseudo-labels from our divide-and-\nconquer pipeline. This points to an inherent issue with\nsupervised datasets: they are heavily biased by human la-\nbelers\u2019 notions of what constitutes an object. By contrast,\nour unsupervised approach focuses on intrinsic relation-\nships among patches, enabling coverage of both instances\nand parts in a coherent hierarchy.\nDesign choices in UnSAMv2 training.In Table 6, we\npresent the ablation studies on design choices for UN-\nSAMV2 model architecture and training procedure. We\nstudy the use of LoRA in the mask decoder in Table 6a. The\nresults show that, with LoRA, UNSAMV2 learns the con-\ncept of granularity efficiently while retaining the strong seg-\nmentation capability of SAM-2. Next, we ablate the num-ber of correction points sampled per mask in one training\nstep in Table 6b. With the granularity scaler, our model can\nidentify the target mask with few clicks, so we use 3 correc-\ntion points per mask for efficiency. In Table 6c, we study\nthe number of masks per image in one training iteration.\nUNSAMV2 benefits from a moderate number of masks per\nstep, which best supports learning granularity while main-\ntaining training stability. Finally, we study the effect of\nthe Fourier feature dimension used to encode the granular-\nity input in Table 6d. We observe that a moderate dimen-\nsion best matches the granularity representation and enables\nUNSAMV2 to distinguish granularity levels smoothly in a\ncontinuous manner.\n6. Conclusion\nWe presented UNSAMV2, a self-supervised framework\nthat equips pretrained segmentation model to segment\nanything at any granularity. By deriving continuous\ngranularity scales from unlabeled data, UNSAMV2\nlearns to traverse part\u2013whole hierarchies and control\nsegmentation with a single scalar. Trained on only 6K\nunlabeled images, it achieves state-of-the-art results\nacross interactive, whole-image, and video segmenta-\ntion. Our results highlight that self-supervised learn-\ning can unlock latent hierarchical structure in vision\nfoundation models, transforming segmentation from dis-\ncrete prediction into continuous, controllable reasoning.\nReferences\n[1] Shahaf Arica, Or Rubin, Sapir Gershov, and Shlomi Laufer.\nCuvler: Enhanced unsupervised object discoveries through\nexhaustive self-supervised transformers. InProceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 23105\u201323114, 2024. 3\n[2] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bo-\njanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and\nNicolas Ballas. Self-supervised learning from images with a\njoint-embedding predictive architecture. InProceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 15619\u201315629, 2023. 3\n[3] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter\nWonka, and Matthias M \u00a8uller. Zoedepth: Zero-shot trans-\nfer by combining relative and metric depth.arXiv preprint\narXiv:2302.12288, 2023. 3\n9\n[4] Shengcao Cao, Jiuxiang Gu, Jason Kuen, Hao Tan, Ruiyi\nZhang, Handong Zhao, Ani Nenkova, Liang-Yan Gui, Tong\nSun, and Yu-Xiong Wang. Sohes: Self-supervised open-\nworld hierarchical entity segmentation.arXiv preprint\narXiv:2404.12386, 2024. 3, 4, 6\n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv \u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. InPro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 9650\u20139660, 2021. 3\n[6] Danhui Chen, Ziquan Liu, Chuxi Yang, Dan Wang, Yan\nYan, Yi Xu, and Xiangyang Ji. Conformalsam: Unlocking\nthe potential of foundational segmentation models in semi-\nsupervised semantic segmentation with conformal predic-\ntion. InProceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 24045\u201324055, 2025. 2\n[7] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fi-\ndler, Raquel Urtasun, and Alan Yuille. Detect what you\ncan: Detecting and representing objects using holistic mod-\nels and body parts. InProceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1971\u20131978,\n2014. 6, 13\n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In2009 IEEE Conference on Computer Vision and\nPattern Recognition, pages 248\u2013255, 2009. 6, 13\n[9] Carl Doersch, Pauline Luc, Yi Yang, Dilara Gokay, Skanda\nKoppula, Ankush Gupta, Joseph Heyward, Ignacio Rocco,\nRoss Goroshin, Joao Carreira, et al. Bootstap: Bootstrapped\ntraining for tracking-any-point. InProceedings of the Asian\nConference on Computer Vision, pages 3257\u20133274, 2024. 2\n[10] Zhirui Gao, Renjiao Yi, Yuhang Huang, Wei Chen,\nChenyang Zhu, and Kai Xu. Partgs: Learning part-aware\n3d representations by fusing 2d gaussians and superquadrics.\narXiv preprint arXiv:2408.10789, 2024. 3\n[11] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A\ndataset for large vocabulary instance segmentation. InPro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 5356\u20135364, 2019. 6, 13\n[12] Abner Guzman-Rivera, Dhruv Batra, and Pushmeet Kohli.\nMultiple choice learning: Learning to produce multiple\nstructured outputs.Advances in neural information process-\ning systems, 25, 2012. 3\n[13] Oliver Hahn, Christoph Reich, Nikita Araslanov, Daniel Cre-\nmers, Christian Rupprecht, and Stefan Roth. Scene-centric\nunsupervised panoptic segmentation. InProceedings of the\nComputer Vision and Pattern Recognition Conference, pages\n24485\u201324495, 2025. 3\n[14] Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah\nSnavely, and William T Freeman. Unsupervised semantic\nsegmentation by distilling feature correspondences.arXiv\npreprint arXiv:2203.08414, 2022. 3\n[15] Bharath Hariharan, Pablo Arbel \u00b4aez, Lubomir Bourdev,\nSubhransu Maji, and Jitendra Malik. Semantic contours from\ninverse detectors. In2011 international conference on com-\nputer vision, pages 991\u2013998. IEEE, 2011. 6, 13\n[16] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalablevision learners. InProceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 16000\u2013\n16009, 2022. 3\n[17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al.\nLora: Low-rank adaptation of large language models.ICLR,\n1(2):3, 2022. 5\n[18] Nan Huang, Wenzhao Zheng, Chenfeng Xu, Kurt Keutzer,\nShanghang Zhang, Angjoo Kanazawa, and Qianqian Wang.\nSegment any motion in videos. InProceedings of the Com-\nputer Vision and Pattern Recognition Conference, pages\n3406\u20133416, 2025. 2\n[19] Hanwen Jiang, Hao Tan, Peng Wang, Haian Jin, Yue Zhao,\nSai Bi, Kai Zhang, Fujun Luan, Kalyan Sunkavalli, Qixing\nHuang, et al. Rayzer: A self-supervised large view synthesis\nmodel.arXiv preprint arXiv:2505.00702, 2025. 3\n[20] Markus Karmann and Onay Urfalioglu. Repurposing stable\ndiffusion attention for training-free unsupervised interactive\nsegmentation. InProceedings of the Computer Vision and\nPattern Recognition Conference, pages 24518\u201324528, 2025.\n3\n[21] Lei Ke, Mingqiao Ye, Martin Danelljan, Yu-Wing Tai, Chi-\nKeung Tang, Fisher Yu, et al. Segment anything in high qual-\nity.Advances in Neural Information Processing Systems, 36:\n29914\u201329934, 2023. 8\n[22] Chanyoung Kim, Woojung Han, Dayun Ju, and Seong Jae\nHwang. Eagle: Eigen aggregation learning for object-centric\nunsupervised semantic segmentation. InProceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3523\u20133533, 2024. 3\n[23] Chung Min Kim, Mingxuan Wu, Justin Kerr, Ken Gold-\nberg, Matthew Tancik, and Angjoo Kanazawa. Garfield:\nGroup anything with radiance fields. InProceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 21530\u201321539, 2024. 3, 5\n[24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. InProceedings of the IEEE/CVF international confer-\nence on computer vision, pages 4015\u20134026, 2023. 2, 3, 4, 6,\n7, 13\n[25] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu,\nChunyuan Li, Jianwei Yang, Lei Zhang, and Jianfeng Gao.\nSegment and recognize anything at any granularity. InEu-\nropean Conference on Computer Vision, pages 467\u2013484.\nSpringer, 2024. 3\n[26] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Interactive\nimage segmentation with latent diversity. InProceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2018. 3\n[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll \u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 6, 13\n10\n[28] Zheng Lin, Nan Zhou, Chen-Xi Du, Deng-Ping Fan, and Shi-\nMin Hu. Refcut: Interactive segmentation with reference\nguidance.arXiv preprint arXiv:2503.17820, 2025. 3\n[29] Qin Liu, Zhenlin Xu, Gedas Bertasius, and Marc Nietham-\nmer. Simpleclick: Interactive image segmentation with sim-\nple vision transformers. InProceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 22290\u2013\n22300, 2023. 3, 6, 7\n[30] Kevin McGuinness and Noel E O\u2019connor. A comparative\nevaluation of interactive segmentation algorithms.Pattern\nRecognition, 43(2):434\u2013444, 2010. 6, 13\n[31] Dantong Niu, Xudong Wang, Xinyang Han, Long Lian, Roei\nHerzig, and Trevor Darrell. Unsupervised universal image\nsegmentation. InProceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n22744\u201322754, 2024. 3\n[32] Maxime Oquab, Timoth \u00b4ee Darcet, Th \u00b4eo Moutakanni, Huy\nV o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193, 2023. 3\n[33] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc\nVan Gool, Markus Gross, and Alexander Sorkine-Hornung.\nA benchmark dataset and evaluation methodology for video\nobject segmentation. InProceedings of the IEEE conference\non computer vision and pattern recognition, pages 724\u2013732,\n2016. 6, 13\n[34] Lu Qi, Jason Kuen, Yi Wang, Jiuxiang Gu, Hengshuang\nZhao, Philip Torr, Zhe Lin, and Jiaya Jia. Open world en-\ntity segmentation.IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2022. 6, 13\n[35] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang\nHu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman\nR\u00a8adle, Chloe Rolland, Laura Gustafson, et al. Sam 2:\nSegment anything in images and videos.arXiv preprint\narXiv:2408.00714, 2024. 1, 2, 3, 4, 6, 7, 13\n[36] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake.\n\u201d grabcut\u201d interactive foreground extraction using iterated\ngraph cuts.ACM transactions on graphics (TOG), 23(3):\n309\u2013314, 2004. 6, 13\n[37] Jianbo Shi and Jitendra Malik. Normalized cuts and image\nsegmentation.IEEE Transactions on pattern analysis and\nmachine intelligence, 22(8):888\u2013905, 2000. 3\n[38] Leon Sick, Dominik Engel, Sebastian Hartwig, Pedro Her-\nmosilla, and Timo Ropinski. Cuts3d: Cutting semantics in\n3d for 2d unsupervised instance segmentation. InProceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 21265\u201321275, 2025. 3\n[39] Oriane Sim \u00b4eoni, Gilles Puy, Huy V V o, Simon Roburin,\nSpyros Gidaris, Andrei Bursuc, Patrick P \u00b4erez, Renaud\nMarlet, and Jean Ponce. Localizing objects with self-\nsupervised transformers and no labels.arXiv preprint\narXiv:2109.14279, 2021. 3\n[40] Oriane Sim \u00b4eoni, Huy V V o, Maximilian Seitzer, Federico\nBaldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov,\nMarc Szafraniec, Seungeun Yi, Micha \u00a8el Ramamonjisoa,\net al. Dinov3.arXiv preprint arXiv:2508.10104, 2025. 3,\n4, 5, 13[41] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\nmamoorthi, Jonathan Barron, and Ren Ng. Fourier features\nlet networks learn high frequency functions in low dimen-\nsional domains.Advances in neural information processing\nsystems, 33:7537\u20137547, 2020. 5\n[42] Bin Wang, Anwesa Choudhuri, Meng Zheng, Zhongpai Gao,\nBenjamin Planche, Andong Deng, Qin Liu, Terrence Chen,\nUlas Bagci, and Ziyan Wu. Order-aware interactive segmen-\ntation.arXiv preprint arXiv:2410.12214, 2024. 3\n[43] Xinlong Wang, Zhiding Yu, Shalini De Mello, Jan Kautz,\nAnima Anandkumar, Chunhua Shen, and Jose M Alvarez.\nFreesolo: Learning to segment objects without annotations.\nInProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 14176\u201314186, 2022. 3\n[44] Xudong Wang, Rohit Girdhar, Stella X Yu, and Ishan Misra.\nCut and learn for unsupervised object detection and instance\nsegmentation. InProceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 3124\u2013\n3134, 2023. 2, 3, 4, 6\n[45] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang,\nChunhua Shen, and Tiejun Huang. Seggpt: Segmenting ev-\nerything in context.arXiv preprint arXiv:2304.03284, 2023.\n2\n[46] Xudong Wang, Ishan Misra, Ziyun Zeng, Rohit Girdhar,\nand Trevor Darrell. Videocutler: Surprisingly simple un-\nsupervised video instance segmentation. InProceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 22755\u201322764, 2024. 3\n[47] XuDong Wang, Jingfeng Yang, and Trevor Darrell. Segment\nanything without supervision.Advances in Neural Informa-\ntion Processing Systems, 37:138731\u2013138755, 2024. 1, 2, 3,\n4, 6, 7, 13\n[48] Xuehao Wang, Zhan Zhuang, Feiyang Ye, and Yu Zhang.\nMtsam: Multi-task fine-tuning for segment anything model.\nInThe Thirteenth International Conference on Learning\nRepresentations, 2025. 2\n[49] Yangtao Wang, Xi Shen, Yuan Yuan, Yuming Du, Maomao\nLi, Shell Xu Hu, James L Crowley, and Dominique Vaufrey-\ndaz. Tokencut: Segmenting objects in images and videos\nwith self-supervised transformer and normalized cut.IEEE\ntransactions on pattern analysis and machine intelligence,\n45(12):15790\u201315801, 2023. 3\n[50] Xiaoyang Wu, Daniel DeTone, Duncan Frost, Tianwei\nShen, Chris Xie, Nan Yang, Jakob Engel, Richard New-\ncombe, Hengshuang Zhao, and Julian Straub. Sonata: Self-\nsupervised learning of reliable point representations. InPro-\nceedings of the Computer Vision and Pattern Recognition\nConference, pages 22193\u201322204, 2025. 3\n[51] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing\nWang, and Feng Zheng. Track anything: Segment anything\nmeets videos.arXiv preprint arXiv:2304.11968, 2023. 2\n[52] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi\nFeng, and Hengshuang Zhao. Depth anything: Unleashing\nthe power of large-scale unlabeled data. InCVPR, 2024. 3\n[53] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiao-\ngang Xu, Jiashi Feng, and Hengshuang Zhao. Depth any-\nthing v2.arXiv:2406.09414, 2024. 3\n11\n[54] Yunhan Yang, Yukun Huang, Yuan-Chen Guo, Liangjun Lu,\nXiaoyang Wu, Edmund Y Lam, Yan-Pei Cao, and Xihui Liu.\nSampart3d: Segment any part in 3d objects.arXiv preprint\narXiv:2411.07184, 2024. 3\n[55] Yunhan Yang, Yufan Zhou, Yuan-Chen Guo, Zi-Xin Zou,\nYukun Huang, Ying-Tian Liu, Hao Xu, Ding Liang, Yan-\nPei Cao, and Xihui Liu. Omnipart: Part-aware 3d genera-\ntion with semantic decoupling and structural cohesion.arXiv\npreprint arXiv:2507.06165, 2025. 3\n[56] Mingqiao Ye, Seoung Wug Oh, Lei Ke, and Joon-Young Lee.\nEntitysam: Segment everything in video. InProceedings of\nthe Computer Vision and Pattern Recognition Conference,\npages 24234\u201324243, 2025. 2\n[57] Yujia Zhang, Xiaoyang Wu, Yixing Lao, Chengyao Wang,\nZhuotao Tian, Naiyan Wang, and Hengshuang Zhao. Con-\ncerto: Joint 2d-3d self-supervised learning emerges spatial\nrepresentations.arXiv preprint arXiv:2510.23607, 2025. 3\n[58] Yian Zhao, Kehan Li, Zesen Cheng, Pengchong Qiao, Xi-\nawu Zheng, Rongrong Ji, Chang Liu, Li Yuan, and Jie Chen.\nGraco: Granularity-controllable interactive segmentation. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 3501\u20133510, 2024. 3, 5,\n6, 7, 14\n[59] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-\ndler, Adela Barriuso, and Antonio Torralba. Semantic under-\nstanding of scenes through the ade20k dataset.International\nJournal of Computer Vision, 127:302\u2013321, 2019. 6, 13\n[60] Tianfei Zhou, Jianwu Li, Shunzhou Wang, Ran Tao, and\nJianbing Shen. Matnet: Motion-attentive transition network\nfor zero-shot video object segmentation.IEEE transactions\non image processing, 29:8326\u20138338, 2020. 3\n12\nUNSAMV2:\nSelf-Supervised Learning Enables Segment Anything at Any Granularity\nSupplementary Material\nA1. Evaluation Datasets\nInteractive Segmentation.Specifically, we adopt the fol-\nlowing 7 datasets as benchmarks.\n\u2022GrabCut[36] consists of 50 images, each containing a\nsingle instance.\n\u2022Berkeley[30] includes 96 images with 100 instances,\nsome of which are more challenging for segmentation.\n\u2022DA VIS[33] contains 50 high-quality videos and we use\n345 frames for evaluation.\n\u2022SA-1B[24] contains 11 million high-resolution images\n(average size 3300\u00d74950 pixels) and 1.1 billion high-\nquality segmentation masks. For evaluation, we randomly\nchoose 1,000 images that are not included in UNSAMV2\ntraining data as our evaluation set.\n\u2022SBD[15] includes 8,498 training images (with 20,172\ninstances) and 2,857 validation images (with 6,671 in-\nstances).\n\u2022PascalPart[7] provides part-level annotations for 20\nclasses from PascalVOC, totaling 193 part categories. We\nevaluate models on the whole validation set.\n\u2022PartImageNet[8] organizes 158 ImageNet classes into\n11 super-categories and defines 40 distinct part cate-\ngories. We utilize its validation set to evaluate model\u2019s\nperformance on segmenting part-level objects, which\nconsists of 1,206 images and 4,553 annotated parts.\nWhole-Image Segmentation.We adopt the following 5\ndatasets to evaluate UNSAMV2\u2019s capability on discovering\nall instances in images.\n\u2022COCO(Common Objects in Context) [27] is a widely\nutilized object detection and segmentation dataset. It con-\nsists of 115,000 labeled training images and 5,000 labeled\nvalidation images. We evaluate our model on COCO\nVal2017with 5000 validation images in a zero-shot\nmanner. We use averaged recall (AR 1000) as the metrics\nfor the whole-image segmentation task.\n\u2022SA-1B[24] consists of 11 million high-resolution im-\nages and 1.1 billion segmentation masks. Following inter-\nactive segmentation, we randomly choose 1,000 images\nthat are not included in UNSAMV2 training procedure as\nevaluation set.\n\u2022LVIS(Large V ocabulary Instance Segmentation) [11] has\n164,000 images with over 1,200 categories and 2 million\nhigh-quality instance-level segmentation masks. It covers\na large number of object categories. We evaluate UN-\nSAMV2 using its 5000 validation images.\n\u2022EntitySeg[34] is an open-world, class-agnostic datasetwith 33,277 images, averaging 18.1 annotated entities per\nimage. We conduct zero-shot evaluation on 1,314 low-\nresolution images in the validation set.\n\u2022ADE20K[59] contains 25,574 training and 2,000 testing\nimages covering 365 scenes, emphasizing semantic-level\nsegmentation. It provides labels for 150 semantic cate-\ngories and 707,868 objects drawn from 3,688 categories.\nWe evaluate zero-shot whole-image segmentation perfor-\nmance on the 2,000-image test split.\nA2. Training Details.\nPseudo mask-granularity data generation.For the di-\nvide stage of our unsupervised data pipeline, we set\u03c4 conf\nto 0.3 and\u03c4 overlap to 0.8. In the conquer stage, we leverage\nDINOv3 [40] ViT-B/16 backbone to extract feature embed-\nding from the last layer of vision transformer and merge\nadjacent patches together based on predefined cosine sim-\nilarity thresholds\u03b8= [0.9,0.8,0.7,0.6,0.5]. We run the\npipeline to generate mask-granularity pairs on 6,000 im-\nages from SA-1B in a fully unsupervised manner. On av-\nerage, we have 112 pseudo-labels on each image. Note\nthat unlike UnSAM [47], UNSAMV2 is designed to learn\ninstance\u2013part relationships rather than only instance rep-\nresentations, so our granularity-aware divide-and-conquer\npipeline intentionally produces fewer pseudo-labels than\nUnSAM, which produces 448 pseudo-labels per image.\nUNSAMV2 Training.We finetune SAM-2-small model\nfor 5 epochs with pseudo-labels on 6,000 images. Dur-\ning finetuning, we freeze the heavy-weight Hiera image en-\ncoder and only train the granularity encoding module, gran-\nularity mask token, and the two-way transformer block in\nthe mask decoder. For the granularity encoder, we first\nadopt Fourier transformation to granularity scalar with di-\nmensiond Fourier = 128and followed by a 3-layer MLP. Fol-\nlowing SAM-2 [35], UNSAMV2 adopts a combination of\nfocal loss and dice loss with a ratio of 20:1. The batch size is\nset to 4, with the learning rate initialized at 1e-4. We apply\nLoRA technology to all projection layers of the transformer,\nsetting the LoRA rank to 8. All experiments are conducted\non either 2 A-100 or 4 RTX 3090 GPUs.\nA3. More Visualizations\nWe present more UNSAMV2\u2019s qualitative results on inter-\nactive image segmentation in Fig. A1, whole image seg-\nmentation in Fig. A2, and video segmentation in Fig. A3.\n13\nGranularity =  0.420.11.0Granularity =  1.00.11.0Granularity =  0.360.11.0Granularity =  0.540.11.0\nGranularity =  0.220.11.0Granularity =  0.450.11.0Granularity =  0.230.11.0Granularity =  0.870.11.0\nGranularity =  0.430.11.0Granularity =  0.920.11.0Granularity =  0.550.11.0Granularity =  0.790.11.0\nGranularity =  0.360.11.0Granularity =  0.770.11.0Granularity =  0.400.11.0Granularity =  0.780.11.0Figure A1.Interactive segmentation results on SA-1B. The top row is previous SOTA GraCo [58] and the bottom row is UNSAMV2.\n14\nGranularity =  0.210.11.0Granularity =  0.150.11.0Granularity =  0.120.11.0Granularity =  0.140.11.0\nGranularity =  0.390.11.0Granularity =  0.780.11.0Granularity =  0.910.11.0Granularity =  0.820.11.0\nGranularity =  0.130.11.0Granularity =  0.170.11.0Granularity =  0.150.11.0Granularity =  0.160.11.0\nGranularity =  0.960.11.0Granularity =  0.910.11.0Granularity =  0.720.11.0Granularity =  0.310.11.0Figure A2.Whole image segmentation on SA-1B. From top to bottom are raw images, segmentation by SAM-2 and UNSAMV2.\n15\nGranularity =  0.400.1 1.0\nGranularity =  0.170.1 1.0\nGranularity =  0.230.1 1.0\nGranularity =  0.520.1 1.0\nGranularity =  0.150.1 1.0\nGranularity =  0.220.1 1.0\nGranularity =  0.940.1 1.0\nGranularity =  0.250.1 1.0\nFigure A3.UNSAMV2\u2019s interactive video segmentation results on YoutubeVIS dataset at various granularity levels.\n16\n",
    "title": "UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity",
    "authors": [
      "Junwei Yu",
      "Trevor Darrell",
      "XuDong Wang"
    ],
    "published": "2025-11-17",
    "arxiv_id": "2511.13714v1",
    "num_pages": 16,
    "num_chars": 58605
  },
  {
    "text": "From Black Box to Insight: Explainable AI\nfor Extreme Event Preparedness\nKiana Vu\u2217\nDepartment of Cybersecurity\nUniversity at Albany, SUNY\nAlbany, NY , USA\nkvu@albany.edu\u02d9Ismet Selc \u00b8uk \u00a8Ozer\nDepartment of Cybersecurity\nUniversity at Albany, SUNY\nAlbany, NY , USA\niozer@albany.eduPhung Lai\nDepartment of Cybersecurity\nUniversity at Albany, SUNY\nAlbany, NY , USA\nlai@albany.edu\nZheng Wu\nDept. of Atmospheric & Environmental Sciences\nUniversity at Albany, SUNY\nAlbany, NY , USA\nzwu26@albany.eduThilanka Munasinghe\nLally School of Management\nRensselaer Polytechnic Institute\nAlbany, NY , USA\nthilankawillbe@gmail.comJennifer Wei\nGoddard Space Flight Center\nNASA\nGreenbelt, MD, USA\njennifer.c.wei@nasa.gov\nAbstract\u2014As climate change accelerates the frequency and\nseverity of extreme events such as wildfires, the need for accurate,\nexplainable, and actionable forecasting becomes increasingly ur-\ngent. While artificial intelligence (AI) models have shown promise\nin predicting such events, their adoption in real-world decision-\nmaking remains limited due to their black-box nature, which\nlimits trust, explainability, and operational readiness. This paper\ninvestigates the role of explainable AI (XAI) in bridging the gap\nbetween predictive accuracy and actionable insight for extreme\nevent forecasting. Using wildfire prediction as a case study, we\nevaluate various AI models and employ SHapley Additive exPla-\nnations (SHAP) to uncover key features, decision pathways, and\npotential biases in model behavior. Our analysis demonstrates\nhow XAI not only clarifies model reasoning but also supports\ncritical decision-making by domain experts and response teams.\nIn addition, we provide supporting visualizations that enhance\nthe interpretability of XAI outputs by contextualizing feature\nimportance and temporal patterns in seasonality and geospatial\ncharacteristics. This approach enhances the usability of AI\nexplanations for practitioners and policymakers. Our findings\nhighlight the need for AI systems that are not only accurate\nbut also interpretable, accessible, and trustworthy, essential for\neffective use in disaster preparedness, risk mitigation, and climate\nresilience planning.\nIndex Terms\u2014AI model, explainable AI, extreme events, wild-\nfires, disaster preparedness\nI. INTRODUCTION\nThe frequency and severity of extreme events, such as wild-\nfires, heatwaves, and floods, have increased significantly due\nto climate change [1], [2], posing serious risks to ecosystems\n[3], public health [4], infrastructure [5], and communities.\nThis urgency has led to growing use of AI and machine\nlearning (ML) for spatiotemporal forecasting, such as wildfire\nand spread prediction [6].\nDespite these advances, the real-world adoption of AI-\ndriven forecasting tools in critical domains like disaster re-\nsponse remains limited. A key barrier is the \u201cblack-box\u201d nature\nof many AI models, like deep neural networks, which make\n\u2217Corresponding authoraccurate predictions but lack transparency. This poses serious\nchallenges for decision-makers in high-stakes environments\nlike emergency response, firefighting, and public safety. With-\nout clear insight into why a model makes certain predictions, it\nbecomes difficult to build trust, assess risks, validate outcomes,\nand effectively use model outputs in operations.\nTo address these challenges, our work adopts NASA\u2019s\nFAIRUST principles [7], which emphasize that AI applica-\ntions and data should be Findable, Accessible, Interopera-\nble, Reusable, Understandable, Secure, and Trustworthy. We\nspecifically focus on leveraging explainable AI (XAI) tech-\nniques to make complex model behavior more Understandable.\nXAI sheds light on how models use input features, revealing\ndecision logic and potential biases. In the context of extreme\nevent forecasting, such as wildfires, XAI helps bridge the\ngap between predictive accuracy and real-world usability,\nempowering domain experts to evaluate model trustworthiness,\nidentify critical risk factors, and make well-informed decisions\nunder conditions of uncertainty.\nIn this paper, we explore the use of XAI in wildfire fore-\ncasting, which is a critical and complex challenge in extreme\nevent prediction.Our contributions are threefold.(1)We\nevaluate several state-of-the-art AI models and use SHapley\nAdditive exPlanations (SHAP) [8] to interpret predictions,\nhighlighting key features and enabling a deeper understanding\nof model behavior, strengths, and limitations.(2)To enhance\ninterpretability, we introduce visualizations that map feature\nimportance across spatial and seasonal dimensions, making\nAI-generated insights more accessible to non-technical stake-\nholders, including emergency planners and policymakers.(3)\nWe show how these explainability tools can support action-\nable insights for extreme event preparedness, bridging the\ngap between model performance and operational utility. Our\nresults emphasize the role of XAI in trustworthy climate risk\nforecasting and responsible AI research with societal impact.arXiv:2511.13712v1  [cs.LG]  17 Nov 2025\nFig. 1. Costs of U.S. extreme disaster events over time [9].\nII. SYSTEMATIZATION OFEXTREMEWEATHEREVENTS\nPREDICTION ANDEXPLANATION\nA. Extreme Weather Events\nExtreme weather events, such as wildfires, heatwaves, hur-\nricanes, floods, and droughts, are statistically rare but highly\nimpactful phenomena that deviate significantly from normal\nconditions [4]. Their impacts and associated costs have been\nrising, reaching hundreds of billions of dollars in recent years\nin the U.S. [9] (Figure 1). In 2024, the National Oceanic\nand Atmospheric Administration (NOAA) reported 27 U.S.\nweather and climate disasters, each causing over one billion\nin damages [10]. Climate change has increased the frequency\nand severity of these events by warming the atmosphere, which\nholds more moisture and intensifies heat and precipitation\nextremes [11]. These escalating costs and increasing frequency\nand intensity of extreme events have motivated significant\nresearch into understanding and predicting extreme events [6].\nSkillful forecasts are crucial for mitigation, adaptation, early\nwarning systems, and building resilience in vulnerable com-\nmunities. However, the chaotic nature of the climate system\nand its complex interactions across atmospheric, oceanic, and\nland processes pose major challenges for reliable prediction.\nWildfires are among the most destructive extreme events,\nbecoming more frequent and intense due to climate change\nand drawing increased scientific attention. These complex\nphenomena are influenced by weather conditions (e.g., tem-\nperature, humidity, wind) and human activity. Wildfires can\nthreaten lives, infrastructure, ecosystems, and economies [12].\nAdvancing wildfire understanding and prediction is, therefore,\nessential and forms the focus of this paper.\nB. AI for Extreme Weather Events Prediction\nDespite challenges in predicting extreme weather, advances\nin understanding its dynamics and development of physics-\nbased models have improved forecast skills in recent decades\n[13], [14]. Recent AI/ML developments have further trans-\nformed forecasting by leveraging diverse data. Models such\nas Tree-based Ensemble, Recurrent Neural Network (RNN),\nConvolutional Neural Network (CNN), and Transformer have\nshown promise in detecting extreme weather patterns, aiding\npredictions of storms, floods, heatwaves, and wildfires. En-\nsemble and tree-based models (e.g., Random Forests, GradientBoosting) are widely applied to floods, heatwaves, and severe\nstorms, offering robustness to noisy data and supporting uncer-\ntainty quantification [15], [16]. Temporal models like RNNs,\nLong Short-Term Memory (LSTM), and Gated Recurrent Unit\n(GRU), are well-suited for capturing long-term dependencies\nin sequential weather data, aiding in the prediction of events\nsuch as heatwaves and droughts [17], [18]. More recently,\nTransformer-based models with attention mechanisms have\nshown strong performance by dynamically weighting relevant\nmeteorological features and time steps, improving forecasts of\ncomplex events like heatwaves and hurricanes [19].\nPhysics-based models like WRF-Fire [20] are widely used\nbut computationally expensive and sensitive to initial con-\nditions. Meanwhile, AI/ML models address these limits by\ncapturing complex, nonlinear patterns [6]. We focus on AI for\nwildfire prediction, one of the most challenging forecasting\ntasks, where accurate timing and location are vital. Beyond\nprediction, we examine interpretability and forecasting oppor-\ntunities, which remain underexplored.\nC. XAI for Extreme Weather Events Prediction\nDespite their advantages, AI models often lack transparency\ndue to their \u201cblack-box\u201d nature, posing challenges for opera-\ntional deployment. This underscores the need for XAI, which\nhelps identify key features and patterns behind predictions. In\nclimate science, interpretability not only fosters user trust but\nalso uncovers insights consistent with physical understanding\n[6]. XAI methods to explain AI-driven forecasts for extreme\nweather events generally fall into two categories (Figure 2):\nintrinsic interpretable model designs and post-hoc techniques.\nFirst, intrinsic XAI methods rely on models that are trans-\nparent by design [21]. Examples include linear regression,\nwhich shows feature influence via coefficients [22], and de-\ncision trees or random forests, which reveal decision paths\nthrough feature thresholds [23], [24]. These models are often\nused in extreme event prediction due to their simplicity and\ninterpretability. Attention-based deep learning models can also\nbe intrinsically interpretable. For example, [25] uses a Trans-\nformer with multi-head self-attention to identify key temporal\nand spatial drivers in next-day wildfire prediction.\nSecond, post-hoc XAI methods are applied after model\ntraining to explain predictions from complex AI/ML models.\nCommon techniques include feature attribution like SHAP [8],\nLIME [26], surrogate model [27], counterfactual explanations\n[6], [28], and visualization-based methods [29], [30]. SHAP\nis widely used in climate science for offering both global and\nlocal interpretability. In extreme weather forecasting, SHAP\nreveals key atmospheric features, enhancing trust and insight\n[31]. It helps assess wildfire risk [24] and finds critical factors\nlike humidity and wind [32].\nMost extreme weather prediction studies favor post-hoc\nXAI methods, as they explain complex AI models better than\ninherently interpretable models. Since high-performing models\nare often opaque, post-hoc tools like SHAP provide critical in-\nsights, build trust, and support informed planning [33]. In this\nwork, we use SHAP to interpret wildfire predictions, ensuring\nexplanations are physically meaningful and informative.\nIII. BACKGROUND\nA. AI Prediction Models\nTo predict extreme events, AI models typically use multiple\nweather and atmospheric features over a period preceding the\nevent. Each featurex iis a time series{x it}L\nt=1, whereLis the\nlength of the time window. An input samplexconsists of time\nseries data of allNfeatures, denoted asx={x i}N\ni=1. This\nformulation captures the evolving dynamics of multiple envi-\nronmental factors over time, which are essential for modeling\ncomplex phenomena like wildfire occurrence.\nAn AI model predictionf(x)takes a time seriesxas input\nand produces a binary predictiony\u2208 {0,1}, wherey= 1indi-\ncates the predicted occurrence of a wildfire, andy= 0denotes\nthe absence of wildfire risk on the prediction day. This binary\nclassification setting is commonly used in operational wildfire\nforecasting systems, where timely and accurate predictions can\nsupport early warning efforts and inform mitigation strategies.\nBy tracking changes in temperature, humidity, wind, and pre-\ncipitation over time, AI models identify patterns that may warn\nof a higher risk of wildfires. Incorporating time series structure\nallows the model to consider the current environmental state\nand to account for trends, lags, and cumulative effects that\ndevelop over the preceding days or weeks.\nB. Model Explanations and SHAP\nThe goal of model explanations is to improve transparency,\ninterpretability, and trust in ML models by capturing how\neach feature influences the model\u2019s decisions and which class\nsuch decisions favor. XAI methods address this by providing\nhuman-understandable reasons for model behavior. SHapley\nAdditive exPlanations (SHAP) [8] is a widely used XAI\nmethod. Given a samplex={x i}N\ni=1wherex irepresents the\nithfeature andNis the number of features, and a prediction\nmodelfthat outputs the probabilityf(x)ofxbelongs to a\ncertain classy, SHAP can be represented as a linear model of\nthe formg(x) =PN\ni=1eixi, where{e i}d\ni=1are the resulting\ncoefficients of the explanation modelg(x), measuring the\nimpact ofx ion the model\u2019s decision. In general, higher values\nofeiimply a higher impact ofx iin the model decision.\nIV. TRANSPARENTWILDFIREMODELING: INSIGHTS\nTHROUGHXAI\nIn this section, we shed light on how XAI can enhance our\nunderstanding of wildfire predictions and support preparedness\nactivities. To achieve this, we examine different AI models\nand use SHAP with an adjusted visualization to interpret the\noutputs of these models. Our analysis reveals 1) how different\nAI models make decisions regarding wildfire risk, 2) which\nfeatures drive predictions, and 3) how insights into these\ncontributing factors can inform and improve preparedness\nstrategies. By revealing the internal reasoning of AI models,\nXAI empowers decision-makers to move beyond black-box\npredictions toward transparent preparedness planning. Theinsights derived from this analysis can guide more effective\nstrategies for wildfire mitigation and emergency management.\nA. AI Models for Wildfire Prediction\nTo ensure generalizable analysis, we employ various AI\nmodels widely used for wildfire prediction, includingdeep\nlearning models, i.e., LSTM, Transformer, GTN, andtree-\nbased models, i.e., Random Forest and XGBoost. They capture\ncomplex temporal patterns in wildfire data and allow compar-\nison of accuracy and interpretability across architectures.\nLSTM is a recurrent neural network designed to capture\nlong-term dependencies using gating mechanisms, making it\neffective for sequential data like wildfire events [59]. Trans-\nformer models rely on self-attention rather than recurrence\nto model sequence dependencies; we adopt an encoder-only\nversion for wildfire risk prediction [60]. GTN extends the\nTransformer by adding dual attention, making it well-suited\nfor multivariate time-series data in environmental settings [61].\nWe also include two ensemble tree-based models, including\nRandom Forest and XGBoost, known for handling noisy and\nheterogeneous data. Random Forest uses multiple decision\ntrees with majority voting to improve robustness [62], while\nXGBoost builds trees sequentially to refine predictions, im-\nproving accuracy and efficiency.\nWe train deep learning models using the Adam optimizer\nwith the following settings: LSTM with a learning rate (lr) of\n0.004and a weight decay (wc) of0.0063, Transformer with\nlr= 0.0001andwc= 0, and GTN withlr= 0.0012and\nwc= 0.0045. All models run for30epochs with a0.1learning\nrate decay every15epochs. We adopt model architectures and\ntraining pipelines from the open-source Mesogeos project [63].\nFor the tree-based models, we usesklearnimplementations\nwith100trees each. Random Forest uses Gini impurity with\na minimum split size of2and no depth limit, while XGBoost\nuses a learning rate of 0.3 and a max depth of6.\nB. Dataset\nWe conducted our analysis of wildfire prediction and XAI\nusing two datasets from distinct geographical regions. The\ndiverse approach allows us to examine how wildfire prediction\nmay vary across different regions, particularly in terms of\nfeature importance, and to draw broader, more generalizable\ninsights for preparedness and mitigation.\n1) General Information:The Mesogeos dataset is a large-\nscale dataset for wildfire modeling in the Mediterranean [63].\nWe use its wildfire danger forecasting subset, containing\n25,722samples:19,353for training,2,262for validation,\nand4,107for testing. Each sample includes24features over\n30days prior to a wildfire event, split into static (i.e., values\nremain the same for all days within a sample) and dynamic\n(i.e., values might change daily) types. Table I provides the\nfull name and explanations for each feature in this dataset.\nThe California Wildfires dataset covers weather conditions\nand wildfire occurrences in California from1984to2025[64].\nThe dataset includes nine feature groups, with seasons one-hot\nencoded. Table II provides the full name for each feature in this\nXAIIntrinsicLinear Models Linear Regression Model: e.g., [34] (Precipitation), [35] (Drought and Heatwave), [36] (Heatwave)\nDecision Tree Random Forests: [23] (Tornado and Hail), [24] (Wildfire), [37] (Wildfire)\nAttention MechanismConvLSTM: [25] (Wildfire); [38] (Wildfire), [39] (Heatwave and Drought), [40] (Tropical Cyclone)\nTranformer/Attention Augmented CNN: [41] (Wildfire), [42] (Precipitation), [43] (Wildfire)\nPost-hocFeature AttributionSHAP: [44] (Heatwave), [39] (Heatwave, Drought), [24] (Wildfire), [37] (Wildfire), [31] (Flooding), [45] (Cyclone), [46] (Anticyclone)\nLIME: [44] (Extreme Heat), [31] (Flooding), [37] (Wildfire)\nPartial Dependence Plots (PDP): [47] (Precipitation), [37] (Wildfire), [48] (Winter Precipitation)\nPermutation Importance: [23] (Tornado and Hail), [49] (Thunderstorm), [50] (Heatwave)\nCounterfactual Explanations: [28] (Heatwaves), [51] (Flooding), [52] (Wildfire)\nSurrogate Methods Linear Regression or Decision Tree: [27] (Floods, Storms), [53] (Arctic Sea-Ice Motion) [54] (Flood)\nVisualization-Based ExplanationRelevance Maps: [29] (Heatwaves), [55] (Decadal Temperature), [56] (Wildfire)\nGradient-Based Saliency Methods: [30] (Wildfire), [57] (Precipitation), [58] (Typhoon)\nFig. 2. Explainable AI (XAI) Techniques.\ndataset. A total of14,976observations are grouped into1,248\nsamples, each representing an11-day window preceding a\nwildfire event. Among them,998samples are used for training\nand250samples for testing.\nWhen performing XAI analysis with each dataset and each\nAI model, we only used samples that were correctly predicted\nas wildfire occurrences by the model in question. This sample\nselection is used to ensure that the results presented in our\nfinal visualizations are both consistent and reliable.\n2) Exploratory Data Analysis: In the Mesogeos dataset, six\nfeatures have missing values across multiple samples. Table\nIII presents their average percentage of missing values, with\nlst_dayandlst_nightmissing in about one-third of the\ntotal entries. The original preprocessing pipeline filled missing\nvalues with zeros [63], but for temperature features in Kelvin,\nthis implies absolute zero, which is a physically unrealistic\ncondition. To address this, we impute missing values using\nthe mean of each feature calculated from the training set.\nWhile this method introduces noticeable skewness, particularly\ninlst_dayandlst_night, mean imputation is more\nphysically plausible than zero-filling and does not significantly\ndegrade data utility. In fact, random checks with the LSTM,\nTransformer, and GTN models show higher accuracy using\nmean imputation compared to zero-filling. Another feature\nwith similar skewness issmi, with6.83%of its values\nimputed. Other dynamic features (e.g.,ndvi,rh) follow near-\nGaussian distributions, while static features (e.g., land cover,\npopulation) cluster near zero, reflecting the Mediterranean\nlandscape (Figure 3).\nIn the California Wildfires dataset, there are a to-\ntal of11features, comprising8original features and\na one-hot encodedseasonfeature represented by three\nmutually exclusive binary variables:season_winter,\nseason_spring, andseason_summer. Features such\nas precipitation, maximum temperature (max_temp), min-imum temperature (min_temp), and average wind speed\n(avg_wind_speed) are treated as independent. Other fea-\ntures are derived or correlated, such astemp_range(from\nmax_tempandmin_temp) andwind_temp_ratio\n(fromavg_wind_speedandmax_temp). Lagged fea-\ntures are computed over a 7-day window. The seasonal-\nity variables are mutually exclusive: ifseason_summer\nis1, thenseason_winterandseason_springare\n0, and vice versa. This dataset has one day with missing\nvalues inprecipitation,min_temp,max_temp, and\ntemp_range. Additionally, there are 12 missing entries in\navg_wind_speed, which also affectwind_temp_ratio.\nWe impute missing values in the independent features\nusing their global means. Missingtemp_rangeand\nwind_temp_ratiovalues are calculated using data from\ntheir defining features. Looking deeper into the dataset,\nthe independent featuresmin_temp,max_temp, and\navg_wind_speedexhibit roughly Gaussian distributions.\nBothmax_tempandavg_wind_speedare right-skewed,\nwhilemin_tempis slightly left-skewed and bimodal, re-\nflecting seasonal temperature shifts. The remaining inde-\npedent feature,precipitation, is highly skewed, with\n90.78%of values equal to zero, producing a near-uniform\ndistribution mirrored in its lagged version. Derived fea-\ntures such astemp_range,wind_temp_ratio, and\nlagged_avg_wind_speedare also right-skewed. Season-\nality features are uniformly distributed, with each binary\nvariable set to1in roughly25%of samples. Figure 4 shows\nrepresentative distributions in this dataset.\nC. Visualizing the Temporal Evolution of SHAP Values\nIn this study, we leverage the SHAP framework [8] to ex-\nplain wildfire predictions. While default SHAP visualizations,\nsuch as bar or summary plots, are effective for static data, they\nare inadequate to capture the temporal dynamics of time series.\nTABLE I\nFEATURES IN THEMESOGEOS DATASET.\nFeatures Abbreviation Full name\nStaticpopulation, slope, dem, roads distance Population, Slope, Elevation, Distance from roads\nlcwetland, lc shrubland, lc grassland Area of wetland, shrubland, grassland\nlcwater bodies, lc forest Area of water bodies, forest\nlcsparse vegetation Area of sparse vegetation\nlcsettlement, lc agriculture Area of settlement land, agricultural land\nDynamicwind speed, ssrd Wind speed, Surface solar radiation downwards\ntp, sp, rh Total precipitation, Surface pressure, Relative humidity\nt2m, d2m, lst night, lst day 2-meter temperature, Dewpoint temp., Night\u2019s and day\u2019s land surface temp.\nsmi, lai, ndvi Soil moisture index, Leaf area index, Normalized difference vegetation index\nTABLE II\nFEATURES IN THECALIFORNIAWILDFIRES DATASET.\nName Explanation\nprecipitation, lagged precipitation Daily precipitation, Cumulative precipitation over the preceding 7 days\nmax temp, min temp, temp range Maximum daily temp., Minimum daily temp., Daily temperature range\navg wind speed, wind temp ratio, lagged avg wind speed Average daily wind speed, Ratio of average wind speed to max temp.\nlagged avg wind speed Average wind speed over preceding 7 days\nseason The season of the observation (Winter, Spring, Summer, Fall)\nTABLE III\nPERCENTAGE OF MISSING VALUES ON SEVERAL FEATURES IN THEMESOGEOS DATASET.\nFeature lstnight lstday smi lai ndvi population\nPercentage (%) 36.43 31.49 6.83 1.69 0.25 0.01\nSpecifically, they do not show how the influence of individual\nfeatures evolves over time, which is crucial for understanding\nsequential patterns leading to wildfire events and identifying\npotential forecast opportunities.\nTo address this limitation, we extract SHAP values for each\nfeature across multiple time steps and create a custom scatter-\nplot visualization. For each feature, we plot its SHAP values\nover the days leading up to a wildfire event, such as30days for\nthe Mesogeos dataset and11days for the California Wildfires\ndataset. In this plot, dot color and size represent the direction\nand magnitude of the feature\u2019s impact, respectively. Using a\nblue-white-red colormap, blue dots indicate negative SHAP\nvalues, white represents near zero, and red denotes positive\nvalues. Dot size is proportional to the absolute SHAP value,\nemphasizing more influential contributions. This visualization\nprovides a clearer and more intuitive view of how key features\naffect model decisions over time, supporting more informed\nand actionable wildfire preparedness.\nD. Experimental Results\n1) Prediction Performance :Table IV presents the predic-\ntion accuracy of five AI models evaluated on the Mesogeos\nand California Wildfires datasets. Deep learning models con-\nsistently outperform tree-based ensemble models across both\ndatasets. On the Mesogeos dataset, the Transformer achieves\nthe highest accuracy at 87.53%, followed closely by LSTM\n(87.00%) and GTN (86.34%). In contrast, Random Forest\nand XGBoost perform worse, with accuracies of 77.23%\nand 75.00%, respectively. We observe a similar trend on the\nCalifornia Wildfires dataset, although the overall accuracies\nare lower due to its smaller size and higher variability. The\nTransformer again leads with 78.71% accuracy, followed by\nGTN (77.60%) and LSTM (73.49%), while Random Forestand XGBoost achieve 76.31% and 71.89%, respectively. These\nresults highlight the strength of deep learning models, partic-\nularly those based on attention mechanisms, in capturing the\ncomplex temporal patterns associated with wildfire prediction.\n2) SHAP Explanations and Visualization:In this section,\nwe enhance interpretability by visualizing SHAP values over\ntime, showing how features influence wildfire predictions\nacross sequential steps. We apply this to multiple AI models\nand both datasets, offering general insights here and detailed\nanalysis in Section IV-E.\nMesogeos Dataset.Figure 5 presents the average SHAP\nvalues across all samples in the Mesogeos dataset using an\nLSTM prediction model. Different from other visualizations\n[24], [37], we display all24features over a30-day window.\nOverall, key features driving the model\u2019s wildfire predictions\ninclude 2-meter air temperature (t2m), relative humidity (rh),\ndaytime and nighttime land surface temperature (lst_day,\nlst_night), and total precipitation (tp). Conversely, dew-\npoint temperature (d2m), soil moisture index (smi), and el-\nevation (dem) negatively impact predictions, indicating lower\nwildfire likelihood. Other features, such as land cover classes,\nhad minimal influence and are excluded from later figures\nto focus on more significant contributors. As expected, data\ncloser to the prediction date have greater impact; for instance,\nwhen predicting day 31, weather data from day 25 onward\ncontribute most. Figure 6 shows how the importance of the top\nfive features evolves over time, with notable changes beginning\naround day 28, matching the increased SHAP magnitudes seen\nin Figure 5. Feature importance also varies across individual\ncases, suggesting the model\u2019s reliance on certain features\ndepends on temporal and environmental context. While some\nfeatures show strong average effects, their impact can shifts\nTABLE IV\nACCURACY(%)OF MODELS TRAINED ONMESOGEOS ANDCALIFORNIAWILDFIRES DATASETS.\nModel LSTM Transformer GTN Random Forest XGBoost\nAccuracyMesogeos 87.00 87.53 86.34 77.23 75.00\nCalifornia Wildfires 76.80 78.71 77.60 76.31 71.89\n(a) lst day\n (b) ndvi\n (c) population\nFig. 3. Representative feature distributions from Mesogeos dataset.\n(a) max temp\n (b) min temp\n (c) precipitation\nFig. 4. Representative feature distributions from California Wildfires dataset.\nbased on local conditions, highlighting the model\u2019s dynamic\nbehavior and the value of SHAP for uncovering such nuances.\nFurthermore, the explanations generated by the Transformer-\nbased models are consistent with the aforementioned observa-\ntions with LSTM (Figure 7).\nIn tree-based methods such as Random Forest and XGBoost,\nthe evolution of feature contributions over the 30-day window\ndiffers from that of the deep learning models (i.e., LSTM,\nTransformer, GTN) when predicting wildfires on day31. In\nFigure 8, contributions come not only from the later days (e.g.,\nday 25 onward) but also from earlier days, such as day 2\ninssrdor day 5 inlst_night. The root cause is that\nthe Random Forest and XGBoost models do not use every\nfeature at every time step for node splits. As a result, some\nfeatures may receive SHAP values of zero on certain days.\nDespite these model architecture and temporal differences,\nthe key features with the highest SHAP values remain largely\nconsistent. In particular, temperature-related features continue\nto show strong correlations with wildfire risk across all models.\nCalifornia Wildfires Dataset.We observe similar patterns\nin the California Wildfires dataset, where later days in the\ntime window have a greater influence on wildfire predictions\ncompared to earlier days. In addition, explanations from deep\nlearning models (Figure 7b) slightly differ from those of tree-\nbased models (Figure 8b). Similar to the Mesogeos dataset,temperature-related features also have the strongest impact on\nthe model\u2019s predictions. SHAP values from individual samples\nshow evolution consistent with the average SHAP values\nacross the test set. Furthermore, explanations indicate that\nspring generally contributes the least to wildfire predictions,\nmeaning a lower wildfire risk during this season.\nE. Discussions\n1) Wildfire Prediction across Datasets and AI Models:\nAcross both datasets and all AI models, we observe several\nconsistent patterns. First, temperature-related features con-\ntribute most significantly to wildfire predictions across all\ncases. In the Mesogeos dataset, relative humidity also plays\na major role in model decisions, while in the California\nWildfires dataset, seasonality emerges as a key influencing\nfactor. Second, the temporal evolution of feature importance\ndiffers slightly between deep learning and tree-based models,\nand among deep learning models themselves. Whereas deep\nlearning models tend to assign greater importance to later days,\ntree-based models rely on both later and some earlier days.\nThis suggests that tree-based models such as Random Forest\nand XGBoost may be less effective in capturing temporal\ndynamics, even if they identify important features consistent\nwith those found by deep learning approaches.\nIn addition, different from LSTM, the Transformer model\ncan extract meaningful signals from both the start and end\nFig. 5. Visualization foraverageSHAP values for the Mesogeos dataset using the LSTM model.\nFig. 6. SHAP value evolution in the top five important features (Mesogeos and LSTM model).\n(a) Mesogeos\n (b) California Wildfires\nFig. 7. Average SHAP values in the Transformer model across datasets.\nof the input sequence. In Figure 7a, features such asndvi,\nlst_night, andd2min the Mesogeos dataset exhibit rel-\natively high absolute SHAP values on both the first day and\nlast day of the time window. Other variables, such asssrd\nandsmi, have lower SHAP values by the last day, but retain\ntheir overall positive contributions to model\u2019s decisions. This\npattern is less pronounced in models trained on the California\nWildfires dataset, likely due to its shorter time window and\nfewer samples. Consequently, Transformers trained on Califor-\nnia data may lack the temporal depth to learn patterns similar\nto those from Mesogeos, resulting in different trends.\n2) Seasonality of wildfires: In the California Wildfires\ndataset, SHAP explanations indicate that seasonality plays\na significant role in predicting wildfire occurrence. AcrossAI models, at least one season-related feature has a high\nSHAP value, withseason_summerbeing the strongest\nin Transformer and Random Forest models. For the LSTM\nand GTN models,season_summerandseason_winter\nhave comparable impacts on model decisions. These results\nsuggest a general consensus among the models that summer\nis the most fire-prone season, while certain winter conditions\nmay also contribute to wildfire risk. However, California\u2019s\ndiverse climate, with wide temperature and precipitation vari-\nation across regions and years [65]. Further investigation into\nthis geographic and climatic variability can clarify seasonal\nwildfire patterns and improve model interpretations.\nIn the Mesogeos dataset, no features explicitly encode\nseasonality. To examine seasonal effects, we visualize av-\n(a) Mesogeos\n (b) California Wildfires\nFig. 8. Average SHAP values in the Random Forest model across datasets.\nerage SHAP values by month (Figure 9). From April to\nAugust, which are typically considered the summer period,\ntemperature-related features strongly contribute positively,\nwhile dewpoint temperature contributes negatively, , which is\nconsistent with the known physical variables that contribute to\nwildfires. SHAP patterns vary by month, suggesting that wild-\nfire risk factors change seasonally. Intuitively, higher temper-\natures and stronger winds increase wildfire risk, especially in\nsummer, while more precipitation and higher dewpoint reduce\nit. During colder months, conditions affect fires differently.\nOur results show that precursors and forecasts vary by region\nand season, highlighting the need to incorporate temporal\ncontext in wildfire prediction and interpretation.\n3) Forecast opportunities beyond weather time scale:\nBased on Section IV-D2, later days in the temporal window\ngenerally exert stronger influence on wildfire predictions,\nwhich is intuitive given that recent conditions are typically\nmore relevant for forecasting. However, certain variables exert\nsignificant effects much earlier. For instance, in Figure 5,\ntpandlst_dayhave high SHAP values as early as day 4,\nwhile in Figure 7b,max_tempandmin_tempcontribute\nmeaningfully by day 3. These early signals suggest that\nsome features encode lasting or cumulative influence, thereby\nextending wildfire predictability. Recognizing these signals\nstrengthens early warning and long-term wildfire planning.\nWe further evaluate how this feature importance translates\ninto model performance (Table V). Using the Transformer\narchitecture, we trained models on subsets of features ranked\nby their importance derived from SHAP. Features with high\nabsolute SHAP values were classified as the most important,\nwhile those with values near zero were considered the least\nimportant. When trained on only ten features, the model\ntrained on the most important subset markedly outperformed\nthe model trained on the least important subset, with an accu-\nracy difference of 3.75%. Interestingly, this margin was larger\nthan the performance gap between the full model (trained\non all twenty-four features) and the model trained on the\nten most important features (3.30%). In addition to accuracy\ngains, prioritizing high-impact features reduced computational\ncost. Training time decreased by 3.86 seconds per epoch\nwhen restricting the training set to ten features, corresponding\nto nearly two minutes saved over a thirty-epoch schedule,while the model did not suffer significant performance degra-\ndation. Collectively, these findings underscore the value of\nexplainability-guided feature selection: by emphasizing early\nand influential signals, forecasting models can achieve high\naccuracy at lower computational cost, ultimately supporting\nmore timely and effective responses to extreme wildfire events.\n4) Results validation using LIME:To complement the\nSHAP-based analyses, we further employed LIME [26] to\ninterrogate our models\u2019 predictions. As shown in Figure 10,\nthe LIME explanations broadly corroborate the SHAP re-\nsults across multiple features. For instance, both methods\nidentifysmias exerting a positive influence on wildfire\nprediction, while also capturing the early effects ofrhand\nd2m. Notable discrepancies emerge for static features such as\npopulation and land cover classes, which is expected given\nthat LIME was originally designed to explain local, instance-\nlevel predictions rather than global trends across the entire\ndataset. Nevertheless, the convergence of LIME and SHAP on\nkey dynamic predictors underscores a degree of consistency\nbetween the two XAI approaches, reinforcing confidence in\nthe interpretability of the models\u2019 outputs. LIME results are\nalso aligned with physical understanding, given that land cover\ntypes can impact the occurrence of wildfires.\n5) Implications for Mitigation and Emergency Manage-\nment:Using SHAP to interpret wildfire predictions from\nvarious AI/ML models reveals that the feature contributions\nto wildfire likelihood generally align with the known physi-\ncal understanding of wildfire occurrence, thereby increasing\nconfidence and trustworthiness of AI/ML predictions. The in-\nsights gained from SHAP-based explanations offer actionable\nopportunities for improving wildfire mitigation and emergency\nresponse strategies. Across both the Mesogeos and California\nWildfires datasets, temperature-related features emerged as the\nmost influential predictors, reinforcing the importance of mon-\nitoring thermal conditions, such as 2-meter air temperature,\nday- and night-time land surface temperature, and dewpoint\ntemperature, as part of early-warning systems.\nBy understanding how AI models respond to these factors\nover time, emergency management agencies can better align\ntheir decision-making processes with evolving risk conditions.\nFor instance, deep learning models consistently identify later\ndays in the time window as more impactful, indicating that\n(a) February\n (b) July\nFig. 9. Average SHAP values for wildfires in February and July (Mesogeos, LSTM model).\nTABLE V\nACCURACY AND AVERAGE TRAINING TIME OFTRANSFORMER MODEL TRAINED ON DIFFERENT COMBINATIONS OFMESOGEOS FEATURES.\nFeature combination Top-5 Top-10 Top-20 Original (24 features)\nAccuracy (%) (Most important) 80.99 83.86 86.83 87.16\nAccuracy (%) (Least important) 73.90 80.11 85.63 87.16\nTraining time per epoch (seconds) 34.22 37.72 40.14 41.58\nFig. 10. Visualization foraverageLIME values for the Mesogeos dataset using the Transformer model.\nreal-time environmental monitoring in the days immediately\npreceding a potential fire event is crucial for accurate fore-\ncasting and rapid response. In addition, the presence of early\nsignals could serve as triggers for proactive mitigation strate-\ngies, including: 1) Pre-deployment of firefighting resources to\nhigh-risk zones, 2) Public advisories and evacuation planning\nin vulnerable regions, and 3) Controlled burns or vegetation\nmanagement ahead of peak fire risk periods.\nMoreover, seasonal patterns revealed by model explanations\nprovide guidance for long-term planning. For example, since\nmost models agree that summer is the most fire-prone period,\npreparedness efforts should ramp up before summer. The\ndifferent SHAP patterns in summer and winter months suggest\nthat incorporating season-related features or temporal markers\ninto prediction models could enhance both interpretability and\neffectiveness in operational settings.\nFinally, the observed limitations of tree-based models in\ncapturing temporal dynamics underscore the need for deploy-\ning temporal-aware architectures (e.g., LSTM, Transformer,\nGTN) in real-world applications where the timing of envi-\nronmental signals is critical. Leveraging model explanations\nnot only improves trust in AI predictions but also supports\nevidence-based policy decisions for wildfire risk mitigation\nand emergency response planning.V. CONCLUSION\nThis paper investigated how existing XAI methods can\nimprove understanding and interpretation of wildfire prediction\nmodels. By visualizing SHAP explanations across various AI\nmodels and two geographically distinct datasets, we found a\nconsistent emphasis on temperature-related features, seasonal-\nrelated, and precipitation, etc. as key drivers of wildfire fore-\ncasts. While this aligns with domain knowledge, the early sig-\nnals revealed by SHAP values offer valuable insights for early\nwarning and emergency planning. These insights can help\nauthorities anticipate wildfire risk well in advance, enabling\nmore proactive resource allocation and mitigation strategies.\nOverall, our findings demonstrate the potential of explainable\nAI to enhance wildfire prediction reliability and support more\neffective disaster preparedness and response.\nFollowing the NASA\u2019s FAIRUST principles, this work is\ngrounded in public, verifiable data and will share code and\nmethods to ensure the research is Findable, Accessible, In-\nteroperable, and Reusable. Crucially, by focusing on making\nwildfire forecasting more Understandable through state-of-the-\nart XAI, we promote interpretability and reliability. Adopting\nFAIRUST principles advances transparency in AI for climate\nscience while fostering open and accessible research.\nREFERENCES\n[1] P. Stott, \u201cHow climate change affects extreme weather events,\u201dScience,\nvol. 352, no. 6293, 2016.\n[2] W. A. Robinson, \u201cClimate change and extreme weather: A review\nfocusing on the continental united states,\u201dJA&WMA, 2021.\n[3] C. Parmesanet al., \u201cImpacts of extreme weather and climate on\nterrestrial biota,\u201dBulletin of the American Meteorological Society, 2000.\n[4] K. L. Ebiet al., \u201cExtreme weather and climate change: population health\nand health system implications,\u201dAnnu. Rev. Public Health, vol. 42, 2021.\n[5] K. V . Goughet al., \u201cVulnerability to extreme weather events in cities:\nimplications for infrastructure and livelihoods,\u201dJBA, 2019.\n[6] G. Camps-Vallset al., \u201cAI for modeling and understanding extreme\nweather and climate events,\u201dNature Communications, vol. 16, 2025.\n[7] A. DiVentiet al., \u201cNasa\u2019s safety, reliability, and mission assurance digital\nfuture,\u201d inRAMS, 2023.\n[8] S. M. Lundberget al., \u201cA unified approach to interpreting model\npredictions,\u201d inNeurIPS, 2017.\n[9] A. B. Smith, \u201c2023: A historic year of u.s. billion-dollar weather and\nclimate disasters,\u201d 2023.\n[10] NCEI, \u201cGlobal Forecast System (GFS),\u201d 2025.\n[11] IPCC, \u201cClimate change 2023: Synthesis report. Contribution of working\ngroups I, II and III to the sixth assessment report of the intergovernmen-\ntal panel on climate change,\u201d 2023.\n[12] G. C. Budget, \u201cGlobal carbon budget 2023,\u201d 2023.\n[13] D. I. Domeisenet al., \u201cAdvances in the subseasonal prediction of\nextreme events: Relevant case studies across the globe,\u201dBAMS, 2022.\n[14] Y . Lyuet al., \u201cImproving subseasonal-to-seasonal prediction of summer\nextreme precipitation over southern china based on a deep learning\nmethod,\u201dGeophysical Research Letters, vol. 50, no. 24, 2023.\n[15] X. Liuet al., \u201cClassified early warning and forecast of severe convective\nweather based on LightGBM algorithm,\u201dACS, 2021.\n[16] X. Xiaoet al., \u201cLong-term forecast of heatwave incidents in China based\non numerical weather prediction,\u201dTAAC, 2024.\n[17] A. Chattopadhyayet al., \u201cAnalog forecasting of extreme-causing weather\npatterns using deep learning,\u201dJAMES, vol. 12, no. 2, 2020.\n[18] A. Dikshitet al., \u201cAn improved SPEI drought forecasting approach using\nthe long short-term memory neural network,\u201dJEM, 2021.\n[19] W. Jianget al., \u201cTransformer-based tropical cyclone track and intensity\nforecasting,\u201dJWEIA, vol. 238, 2023.\n[20] J. L. Coenet al., \u201cWrf-fire: coupled weather\u2013wildland fire modeling\nwith the weather research and forecasting model,\u201dJAMC, vol. 52, 2013.\n[21] C. Rudin, \u201cStop explaining black box machine learning models for high\nstakes decisions and use interpretable models instead,\u201dNature machine\nintelligence, vol. 1, no. 5, pp. 206\u2013215, 2019.\n[22] R. Yanget al., \u201cInterpretable machine learning for weather and climate\nprediction: A review,\u201dAtmospheric Environment, 2024.\n[23] E. D. Lokenet al., \u201cComparing and interpreting differently designed\nrandom forests for next-day severe weather hazard prediction,\u201dWeather\nand Forecasting, vol. 37, no. 6, pp. 871\u2013899, 2022.\n[24] R. Cilliet al., \u201cExplainable AI detects wildfire occurrence in the\nMediterranean countries of Southern Europe,\u201dScientific reports, 2022.\n[25] A. Masrur, M. Yuet al., \u201cCapturing and interpreting wildfire spread\ndynamics: attention-based spatiotemporal models using ConvLSTM\nnetworks,\u201dEcological Informatics, vol. 82, 2024.\n[26] M. T. Ribeiro, S. Singh, and C. Guestrin, \u201c\u201cWhy should i trust you?\u201d\nexplaining the predictions of any classifier,\u201d inACM SIGKDD, 2016.\n[27] M. Ronco, J. M. T \u00b4arragaet al., \u201cExploring interactions between socioe-\nconomic context and natural hazards on human population displace-\nment,\u201dNature Communications, vol. 14, no. 1, 2023.\n[28] J. T. Trok, E. A. Barneset al., \u201cMachine learning\u2013based extreme event\nattribution,\u201dScience Advances, vol. 10, no. 34, 2024.\n[29] J. Wei, A. Bora, V . Oommen, C. Donget al., \u201cXAI4Extremes: An inter-\npretable ML framework for understanding extreme-weather precursors\nunder climate change,\u201darXiv, 2025.\n[30] G. Camps-Vallset al., \u201cAI for extreme event modeling and understand-\ning: Methodologies and challenges,\u201darXiv, 2024.\n[31] L. Peng, L. Gao, F. Hong, and J. Sun, \u201cEvaluating pavement deteriora-\ntion rates due to flooding events using explainable ai,\u201dBuildings, 2025.\n[32] A. Abdollahi and B. Pradhan, \u201cXAI for interpreting the contributing\nfactors feed into the wildfire susceptibility prediction model,\u201dScience\nof the Total Environment, vol. 879, p. 163004, 2023.[33] B. Liao, T. Zhouet al., \u201cTackling the wildfire prediction challenge: An\nXAI model combining XGBoost with SHAP for enhanced interpretabil-\nity and accuracy,\u201dForests, vol. 16, no. 4, 2025.\n[34] G. R. Hermanet al., \u201c\u201cDendrology\u201d in numerical weather prediction:\nWhat random forests and logistic regression tell us about forecasting\nextreme precipitation,\u201dMonthly Weather Review, 2018.\n[35] S. Mukherjeeet al., \u201cCompound drought and heatwaves at a global scale:\nThe role of natural climate variability-associated synoptic patterns and\nland-surface energy budget anomalies,\u201dJGR: Atmospheres, 2020.\n[36] P. Amoateyet al., \u201cEvaluating the association between heatwave vul-\nnerability index and related deaths in Australia,\u201dEIA Review, 2025.\n[37] D. Fanet al., \u201cExplainable AI integrated feature engineering for wildfire\nprediction,\u201darXiv, 2024.\n[38] H. S. Andrianarivony and M. A. Akhloufi, \u201cMachine learning and deep\nlearning for wildfire spread prediction: A review,\u201dFire, vol. 7, 2024.\n[39] O. J. Pellicer-Valeroet al., \u201cExplainable earth surface forecasting under\nextreme events,\u201darXiv, 2024.\n[40] Y . Liuet al., \u201cApplication of deep convolutional neural networks for\ndetecting extreme weather in climate datasets,\u201darXiv, 2016.\n[41] M. Marjani, M. Mahdianpariet al., \u201cApplication of XAI in predicting\nwildfire spread: An ASPP-enabled CNN approach,\u201dIEEE GRSL, 2024.\n[42] W. Huang, \u201cExtreme precipitation forecasting using attention augmented\nconvolutions,\u201darXiv preprint arXiv:2201.13408, 2022.\n[43] I. Prapaset al., \u201cTelevit: Teleconnection-driven transformers improve\nsubseasonal to seasonal wildfire forecasting,\u201d inICCV, 2023.\n[44] F. Shafiqet al., \u201cExtreme heat prediction through deep learning and\nXAI,\u201dPloS one, vol. 20, no. 3, 2025.\n[45] S. Liuet al., \u201cEvaluation of tropical cyclone disaster loss using machine\nlearning algorithms with an XAI approach,\u201dSustainability, 2023.\n[46] H. Zhanget al., \u201cUsing XAI and transfer learning to understand and\npredict the maintenance of Atlantic blocking with limited observational\ndata,\u201dJGR: Machine Learning and Computation, 2024.\n[47] P. Gibsonet al., \u201cTraining ML models on climate model output yields\nskillful interpretable seasonal precipitation forecasts,\u201dCEE, 2021.\n[48] C. C. Ibebuchi, \u201cUncertainty in machine learning feature importance for\nclimate science: a comparative analysis of SHAP, PDP, and gain-based\nmethods,\u201dTheoretical and Applied Climatology, vol. 156, 2025.\n[49] M. J. Molinaet al., \u201cA benchmark to test generalization capabilities of\ndeep learning methods to classify severe convective storms in a changing\nclimate,\u201dEarth and Space Science, vol. 8, no. 9, 2021.\n[50] N. J. Leach, A. Weisheimeret al., \u201cForecast-based attribution of a winter\nheatwave within the limit of predictability,\u201dPNAS, vol. 118, 2021.\n[51] X. Chenet al., \u201cCounterfactual analysis of extreme events in urban\nflooding scenarios,\u201dJournal of Hydrology: Regional Studies, 2025.\n[52] M. P. Thompson and J. F. Carriger, \u201cAvoided wildfire impact modeling\nwith counterfactual probabilistic analysis,\u201dFFGC, vol. 6, 2023.\n[53] L. Hoffmanet al., \u201cEvaluating the trustworthiness of XAI methods\napplied to regression predictions of Arctic sea-ice motion.\u201dAIES, 2025.\n[54] N. Fraehr, Q. J. Wang, W. Wu, and R. Nathan, \u201cAssessment of surrogate\nmodels for flood inundation: The physics-guided LSG model vs. state-\nof-the-art machine learning models,\u201dWater Research, vol. 252, 2024.\n[55] P. L. Bommeret al., \u201cFinding the right XAI method\u2014a guide for the\nevaluation and ranking of XAI methods in climate science,\u201dAIES, 2024.\n[56] Y . Zhouet al., \u201cComparative and interpretative analysis of CNN and\ntransformer models in predicting wildfire spread using remote sensing\ndata,\u201dJGR: Machine Learning and Computation, vol. 2, no. 2, 2025.\n[57] L. Chenet al., \u201cA machine learning model that outperforms conventional\nglobal subseasonal forecast models,\u201dNature Communications, 2024.\n[58] M. Higaet al., \u201cDomain knowledge integration into deep learning for\ntyphoon intensity classification,\u201dScientific reports, vol. 11, 2021.\n[59] R. C. Staudemeyer and E. R. Morris, \u201cUnderstanding LSTM\u2013a tutorial\ninto long short-term memory recurrent neural networks,\u201darXiv, 2019.\n[60] A. Vaswaniet al., \u201cAttention is all you need,\u201dNeurIPS, vol. 30, 2017.\n[61] M. Liuet al., \u201cGated transformer networks for multivariate time series\nclassification,\u201darXiv, 2021.\n[62] L. Breiman, \u201cRandom forests,\u201dMachine learning, vol. 45, 2001.\n[63] S. Kondylatoset al., \u201cMesogeos: A multi-purpose dataset for data-driven\nwildfire modeling in the Mediterranean,\u201dNeurIPS, vol. 36, 2023.\n[64] C. E. Yavas, C. Kadlec, J. Kim, and L. Chen, \u201cCalifornia weather and\nfire prediction dataset (1984\u20132025) with engineered features,\u201d 2025.\n[65] J. Null and H. M. Mogil, \u201cThe weather and climate of California,\u201d\nWeatherwise: The Power, the Beauty, the Excitement, vol. 63, 2010.\n",
    "title": "From Black Box to Insight: Explainable AI for Extreme Event Preparedness",
    "authors": [
      "Kiana Vu",
      "\u0130smet Sel\u00e7uk \u00d6zer",
      "Phung Lai",
      "Zheng Wu",
      "Thilanka Munasinghe",
      "Jennifer Wei"
    ],
    "published": "2025-11-17",
    "arxiv_id": "2511.13712v1",
    "num_pages": 10,
    "num_chars": 49866
  },
  {
    "text": "From Power to Precision: Learning Fine-grained Dexterity\nfor Multi-fingered Robotic Hands\nJianglong Ye\u2217, Lai Wei\u2217, Guangqi Jiang, Changwei Jing\nXueyan Zou, Xiaolong Wang\nUC San Diego\njianglongye.com/power-to-precision\nFig. 1: Our system enables multi-fingered robotic hands to perform a diverse set of manipulation tasks, ranging from precise\nmanipulation of small objects (e.g., pens, nuts, batteries) to power grasps of larger items (e.g., frying pans). This versatility\nand dexterity are achieved through a co-design framework for both control and fingertip-geometry optimization.\nAbstract\u2014 Human grasps can be roughly categorized into two\ntypes: power grasps and precision grasps. Precision grasping\nenables tool use and is believed to have influenced human\nevolution. Today\u2019s multi-fingered robotic hands are effective\nin power grasps, but for tasks requiring precision, parallel\ngrippers are still more widely adopted. This contrast high-\nlights a key limitation in current robotic hand design: the\ndifficulty of achieving both stable power grasps and precise,\nfine-grained manipulation within a single, versatile system. In\nthis work, we bridge this gap by jointly optimizing the control\nand hardware design of a multi-fingered dexterous hand,\nenabling both power and precision manipulation. Rather than\nredesigning the entire hand, we introduce a lightweight fingertip\ngeometry modification, represent it as a contact plane, and\njointly optimize its parameters along with the corresponding\ncontrol. Our control strategy dynamically switches between\npower and precision manipulation and simplifies precision\ncontrol into parallel thumb\u2013index motions, which proves robust\nfor sim-to-real transfer. On the design side, we leverage large-\nscale simulation to optimize the fingertip geometry using a\ndifferentiable neural-physics surrogate model. We validate our\napproach through extensive experiments in both sim-to-real\nand real-to-real settings. Our method achieves an 82.5% zero-\nshot success rate on unseen objects in sim-to-real precision\ngrasping, and a 93.3% success rate in challenging real-world\ntasks involving bread pinching. These results demonstrate that\nour co-design framework can significantly enhance the fine-\n\u2217Equal contribution.grained manipulation ability of multi-fingered hands without\nreducing their ability for power grasps.\nI. INTRODUCTION\nA classical taxonomy divides human grasps into two main\ncategories: thepower grasp, which secures objects against\nthe palm with the fingers, and theprecision grasp, which\nplaces the thumb in opposition to other fingertips [1], [2].\nWhile both are fundamental, the precision grasp is particu-\nlarly associated with the fine-grained manipulation required\nfor tool use in early humans, which significantly shaped the\nevolutionary trajectory of our species [3]\u2013[5].\nRecent advancements have shown that multi-fingered\nrobotic hands are effective in power grasp, as more con-\ntact points provide greater stability [6]\u2013[9]. In contrast, for\nfine-grained manipulation tasks requiring precision, two-\nfinger parallel grippers are more widely adopted, with im-\npressive applications ranging from folding shirts to gear\ninsertion [10]\u2013[14]. Therefore, replicating the human-level\ndexterity in multi-fingered hands, especially for precision-\noriented manipulation, remains a fundamental challenge.\nOur goal is to enable a multi-fingered dexterous hand\nwith reliable precision manipulation while preserving strong\npower grasp capability. We aim for the following goals:\n(i) plug-and-play compatibility with existing commerciallyarXiv:2511.13710v1  [cs.RO]  17 Nov 2025\navailable multi-fingered hands by augmenting fingertip ge-\nometry rather than designing a new hand from scratch, (ii)\nachieving both power and precision manipulation in a unified\nhardware platform and control strategy, and (iii) validation\nwith both sim-to-real transfer using large-scale simulation\nand real-to-real policies using teleoperation.\nTo achieve these goals, we adopt a joint control\u2013design\noptimization framework. On the control side, we switch\nbetween power and precise manipulation, where precise\nmanipulation is simplified into coordinated thumb\u2013index\nactions and parallel finger motions. We also propose a\nneural switcher that dynamically switch to corresponding\ngrasp mode based on the object. On the design side, we\nleverage large-scale simulation with a neural physics surro-\ngate model to co-optimize fingertip geometry and control\nvariables, yielding physically effective fingertip designs. To-\ngether, these components provide a simple and generalizable\nsolution for both precise and power manipulation.\nWe validate the proposed system in both sim-to-real and\nreal-to-real settings. In the sim-to-real setting, we focus on\ngrasping tasks, where objects are categorized into power\ngrasps and precise grasps. Demonstrations are optimized\nwith multiple objectives, and a policy is learned from them.\nReal-world experiment results show that our method out-\nperforms the SOTA method on precise grasps by a large\nmargin. In the real-to-real setting, we tackle more difficult\ncompositional task and precision task. Demonstrations are\ncollected using our switchable teleoperation, where precise\nmanipulation use optimized control. The trained policies are\ncapable of picking up an M4 hex nut with precision and\npower grasping a pan handle, as shown in Fig. 1.\nIn summary, we claim the following contributions:\n\u2022Control optimization for precise manipulation, including\nthumb-index motion generation for both grasp synthesis\nand real-world teleoperation.\n\u2022Design optimization with a contact-plane geometry rep-\nresentation and a neural-physics surrogate distilled from\nlarge-scale simulations.\n\u2022Extensive experiments that demonstrate the effectiveness\nof our co-design framework in both simulation and the\nreal world, showing that our system improves precise\nmanipulation ability without compromising power grasp.\nII. RELATEDWORK\nPower to Precise Manipulation.Power manipulation\nrefers to using whole-hand contacts to apply large, robust\nforces that move or stabilize objects without requiring high\npose accuracy (e.g., holding a hammer, grasping a large\nbottle). Recent studies [6]\u2013[9], [15]\u2013[19] have shown that\nmulti-fingered robotic hands perform well in power manip-\nulation, mainly because additional contact points increase\nstability. For example, [6], [7] leverage large-scale simulation\nto learn dexterous grasping policies and employ point cloud\nobservations for robust sim-to-real transfer. [18], [19] use\nVR devices to collect high-quality teleoperation data and\ndemonstrate impressive dexterous hand tasks.In contrast,precise manipulationrefers to using fingertip-\nscale, finely controlled motions and contacts to achieve\naccurate object poses and delicate interactions (e.g., inserting\na key, turning a screw). For these precise tasks, parallel\ngrippers are more widely adopted [10]\u2013[14]. How to achieve\nprecise manipulation, especially with multi-fingered hands,\nis still an open problem for the community. To this end,\nprevious works [20]\u2013[24] either design better hardware or\nuse more accurate sensors such as tactile sensors. [20], [21]\noptimize hardware design to achieve precise manipulation,\nwhile the more general approaches [22]\u2013[24] employ tactile\nsensing to facilitate precise manipulation. Instead of design-\ning new hardware or using tactile sensing, our approach\noptimizes both control and finger tip geometry for precise\nmanipulation.\nComputational Design and Co-Design.Previous compu-\ntational methods for mechanical design often use gradient-\nbased optimization and task-aware planning over geometry\nor topology to create structures for target tasks [25], [26].\nHowever, these pipelines can be computationally expen-\nsive and may not generalize well to new objects or tasks.\nDifferentiable simulators address this by enabling the joint\noptimization of morphology and control within a single\nframework [27]. Recent work has increasingly utilized co-\ndesign as a tool for manipulation, such as in the creation\nof dexterous hands [28], fingertips [29], and grippers [30],\n[31]. Rather than designing a completely new dexterous\nhand, we enhance an existing hand by optimizing its ge-\nometry. In our co-design framework, this is achieved by\nattaching a newly optimized fingertip cover that improves\nprecise manipulation. During optimization, we model the\nfingertip geometry as a contact plane and leverage large-scale\nsimulation by using a neural dynamics surrogate.Imitation\nLearning.Imitation learning is a common framework for\nlearning robot control policies from demonstrations, and it\nhas been applied in many recent works [18], [19], [32]\u2013\n[42]. Demonstrations may be obtained through teleopera-\ntion [19], [43]\u2013[45], simulation [6], [46]\u2013[48], real-world\npolicy trial-and-error [49], [50], human videos [37], [39],\n[51], or a mixture of these sources [38], [52], [53]. In this\nwork, we validate our approach on real-to-real tasks using\nteleoperation data and on sim-to-real tasks using simulation\ndata. We apply existing imitation learning methods [6], [10]\nto our collected demonstrations. Our co-design framework\nimproves the robustness of both demonstrations and the\nrobotic hand, leading to more effective precise manipulation\npolicies in simulation and the real world.\nIII. METHOD\nA. Overview\nThis work focuses on learning manipulation skills for\nmulti-fingered hands using imitation learning. In this frame-\nwork, a control policy\u03c0learns from expert demonstrations\nD= (O,A), consisting of observationsOand actionsA\ncollected through teleoperation, simulation, or other methods.\nTo achieve precise manipulation, we introduce an approach\nthat jointly optimizes both control (Sec. III-B) and design\n(Sec. III-C) of dexterous hands. We cross validate our\napproach on tasks under both sim-to-real and real-to-real\nsettings, where data is collected through optimization in\nsimulation and teleoperation in the real world.\nControl Optimization.Our control method follows two\nprinciples. The first iscategorization, where we switch\nbetween power and precision grasps based on the target\nobject. For example, in ourCooking Setuptask, a power\ngrasp is used for a pot handle, while a precision pinch\ngrasp is used for a thin asparagus. The second principle is\nsimplification. We achieve this by reducing the degrees of\nfreedom of the hand and the number of contact points. We\nalso constrain finger motion to be parallel movements during\nprecision grasps. Although it is possible to use more complex\nmotions for precise manipulation, our experiments show that\nsimpler motions are effective for both retargeting and sim-\nto-real transfer.\nDesign Optimization.An effective control strategy can\nstill be limited by the hand\u2019s physical design. To address this,\nwe apply a co-design framework to optimize the fingertip\ngeometry for precise manipulation. We model the contact\nregion between the thumb and index finger as a plane and\nsearch for the optimal plane. This optimization process uses\na learned, differentiable forward dynamics model along with\nseveral objectives, which allows us to leverage large-scale\nsimulation to find a better fingertip design.\nB. Control Optimization for Precise Manipulation\nThe robot\u2019s action spaceAis defined as the target joint\npositionsq\u2208Rd, whered= 19(7 for the arm and 12 for\nthe hand). The collected data, in both simulation and the real\nworld, consist of observationsOand trajectories of actions\n{qt}T\nt=1, where eachq t\u2208 A.\nSim-to-real Grasping.We proof the effectiveness of the\nproposed optimization pipeline in Sim-to-real settings. In\nsimulation, a common way to collect demonstrations for\ndexterous grasping is to first optimize for force closure\nand then apply motion planning combined with simulation\nfiltering [6], [7], [48]. Although this framework can produce\ngrasps that work even for tiny objects in simulation, the poses\nare often complex because all fingers are encouraged to apply\nforce to the object. Deploying these grasps to the real world\nis impractical due to the sim-to-real gap.\nWe categorize grasping poses based on the type of target\nobject. For simplicity, we focus on two categories: power\ngrasp for large objects, and precision grasp for small or\nthin objects. For power grasp optimization, we use the\noriginal force closure objective from [54]. For precision\ngrasp, we introduce a variant objective. Given object mesh\nOand joint configurationq, we samplencontacts on\nthe thumb andnon the index fingertip. Each contact has\npositionx i\u2208R3and normalc i\u2208R3. The grasp map\nisG=\u0014I3\u00b7\u00b7\u00b7I 3\n[x1]\u00d7\u00b7\u00b7\u00b7[x 2n]\u00d7\u0015\n,where[x] \u00d7is the skew-\nsymmetric matrix ofx. We minimizeE precise =\u2225G c\u2225 2,\nwithc= [c\u22a4\n1, . . . , c\u22a4\n2n]\u22a4, which encourages the net wrench\nFig. 2:Control Optimization.For precise grasps control,\nwe optimize for opposite force closure and parallel finger\nmotions, which significantly improve sim-to-real transfer.\nto approach zero for the thumb-index grasp. We also fix other\nfingers during optimization to reduce the degrees of freedom.\nAfter optimizing the grasp pose, a common way for\npower grasp to obtain pre-grasp and overshoot-grasp poses\nis based on the object\u2019s signed distance function (SDF),\nwhich pushes fingers toward the object surface [6], [48]. We\npropose to use simple parallel motions for precision grasps.\nLetdbe the normalized direction vector from the thumb\ncontact points to the index contact points. To generate a\npre-grasp pose, the thumb and index move apart by a small\ndistance\u03b1. The required joint velocity are calculated using\nthe Jacobian pseudoinverseJ\u2020as\u2206q thumb =\u2212\u03b1J\u2020\nthumbd,\n\u2206qindex =\u03b1J\u2020\nindexd. Experimental results show that this\nsimple thumb-index grasp with parallel motion is robust for\nsim-to-real deployment. The finger motion and its real-world\ncounterpart are visualized in Fig. 2.\nAll demonstrations are filtered using the ManiSkill simu-\nlator [55]. After data collection, we train two DexSimple [6]\npolicies, one on power grasp data and one on precision grasp\ndata. We then add a switcher consisting of PointNet [56]\nand an MLP to predict whether an object should be grasped\nwith a power grasp or a precision grasp, and apply the\ncorresponding policy actions. The policy is deployed to real\nworld in a zero-shot fashion.\nReal-to-real Tasks.We tackle both compositional tasks\nthat combine power and precision grasps, and pure precision\ntasks in a real-to-real setting. Teleoperation is used to collect\ndemonstrations. The standard position-based retargeting [57]\nstruggles with fine-grained actions such as pinching a nut.\nTo address this, we switch between normal retargeting and a\npinch controller. The hand pose is optimized by minimizing\nEprecise =\u2225G c\u2225 2,wherecrepresents contact normals\nbetween the thumb and index finger (rather than between\nhand and object). We apply the same parallel finger motions\nused in the sim-to-real setting, where the thumb and index\nmove along directiondwith joint updates\u2206q=J\u2020d.\nFinally, an ACT policy [10] is trained on these teleoperated\ndemonstrations for deployment.\nC. Design Optimization for Precise Manipulation\nWe aim to optimize hand geometry to enhance precision\nmanipulation without compromising power manipulation. To\nthis end, we represent geometry using a simple contact plane\nP, parameterized by a reference pointpand a unit normal\nFig. 3:Design Optimization.We optimize fingertip geome-\ntry (represented as a contact plane) under multiple objectives.\nThe resulting fingertip cover improves precision manipula-\ntion in real-world.\nvectorn:P={x\u2208R3|n\u22a4(x\u2212p) = 0}.This simplifies\nsimulation and improves robustness in sim-to-real transfer.\nGivenP, we project a slightly inflated convex hull of the\nfingertip onto it and 3D print the resulting union geometry.\nThe fingertip cover is easy to assemble and generalizes well\nto different multi-fingered hands.\nDuring plane parameter optimization, we jointly optimize\nPand the joint positionqunder multiple objectives using\ngradient descent. (Note thatqhere is not used for grasp\noptimization; it assumes that no object is present.) The\nobjectives and the real-world fingertip covers are shown in\nFig. 3.\nGeometric Objectives.The first two objectives encourage\nthumb-index contact while preventing penetration:\nEatt=NX\ni=1d(xi, P), E rep=X\nv\u2208S(F)[\u03d5P(v)<0]d(v, P).\n(1)\nHere,x iare candidate contact points on the thumb and index,\nS(F)is the surface point cloud of finger meshesF,d(\u00b7, P)\nis the point-to-plane distance,\u03d5 Pis the signed distance to\nP, and[\u00b7]denotes the indicator function. The attraction term\npulls sampled contacts towardP, while the repulsion term\npenalizes surface points that crossP.\nManipulability Objective.To further ensure stable mo-\ntion, we include an objective that encourages the thumb and\nindex to move in parallel along the plane normal direction.\nWe measure this using directional manipulability [58]:\nEmani=\u2212(\u2225J thumbn\u22252+\u2225J indexn\u22252),(2)\nwhereJ thumb andJ index are the Jacobians of the thumb and\nindex, respectively.\nNeural Physics Objective.While the previous terms focus\non kinematics and geometry, we further introduce a neural\nphysics objective to leverage large-scale simulation. We first\noptimize1k plane parameters from different initializations\nusing the above objectives, and then evaluate them in sim-\nulation on the grasping task introduced in Sec. III-B. The\nsimulation outcomes are distilled into a neural surrogate\nmodelf, implemented as a PointNet encoder followed by\nan MLP:\nf: (P, q, o)7\u2192\u02c6s,(3)where(P, q)denotes the plane parameters and joint configu-\nration,ois the object observation (point cloud), and\u02c6s\u2208[0,1]\nis the predicted task success probability. We incorporate this\nsurrogate into the optimization as an energy term:\nEphys=\u2212f(P, q, o),(4)\nwhich encourages geometries and poses that maximize the\npredicted success probability. This neural term provides\ngradient feedback during optimization: we sample a batch\nof objects and jointly optimize a sharedPwith diverseq.\nIn this way,Pis refined toward geometries that are not\nonly kinematically consistent but also physically effective\nfor manipulation.\nIV. EXPERIMENT\nA. Experimental Settings\nOur method is validated on two platforms: XArm robotic\narm + XHand dexterous hand (7+12 DOFs, referred to as\nxArm below), as well as Unitree G1 Humanoid + Inspire\ndexterous hand (7+6 DOFs, referred to as G1 below). We\ncross-validate it on tasks under both sim-to-real and real-to-\nreal settings.\nSim-to-real Tasks.We focus on thegraspingtask in\nthe sim-to-real setting, following the Dex1B [6] benchmark.\nOur dataset includes 7k Objaverse [59] objects and 1k\nprimitive shapes (spheres, boxes, cylinders) of various sizes.\nThe data are categorized by grasp type. Objaverse objects\nwith successful poses optimized byE precise (Sec. III-B) are\nassigned to precision grasps, while the rest are used for\npower grasps. All primitive shapes are used for precision\ngrasps. In total, 6k objects are used for power grasps and\n3k for precision grasps. We hold out30%of the objects for\ntesting. We collect 30k trajectories for power and precision\ngrasps, respectively.\nThe success criterion in Dex1B for grasping is to lift an\nobject from the table to a certain height while maintaining\ncontact between the fingers and the object. During data\ncollection, Dex1B additionally applies lateral external forces\nto the objects to ensure that the optimized grasp pose is\nrobust, but these external forces are removed during final\nevaluation. To ensure better sim-to-real transfer, we keep\nexternal forces during all evaluations.\nWe adopt the DexSimple policy [6] for the grasping policy.\nThe neural switcher (Sec. III-B) consists of a PointNet [56]\nand an MLP, with hidden dimensions (256, 128). The neural\nphysics model (Sec. III-C) also consists of a PointNet and\nan MLP with the same hidden dimensions.\nReal-to-real Tasks.Compared to sim-to-real experiments,\nwe focus on more difficult compositional task and precision\ntask in the real-to-real setting. The task description are as\nbelow:\n\u2022Cooking Setup.A compositional task in which the\nrobot must sequentially pinch-grasp an asparagus spear\nfrom the cutting board, place it into the frying pan, then\nregrasp the pan handle and lift it off the stove.\nOptimization Power G. SR (%) Precise G. SR (%)\nMethod Design Control Seen Unseen Seen Unseen\nSimulation (xArm)\nDex1B [6] 59.60 55.88 56.38 53.91\nOurs\u271360.64 54.12 61.54 59.04\nOurs \u2713 \u2713 61.52 53.35 64.74 64.17\nSimulation (G1)\nDex1B [6] 60.06 56.46 44.75 44.44\nOurs\u271360.54 55.42 45.62 45.26\nOurs \u2713 \u2713 59.93 57.67 49.91 49.32\nZero-shot Sim-to-Real (xArm)\nDex1B [6] \u2013 60.00 \u2013 12.50\nOurs\u2713\u2013 70.00 \u2013 20.00\nOurs \u2713 \u2713 \u2013 80.00 \u2013 82.50\nTABLE I:Main Results for Sim-to-real Grasping Task.\nThe top two parts report simulation success rates for G1 and\nxArm, and the bottom part shows zero-shot sim-to-real re-\nsults. Our method with joint control and design optimization\nconsistently outperforms Dex1B on precise grasps, especially\nin real world precision grasps. Power G. and Precise G.\ndenote Power Grasp and Precision Grasp, respectively.\n\u2022Multi-pen Grasp.The robot is required to grasp two\nmarker pens and place them into a box within a sin-\ngle attempt. Specifically, it first pinch-grasps one pen\nand dexterously rolls it into the palm, secured by the\nremaining three fingers. It then pinch-grasp the second\npen, before dropping both pens into the box.\n\u2022Nut onto Peg.In this task, the robot must precisely\npinch-grasp an M4 hex nut (inner diameter\u03a6 =\n4.1mm) from the tabletop and accurately insert it onto\nan upright M3 bolt (outer diameter\u03a6 = 2.9mm).\nThe small clearance between the nut and bolt requires\nfine dexterity and precise alignment, making this task\nparticularly challenging for precise manipulation.\n\u2022Bread Pinch.In this task, the robot is required to\npinch-grasp a single slice of toast from the table. Since\nexcessive downward pressing may deform the bread or\neven trigger the emergency stop, the robot must execute\nthe grasp with precise control.\n\u2022Battery Insert.This task involves a sequence of precise\nmanipulations in which the robot must grasp a battery\nfrom the table, align it with the charging socket, place\nit in position, and apply a controlled push to ensure it\nis fully secured. This task is conducted on G1 setup.\nWe employ the teleoperation framework from [19] to col-\nlect demonstration data, where the teleoperator\u2019s wrist pose\nis mapped to the XArm end-effector, and the teleoperator\u2019s\nfinger motions are retargeted to the corresponding XHand\nfinger positions. For the baseline retargeting method [57],\nwe adopt the dexpilot retargeting scheme. For the scripted\nbaseline, we record joint configurations from manually exe-\ncuted passive grasping motions. In our optimized retargeting\napproach, the Euclidean distance between the teleoperator\u2019s\nthumb and index fingertips is mapped to the opening angle of\nthe corresponding XHand fingers. We collected 15 successful\nFig. 4: Real-world test objects used for evaluating zero-shot\nsim-to-real grasping.\ndemonstrations for each task.\nAfter data collection, we use [10] for the autonomous\npolicy to verify the policy learning ability in our design. The\naction trunking transformer (ACT) is implemented by [55],\nwhere the input is third-person camera view with XArm-\nXHand joint position as proprioception information. The\noutput action is joint targets of Robot Arm and Hand. It is\nworth noting that we do not employ processed or quantified\ngripper values for hand supervision, but instead rely solely\non raw joint command signals. This aims to validate that our\ncontrol optimization would not affect the performance of the\npolicy. We tested 15 times for each task on the real robot.\nB. Sim-to-real Results\nMain Results.The main results of the sim-to-real grasping\ntask are reported in Tab. I. We mainly compare against\nDex1B [6], a state-of-the-art sim-to-real grasping policy.\nThe simulation results are presented in the top part of\nTab. I, showing policy success rates for power and precision\ngrasps on both training and testing splits. Both Dex1B and\nour policy are trained with 30K demonstrations per grasp\ntype. The key differences are that our precision grasp demon-\nstrations are collected using the method described in Sec. III-\nB (Control optimization in the table), and our fingertip\ngeometry is optimized using the method described in Sec. III-\nC (Design optimization in the table). Since all policies are\ntrained on successful demonstrations, the Dex1B policy still\nachieves strong performance (53.91%) on tiny objects in\nthe precision grasp category. Our policy outperforms Dex1B\non precision grasps by about10%(64.17%vs53.91%),\nprimarily because parallel finger motions and flat contact\nplanes provide greater robustness when the policy predicts\nimperfect grasping poses. At the same time, our policy\nachieves comparable results on power grasps, indicating that\nthe added design optimization does not compromise the\ndexterous hand\u2019s capability for power grasps.\nThe zero-shot sim-to-real results are reported in the bottom\npart of Tab. I. The real-world testing objects are shown in\nFig. 4, and all objects are unseen during training. The policy\nrollouts are visualized in Fig. 5(a) and (b). We evaluate\n5 trials per object, resulting in 40 trials in total for each\ncategory. Unlike the simulation results, our method with opti-\n(a) (b)\n(c) (d)\n(e) (f)\n(g) (h)\nFig. 5: Task policy rollouts across eight tasks. (a)-(b) and (g) illustrate sim-to-real transfer of the precision grasp policy on\nthe xArm and G1 setup. (c)-(f) and (h) show real-to-real executions on five distinct tasks: Cooking Setup, Multi-pen Grasp,\nNut onto Peg, Bread Pinch, and Battery Insert on xArm and G1.\nmized design outperforms Dex1B by a large margin (82.50%\nvs.12.50%) in real-world deployment. The main reason is\nthat all-finger grasping in Dex1B is often too complex for\nsmall objects, making it impractical to deploy under sim-\nto-real gaps in sensing, calibration, and dynamics. We also\nobserve that our policy with control-only optimization does\nnot achieve strong performance (20%), which indicates that\ndesign optimization is crucial for robust sim-to-real transfer,\nas the flat plane provides significantly more contact area.\nAblation Study.We conduct ablation studies in simulation\nto validate the effectiveness of our proposed techniques.\nSpecifically, we ablate the control optimization, the design\noptimization, and theE phys objective (see Sec. III). In-\nstead of training an imitation learning policy on successful\ndemonstrations and evaluating performance as in Tab. I, we\ndirectly report the optimization success rates for precise\ngrasp data collection, which serve as a more direct metric\nof effectiveness.\nOur control-only optimization (C. only in the table)\nachieves a lower success rate (0.41%vs.2.75%), since\nreducing the DOFs makes the optimization problem more\nchallenging. Nevertheless, this control optimization is still\nvaluable because the resulting parallel motions are moreOptimization Objective Opt. SR (%)\nMethod Design Control Physics\nDex1B [6] 2.75\nC. only\u27130.41\nC.+D. (w/oE phys)\u2713 \u27133.77\nC.+D.\u2713 \u2713 \u27135.35\nTABLE II:Ablation Study for Precise Grasp Optimiza-\ntion in Simulation.We report the optimization success\nrate (Opt. SR; %) for control-only (C.), joint control and\ndesign (C.+D.), and the physics objectiveE phys. Joint C.+D.\nachieves the highest SR (5.35%). Incorporating theE phys\nobjective improves performance from3.77%to5.35%.\nrobust for learning and sim-to-real transfer. Our joint control\nand design optimization (C.+D. in the table) achieves the\nhighest success rate (5.35%), highlighting the effectiveness\nof design optimization. We also evaluate design optimization\nwithout theE physobjective (C.+D. w/oE physin the table).\nThe performance gap (3.77%vs.5.35%) indicates that the\nEphysobjective distilled from large-scale simulation helps\nfind more physically plausible fingertip geometries.\nSetting Compositional Task Precision Task\nMethod Design Control Cooking Setup Multi-pen Grasp Nut onto Peg Bread Pinch Battery Insert\nAutonomous Policy\nBaseline Original Retargeting [57] 20.0% 53.3% \u2013 60.0% 13.3%\nOurs Optimized Optimized 73.3% 66.7% 66.7% 93.3% 66.7%\nTeleoperation\nBaseline Original Retargeting [57] 41.7% 57.7% 6.6% 57.1% 26.7%\nOurs Optimized Optimized 88.2% 50.0% 68.2% 93.8% 80.0%\nTABLE III: Results under autonomous policy and teleoperation on compositional and precision tasks.\nSetting Success Rate (%)\nDesign Control Policy Teleoperation\nOriginal Retargeting [57] 60.0% 57.1%\nOriginal Manual Script 73.3% 57.1%\nManual Design Manual Script 60.0% 50.0%\nOptimized (Ours) Optimized (Ours)93.3% 93.8%\nTABLE IV: Ablation study on the bread pinch task. We\nevaluate different combinations of fingertip designs and\ncontrol methods.\nC. Real-to-real Results\nMain Results.The Table III highlights the effectiveness\nof the proposed co-optimized control and design framework\nacross both autonomous policy and teleoperation settings, for\nboth compositional and precision manipulation task.\nIn both teleoperation and autonomous settings, the base-\nline system shows limited success due to the original finger-\ntip design, which makes it difficult to reliably pinch small\nor thin objects such as asparagus in the Cooking Setup,\npens in the Multi-pen Grasp, and thin-cut toast in the Bread\nPinch task. With our optimized control\u2013design, success rates\nincrease significantly: for example, from 20.0% to 73.3%\nin Cooking Setup and from 0.0% to 66.7% in Nut onto\nPeg task for the autonomous policies. This demonstrates\nmarked improvements in both compositional and fine-grained\nprecision manipulation skills. Meanwhile, we show that both\nteleoperation, which maps the operator fingertip distance to\nthe optimized path, and autonomous execution, which learns\nfrom joint positions, confirm the effectiveness of our system.\nTable IV reports the ablation study results on the Bread\nPinch task. Our optimized fingertip design combined with\ncontrol consistently outperforms all variants, surpassing both\nthe naive implementation and the manually crafted geome-\ntry\u2013control design.\nQualitative ResultsTo further illustrate the effectiveness\nof our proposed method, Fig. 5 presents task rollouts across\neight representative scenarios. Subfigures (a), (b) and (g)\ndemonstrate successful sim-to-real transfer of the precision\ngrasp policy, where the robot reliably manipulates small\nobjects like a screwdriver, banana and glue stick using the\nco-optimized fingertip design and control strategy.\nSubfigures (c)-(f) and (h) highlight real-to-real executions\nof complex and fine-grained tasks: Cooking Setup, Multi-penGrasp, Nut onto Peg, Bread Pinch, and Battery Insert. These\ntasks involve diverse object geometries, contact constraints,\ndynamic demands and precision demands. In each case, the\nrobot achieves stable and repeatable performance, showcas-\ning the system\u2019s ability to generalize across manipulation\ncontexts. The visual results confirm that the co-optimization\napproach leads to robust dexterity even in challenging real-\nworld scenarios.\nV. CONCLUSION\nWe introduced a unified framework that enables multi-\nfingered robotic hands to perform both power and precision\ngrasps through a combination of control-policy learning and\nfingertip geometry optimization. By simplifying precision\ncontrol into parallel thumb-index motions and co-designing\nfingertip covers using a neural-physics surrogate, our method\nachieves robust, generalizable manipulation without requir-\ning complex hardware modifications.\nExtensive evaluations in both sim-to-real and real-world\ntasks show significant improvements over existing ap-\nproaches, particularly in fine-grained precision grasping.\nThese results highlight the effectiveness of combining\nlightweight mechanical design with data-driven control, of-\nfering a practical path toward more dexterous and adaptable\nrobotic systems.\nREFERENCES\n[1] J. R. Napier, \u201cThe prehensile movements of the human hand.\u201dThe\nJournal of Bone and Joint Surgery. British volume, 1956.\n[2] T. Feix, J. Romero, H.-B. Schmiedmayer, A. M. Dollar, and D. Kragic,\n\u201cThe grasp taxonomy of human grasp types,\u201dTransactions on Human-\nMachine Systems, 2015.\n[3] M. M. Skinner, N. B. Stephens, Z. J. Tsegai, A. C. Foote, N. H.\nNguyen, T. Gross, D. H. Pahr, J.-J. Hublin, and T. L. Kivell, \u201cHuman-\nlike hand use in australopithecus africanus,\u201dScience, 2015.\n[4] T. L. Kivell, \u201cEvidence in hand: recent discoveries and the early\nevolution of human manual manipulation,\u201dPhilosophical Transactions\nof the Royal Society B: Biological Sciences, 2015.\n[5] F. A. Karakostis, D. Haeufle, I. Anastopoulou, K. Moraitis, G. Hotz,\nV . Tourloukis, and K. Harvati, \u201cBiomechanics of the human thumb\nand the evolution of dexterity,\u201dCurrent Biology, 2021.\n[6] J. Ye, K. Wang, C. Yuan, R. Yang, Y . Li, J. Zhu, Y . Qin, X. Zou, and\nX. Wang, \u201cDex1b: Learning with 1b demonstrations for dexterous\nmanipulation,\u201d inRobotics: Science and Systems (RSS), 2025.\n[7] J. Chen, Y . Ke, and H. Wang, \u201cBodex: Scalable and efficient robotic\ndexterous grasp synthesis using bilevel optimization,\u201d inInternational\nConference on Robotics and Automation (ICRA), 2025.\n[8] R. Singh, A. Allshire, A. Handa, N. Ratliff, and K. Van Wyk, \u201cDextrah-\nrgb: Visuomotor policies to grasp anything with dexterous hands,\u201d\narXiv preprint arXiv:2412.01791, 2024.\n[9] H.-S. Fang, H. Yan, Z. Tang, H. Fang, C. Wang, and C. Lu, \u201cAnydex-\ngrasp: General dexterous grasping for different hands with human-level\nlearning efficiency,\u201darXiv preprint arXiv:2502.16420, 2025.\n[10] T. Z. Zhao, V . Kumar, S. Levine, and C. Finn, \u201cLearning fine-grained\nbimanual manipulation with low-cost hardware,\u201dRSS, 2023.\n[11] T. Z. Zhao, J. Tompson, D. Driess, P. Florence, K. Ghasemipour,\nC. Finn, and A. Wahid, \u201cAloha unleashed: A simple recipe for robot\ndexterity,\u201d inCoRL, 2024.\n[12] K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai,\nL. Groom, K. Hausman, B. Ichter,et al., \u201c\u03c00: A vision-language-\naction flow model for general robot control,\u201darXiv, 2024.\n[13] P. Intelligence, K. Black, N. Brown, J. Darpinian, K. Dhabalia,\nD. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai,et al., \u201c\u03c00.5: a\nvision-language-action model with open-world generalization,\u201dArXiv,\n2025.\n[14] G. R. Team, S. Abeyruwan, J. Ainslie, J.-B. Alayrac, M. G. Arenas,\nT. Armstrong, A. Balakrishna, R. Baruch, M. Bauza, M. Blokzijl,\net al., \u201cGemini robotics: Bringing ai into the physical world,\u201darXiv\npreprint arXiv:2503.20020, 2025.\n[15] H. Zhang, S. Christen, Z. Fan, O. Hilliges, and J. Song, \u201cGraspxl:\nGenerating grasping motions for diverse objects at scale,\u201d inECCV,\n2024.\n[16] Y . Zhong, X. Huang, R. Li, C. Zhang, Z. Chen, T. Guan, F. Zeng,\nK. N. Lui, Y . Ye, Y . Liang,et al., \u201cDexgraspvla: A vision-language-\naction framework towards general dexterous grasping,\u201darXiv preprint\narXiv:2502.20900, 2025.\n[17] J. He, D. Li, X. Yu, Z. Qi, W. Zhang, J. Chen, Z. Zhang, Z. Zhang,\nL. Yi, and H. Wang, \u201cDexvlg: Dexterous vision-language-grasp model\nat scale,\u201darXiv preprint arXiv:2507.02747, 2025.\n[18] X. Cheng, J. Li, S. Yang, G. Yang, and X. Wang, \u201cOpen-television:\nTeleoperation with immersive active visual feedback,\u201darXiv preprint\narXiv:2407.01512, 2024.\n[19] R. Ding, Y . Qin, J. Zhu, C. Jia, S. Yang, R. Yang, X. Qi, and X. Wang,\n\u201cBunny-visionpro: Real-time bimanual dexterous teleoperation for\nimitation learning,\u201darXiv preprint arXiv:2407.03162, 2024.\n[20] J. Zhao and E. H. Adelson, \u201cGelsight svelte hand: A three-finger,\ntwo-dof, tactile-rich, low-cost robot hand for dexterous manipulation,\u201d\narXiv preprint arXiv:2309.10886, 2023.\n[21] Y . Hong, Y . Zhao, J. Berman, Y . Chi, Y . Li, H. Huang, and J. Yin,\n\u201cAngle-programmed tendril-like trajectories enable a multifunctional\ngripper with ultradelicacy, ultrastrength, and ultraprecision,\u201dNature\nCommunications, vol. 14, no. 1, p. 4625, 2023.\n[22] W. K. Do, A. K. Dhawan, M. Kitzmann, and M. Kennedy, \u201cDensetact-\nmini: an optical tactile sensor for grasping multi-scale objects from\nflat surfaces,\u201d in2024 IEEE international conference on robotics and\nautomation (ICRA). IEEE, 2024, pp. 6928\u20136934.\n[23] Z. Yu, W. Xu, S. Yao, J. Ren, T. Tang, Y . Li, G. Gu, and C. Lu, \u201cPrecise\nrobotic needle-threading with tactile perception and reinforcement\nlearning,\u201d inConference on Robot Learning. PMLR, 2023, pp. 3266\u2013\n3276.\n[24] A. Bronars, S. Kim, P. Patre, and A. Rodriguez, \u201cTexterity: Tactile ex-\ntrinsic dexterity,\u201d in2024 IEEE International Conference on Robotics\nand Automation (ICRA). IEEE, 2024, pp. 7976\u20137983.\n[25] F. Chen and M. Y . Wang, \u201cDesign optimization of soft robots: A\nreview of the state of the art,\u201dIEEE Robotics & Automation Magazine,\nvol. 27, no. 4, pp. 27\u201343, 2020.\n[26] D. Feshbach, W.-H. Chen, L. Xu, E. Schaumburg, I. Huang, and\nC. Sung, \u201cAlgorithmic design of kinematic trees based on csc dubins\nplanning for link shapes,\u201d inInternational Workshop on the Algorith-\nmic Foundations of Robotics (WAFR), 2024.\n[27] R. Calandra, A. Seyfarth, J. Peters, and M. P. Deisenroth, \u201cBayesian\noptimization for learning gaits under uncertainty: An experimental\ncomparison on a dynamic bipedal walker,\u201dAnnals of Mathematics\nand Artificial Intelligence, vol. 76, pp. 5\u201323, 2016.\n[28] P. Mannam, X. Liu, D. Zhao, J. Oh, and N. Pollard, \u201cDesign and\ncontrol co-optimization for automated design iteration of dexterous\nanthropomorphic soft robotic hands,\u201d in2024 IEEE 7th International\nConference on Soft Robotics (RoboSoft). IEEE, 2024, pp. 332\u2013339.\n[29] K. Ikemura, Y . Dong, D. Blanco-Mulero, A. Longhini, L. Chen, and\nF. T. Pokorny, \u201cEfficient end-effector co-design by demonstration for\ndeformable fragile object manipulation,\u201d in1st Workshop on Robot\nHardware-Aware Intelligence.\n[30] R. Liu, J. Liang, S. Sudhakar, H. Ha, C. Chi, S. Song, and C. V ondrick,\n\u201cPaperbot: Learning to design real-world tools using paper,\u201darXiv\npreprint arXiv:2403.09566, 2024.[31] S. Yi, X. Bai, A. Singh, J. Ye, M. T. Tolley, and X. Wang, \u201cCo-design\nof soft gripper with neural physics,\u201darXiv preprint arXiv:2505.20404,\n2025.\n[32] C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine, \u201cOne-shot visual\nimitation learning via meta-learning,\u201d inConference on robot learning.\nPMLR, 2017, pp. 357\u2013368.\n[33] A. Mandlekar, D. Xu, R. Mart \u00b4\u0131n-Mart \u00b4\u0131n, S. Savarese, and L. Fei-\nFei, \u201cLearning to generalize across long-horizon tasks from human\ndemonstrations,\u201darXiv preprint arXiv:2003.06085, 2020.\n[34] Y . Zhu, P. Stone, and Y . Zhu, \u201cBottom-up skill discovery from\nunsegmented demonstrations for long-horizon robot manipulation,\u201d\nIEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 4126\u20134133,\n2022.\n[35] S. Nasiriany, T. Gao, A. Mandlekar, and Y . Zhu, \u201cLearning and\nretrieval from prior data for skill-based imitation learning,\u201darXiv\npreprint arXiv:2210.11435, 2022.\n[36] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni,\nL. Fei-Fei, S. Savarese, Y . Zhu, and R. Mart \u00b4\u0131n-Mart \u00b4\u0131n, \u201cWhat matters\nin learning from offline human demonstrations for robot manipula-\ntion,\u201darXiv preprint arXiv:2108.03298, 2021.\n[37] Y . Qin, Y .-H. Wu, S. Liu, H. Jiang, R. Yang, Y . Fu, and X. Wang,\n\u201cDexmv: Imitation learning for dexterous manipulation from human\nvideos,\u201d inECCV, 2022.\n[38] A. Mandlekar, S. Nasiriany, B. Wen, I. Akinola, Y . Narang, L. Fan,\nY . Zhu, and D. Fox, \u201cMimicgen: A data generation system for\nscalable robot learning using human demonstrations,\u201darXiv preprint\narXiv:2310.17596, 2023.\n[39] J. Ye, J. Wang, B. Huang, Y . Qin, and X. Wang, \u201cLearning continuous\ngrasping function with a dexterous hand from human demonstrations,\u201d\nRA-L, 2023.\n[40] S. Young, D. Gandhi, S. Tulsiani, A. Gupta, P. Abbeel, and L. Pinto,\n\u201cVisual imitation made easy,\u201d inConference on Robot learning.\nPMLR, 2021, pp. 1992\u20132005.\n[41] R.-Z. Qiu, Y . Song, X. Peng, S. A. Suryadevara, G. Yang, M. Liu,\nM. Ji, C. Jia, R. Yang, X. Zou, and X. Wang, \u201cWildlma: Long horizon\nloco-manipulation in the wild,\u201darXiv preprint arXiv:2411.15131,\n2024.\n[42] L. Wei, J. Ma, Y . Hu, and R. Zhang, \u201cEnsuring force safety in vision-\nguided robotic manipulation via implicit tactile calibration,\u201darXiv\npreprint arXiv:2412.10349, 2024.\n[43] A. Mandlekar, Y . Zhu, A. Garg, J. Booher, M. Spero, A. Tung, J. Gao,\nJ. Emmons, A. Gupta, E. Orbay,et al., \u201cRoboturk: A crowdsourcing\nplatform for robotic skill learning through imitation,\u201d inConference\non Robot Learning. PMLR, 2018, pp. 879\u2013893.\n[44] A. Brohan, N. Brown, J. Carbajal, Y . Chebotar, J. Dabis, C. Finn,\nK. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu,et al., \u201cRt-1:\nRobotics transformer for real-world control at scale,\u201darXiv preprint\narXiv:2212.06817, 2022.\n[45] P. Wu, Y . Shentu, Z. Yi, X. Lin, and P. Abbeel, \u201cGello: A general, low-\ncost, and intuitive teleoperation framework for robot manipulators,\u201d in\nIROS, 2024.\n[46] Y . Wang, Z. Xian, F. Chen, T.-H. Wang, Y . Wang, K. Fragkiadaki,\nZ. Erickson, D. Held, and C. Gan, \u201cRobogen: Towards unleashing\ninfinite data for automated robot learning via generative simulation,\u201d\narXiv preprint arXiv:2311.01455, 2023.\n[47] S. James, Z. Ma, D. R. Arrojo, and A. J. Davison, \u201cRlbench: The\nrobot learning benchmark & learning environment,\u201dIEEE Robotics\nand Automation Letters, vol. 5, no. 2, pp. 3019\u20133026, 2020.\n[48] R. Wang, J. Zhang, J. Chen, Y . Xu, P. Li, T. Liu, and H. Wang,\n\u201cDexgraspnet: A large-scale robotic dexterous grasp dataset for general\nobjects based on simulation,\u201d inICRA, 2023.\n[49] L. Pinto and A. Gupta, \u201cSupersizing self-supervision: Learning to\ngrasp from 50k tries and 700 robot hours,\u201d in2016 IEEE international\nconference on robotics and automation (ICRA). IEEE, 2016, pp.\n3406\u20133413.\n[50] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang,\nD. Quillen, E. Holly, M. Kalakrishnan, V . Vanhoucke,et al., \u201cScalable\ndeep reinforcement learning for vision-based robotic manipulation,\u201d in\nConference on robot learning. PMLR, 2018, pp. 651\u2013673.\n[51] S. Bahl, R. Mendonca, L. Chen, U. Jain, and D. Pathak, \u201cAffordances\nfrom human videos as a versatile representation for robotics,\u201d in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2023, pp. 13 778\u201313 790.\n[52] S. Dass, K. Pertsch, H. Zhang, Y . Lee, J. J. Lim, and S. Nikolaidis,\n\u201cPato: Policy assisted teleoperation for scalable robot data collection,\u201d\narXiv preprint arXiv:2212.04708, 2022.\n[53] A. O\u2019Neill, A. Rehman, A. Maddukuri, A. Gupta, A. Padalkar,\nA. Lee, A. Pooley, A. Gupta, A. Mandlekar, A. Jain,et al., \u201cOpen\nx-embodiment: Robotic learning datasets and rt-x models: Open x-\nembodiment collaboration 0,\u201d in2024 IEEE International Conference\non Robotics and Automation (ICRA). IEEE, 2024, pp. 6892\u20136903.\n[54] T. Liu, Z. Liu, Z. Jiao, Y . Zhu, and S.-C. Zhu, \u201cSynthesizing diverse\nand physically stable grasps with arbitrary hand structures using\ndifferentiable force closure estimator,\u201dRA-L, 2021.\n[55] S. Tao, F. Xiang, A. Shukla, Y . Qin, X. Hinrichsen, X. Yuan, C. Bao,\nX. Lin, Y . Liu, T. kai Chan, Y . Gao, X. Li, T. Mu, N. Xiao, A. Gurha,\nZ. Huang, R. Calandra, R. Chen, S. Luo, and H. Su, \u201cManiskill3:\nGpu parallelized robotics simulation and rendering for generalizable\nembodied ai,\u201darXiv, 2024.\n[56] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, \u201cPointnet: Deep learning\non point sets for 3d classification and segmentation,\u201d inCVPR, 2017.\n[57] Y . Qin, W. Yang, B. Huang, K. Van Wyk, H. Su, X. Wang, Y .-W. Chao,\nand D. Fox, \u201cAnyteleop: A general vision-based dexterous robot arm-\nhand teleoperation system,\u201d inRobotics: Science and Systems (RSS),\n2023.\n[58] T. Yoshikawa, \u201cManipulability of robotic mechanisms,\u201dThe interna-\ntional journal of Robotics Research, 1985.\n[59] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. Vander-\nBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi, \u201cObjaverse:\nA universe of annotated 3d objects,\u201d inProceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, 2023, pp.\n13 142\u201313 153.\n",
    "title": "From Power to Precision: Learning Fine-grained Dexterity for Multi-fingered Robotic Hands",
    "authors": [
      "Jianglong Ye",
      "Lai Wei",
      "Guangqi Jiang",
      "Changwei Jing",
      "Xueyan Zou",
      "Xiaolong Wang"
    ],
    "published": "2025-11-17",
    "arxiv_id": "2511.13710v1",
    "num_pages": 9,
    "num_chars": 43884
  },
  {
    "text": "Rare Genomic Subtype Discovery from RNA-seq via\nAutoencoder Embeddings and Stability-Aware\nClustering\nAlaa Mezghiche1,*\n1Department of Computer Science, University of Science and Technology\nHouari Boumediene (USTHB), Algiers, Algeria\n*Correspondence:alaa.mezghiche@etu.usthb.dz\nAbstract\nUnsupervised learning on high-dimensional RNA-seq data can reveal molecular\nsubtypes beyond standard labels. We combine an autoencoder-based representation\nwith clustering andstabilityanalysis to search forrarebut reproducible genomic\nsubtypes. On the UCI \u201cGene Expression Cancer RNA-Seq\u201d dataset (801 samples,\n20,531 genes; BRCA, COAD, KIRC, LUAD, PRAD), apan-canceranalysis shows\nclusters aligning almost perfectly with tissue-of-origin (Cram\u00b4 er\u2019s V= 0.887), serving\nas a negative control. We therefore reframe the problemwithinKIRC ( n= 146):\nwe select the top 2,000 highly variable genes, standardize them, train a feed-forward\nautoencoder (128-dimensional latent space), and run k-means for k= 2. . .10. While\nglobal indices favor small k, scanning kwith a pre-specified discovery rule (rare\n<10% and stable with Jaccard \u22650.60 across 20 seeds after Hungarian alignment)\nyields a simple solution at k= 5 (silhouette = 0 .129, DBI = 2 .045) with a rare\ncluster C0 (6.85% of patients) that is highly stable (Jaccard = 0 .787). Cluster-vs-\nrest differential expression (Welch\u2019s t-test, Benjamini\u2013Hochberg FDR) identifies\ncoherent markers, including strong down-regulation of gene 11713 ,gene 13678 ,\ngene 16402 ,gene 3777 and up-regulation of gene 751,gene 17397 ,gene 2760.\nOverall, pan-cancer clustering is dominated by tissue-of-origin, whereas a stability-\naware within-cancer approach reveals a rare, reproducible KIRC subtype.\nKeywords:RNA-seq; unsupervised learning; autoencoder; clustering stability; rare\nsubtypes; KIRC; differential expression; UMAP; pan-cancer; within-cancer.\n1arXiv:2511.13705v1  [cs.LG]  17 Nov 2025\n1 Introduction\nHigh-throughput RNA sequencing (RNA-seq) has transformed our ability to profile gene\nexpression at scale, enabling large consortia such as The Cancer Genome Atlas (TCGA)\nto generate transcriptomic data across many tumor types and patients. Unsupervised\nanalysis of such data has the potential to uncovermolecular subtypes, or groups of\npatients sharing coherent expression programs that may not correspond directly to current\nhistopathological classifications.\nThe UCI \u201cGene Expression Cancer RNA-Seq\u201d dataset is a curated subset (801 patients,\n20,531 genes) of the TCGA Pan-Cancer RNA-seq collection, covering five tumor types:\nbreast invasive carcinoma (BRCA), colon adenocarcinoma (COAD), kidney renal clear\ncell carcinoma (KIRC), lung adenocarcinoma (LUAD) and prostate adenocarcinoma\n(PRAD) [ 1]. This dataset has been widely used as a benchmark for supervised cancer\nclassification and feature selection, including autoencoder-based biomarker identification [ 2]\nand comparative studies of machine learning models on RNA-seq data.\nMost existing work on this dataset focuses on predicting the known tumor type from\ngene expression. In contrast, our goal is to explore whether unsupervised methods can\ndiscoverraretranscriptomic subtypes within a given cancer type. Such rare subtypes, if\nstable and biologically coherent, could correspond to clinically meaningful subgroups (e.g.,\ndistinct prognostic profiles or therapy responses).\nAutoencoders provide a natural way to compress high-dimensional gene expression into\na lower-dimensional latent space while preserving non-linear structure. Clustering in this\nlatent space, combined with interpretability tools, can help reveal underlying patterns [ 2].\nHowever, pan-cancer clustering is strongly driven by tissue-of-origin, which can mask\nfiner-grained within-cancer structure.\nIn this work, we:\n1.Perform a pan-cancer unsupervised analysis as a sanity / negative control, confirming\nthat clusters align with tissue labels and therefore mostly re-discover known structure.\n2.Reframe the task within a single cancer type (KIRC), using an autoencoder to learn\na compact representation of highly variable genes.\n3.Usek-means clustering with model selection and cluster stability analysis to identify\nrare but reproducible clusters.\n4.Apply simple differential expression analysis (cluster-vs-rest) to derive a cluster-\nspecific gene signature.\n2\n2 Materials and Methods\n2.1 Dataset\nWe use the \u201cGene Expression Cancer RNA-Seq\u201d dataset from the UCI Machine Learning\nRepository [1]. This dataset consists of:\n\u2022801 tumor samples (patients).\n\u202220,531 gene expression features measured by Illumina HiSeq RNA-seq.\n\u2022Five tumor types (\u201cClass\u201d label): BRCA ( n= 300), KIRC ( n= 146), LUAD\n(n= 141), PRAD (n= 136), COAD (n= 78).\nExpression values are provided in a matrix where rows are samples and columns are\ngenes (named gene 0togene 20530 ). Class labels are provided separately and aligned to\nsamples by an ID field ( sample X). We merged features and labels using an inner join on\nthe sample ID, and subsequently set the sample ID as the row index.\n2.2 Pan-cancer negative control\nAs an initial experiment, we considered all 801 samples together (pan-cancer analysis).\nThe objective was to confirm that the unsupervised pipeline is sensible and that the latent\nspace captures known structure.\nPreprocessing.We extracted the gene expression matrix X\u2208R801\u00d720531and verified\nthat all values were finite and non-negative. We applied a log(1 +x) transform to reduce\nskewness. To reduce noise and dimensionality, we selected the top 2,000 most variable\ngenes across all samples based on the empirical variance, then standardized each gene to\nzero mean and unit variance (z-scoring).\nPan-cancer autoencoder and clustering.We trained a feed-forward autoencoder\non the standardized matrix Xscaled\u2208R801\u00d72000and obtained latent codes Z\u2208R801\u00d7128\n(architecture described in Section 2.4). We then applied k-means clustering on Zfor\nk= 2, . . . , 10 and computed the silhouette score for each k, which increases from 0 .174 at\nk= 2 to a maximum of 0 .286 at k= 6 before slightly decreasing (Fig. 1). We chose k= 6\nfor detailed inspection as a reasonable pan-cancer solution.\nEvaluation.We constructed a contingency table between tumor types and clusters,\nrow-normalized it, and measured association using Pearson\u2019s chi-square test and Cram\u00b4 er\u2019s\nV.\n3\n2.3 Within-cancer reframing: focus on KIRC\nTo avoid the dominant tissue-of-origin signal, we restricted the analysis to one cancer\ntype: KIRC (kidney renal clear cell carcinoma). We filtered the merged dataset to keep\nonly samples with Class=KIRC , yielding n= 146 patients. All subsequent analyses were\nperformed on this subset.\n2.3.1 Highly variable genes and scaling\nWithin KIRC, we computed the variance of each gene across the 146 samples and selected\nthe top 2,000 most variable genes (or fewer if fewer genes had non-zero variance). Let\nXKIRC\u2208R146\u00d7Gdenote the KIRC expression matrix for these genes, withG\u22642000.\nWe standardized each gene to zero mean and unit variance:\nXz= StandardScaler(X KIRC),\nresulting inX z\u2208R146\u00d7G, which serves as input to the autoencoder.\n2.4 Autoencoder architecture and training\nWe used the same architectural template for both pan-cancer and KIRC analyses, instan-\ntiated with the appropriate input dimension ( Din= 2000 for pan-cancer; Din=Gwithin\nKIRC).\nThe autoencoder is a fully connected neural network implemented in PyTorch:\n\u2022Input dimension:D in(number of selected genes).\n\u2022Hidden layer 1: size H1=min(1024 ,max (256, Din/2)), with ReLU activation and\ndropout (p= 0.1).\n\u2022Hidden layer 2: sizeH 2= min(512,max(128, D in/4)), with ReLU activation.\n\u2022Latent layer: sizeD lat= 128 (linear).\n\u2022Decoder: symmetric to the encoder with ReLU activations.\nThe encoder f\u03b8:RDin\u2192R128maps input samples to latent codes z, and the decoder\nreconstructs back to input space. We trained the model using mean squared error (MSE)\nreconstruction loss:\nL=1\nNNX\ni=1\u2225xi\u2212\u02c6x i\u22252\n2,\nwith the Adam optimizer (learning rate 10\u22123, weight decay 10\u22125). We used mini-batches\nof size 256, a 85%/15% train/validation split, and early stopping (patience = 15).\n4\nA typical learning curve for the KIRC autoencoder is shown in Fig. 2: training and\nvalidation MSE decrease rapidly in the first 5 epochs, then flatten; the validation curve\nstabilizes around 0.46 while the training curve continues to decrease more slowly.\nAfter training, we embeddedallKIRC samples by passing Xzthrough the encoder in\nevaluation mode (without dropout), obtaining a latent matrixZ\u2208R146\u00d7128.\n2.5 Clustering, model selection and stability\n2.5.1 Within-KIRC clustering\nWithin KIRC, we applied k-means clustering on the latent codes Zforkin{2, . . . , 10},\nusingn init= 10 and a fixed random seed (42). For eachk, we computed:\n\u2022The silhouette score (higher is better).\n\u2022The Davies\u2013Bouldin index (DBI; lower is better).\nThe silhouette curve peaks at k= 2 with a score of 0 .140, then slowly declines as k\nincreases (Fig. 3). DBI also decreases gradually with kand reaches 2 .045 at k= 5 before\ncontinuing to drop (Fig. 4). The purely \u201cbest\u201d solution in terms of these global indices is\ntherefore a small k(particularly k= 2), but this produces only two large clusters (sizes\n69 and 77) with no rare subtypes.\nTo explicitly search for rare but meaningful clusters, we supplemented these global\nmetrics with a stability analysis described below.\n2.5.2 Cluster stability via Jaccard index\nFor each value of k, we ran k-means R= 20 times with different random seeds, obtaining\nlabelings \u2113(1), . . . , \u2113(R). Because cluster labels are arbitrary up to permutation, we aligned\nall runs to a reference labeling (e.g.,\u2113(1)) using the Hungarian algorithm.\nFor each cluster cin the reference solution, we computed its Jaccard similarity across\nruns:\nJc=1\nR\u22121RX\nr=2|Sc\u2229S(r)\nc|\n|Sc\u222aS(r)\nc|,\nwhere Scis the set of samples assigned to cluster cin the reference run, and S(r)\ncis the\naligned set in runr.\nWe then summarized, for allkand clusters, the triplet:\n\u2022Prevalencep c=|S c|/N,\n\u2022Jaccard stabilityJ c,\n\u2022Indicators for beingrare(p c<0.10) andstable(J c\u22650.60).\n5\nAcross allk, rareandstable clusters appear at:\n(k,cluster)\u2208 {(5,0),(8,5),(10,7),(10,9)}.\nAmong these, the configuration with k= 5 is the simplest and has the largest global\nsilhouette (0.129) and reasonably low DBI (2.045). We therefore focus on the KIRC\nclustering solution withk= 5.\n2.5.3 Final clustering solution atk= 5\nFork= 5, we refitted k-means with ninit= 30 to obtain stable labels c1, . . . , c 146\u2208\n{0,1,2,3,4}. The resulting cluster sizes and prevalences are:\n[10,19,31,53,33]\u2194[6.85%,13.0%,21.2%,36.3%,22.6%].\nCluster C0 is the rare cluster with size 10 (6.85%). The global clustering metrics at\nk= 5 are:\nsilhouette = 0.129,DBI = 2.045.\nThe barplot in Fig. 5 highlights C0 in red and shows the distribution of cluster sizes.\nStability analysis for k= 5 (Fig. 6) confirms that C0 is highly stable across runs with a\nJaccard index of 0.787, while other clusters have Jaccard indices between 0.44 and 0.76.\n2.6 Cluster interpretability via differential expression\nTo characterize the rare KIRC cluster C0, we performed a cluster-vs-rest differential\nexpression (DE) analysis on the standardized KIRC expression matrix Xz. For C0 (the 10\nin-cluster samples) and the remaining 136 out-of-cluster samples, we computed for each\ngeneg:\n\u2022The effect size: \u2206z g=\u00b5 g,in\u2212\u00b5 g,out.\n\u2022A Welch\u2019st-testp-value comparing in-cluster vs out-of-cluster samples.\n\u2022Benjamini\u2013Hochberg FDR correction over all genes.\nThe top hits (sorted by FDR and |\u2206zg|) are reported in Table 2. A volcano plot\n(Fig. 8) shows the global distribution of genes; those with large |\u2206zg|and low FDR are\nannotated. A heatmap of the top 20 up- and top 20 down-regulated genes (Fig. 9) confirms\nthat C0 patients share a coherent molecular pattern distinct from other KIRC samples.\n6\n2.7 Visualization of latent structure\nTo visualize the structure of the latent space, we applied UMAP to the KIRC latent codes\nZ:\nU= UMAP(n neighbors= 15,min dist = 0.3,random state = 42).\nThe resulting 2D embedding (Fig. 7) shows that C0 samples are concentrated in a compact\nregion of the latent space, separated from the bulk of other KIRC samples, further\nsupporting the idea that C0 represents a coherent subtype rather than scattered noise.\n3 Results\n3.1 Pan-cancer clusters re-discover tissue-of-origin\nFor the pan-cancer analysis, the k= 6 solution yields the cluster size and stability profile\nshown in Table 1. All clusters are extremely stable (Jaccard indices \u22650.994), including\ntwo rare clusters (prevalence<10%).\nTable 1: Pan-cancer k-means clusters atk= 6.\nCluster Size Prevalence Jaccard Rare (<10%) Stable (\u22650.60)\n0 240 0.300 0.998 No Yes\n1 136 0.170 1.000 No Yes\n2 65 0.081 0.994 Yes Yes\n3 136 0.170 1.000 No Yes\n4 145 0.181 1.000 No Yes\n5 79 0.099 1.000 Yes Yes\nThe contingency table between tumor type (rows) and cluster (columns) is:\nClass 0 1 2 3 4 5\nBRCA 240 0 60 0 0 0\nCOAD 0 0 0 0 0 78\nKIRC 0 0 1 0 145 0\nLUAD 0 0 4 136 0 1\nPRAD 0 136 0 0 0 0\nRow-normalizing this table gives:\n7\nClass 0 1 2 3 4 5\nBRCA 0.80 0.00 0.20 0.00 0.00 0.00\nCOAD 0.00 0.00 0.00 0.00 0.00 1.00\nKIRC 0.00 0.00 0.01 0.00 0.99 0.00\nLUAD 0.00 0.00 0.03 0.96 0.00 0.01\nPRAD 0.00 1.00 0.00 0.00 0.00 0.00\nThe chi-square test of independence between tumor type and cluster assignment is\nhighly significant ( p\u2248 0), with Cram\u00b4 er\u2019s V= 0.887. Thus, the pan-cancer clusters\nessentially recover the five tissues-of-origin:\n\u2022Cluster 0: mostly BRCA (240/300).\n\u2022Cluster 1: pure PRAD (136/136).\n\u2022Cluster 3: almost pure LUAD (136/141, plus a few LUAD samples in other clusters).\n\u2022Cluster 4: almost pure KIRC (145/146).\n\u2022Cluster 5: almost pure COAD (78/78, plus 1 LUAD).\n\u2022Cluster 2: a small residual cluster (65 samples) with the remaining BRCA and a\nfew LUAD/KIRC samples.\nAlthough there are two rare clusters (2 and 5), they correspond to leftover fractions\nof known tumour types rather than novel cross-cancer subtypes. We therefore treat\nthis experiment as anegative controlthat validates the pipeline but does not produce\nsurprising biology.\n3.2 Within KIRC: rare and stable subtype C0 atk= 5\nIn contrast, the within-KIRC analysis allows us to search for more subtle heterogeneity.\nAs noted above, pure clustering metrics favour k= 2 (silhouette 0 .140, DBI 2 .455), but\nthis solution contains only two large clusters:\nCluster Size Prevalence Rare\n0 69 0.473 No\n1 77 0.527 No\nBy scanning kand examining cluster stability, we identify several rare and stable\nclusters (Table in the Methods section). Among them, cluster C0 atk= 5 has:\n\u2022Size = 10 patients (6.85% of KIRC).\n8\n\u2022Jaccard stability = 0.787 across 20 seeds.\n\u2022Clear separation in the UMAP latent space (Fig. 7).\nTherefore, we focus on this k= 5 solution and interpret C0 as a candidate rare genomic\nsubtype of KIRC.\n3.3 Differential expression and marker genes of C0\nThe DE analysis for C0 identifies a set of strongly altered genes. The top 15 markers are\nsummarized in Table 2 (values taken directly from the analysis):\nTable 2: Top 15 marker genes for the rare KIRC cluster C0 (cluster-vs-rest DE on\nstandardized expression). Negative effects indicate down-regulation in C0; positive effects\nindicate up-regulation.\nGene Effect \u2206z p-value FDR\ngene 3777\u22121.459 6.34\u00d710\u2212311.27\u00d710\u221227\ngene 274\u22121.050 3.38\u00d710\u2212232.73\u00d710\u221220\ngene 8185\u22121.386 4.10\u00d710\u2212232.73\u00d710\u221220\ngene 2715\u22121.013 1.80\u00d710\u2212219.01\u00d710\u221219\ngene 5659\u22121.178 5.86\u00d710\u2212162.35\u00d710\u221213\ngene 17397+1.351 1.75\u00d710\u2212145.84\u00d710\u221212\ngene 13678\u22122.012 2.84\u00d710\u2212148.10\u00d710\u221212\ngene 2760+1.131 1.82\u00d710\u2212134.54\u00d710\u221211\ngene 17009\u22121.406 4.11\u00d710\u2212139.12\u00d710\u221211\ngene 9561\u22120.755 6.57\u00d710\u2212131.31\u00d710\u221210\ngene 11713\u22123.332 2.85\u00d710\u2212125.19\u00d710\u221210\ngene 16402\u22122.612 5.92\u00d710\u2212129.87\u00d710\u221210\ngene 751+1.915 9.12\u00d710\u2212121.40\u00d710\u22129\ngene 17921\u22120.674 3.17\u00d710\u2212114.53\u00d710\u22129\ngene 5945\u22121.457 3.80\u00d710\u2212115.07\u00d710\u22129\nThe volcano plot (Fig. 8) highlights these genes as red points with labels; they occupy\nthe extremes of the \u2206 zaxis and the top of the \u2212log10(FDR ) axis, confirming both strong\neffect size and statistical significance. The heatmap (Fig. 9) of the top 20 up- and 20\ndown-regulated genes (with C0 samples placed on the left) shows a clear block structure:\nC0 samples share strong down-regulation of several genes (deep blue) and up-regulation\nof a smaller set (red), while the rest of the KIRC cohort shows more heterogeneous\nexpression.\n4 Discussion\nThis work presents a small but illustrative pipeline for discovering rare transcriptomic\nsubtypes in cancer using autoencoders, clustering and simple statistical interpretability.\n9\nUsing the UCI Gene Expression Cancer RNA-Seq dataset as a testbed, we show that:\n1.Pan-cancer unsupervised clustering primarily re-discovers tissue-of-origin, with\nclusters essentially corresponding to BRCA, COAD, KIRC, LUAD and PRAD. The\nhigh Cram\u00b4 er\u2019s Vvalue and nearly block-diagonal contingency table confirm that\nthe model captures known structure but does not reveal unexpected cross-cancer\nsubtypes in this dataset.\n2.Restricting to a single cancer type (KIRC) and focusing on highly variable genes\nallows us to search for finer-grained within-cancer heterogeneity without the con-\nfounding effect of tissue differences.\n3.An autoencoder-based latent representation, combined with k-means clustering and\nJaccard-based stability analysis, can highlight rare but robust clusters. In our case,\ncluster C0 represents only \u223c7% of KIRC patients but remains stable across multiple\nrandom seeds.\n4.Differential expression analysis yields a compact list of marker genes for the rare\ncluster, suggesting that it corresponds to a distinct transcriptional state rather than\nnoise.\nBecause the UCI dataset uses anonymized gene identifiers ( gene X) and aggregates\nsamples from a larger TCGA resource, we cannot directly map our markers to known gene\nsymbols or pathways in this setting. As a result, our current analysis is methodological:\nit demonstrates that the combination of autoencoders, clustering, and stability analysis\ncan identify candidate rare subtypes, but does not yet provide biological interpretation.\nA natural next step is to re-run the same pipeline on the full TCGA KIRC RNA-seq\ndata where genes have standard identifiers. This would enable downstream analyses such\nas Gene Ontology and pathway enrichment, comparison with known KIRC subtypes, and\ncorrelation with clinical variables such as survival, stage, and treatment.\n5 Conclusion\nWe presented an unsupervised pipeline for discovering rare genomic subtypes within\ncancer using autoencoders, clustering, cluster stability analysis, and differential expression.\nApplied to the KIRC subset of the UCI Gene Expression Cancer RNA-Seq dataset, this\napproach identified a rare ( \u223c7%) but stable cluster with a distinct expression signature.\nWhile limited by anonymized gene identifiers and lack of clinical annotations in the\nbenchmark dataset, this work provides a proof-of-concept that can be extended to richer\ndatasets such as TCGA with full clinical and genomic annotations. Ultimately, com-\nbining unsupervised representation learning, stability-aware clustering and interpretable\nsignatures may help uncover clinically actionable subgroups in cancer.\n10\nData and code availability\nAll expression data used in this study are available from the UCI Machine Learning Reposi-\ntory under the title \u201cGene Expression Cancer RNA-Seq\u201d (dataset ID 401) [ 1]. The analysis\ncode (autoencoder training, clustering, stability analysis and differential expression) is\nimplemented in Python (PyTorch, scikit-learn, statsmodels) and is available at: https:\n//github.com/alaa-32/Discovering-Rare-Genomic-Subtypes-from_RNA-seq.git.\nReferences\n[1]S. Fiorini. Gene Expression Cancer RNA-Seq [Dataset]. UCI Machine Learning\nRepository, 2016.https://doi.org/10.24432/C5R88H.\n[2]F. Al Abir, S. M. Shovan, M. A. M. Hasan, A. Sayeed, and J. Shin. Biomarker\nidentification by reversing the learning mechanism of an autoencoder and recursive\nfeature elimination.Molecular Omics, 18:652\u2013661, 2022.\n[3]J. N. Weinstein, E. A. Collisson, G. B. Mills,et al.The Cancer Genome Atlas\nPan-Cancer analysis project.Nature Genetics, 45(10):1113\u20131120, 2013.\n[4]Rousseeuw PJ. Silhouettes: A graphical aid to the interpretation and validation of\ncluster analysis.J Comput Appl Math. 1987;20:53\u201365.\n[5]Davies DL, Bouldin DW. A cluster separation measure.IEEE TPAMI. 1979;(2):224\u2013\n227.\n[6]Munkres J. Algorithms for the assignment and transportation problems.SIAM.\n1957;5(1):32\u201338.\n[7]Benjamini Y, Hochberg Y. Controlling the false discovery rate.JRSS B. 1995;57(1):289\u2013\n300.\n[8]McInnes L, Healy J, Melville J. UMAP: Uniform Manifold Approximation and Projec-\ntion.arXiv:1802.03426. 2018.\n11\nFigure 1: Pan-cancer k-means silhouette score as a function of k, computed on the\nautoencoder latent space (Z) for all 801 samples. The maximum occurs atk= 6.\nFigure 2: Example autoencoder training curve (KIRC): reconstruction MSE vs epoch for\ntraining and validation splits.\n12\nFigure 3: Within-KIRC k-means silhouette score as a function of k. The best score is at\nk= 2, but rare subtypes emerge only at largerk.\nFigure 4: Within-KIRC Davies\u2013Bouldin index (DBI) as a function of k. Lower values\nindicate more compact and separated clusters.\n13\nFigure 5: Cluster sizes for KIRC at k= 5. Cluster C0 (size 10, prevalence 6.8%) is\nhighlighted in red as the rare subtype.\nFigure 6: Cluster stability for KIRC at k= 5, measured by Jaccard index across 20\nrandom seeds after Hungarian alignment. C0 (red) has Jaccard 0.787 (above the 0.60\nstability threshold, dashed line).\n14\nFigure 7: UMAP of the KIRC latent space, with rare cluster C0 highlighted in red and\nother clusters in grey. C0 occupies a compact region of the latent space.\nFigure 8: Volcano plot for C0 (cluster-vs-rest DE on standardized expression). Each point\nis a gene; red labelled points denote the top markers listed in Table 2.\n15\nFigure 9: Heatmap of standardized expression ( z-scores) for the top 20 up-regulated and\ntop 20 down-regulated genes in C0. Samples are ordered with C0 on the left.\n16\n",
    "title": "Rare Genomic Subtype Discovery from RNA-seq via Autoencoder Embeddings and Stability-Aware Clustering",
    "authors": [
      "Alaa Mezghiche"
    ],
    "published": "2025-11-17",
    "arxiv_id": "2511.13705v1",
    "num_pages": 16,
    "num_chars": 21951
  },
  {
    "text": "Generalist Foundation Models Are Not Clinical\nEnough for Hospital Operations\nLavender Y.Jiang1,2,3*\u2020, AngelicaChen1\u2020, XuHan2, Xujin ChrisLiu2,4,\nRadhika Dua1,2,3, Kevin Eaton5,6, Frederick Wolff2,7, Robert Steele2,8,\nJeff Zhang9,10, Anton Alyakin2,11, Qingkai Pan2, Yanbing Chen2,12,\nKarl L. Sangwon2,5, Daniel A. Alber2,5, Jaden Stryker2,\nJin Vivian Lee2,3,11, Yindalon Aphinyanaphongs6,9,10,\nKyunghyun Cho1,3,13, Eric Karl Oermann1,2,3,5,9,14*\n1Courant Institute School of Mathematics, Computing, and Data Science, New\nYork University, 60 5th Ave, New York, 10001, NY, USA.\n2Department of Neurosurgery, NYU Langone Health, 550 First Avenue, New\nYork, 10016, NY, USA.\n3Global AI Frontier Lab, New York University, 1 Metrotech Center, Fl. 22,\nBrooklyn, 11201, NY, USA.\n4Electrical and Computer Engineering, Tandon School of Engineering, 6\nMetroTech Center, Brooklyn, 11201, NY, USA.\n5Grossman School of Medicine, New York University, 550 First Avenue, New\nYork, 10016, NY, USA.\n6DepartmentofMedicine,NYULangoneHealth,550FirstAvenue,NewYork,\n10019, NY, USA.\n7Department of Computer Science, ETH Zurich, Universit\u00e4tstrasse 6, City,\n8092, Zurich, Switzerland.\n8Department of Surgery, NYU Langone Health, 1 Park Avenue, New York,\n10016, NY, USA.\n9Division of Applied AI Technologies, NYU Langone Health, 227 East 30th\nStreet, New York, 10016, NY, USA.\n10Department of Population Health, NYU Langone Health, 450 First Avenue,\nNew York, 10019, NY, USA.\n11School of Medicine, Washington University of St. Louis, 660 S. Euclid Ave.,\nSt. Louis, 63110, MO, USA.\n12School of Global Public Health, New York University, 708 Broadway, New\nYork, 10003, NY, USA.\n1arXiv:2511.13703v1  [cs.CL]  17 Nov 2025\n13Prescient Design, Genentech, 149 5th Ave. 3rd floor, New York, 10019, NY,\nUSA.\n14Department of Radiology, NYU Langone Health, 450 First Avenue, New\nYork City, 10019, NY, USA.\n*Corresponding author(s). E-mail(s): Lavender.Jiang@nyu.edu;\nEric.Oermann@nyulangone.org;\nContributing authors: Angelica.Chen@nyu.edu; Xu.Han@nyulangone.org;\nxl3942@nyu.edu; rd3571@nyu.edu ; Kevin.Eaton@nyulangone.org;\nwfrederick@ethz.ch; Robert.Steele@nyulangone.org;\nJeff.Zhang@nyulangone.org; Anton.Alyakin@nyulangone.org;\nivrinom@gmail.com; yc6785@nyu.edu; Karl.Sangwon@nyulangone.org;\nDaniel.Alber@nyulangone.org; Jaden.Stryker@nyulangone.org;\njin.v.lee@wustl.edu; yin.a@nyulangone.org; Kyunghyun.Cho@nyu.edu;\n\u2020These authors contributed equally to this work.\nAbstract\nHospitalsandhealthcaresystemsrelyonoperationaldecisionsthatdeterminepatientflow,\ncost, and quality of care. Despite strong performance on medical knowledge and conver-\nsational benchmarks, foundation models trained on general text may lack the specialized\nknowledgerequiredfortheseoperationaldecisions.WeintroduceLang1,afamilyofmod-\nels (100M\u20137B parameters) pretrained on a specialized corpus blending 80 billion clinical\ntokens from NYU Langone Health\u2019s electronic health records (EHRs) and 627 billion\ntokens from the internet. To rigorously evaluateLang1in real-world settings, we devel-\noped the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331\nEHRnotesthatevaluatesfivecriticaltasks:30-dayreadmissionprediction,30-daymortal-\nity prediction, length of stay, comorbidity coding, and predicting insurance claims denial.\nIn zero-shot settings, both general-purpose and specialized models underperform on four\noffivetasks(36.6%\u201371.7%AUROC),withmortalitypredictionbeinganexception(upto\n94.2%AUROC).Afterfinetuning,Lang1-1Boutperformsfinetunedgeneralistmodelsup\nto70\u00d7largerandzero-shotmodelsupto671\u00d7larger,improvingAUROCby3.64%\u20136.75%\nand 1.66%\u201323.66% respectively. We also observed cross-task scaling with joint finetun-\ning on multiple tasks leading to across the board improvement on other tasks.Lang1-1B\neffectivelytransfersto out-of-distribution settings, including other clinical tasks and an\nexternal health system. Our findings suggest that predictive capabilities for hospital oper-\nationsrequireexplicit supervised finetuning, and that this finetuning process is made\nmore efficient by in-domain pretraining on EHR. Our findings support the emerging view\nthat specialized LLMs can compete with generalist models in specialized tasks, and show\nthat effective healthcare systems AI requires the combination of in-domain pretraining,\nsupervised finetuning, and real-world evaluation beyond proxy benchmarks.\nKeywords:pretraining, finetuning, Electronic Health Records, hospital operations, clinical\nprediction tasks, domain-specific models\n2\n1 Main\nHealthcaresystemsfacehigh-stakesoperationaldecisionsdaily:whichpatientsareatimminent\nrisk of decline, who can be safely discharged, how many beds will be available for new\nadmissions.Thesedecisionsdirectlyimpactresourceallocation,carecoordination,andpatient\noutcomes[1\u20133].Ashealthcaresystemsfaceincreasingpatientvolume,thereisaneedfortools\nthat can analyze complex clinical data to inform these critical decisions. Foundation models,\nwith their powerful text comprehension capabilities and versatility in specialized domains\n[4\u20139], have emerged as a promising technology for optimizing hospital operations.\nHowever, deploying LLMs in clinical settings is fraught with challenges. While LLMs\nshow promise in various clinical tasks [10\u201325], there is disagreement on whether smaller\nspecializedmodels(\"specialists\")canoutperformgeneral-purposemodels(\"generalists\")[26\u2013\n28]. Many evaluations rely on proxy benchmarks that weakly reflect real-world clinical\nconstraints like data scarcity and temporal shifts [17, 29\u201335]. Data privacy concerns further\nlimitthepretrainingdataforthevastmajorityoftheclinicalLLMs[36]toasmallsetofpub-\nliccorpora,MIMIC[37]andPubMed[38],eventhoughlarge-scaleEHRdatasetsareknown\nto improve out-of-domain generalization [28, 39\u201342].\nIn this work, we focus on hospital operation tasks that represent the daily challenge of\nhealthcaredeliveryandexplorethetradeoffsbetweenoff-the-shelfgeneralistsandspecialized\nmodels trained on a health system\u2019s internal patient notes. Our contributions are as follows:\n1.Lang1: a family of models specialized for hospital operations.We presentLang1, a\nsuiteofdecoderLLMs(100M,1B,and7B),pretrainedfromscratchonamixof80billion\ntokensofEHRnotesand627billiontokensofinternettexts.Aftertask-specificfinetuning,\nLang1outperformsboth off-the-shelf generalist LLMs (such asDeepseek R1671B) and\nthe parameter-efficient finetuned variant (LoRA finetunedDeepseek Distilled Llama\n70B), on theReMedEbenchmark. Instruction finetuned on one or more tasks,Lang1\ntransferszero-shottorelatedtasksandtoadifferenthospital,surpassinggeneralistmodels\nof similar scales.\n2.ReMedE:anoperations-groundedevaluationsuite.Weconstructaninternalevaluation\nbenchmarkconsistingoffiveclinicallyimportantclassificationtasksfromamulti-hospital\nacademic health system across 10 years, where each task contains 87,974 to 421,429\nreal patients. The benchmark has time-based splits to mimic deployment settings and\ndata-efficiency protocols to reflect real-world constraints.\n3.EngineeringPrinciplesforclinicalutilities.Weanalyzethetrainingdynamicsandshow\nthatpretrainingonnexttokenpredictionofunlabeledclinicalnotesandwebtextcontributes\nto emergent skills on comprehension tasks but is insufficient for excelling onReMedE,\nwhich specifically requires supervised finetuning (SFT). However, SFT is made more\nefficient by in-domain pretraining, and larger models pretrained on more clinical data\nimprove temporal robustness.\nOverall,ourresultssuggestthathealthsystemswiththecapacityforin-housemodeldevel-\nopmentcangainclearadvantagesfromsmallerspecializedmodels,providingapracticaland\ndata-efficientpathwaytorobustoperationalpredictionwithminimaltask-specificsupervision.\n3\n\u201cSepsis\u201d a.Data Collection b.Pre-training c.Fine-Tuning and Transfer Learning \ne.Ablation Experiments \nInternet EHR \nDataset \nPre-trained \nLang1 Patient is febrile and \nmay have [...] \nDeepSeek-R1 \n Llama-3-70B \nGPT-4o \n MedQA \nLeaderboard Models \nFine-tuned \nLang1 B. No Multiple-Choice based Fine-tuning tasks Test on Unseen Task \nClinical Notes Web Texts \nQ. How long will pt \nstay at hospital ?Admission \nNotes \nQ. [Mortality] Given {admission note}, \nwill this patient die during admission? \nd.Comparison to Generalist Language Models \nvs\nClinical Notes Web Texts \nData Mix  Variation \nPre-train \nModel Scale  Variation \nData Scale  Variation \nClinical \nFine-Tuning \n\u201cA. 0 to 2 days\u201d \nPredicted Probability: 0.4\nGround Truth: 0.6\nCompute \nLoss\nUpdate Weights \nGeneralist Language Models Q. Will pt be \nre-admitted ?Discharge \nNotes \nA. Yes\nB. No\nEval Hospital  Variation \nNYU Boston \nEval Task Type  Variation \nNonclinical \nFine-tuned \nLang1 Pre-trained Trajectories  \n\u2026\nckpt 1k ckpt 2k ckpt 3k \nckpt + \nLow data FT\nckpt + \nmax data FTEval Time  Variation \nPerformance Consistency  \nAcross Real-Time Fig. 1: Overview. (a) We mix unlabeled EHR notes and web texts as our pretrain corpus.\n(b) We pretrain using next token prediction. (c) Instruction finetuning in multiple choice\nformat enables cross-task transfer. (d) We compare Lang1 to off-the-shelf generalist models.\n(e) In order to derive design principles, we do ablations on data mix, model scale, pretrain\ntrajectories, data scale, eval task type, eval hospital, and eval time.\n1.1 Overview\nOur work consists of five stages: data collection, pretraining, finetuning, evaluation, and\nablations. Asshown inFigure 1,we first collectunlabeled clinicalnotes fromNYU Langone\nEHR and web texts from the internet and mix them to form the corpus (Figure 1a). We\npretrainLang1vianexttokenprediction(Figure1b).WeinstructionfinetuneLang1topredict\nlabelsforreal-worldclinicaltasks,enablingcross-tasktransfer(Figure1c).Wethencompare\nfinetunedLang1with off-the-shelf generalist models (Figure 1d) and perform ablations of\nthe data mix, model scale, eval task type, pretraining trajectory, finetune data scale, eval data\ntime, and eval hospital to derive design principles for clinical utilities (Figure 1e).\nWe evaluateLang1usingReMedE, an internal benchmark of real-world, high-impact\nclinicaltasksbeyonddiagnosis.Unlikerecentbenchmarksthatfocusonmulti-turndiagnostic\ndialogue[15,24,29],whichcapturesanimportantbutnarrowpartofclinicaldecisionmaking,\nReMedEisbasedon668,331EHRnotesandemphasizesoperationaltasksthatbetterrepresent\nthe day-to-day challenges of healthcare delivery [1\u20133]. These tasks support practical goals\nsuch as reducing costs, optimizing resource use, and improving continuity of care.\nReMedEincludes five predictive tasks drawn from real-world hospital workflows: 30-\nday all-cause readmission, in-hospital mortality, binned length of stay, insurance denial, and\nimputation of binned Charlson Comorbidity Index (CCI, a measure of patient comorbidity\nburden). These tasks reflect critical decisions tied to patient outcomes, resource planning,\n4\nand healthcare operations (See Methods 5.2.2 for more details). To assess model robustness\nto temporal distribution shifts, each task is evaluated across three non-overlapping test splits\ndrawnfromdistincttimeperiods(AppendixA).ReMedEsupportsflexiblefew-shotevaluation\nacross a range of language model interfaces and task formats. It is easily extensible for new\ntasks and evaluation settings. We plan to releaseReMedEas a secure evaluation service,\nallowing trusted researchers to submit models and receive standardized evaluation results\nwithout direct access to patient data. This design safeguards patient privacy while enabling\nfair and reproducible model comparison.\n2 Results\nReadmission In\nHospital\nMortalityInsurance\nDenialLOS CCI0.300.400.500.600.700.800.901.00ROC AUCLang1-1B\nMedMobile-3.8B\nLlama 3.2 1B\nLlama 2 7B\nLlama 3.3 70B Chat\nDeepSeek R1 Distill Llama 70B \nDeepSeek R1\ngpt-4o-sampling-prob\nRandom-guess AUC\nBest per task (zero-shot)\n(a) Both generalists and specialists underperform zero-shot. Yellow triangles indicate the best zero-shot\nperformancepertask.WhilethebestmortalitypredictionAUROCis94.2%,performanceofothertasks\n(readmission, insurance denial, LOS, CCI) range from 36.6%\u201371.7% AUROC.\nReadmission In\nHospital\nMortalityInsurance\nDenialLOS CCI0.50.60.70.80.91.0ROC AUCBest Zero-shot (max 671B)\nLlama 3.2 1B (finetuned)\nDSR Distill Llama 70B (LoRA finetuned)\nLang1-100M (finetuned)\nLang1-1B (finetuned)\nLang1-7B (finetuned)\nRandom-guess AUC\nBest per task (zero-shot)\nBest per task (finetuned)\n(b)FinetunedLang1-1B(purple)outperformbestzero-shotperformance(magenta)by1.66%to23.66%\nAUROCandfinetunedLlama 3.2 1B(lightblue)andLoRAfinetunedLlama 70B(deepblue)by3.64%\nto 6.75% AUROC. Yellow stars indicate the best finetuned performance per task.\nFig. 2: Finetuned small specialists outperform strong generalists on ReMedE.\nLarge generalist models underperform on real-world clinical predictive tasks..We\nevaluatebothlargegeneralistfoundationmodelsandmedQAleaderboardmodelsunderzero-\nshotinference,andfindthattheyunderperformonReMedEtasks.Afterfinetuning,Lang1-1B\noutperform both two finetuned models (Llama 3.2 1Band LoRADeepseek R1 Distill\nLlama 3.2 70B)by3.64%to6.75%AUROC.Lang1-1Balsooutperformsthebestzero-shot\nperformance (including models up toDeepSeek R1 671B) by 1.66% to 23.66% AUROC.\n5\nFigure2ashowsthatonspecializedtaskssuchasinsurancedenial,bothleaderboardmodels\n(Llama 2 7B\u2014lightblue,Llama 3.2 1B\u2014blue,MedMobile\u2014grey)andourownpretrained\nmodels (Lang1-1B\u2014purple) underperform in the zero-shot setting. While mortality predic-\ntionhasupto94.2%AUROC,theotherfourtasksrangebetween36.6%-71.7%AUROC.This\nshows that even the strongest generalist models falter on these specialized operational tasks.\nFigure 2b compares the best zero-shot result per task against a few finetuned models,\nincludingLang1(100M, 1B and 7B),Llama 3.2 1B, and a parameter-efficient finetuned\nversionofDeepseek R1 Distill Llama 70B.Acrossallfivetasks,Lang1-1BandLang1-7B\n(purple bars) achieve higher AUROC than the best zero-shot model (magenta) and the other\nfinetunedmodels(lightblueanddarkblue).Theimprovementsovertheotherfinetunedmodels\nrange from 3.64%\u20136.75%. The improvements over the best zero-shot baseline range from\n1.66% (mortality) to 23.66% (insurance denial), with the largest gains observed at insurance\ndenial prediction, which isDeepseek R1\u2019s worst performing task.\n0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0\nNumber of Pretraining Tokens 1e110.250.300.350.400.450.500.550.60AccuracyPubMedQA\n0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0\nNumber of Pretraining Tokens 1e11SciQ\nLang1-1B\nLang1-7B\nrandom guess\n(a) Reading comprehension performance increases from pretraining.\n0.300.350.400.450.500.55ROC AUC\nReadmission\n In-Hospital Mortality\n0 1 2 3 4\nNumber of Pre-training Tokens 1e11\nInsurance Denial\n0 1 2 3 4\nNumber of Pre-training Tokens 1e110.300.350.400.450.500.55ROC AUC\nLength of Stay Prediction\n0 1 2 3 4\nNumber of Pre-training Tokens 1e11\nComorbidity Prediction\nLang1-1B\nLang1-7B\nrandom guess\n(b) Clinical classification performance does not rapidly emerge from pretraining.\nFig. 3: Zero-shot clinical classification performance does not increase over the course of\npretraining, unlike reading comprehension. Error bars depict the 95% confidence interval.\nClinical performance does not emerge during pretraining, unlike reading comprehen-\nsion.Wetrackedzero-shotperformanceofLang1(1Band7B)throughoutpretraining,asa\nfunctionofthenumberoftokensseen.Oncomprehensiontasks(Method5.2.4fordatadetails),\naccuracy increased with additional pretraining data (Figure 3a), consistent with the intuition\nthatlanguagemodelsimproveontext-basedreasoningtasksastheyareexposedtomoredata.\nIncontrast,zero-shotAUROConclinicalclassificationtasks(ReMedE)remainedclosetoor\n6\nbelowrandomchanceacrosstheentirepretrainingtrajectory(Figure3b).Wehypothesizethat\nthe mapping from clinical notes to outcomes does not emerge from learning next token pre-\ndiction on unlabeled texts alone, but must be learned through either task-specific finetuning\nor alternative pretraining objectives.\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0\nT otal # of T okens Seen (Pretraining + Finetuning) 1e110.40.50.60.7Readmission ROC AUCRandom guess\nNo pretrain, no finetuneNo pretrain, full finetuneFull pretrain, full finetune\n0246# Finetune T okens1e8\n(a) Finetuning is more token-efficient for performance gains, but pretraining still provides value. At any\nfixed token budget (a vertical slice on thexaxis), using more finetuning tokens (darker colors) yields a\nhigherROCAUCforLang1-1B\u2019scheckpointtrajectory.Nonetheless,acleargapremainsbetweenfully\nfinetuning without pretraining (purple diamond) and the fully pretrained model (yellow star).\n10121013\nT otal Number of Training T okens (Pretrain + Finetune)0.5750.6000.6250.6500.6750.7000.7250.7500.775Readmission T est ROC AUC (2024)\nModel\nLang1-1b (314.6B pretrain tokens)\nLang1-1b (419.4B pretrain tokens)\nLlama-2-7B (2T pretrain tokens)\nLlama-3.2-1B (9T pretrain tokens)Experiment\nBest Finetuning Run\nLow Finetune Data\n(b) Clinically pretrained models (purpleLang1-\n1b),whenfinetuned,outperformgeneralistmodels\nof similar size (blueLlamamodels), especially\ninlowdataregime(cross).Arrowstrackthesame\npretrained model as it is finetuned on different\nnumbers of examples.\n10 12 14 16 18 20\nPerplexity of Answer given Question ppl(A|Q)0.500.550.600.650.700.75Readmission ROC AUC\nModel\nLang1-1b (314.6B tokens)\nLlama-3.2-1B (9T tokens)\nLlama-2-7B (2T tokens)\nExperiment\nFinetuned\nZero-shotExperiment\nFinetuned\nZero-shot(c) Lower model perplexity onReMedEanswer-\nquestion pairs is associated with better zero-\nshot (cross) and finetuned (circle) performance.\nLang1(purple)haslowerperplexitydespitefewer\ntotalpretraintokens(thoughmoreclinicaltokens).\nArrows track the same model\u2019s performance.\nFig. 4: Finetuning is are more token-efficient than pretraining for performance gains\n(Figure 4a), but in-domain pretraining enables sample-efficient finetuning (Figure 4b). This\nadvantage is also associated with lower perplexity on in-domain tasks (Figure 4c).\nCompute is better spent on finetuning, but pretraining makes finetuning more efficient.\nFigure 4a examines the pretraining and finetuning trajectory of theLang1-1Breadmission\nmodelunderafixedtotaltokenbudget(pretraining+finetuning,i.e.,eachverticalslice).Dur-\ning pretraining ofLang1-1B, checkpoints were saved after each one million training tokens.\n7\nEach pretrain checkpoint is finetuned using 100\u2013362,259 discharge notes with readmission\nlabel (2.0M\u2013742.0M tokens). The compute budget is the sum of pretraining token for that\ncheckpoint and the number of finetuning tokens used to finetune that particular checkpoint.\nWithin each slice, increasing the proportion of finetuning tokens (darker colors) consistently\nimprovesperformance.Atthesametime,pretrainingstillprovidesvalue:evenwithmaximal\nfinetuning data, models initialized from pretraining (purple diamond) outperform the ran-\ndomlyinitializedone(yellowstar)by7.03%AUROC.AsimilarpatternappearsinFigure4b:\nwhenfinetuningdataarescarce,Lang1-1B(purple)outperformsgeneralistmodelsofcompa-\nrable scale (blueLlama-2-7BandLlama-3.2-1B) that were pretrained on more nonclinical\ntokens, demonstrating the efficiency gains of domain-specific pretraining. In fact, Figure 4c\nshows thatLang1-1B, despite being trained on fewer tokens, achieves lower perplexity on\nanswer\u2013question pairs and stronger zero-shot and finetuned performance thanLlama-2-7B\nandLlama-3.2-1B.Ablationsshowthatlargermodelstrainedonmorerecentdatahavebetter\nperformance (Appendix D).\n     \nT ask-specific ROC AUCs                               Finetuning Dataset(s)48% 56% 55% 54% 43%\n77% 84% 64% 77% 57%\n70% 96% 64% 80% 58%\n63% 47% 77% 65% 58%\n69% 78% 63% 90% 61%\n50% 30% 52% 51% 87%\n76% 95% 76% 90% 88%No Finetuning\n/zero-width-space /zero-width-space /zero-width-space /zero-width-space \n0.30.40.50.60.70.80.9\nReadmission\nMortality\nLOS\nCCI\nInsurance\nDenial\nReadmission Mortality LOS CCI Insurance\nDenial\n(a)FinetuningLang1-1Bcanoftentransferacross\ntasks. The heatmap shows the finetuned model\u2019s\nperformance when finetuned on a subset of\nReMedEtasks (yaxis) and evaluated on all five\nReMedEtasks (xaxis).\nLang1-1B Llama-3.2-1B\nPretrained modelzero-shotNYU\nReadmissionMIMIC\nReadmissionFinetune setting51.6%\n\u00b11.5%45.5%\n\u00b11.5%\n66.4%\n\u00b11.4%60.8%\n\u00b11.4%\n67.7%\n\u00b11.4%58.3%\n\u00b11.5%External Validation of Finetuning Lang1\nfor MIMIC III Readmission\n0.500.550.600.650.700.750.80\nROC AUC on MIMIC III Readmission\n(b) Finetuning on NYU Readmission (purple)\ntransferswell(darkeryellow)toMIMICIIIRead-\nmission (green). Overall performance is better\non finetuningLang1-1B(purple) compared to\nLlama 3.2 1B(blue).\nFig. 5:Lang1is able to transfer to unseen task (Figure 5a) and a different health system\n(Figure 5b).\nLang1is able to transfer to unseen task and a different health system.The heatmaps\nin Figure 5a showLang1-1Bfinetuned on one or all ReMedE tasks (rows) and evaluated on\nall five tasks (columns). Overall, Lang1-1B achieves strong single-task (diagonal) and joint-\ntask (last row) performance, and is well calibrated (Appendix E). Many tasks transfer. For\nexample,finetuningonreadmission(secondrow)boostsperformanceontheotherfourtasks.\nHowever,thistransfercouldbeasymmetric,i.e.,mortalityhelpsLOS(thirdrow,thirdcolumn),\nbut LOS does not help mortality (fourth row, second column), which can be explained using\ndomain knowledge (Appendix F). Compared toLang1-1B,Llama-3.2-1Bhas overall worse\nperformanceandadifferenttransferpattern(AppendixC),suggestingthatthepatterndepends\non the pretrained model.\n8\nFigure5bshowsLang1-1BandLlama-3.2-1Bfinetunedusingdifferentreadmissiondata\n(MIMICIIIorNYU)andtestonMIMIC.FinetuningLang1-1Bshowsbetterperformanceon\nbothdata.ForLang1-1B,finetuningonMIMICisslightlybetterthanNYUby1.2%AUROC.\nHowever, forLlama-3.2-1B, finetuning on NYU is surprisingly better than finetuning on\nMIMIC by 2.5% AUROC. We suspect this is because NYU+Readmission has more labeled\npairs than MIMIC readmission, suggesting that nonclinical models may benefit more from a\nlarge number of slightly out-of-distribution examples compared to a smaller number of in-\ndistributionexamples.Indeed,downsamplingNYUdatasettothesamesizeasMIMICwould\nleadtosimilarresultsforLlama-3.2-1B(FigureM11b).AppendixMextendsthisanalysisto\nMIMIC mortality and LOS with similar observations.\n3 Discussion\nWepresentwhatis,toourknowledge,thefirstcomprehensivestudyofLLMsforhospitaland\nhealth-systemoperations.Wedetailafull-cycleapproach,inwhichwebuild,benchmarkand\ntesttherobustnessofthesemodelsaspartofthetheiroperationaldeploymentwithintheNYU\nLangoneHealthSystem.Ourworkhasimplicationsforthebroaderdebatebetweengeneralist\nand specialist models, LLM generalizability, and pretraining-finetuning dynamics.\nWhy Operational Tasks Matter.Much of the current excitement in medical AI centers\non diagnostic reasoning [12, 29, 35, 43\u201345]. These are valuable directions, but they do not\nfully capture the day-to-day challenges physicians face. Healthcare is as much operational as\nit is clinical. Physicians spend only 26% of their time in direct patient care, with much of the\nremainder devoted to documentation, insurance, and scheduling [1]. Operational outcomes\nsuch as readmission, insurance denial, and length of stay directly shape costs, capacity, and\ncontinuity of care. Predicting them is actionable, and improving them is measurable. By\nadvancing operational tasks alongside diagnostic ones, we can reduce costs, improve care\ndelivery, and make healthcare more accessible. Notably, we find that many of these tasks\nrequire specialized finetuning, and are not out-of-the-box accessible to generalist models,\nlikely reflecting their poor representation in web-scale datasets.\nOn The Need for Real-World Evaluation.Our results show the limitations of relying\non proxy benchmarks as evidence of readiness for clinical deployment. Several MedQA\nleaderboard models under-perform on ReMedE, suggesting that proxy benchmark success\ndoes not establish clinical utility. Evaluating models directly on real-world, task-specific\noutcomes is therefore essential, not only to identify models that genuinely improve hospital\noperations, but also to avoid overestimating the safety or value of systems based solely on\nproxy measures [17, 33, 34].\nThe Costs of Training.A common concern about specialized models is whether they can\nbe trained affordably outside of industrial labs. Our experience withLang1suggests that\nthe answer is yes. Training the 1B-parameter model on 314.5B tokens (150k steps) required\nroughly 30 days on 64 H100s, which at AWSp5.48xlargepricing corresponds to a cost\nof about $180,000. While non insignificant, this figure is orders of magnitude lower than\nthe multimillion-dollar budgets required for frontier generalist models [5, 6, 46, 47], yet\n9\ndeliverssubstantialperformancegainsonreal-worldhospitaltasks.Foralargehealthsystem,\nsuch costs are comparable to routine IT infrastructure upgrades and therefore financially and\noperationally feasible and profitable. This supports recent position papers [48, 49] that argue\nsmall, specialized models are the future of agentic AI because they can be more reliable,\neconomical, and aligned with domain-specific needs.\nIn-HouseModels.Fromanoperationalstandpoint,traininganddeployingmodelsin-house\ncanmakemoresensethanrelyingexclusivelyongeneralistmodelsdeliveredviacommercial\nAPIs. In this paradigm, hospitals not only reduce long-term costs and safeguard patient data,\nbut also retain the ability to adapt models as documentation practices, coding standards, and\npatient populations shift over time, a need highlighted by our temporal robustness results. In\ncontrast,thedependenceonexternalAPIscanintroduceongoingexpenses,privacyrisks,and\nlimited control over model behavior. Our findings suggest that even modestly sized models,\ntrained locally, can outperform larger off-the-shelf alternatives, reframing clinical AI from\n\u201crentingintelligence\"to\u201cbuildinginstitutionalassets\"thatevolvewiththehealthsystemitself.\nPretraining Is Not All You Need.While the reading comprehension capabilities emerge\ndirectly from large-scale pretraining, our finding provides evidence that high-stake objective\npredictions represent a different class of problem. Strong performance onReMedEtasks\nrequire explicit finetuning and does not emerge from pretraining alone, even with domain-\nspecific data. This reliance on finetuning is shared by general-purpose chatbots, but the\nunderlying tasks are fundamentally different. Chatbot finetuning aligns emergent generalist\nskillstosubjective,preference-basedgoals[50].Incontrast,ReMedEtasksrequirefinetuning\nto build a new, non-emergent predictive skill against anobjective, grouth-truth target. We\nhypothesize that this is because clinical predictions rely on complex relationships not well\ncapturedbyanext-token-predictionobjective.Ourexperimentsshowthatthesetasksdemand\ndomain-specific supervision built on top of in-domain pretraining.\nWhyTransferMatters.Healthcaretasksoftensufferfromlimitedlabelsduetotheexpertise\nrequired for annotation, and in some cases it is practically impossible to obtain large labeled\ndatasets (e.g., for rare conditions). Our experiments show that instruction finetuning enables\ntransfer across related tasks, where supervision on one outcome (e.g.,readmission) improves\nperformanceonothers(e.g.,mortality,lengthofstay).WealsoshowedthatfinetuningLang1\non NYU can transfer to a different hospital. This capability is especially valuable in clinical\nsettings because it reduces dependence on costly annotation pipelines and makes it possible\nto tackle tasks where labels are scarce or unattainable, effectively maximizing the value of\nindividual annotations.\n4 Conclusions\nWe introducedLang1, a family of domain-specialized models trained on EHR and web\ntext. To evaluateLang1, we developedReMedE, an operations-grounded benchmark for\nevaluating language models across five operationally significant tasks. We find that large\ngeneralist models underperformLang1-1Bby 1.66% - 23.66% AUROC, despite their strong\nperformance on general and proxy medical benchmarks.Lang1-1Balso outperformed other\n10\nfinetuned models up to70\u00d7larger by 3.64%\u20136.75%. These results demonstrate that success\non proxy benchmarks does not guarantee effectiveness in deployment-critical settings.\nOuranalysisshowsthatclinicalpredictioncapabilitiesdonotarisefrompretrainingalone;\nexplicitfinetuningisnecessary,thoughin-domainpretrainingimprovesdataefficiency.Lang1\nalso demonstrates promising transfer across related tasks and health systems, suggesting that\ncarefully designed instruction finetuning can reduce dependency on expensive annotation.\nTraining costs forLang1are substantially smaller than frontier models and affordable for\nlarge health systems, making specialized models both effective and practical.\nFuture works include expandingReMedEto additional institutions, more diverse patient\npopulations,andtasktypesbeyondclassificationoutcomes.Thisisimportantforestablishing\nshared standards for evaluating models in real operational contexts.\nOurfindingschallengetheassumptionthatever-largerinternet-trainedmodelswillgener-\nalizetoalldomainsandinsteadpointtoamorehopefuldirectionforclinicalAI.Inhealthcare,\nwhere patient safety is paramount, progress must be deliberate and grounded in real oper-\national outcomes. Benchmarks likeReMedEand specialized models such asLang1show\nthat smaller, domain-specific systems can be accurate, affordable, and adaptable, offering a\nscalable path forward that directly supports hospital operations and improves patient care.\n11\n5 Methods\n5.1 Data Collection\nData are extracted via structured query language (SQL) scripts to query the NYU Langone\nEHR,prototypedinaninteractiveweb-basededitor(ClouderaHue),andexportedascomma-\nseparated files (CSVs) to an on-prem high-performance computing (HPC) cluster.\nPreprocessing.RawCSVnotes(includingpathology,radiology,andgeneralhospitalnotes)\nare loaded with standard ASCII-encoding using Python Dask [51] for distributed processing.\nWe concatenate narrative fields, standardize punctuation, spacing and formatting via regular\nexpressionsubstitutions,removenon-ASCIIandmalformedcharacters,removeerrantwhites-\npace and newlines, and filter out short notes (less than 10 words or with placeholder values\nsuch as<NA>).\n5.2 Datasets\n5.2.1 Pretraining Dataset\nWebtexts.WeuseSlimPajama(627Btokens)[52],alargeextensivelydeduplicated,multi-\ncorpora, open-source dataset for training LLMs. Its sources include Commoncrawl [53], C4\n[54], Github, Books [55, 56], Arxiv, Wikipedia and StackExchange.\nNYU Notes.This dataset is previously created using unlabeled inpatient hospital notes\nsigned bymedical professionalsfrom the NYULangone HealthEHR1for patientencounters\nstarting from January 2011 to May 2020. NYU Notes contains 387,144 patients, 7,247,694\nnotes,and4,112,249,482wordsintotal.NYUNotesareusedtotrainandevaluateNYUTron.\n[42]\nNYU Notes+.This dataset builds on NYU Notes by including a wider range of note types\nandcoveringalongertimespan,resultinginatotalwordcount14.5timesgreaterthanthatof\nNYU Notes. NYU Notes+contains unlabeled hospital notes, pathology note and radiology\nnotesfromNYULangoneHealthEHRfrom2003to2023.Itcomprises11,689,342patients,\n180,487,092 notes, and 59,917,646,788 words.\n5.2.2 Finetuning Datasets and ReMedE Test Set\nWe derive five task-specific labeled datasets by combining NYUTron [42] finetune datasets,\nwith the addition of 2024 temporal test set to approximate deployment robustness. The 2024\ntemporal tests set are used for ReMedE. See Appendix A for a visualization of the data\nsplit timeline and Appendix B for detailed dataset statistics. Appendix L shows that a small\npercentage of patient overlap does not over-estimate model performance on readmission. For\nboth zero-shot evaluation and finetuning (section 5.4), the dataset were converted to multiple\nchoice format (Appendix H).\n1ThisstudyisapprovedbytheInstitutionalReviewBoard(IRB)atNYULangoneHealth.Themethodsarecarriedoutinaccordance\nwith the IRB\u2019s relevant guidelines and regulations.\n12\nNYU+Readmission.Readmission occurs when a patient returns to the hospital shortly\nafterdischarge.Predictingreadmissionsiscriticalforidentifyingpatientswhoneedlongerstay\nor post-discharge support, and it also serves as a key hospital quality metric. This finetuning\ndatasetcontainsdischargenoteswith30-dayall-causereadmissionlabels.Thenotescomprise\nof a subset of NYU+Notes whose encounters end between January 2013 and November\n2021, and additional discharge notes from 2024 for the temporal test. Rehabilitation, dialysis\nand palliative care notes are excluded to focus on modeling acute readmission. A positive\nlabel is assigned if the patient is readmitted within 30 days of discharge, and a negative label\notherwise. We split the dataset into five sets. The first three sets are train, validation, and test\nset with a 8:1:1 ratio from 2013 to May 2021. The 2021 temporal test includes notes from\nJune to December 2021. The 2024 temporal test set includes notes from 2024. The positive\nclass ratio range from 10.81% to 11.29% across these five sets. The dataset contains 421,429\npatients, 604,326 notes and 607,877,177 words in total.\nNYU+In-HospitalMortality.In-hospitalmortalitypredictionidentifiespatientsathighest\nriskofdeathduringtheiradmission,enablingtimelypalliativecareconsultationsandgoals-of-\ncare discussions that align treatment with patient prognosis. This finetuning dataset contains\nHistory and physical (H&P) notes with in-hospital mortality labels. The notes comprise of\na subset of NYU+Notes for encounters ending between January 2013 and November 2021,\nwith additional H&P notes from 2024 for the temporal test. A positive label is assigned is\nthe discharge disposition is \u201cExpired\", and a negative label otherwise. We split the dataset\ninto five sets. The first three sets are train, validation, and test set with a 8:1:1 ratio from\n2013 to May 2021. The 2021 temporal test includes notes from June to December 2021. The\n2024 temporal test set includes notes from 2024. The positive class ratio range from 1.78%\nto 1.93% across these five sets. In total, the dataset contains 395,991 patients, 566,748 notes,\nand 608,603,182 words.\nNYU+Length of Stay (LOS).LOS is the number of days a patient remains hospitalized.\nPredicting LOS is essential for bed management, staffing allocation, and discharge planning.\nThis finetuning dataset contains H&P notes with label for binned length of stay. The dataset\ncomprises of a subset of NYU+Notes for encounters ending between January 2013 and\nNovember 2021, with additional H&P notes from 2024 for the temporal test. We assign the\nlabelsbasedonquantile.Weassignedlabel0foranLOSbelowthe25%quantile(0to2days),\nlabel 1 for an LOS between the 25% and 50% quantile (3 days), label 2 for an LOS between\nthe 50% and 75% quantile (4 to 5 days) and label 3 for an LOS above the 75% quantile (>5\ndays).Wesplitthedatasetintofivesets.Thefirstthreesetsweretrain,validation,andtestset\nwith a 8:1:1 ratio from 2013 to May 2021. The 2021 temporal test includes notes from June\nto December 2021. The 2024 temporal test set includes notes from 2024. The majority class\nratio (0 to 2 days) range from 41.49% to 45.64% across these five sets. The minority class\nratio(morethan5days)rangefrom23.92%to26.34%.Intotal,thedatasetcontains395,991\npatients, 566,748 notes and 608,603,182 words.\nNYU+Insurance Denial.Insurance denials occur when payers reject claims for hospi-\ntal services. Predicting denials allows hospitals to proactively address documentation gaps,\nreducing administrative burden and preventing unexpected out-of-pocket costs for patients.\n13\nThis finetuning dataset contains H&P notes with insurance denial label. The notes comprise\nof a subset of NYU+Notes whose encounters ends between May 1, 2021 and April 30,\n2022, with additional H&P notes from Janurary 2024 for the temporal test. A positive label\nis assigned if claim status is \u201cfinal, adverse determination\" (initial rejection by insurance and\nagainrejectedfollowingappeal)or\u201cfinal,favorabledetermination\"(initialrejectionbyinsur-\nance and approved following appeal), an negative label otherwise. We split the dataset into\nfive sets. The first three sets are train, validation, and test set with a 8:1:1 ratio from May\n2021-Feb 2022. The 2022 temporal test include notes from March-Apr 2022. The 2024 tem-\nporal test set includes notes from January 2024. The positive class ratio range from 12.01%\nto 13.90%. The dataset contains 87,974 patients, 97,837 notes, and 89,147,715 words.\nNYU+Charlson Comorbidity Index (CCI).CCI is a standard score used to quantify a\npatient\u2019schronicillnessbasedontheirmedicalhistory[57].Itisusefulforsupportingaccurate\nriskstratificationforpatientswithlimitedhistoricaldata.However,thisheuristicscorecannot\nbecomputedwhenapatient\u2019smedicalhistoryisunknown.Thisdatasetaddressesthisproblem\nby providing History & Physical (H&P) notes paired with their corresponding binned CCI\nscores. It allows models to be trained to impute the CCI score directly from unstructured\nclinical text. The target CCI is calculated using ICD code associated with the encounter in\ntheEHRfollowingthescoringfunctionin[58].EncounterswithnoassociatedICDcodesare\nexcluded.TheCCIisdiscretizedintofiveclasses:comorbidityindexof0(<50thpercentile),\ncomorbidity index of1\u22122(50\u221275% percentile), comorbidity index of3\u22124(75-90th\npercentile), and comorbidity index of5\u22127(90-99th percentile), and comorbidity index>7\n(>99thpercentile).Wesplitthedatasetintofivesets.Thefirstthreesetsaretrain,validation,\nand test set with a 8:1:1 ratio from 2013 to May 2021. The 2021 temporal test includes notes\nfrom June to December 2021. The 2024 temporal test set includes notes from 2024. The\nmajority class (score 0) ratio range from 68.40% to 69.47%. The minority class (score more\nthan 7) ratio range from 0.059% to 0.17%. In total, the dataset has 306,741 patients, 443,915\nnotes, and 524,739,038 words.\n5.2.3 External Validation Datasets\nWealsocreateexternalvalidationdatasetsfromMIMICIII[59],whichissourcedfromBeth\nIsrael hospital in Boston Massachusetts.\nMIMICIIIReadmission.Thelabeleddatasethas6%positivelabels,with52,725examples\nand a 70% train, 15% validation and 15% test split. More dataset construction details are in\n[60]\u2019s Appendix A.\nMIMICIIIMortality.Thelabeleddatasethas10.55%positivelabels,with5658examples\nand 80% train, 10% validation and 10% test split. The dataset is constructed from MIMIC-\nIII clinical notes. As no explicit \"Admission Note\" label exists, we identify notes by filtering\ndescriptionsfor\"admission\"whileexcluding\"readmission\"(\u224819Knotes).Topreventpatient\nbias,weselectasinglenoteperhospitalstayusingaprioritizationheuristic.Theheuristicfirst\nprioritizesPhysicianNote>GeneralNote>NurseNote,thenprioritizes\"resident/attending\"\n>\"resident\">\"attending\">\"fellow\">\"h&p\".Finallytheheuristicpreferslongernotes.This\n14\nfilters down to\u22486K notes. We further refine the dataset by removing notes written>120\nhours after admission or associated with a negative length of stay. The final dataset contains\n5,658 unique admission notes.\nMIMICIIILOS.Thelabeleddatasetusesthesame5,658admissionnotesasthemortality\ntask,withameanLOSof7.96days.Similarly,thesplitis80%train,10%validationand10%\ntest. The continuous LOS values are discretized into bins. We adapt the NYU+LOS scheme\n(Methods5.2.2)tohandleMIMIC-III\u2019scontinuousrangebytreatingtheoriginalintegerbins\nas upper bounds. For example, the \"3 days\" bin is modified to capture the continuous range\n2<LOS\u22643.\n5.2.4 Comprehension Datasets\nWeevaluatetheperformanceofLang1checkpointsoncomprehensiondatasetstoanalyzethe\nemergence of its nonclinical abilities.\nSciQ [61].The dataset contains 13.7K multiple choice science exam questions with con-\ntexts. An example question isMesophiles grow best in moderate temperature,\ntypically between 25\u25e6C and 40\u25e6C (77\u25e6F and 104\u25e6F). Mesophiles are\noften found living in or on the bodies of humans or other animals.\nThe optimal growth temperature of many pathogenic mesophiles is 37\u25e6C\n(98\u25e6F), the normal human body temperature. Mesophilic organisms have\nimportant uses in food preparation, including cheese, yogurt, beer\nand wine. \\n Question: What type of organism is commonly used in\npreparation of foods such as cheese and yogurt? \\n Answer:\nPubmedQA [62].The dataset contains 1k expert-annotated biomedical question answer-\ning dataset collected from PubMed abstracts. An example question isAbstract:\nComplex regional pain syndrome type I is treated symptomatically ...\nEarly cast-related complaints predicted the development of complex\nregional pain syndrome (relative risk, 5.35; 95% confidence interval,\n2.13 to 13.42)\\n Question: Can vitamin C prevent complex regional pain\nsyndrome in patients with wrist fractures?\\n Answer:\n5.3 PretrainingLang1\nWe pretrain a family of Llama-style decoders (Lang1-100M,Lang1-1B,Lang1-7B) on a\nmixtureofwebtextsandNYUNotes+(section5.2.1)usingnexttokenprediction(Figure1a,b).\nDetailed demographic statisitics is in Appendix G. Unless otherwise noted,Lang1models\naretrainedwithequalsamplingfrombothclinicalandgeneralsources,whichissupportedby\nour pretraining ablations (Appendix D). For tokenization, we use theLlama-2-7Btokenizer\n(SentencePiece, 32k vocabulary). The 100M-parameter model follows theSmol-Llama-\n101Marchitecture with a 1024 context length; the 1B model followsTinyLlama-1.1Bwith\na 2048 context length; and the 7B model followsLlama-2-7Bwith a 4096 context length\nWepretrainon8to64nVidia80GBH100swithNVLink.WeusedtheLitGPT[63]library\nand Fully Sharded Data Parallel [64].\n15\nWe run a few trials of manual hyperparameter search based on speed, performance and\ntraining stability. For all models we use AdamW optimizer with linear warmup (2000 steps),\nbeta1 of 0.9, beta2 of 0.95, eps of 1e-8 and cosine cycle decay down to a minimum learning\nrate of 4e-5. We use a seed of 3407, weight decay of 0.1, and gradient clipping of 1. In terms\nofFSDPsharding,weshardgradientandoptimizerformodelsupto1B,andfullshardingfor\n7B model. For effective batch size, we use 4096 for 100M model for controlled comparison\nwith NYUTron [42], and 1024 for 1B and 7B models.\nWeimplementamonitoringpipelinethatautomaticallytriggeredfew-shotevaluationsand\ngenerations at fixed intervals of pretraining steps. Slack Weights & Biases (W&B) alerts are\nconfigured to report loss spikes. Upon detection of anomalies in loss or output, we revert to\nthe most recent stable checkpoint. Validation loss is computed periodically on the held-out\n0.1% validation split and used to determine early stopping and model checkpoint selection.\n5.3.1 Pretrained Models\nWe pretrain the following variants ofLang1models (Table 1). Ablations (Appendix D)\nshow that larger models trained on more clinical data has better performance, and mixing\nin web texts does not hurt much. When we refer toLang1without specifying data sources,\nwe mean the one trained with NYUNotes+and WebTexts. For instance,Lang1-1Brefers to\nlang1-1B-NYUNotes+,WebTexts.\nTable 1: Pretrained Model Specifications\nModel Name Model Size Pretrain Data\nlang1-100m-NYUNotes100m NYUNotes\nlang1-100m-NYUNotes+100m NYUNotes+\nlang1-100m-NYUNotes+,WebTexts100m NYUNotes+,WebTexts\nlang1-1B-NYUNotes1B NYUNotes\nlang1-1B-NYUNotes+1B NYUNotes+\nlang1-1B-NYUNotes+,WebTexts1B NYUNotes+,WebTexts\nLang1-7B-NYUNotes+,WebTexts7B NYUNotes+,WebTexts\n5.4 Finetuning\nWefinetuneLang1models(andtheirtrajectoriesofcheckpoints)andotherpretrainedmodels\n(Table 2) on ReMedE tasks using multiple choice format (Figure 1 panel c). The labeled\nclinical notes are converted to multiple choice format (Appendix H), and we train the model\nto predict the correct multiple choice option. For fair comparison with NYUTron, we right\ntruncateallclinicalnotestoamaximumof512tokens.Allfinetuningjobsaredoneonenode\nof 8 nVidia 80GB H100s with NVLink.\nBefore each full finetuning, we run 5 hyperparameter search trials up to 100 steps using\nHydraandOptuna.Werandomsearchlearningrate(basedon[65]\u2019srecommendation)inlog\nscale over the closed interval 1e-6 to 1e-3. We used an AdamW optimizer with beta1 of 0.9,\nbeta2 of 0.999, eps of 1e-5. We used a weight decay of 0.02 and no gradient clipping. We\nused a cosine annealing scheduler with no warmup and a max steps of 5120. The best trial is\nselected based on both maximum validation AUROC and minimum validation loss.\n16\nTable 2: Additional Model Specifications\nModel Name Model Size Pretrain Data\nllama-3.2-1b1B Unnamed public mix (9T tokens)\nllama-2-7b7B Unnamed public mix (2T tokens)\nDeepSeek-R1-Distill-Llama-70B70B Unnamed public mix (2T tokens) + reasoning data (800k samples)\nFor full finetuning, we used the best learning rate from hyperparameter search, and train\nfor maximum 5120 steps with early stopping based on Micro-AUROC and a patience of 300\nsteps. The probabilities for calculating AUROC is obtained via normalizing the logits of the\nmultiplechoiceoptions(e.g.,\u201cA\u201d).WetrainallparametersexceptforDeepSeek-R1-Distill-\nLlama-70B,whichhastobefinetunedusinglow-rankadaptationfinetuning[66]tomeetthe\nmemory constraint (Appendix I).\nFor multitask finetuning, we mix examples from each task evenly within each training\nbatch. We increase the total training steps scaled by the number of tasks.\n5.5 Evaluation\nPretrainingevaluation.Wemonitoredthetoken-levelcrossentropylossandperplexityfor\nboth training and validation.\nZero-shotandfew-shotevaluation.ReMedEisbuiltonLMEvalHarness[67].Weimple-\nmentedthetasksasmultiplechoicequestionsandAUROCscoreasametric.Weimplemented\nachild classofLocalCompletionsAPIto connecton-premmodels.For modelswhoselog-\nits are not accessible (e.g., on-prem GPT-4o), we implemented custom sampling function to\napproximate (Appendix J) the probability via counting choices from 10 generations using a\ntemperature of 1.\nFinetuningevaluation.Wecollectedthelogitsofthemultiplechoiceoptions,normalizedit\nas probabilities, and calculated AUROC using sklearn\u2019s implementation (same as ReMedE\u2019s\nAUROC backend for consistency). For multiclass classification, we used One-Versus-Rest\n(OVR) AUROC.\nUncertainty.To capture the uncertainty arose from the randomness of our test set, we\ncalculated 95% confidence intervals (CI =\u00b11.96 x SD) by resampling each test set 1000\ntimes using the quantile bootstrap method from scipy.\nTemporal Shift.To better mimic deployment conditions under temporal distribution shift,\nall AUROCs are reported on test data from 2024 \u2013 drawn from a period after the pretraining\ndata \u2013 unless otherwise noted. See Appendix A for a visualization of the test timeline.\nGeneralist Models.We compared against generalist frontier models, including\nDeepSeek R1(served via vLLM [68]),DeepSeek R1 Distilled Llama 70B(vLLM),\nLlama 3.3 70B Chat(vLLM), and on-premisesGPT-4o(Azure-hosted service). We also\n17\nevaluated MedQA leaderboard models using in-memory inference, such asLlama 3.2 1B,\nLlama 2 7B, andMedMobile.\nAcknowledgements.E.K.O. is supported by the National Cancer Institute\u2019s Early Surgeon\nScientistProgram(3P30CA016087-41S1)andtheW.M.KeckFoundation.L.Y.J.issupported\nby Apple AIML PhD fellowship. L.Y.J. and A.C. are supported by NSF Award 1922658.\nK.C., E.K.O., L.Y.J. and A.C. are supported by Institute for Information & communications\nTechnology Promotion (IITP) grant funded by the Korea government (MSIT) (No. RS-2019-\nII190075ArtificialIntelligenceGraduateSchoolProgram(KAIST);No.RS-2024-00509279,\nGlobalAIFrontierLab).WewouldliketoacknowledgeJ.Golfinos,whosevisionandsupport\nmade this project possible. We would like to acknowledge Michael Costantino, Ph.D., Ali\nSiavosh-Haghighi,Ph.D.,KevinYie,M.S.,NeelimaSharma,TedumSampsonfromtheNYU\nLangone High Performance Computing (HPC) team. Without their tireless assistance in\nbuilding and maintaining our GPU cluster none of this research would have been possible.\nWe would also like to thank Dr. Dafna Bar-Sagi,Ph.D., and Nader Mherabi whose support\nfor this research has made everything possible. Thanks to He He, Ph.D., Eunsol Choi, Ph.D.,\nCarlos Fernandez-Granda, Ph.D., Julia Kempe, Ph.D., Vasant Dhar, Ph.D., Keunwoo Choi,\nPh.D., Jesse Swanson, Gavin Zihao Yang, William Merrill, Ph.D., Nicholas Lourie, Sophie\nHao,Ph.D.,VishakhPadmakumar,Ph.D.,MichaelHu,RobertJSteele,YueyingLi,Yunzhen\nFeng, Guillermo Sapiro, Ph.D., Oussama Elaqchar, Kai Xu, Varun Yerram, Itay Itzhak, Jeff\nHammerbacher for their valuable discussions.\nDeclarations\nEthicalapproval.ThisstudywasapprovedbytheInstitutionalReviewBoard(IRB)atNYU\nLangone Health (study protocol s21-01189). The methods were carried out in accordance\nwith the IRB\u2019s relevant guidelines and regulations.\nDataAvailability.Theclinicaldatausedforthepretraining,finetuning,validation,andtest\nsets were collected from the NYU Langone Health System EHR maintained by the NYULH\nDatacoreteam.Textdatawasstrippedofrichtextfeaturesanddirectlyincludedinthedataset\n\"as-is\",andwasaugmentedwithstructuredfeatureswherenoted.Itconsistsoftheproduction\nmedical records of NYU Langone and cannot be made publicly available. For the external\nvalidation task, the datasets were obtained from MIMIC III, and are publicly available from\ntheir website.\nCodeAvailability.Thisworkusesseveralopen-sourcelibrariesincludingPyTorch,LitGPT,\nTransformerslibrary,LMEvalHarness,andhydra.Ourexperimentalframeworkinvolvesthe\nutilization of these libraries and in some cases modification of them. We will release code to\nreplicate the pretraining, finetuning, and testing of the models described in this paper at the\ntime of publication. We include detailed methods and implementation steps in the Methods\nand Supplementary Information to allow for independent replication.\nAuthor\u2019s contribution.E.K.O. and K.C. supervised the project. L.Y.J and X.H. collected\npretrain,finetuneandevaluationdata(exceptNYU+InsuranceDenialandMIMIC-III).L.Y.J.\nand A.C. engineered the software for pretrain and finetune. A.C., X.H., L.Y.J. and J.Z. engi-\nneered the software for evaluation. L.Y.J., A.C., X.H., R.D., X.C.L. ran experiments. L.Y.J.,\nA.C.,X.C.L.,X.H.,J.Z.,R.D.andK.C.debuggedandtestedthesoftware.A.C.,L.Y.J.,K.C.,\n18\nX.C.L., R.S., A.A., K.L.S, Y.C., and Q.P. created figures. A.C., L.Y.J., K.C., E.K.O. concep-\ntualized the training dynamics and transfer experiments. J.S. hosted deepseek and llama 70b\ninference server. K.E. collected NYU+ Insurance Denial data. Q.P. and L.Y.J. check pretrain\ndata quality and cleaned the data. F.W. processed the MIMIC-III Mortality and LOS data.\nA.C., L.Y.J, R.D., Y.C., D.A. and J.V.L. reviewed related literature. K.C., E.K.O., Y.A. pro-\nvided guidance and feedback throughout the project. L.Y.J., A.C., X.H., R.D., A.A., D.A.,\nJ.V.L., Q.P., Y.C., R.J.S., K.C., E.K.O. wrote the initial draft. All authors edited and revised\nthe manuscript.\nConflict of interestE.K.O. reports consulting with March AI, Sofinnova Inc., Google\nInc.,incomefromMerck&Co.,andMiratiTherapeutics,andequityinArtisightInc.K.C.is\nemployedbyPrescientDesign,asubsidiaryofRoche.A.C.isemployedbyGoogleDeepmind.\nQ.P. is employed by Faction Imaging Inc. J.S. is employed by March AI. There are no other\npotential conflicts of interest. The work presented herein was performed exclusively within\nthe NYU Langone Health System.\n19\nAppendix A Data Timeline\nLang1\u2019s pretrain data covers a wider time window thanNYUTron[42].NYUTron\u2019s\npretrain data is from 2013 to May 2021.Lang1\u2019s pretrain data covers a longer span, from\n2003 to 2023. Overall,Lang1\u2019s pretrain corpus is more than 10 times the size of NYUTron.\nLang1uses the same finetuning dataset for training asNYUTron, but adds additional\ntemporal test.For bothNYUTronandLang1finetuning, we have a temporal test set to\nbetter mimic the deployment scenario, where the test set comes from the future of training\nset. ForNYUTron, the temporal test set is between June to December of 2021. ForLang1,\nthe temporal test set is in 2024. All performance we report in main is performance on 2024\ntemporal test set, unless otherwise specified.\nNYUTron Pre-training \nLang1 Pre-training \n2003 Finetune Temporal Test \n2024 \nJan 2024 Dec 2024 Finetune \nTrain/Val/Test \nMay 2021 Finetune Temporal Test  \nJun-Dec 2021 \nDec 2021 2013 \nFig.A1:IllustrationoftimelineofpretrainandfinetunedatasetforbothLang1andNYUTron.\nLang1covers a wider time window for pretraining and added additional temporal test set in\n2024 to capture temporal distribution shift.\nTemporal test is important and difficult.Figure A2 shows that bothLang1-1B(purple)\nandNYUTron(pink)performworseastestdataaresampledfromafurtherfuture,illustrating\nthe importance of evaluating with temporal test to capture deployment scenario.\nJan 2013 -\nMay 2021June 2021 -\nDec 202120240.500.550.600.650.700.750.800.850.90Readmission ROC AUCModel Performance Over Time\nLang1-1B\nNYUTron\nFig. A2: Models perform worse on temporal test and exhibit different level of degradation in\nface of temporal shift.\n20\nAppendix B ReMedE Dataset Statistics\nThe following tables show the note counts (Table B1) and patient counts (Table B2) across\neach split, and class ratio for each task: readmission (Table B3), mortality (Table B4), length\nof stay (Table B5, comorbidity imputation (Table B6, and Insurance denial (Table B7).\nTable B1: Distinct note counts for each ReMedE task across five splits.\nTask Train Val Test Temporal Test 2021 Temporal Test 2024 Total\nReadmission 362,259 45,282 45,283 53,916 97,586 604,326\nCCI 256,676 32,085 32,085 42,137 80,932 443,915\nLength of Stay 334,515 41,814 41,815 51,018 97,586 566,748\nInsurance Denial 41,842 2,325 2,325 9,299 42,046 97,837\nMortality 334,515 41,814 41,815 51,018 97,586 566,748\nTable B2: Distinct patient counts for each ReMedE task across five splits.\nTask Train Val Test Temporal Test 2021 Temporal Test 2024 Total\nReadmission 269,140 42,692 42,603 46,003 78,453 421,429\nCCI 188,298 30,085 30,098 251,804 64,873 306,741\nLength of Stay 248,486 39,304 39,331 43,358 78,453 395,991\nInsurance Denial 39,422 2,319 2,313 9,037 37,821 87,974\nMortality 248,674 39,317 39,357 43,358 78,453 395,991\nTable B3: Readmission label ratios by split (Total notes =\n604,326; total words = 607,877,177).\nSplit Not Readmitted Readmitted within 30 days\nTrain 0.891495 0.108505\nVal 0.891944 0.108056\nTest 0.893293 0.106707\nTemporal Test (2021) 0.888456 0.111544\nNew Temporal (2024) 0.887115 0.112885\n21\nTable B4: In-hospital mortality label\nratiosbysplit(Totalnotes=566,748;total\nwords = 608,603,182).\nSplit Survived Died\nTrain 0.981161 0.018839\nVal 0.981250 0.018750\nTest 0.980916 0.019084\nTemporal Test (2021) 0.980693 0.019307\nNew Temporal (2024) 0.982241 0.017759\nTable B5: Length of stay label ratios by split (Total notes =\n566,748; total words = 608,603,182).\nSplit 0\u20132 days 3 days 4\u20135 days >5 days\nTrain 0.417306 0.176575 0.165888 0.240231\nVal 0.415722 0.180562 0.164490 0.239226\nTest 0.414851 0.176611 0.167452 0.241086\nTemporal Test (2021) 0.418597 0.153201 0.164805 0.263397\nNew Temporal (2024) 0.456418 0.144529 0.159757 0.239297\nTable B6: Charlson Comorbidity Index (CCI) label ratios by split (Total\nnotes = 443,915; total words = 524,739,038).\nSplit Score 0 Score 1\u20132 Score 3\u20134 Score 5\u20137 Score >7\nTrain 0.694681 0.224840 0.054251 0.025324 0.000904\nVal 0.689169 0.229547 0.055166 0.025526 0.000592\nTest 0.693502 0.226866 0.053327 0.025370 0.000935\nTemporal Test (2021) 0.685811 0.228327 0.059164 0.025821 0.000878\nNew Temporal (2024) 0.684043 0.217355 0.067513 0.029370 0.001717\nTable B7: Insurance denial label ratios by\nsplit (Total notes = 97,837; total words =\n89,147,715).\nSplit Approved (0) Denied (1)\nTrain 0.877850 0.122150\nVal 0.867097 0.132903\nTest 0.873978 0.126022\nTemporal Test (2021) 0.879880 0.120120\nNew Temporal (2024) 0.861033 0.138967\n22\nAppendix C TransferPatternofLlama 3.2 1bv.s.Lang1 1B\n     \nT ask-specific ROC AUCs                               Finetuning Dataset(s)48% 56% 55% 54% 43%\n77% 84% 64% 77% 57%\n70% 96% 64% 80% 58%\n63% 47% 77% 65% 58%\n69% 78% 63% 90% 61%\n50% 30% 52% 51% 87%\n76% 95% 76% 90% 88%No Finetuning\n/zero-width-space /zero-width-space /zero-width-space /zero-width-space \n0.30.40.50.60.70.80.9\nReadmission\nMortality\nLOS\nCCI\nInsurance\nDenial\nReadmission Mortality LOS CCI Insurance\nDenial\n(a)Lang1-1B\u2019s transfer heatmap.\n     \nT ask-specific ROC AUCs                               Finetuning Dataset(s)56% 62% 49% 52% 38%\n72% 83% 62% 71% 71%\n69% 95% 63% 78% 61%\n56% 48% 73% 60% 53%\n68% 77% 62% 88% 69%\n52% 35% 54% 51% 85%\n67% 79% 64% 75% 76%No Finetuning\n/zero-width-space /zero-width-space /zero-width-space /zero-width-space \n0.40.50.60.70.80.9\nReadmission\nMortality\nLOS\nCCI\nInsurance\nDenial\nReadmission Mortality LOS CCI Insurance\nDenial (b)Llama 3.2 1B\u2019s transfer heatmap.\nFig. C3:Lang1-1BandLlama 3.2 1Btransfers differently.The heatmap shows the two\ndifferentbasemodels\u2019performancewhenfinetunedonasubsetofReMedEtasks(yaxis)and\nevaluatedonallfiveReMedEtasks(xaxis).Overall,Lang1-1Bhashigherper-taskandjoint\nperformance, and shows a different transfer pattern thanLlama 3.2 1B.\nLlama-3.2-1BhasoverallworseperformancethanLang1-1B.ComparedtoFigureC3a,\nFigureC3bhasworsesingle-task(diagonal)andjoint-task(lastrow)performance,suggesting\nthat the specific transfer pattern could be highly model specific.\nBoth models exhibit some similar patterns.(i) finetuning on readmission (second row)\nboostsperformanceontheotherfourtasks,and(ii)transfercouldbeasymmetric,i.e.,mortality\nhelps LOS (third row, third column) but LOS does not help mortality (fourth row, second\ncolumn), which can be explained using domain knowledge (Appendix F).\nLang1-1BandLlama-3.2-1Balso have different patterns.(i) joint finetuning (last row)\nhelpsLang1-1BbuthurtsLlama-3.2-1B,and(ii)finetuningoninsurancedenial(fourthrow)\nlowersLang1-1B\u2019s LOS performance (third column) while improving it forLlama-3.2-1B.\nThese results suggest thatinstruction finetuning enables cross-task transfer, though the\nspecific transfer patterns depend on model pretraining.\n23\nAppendix D Pretraining ablations\nFigure D4 presents pretraining ablation results evaluated on 2024 readmission temporal test\nset. For pretraining, we control for the model architecture (encoder v.s. decoder), model size\n(100Mv.s.1B),andpretraindata(NYUNotes,NYUNotes+,NYUNotes+andwebtexts).For\nfinetuning, we evaluated on three clinical predictive tasks: readmission prediction, insurance\ndenialprediction,andLOSprediction.Thesethreetaskswerechosenfortheirdistincttransfer\npattern in Figure C3a.\nTraining larger models on more recent clinical data improves temporal robustness.\nFigure D4a shows the ablation results for readmission prediction. On the left, holding the\nmodel architecture fixed as a decoder, we vary pretraining data. Compared to models trained\nonly on EHR from 2013 to 2021 (D NYUNotes), adding more recent clinical data (D NYUNotes+,\nspanning2003to2023)andfurthermixingingeneral-domainSlimPajama(D NYUNotes+,WebText )\nimproves performance for the 1B model but not the 100M model, suggesting that larger\nmodels are better able to leverage additional clinical data.This also justifies our choice of\nequallymixingEHRtextsandwebtexts,sinceaddingwebtextsdoesnotsignificantlyhurt\nthedownstreamclinicaltaskperformancewhileinstillinggeneral-purposeknowledge.Onthe\nright, holding the pretraining data fixed toD NYUNotes, varying model architecture shows that\nscalingto1Bhelps.Weobservesimilarpatternsforinsurancedenialprediction(FigureD4b)\nand LOS prediction (Figure D4c).\nDNYUNotes DNYUNotes+DNYUNotes+,WebText\nPretraining Dataset0.700.720.740.760.780.80Readmission ROC AUCLang1-100m\nLang1-1B\nNYUTron-100m\n(encoder)Lang1-100m\n(decoder)Lang1-1B\n(decoder)\nModel Architecture0.700.720.740.760.780.80Readmission ROC AUC\n(a) Readmission prediction\nDNYUNotes DNYUNotes+DNYUNotes+,WebText\nPretraining Dataset0.760.780.800.820.840.860.880.90Insurance Denial ROC AUCLang1-100m\nLang1-1B\nNYUTron-100m\n(encoder)Lang1-100m\n(decoder)Lang1-1B\n(decoder)\nModel Architecture0.760.780.800.820.840.860.880.90Insurance Denial ROC AUC\n(b) Insurance denial prediction\nDNYUNotes DNYUNotes+DNYUNotes+,WebText\nPretraining Dataset0.700.720.740.760.780.80LOS (Length of Stay) ROC AUCLang1-100m\nLang1-1B\nNYUTron-100m\n(encoder)Lang1-100m\n(decoder)Lang1-1B\n(decoder)\nModel Architecture0.700.720.740.760.780.80LOS (Length of Stay) ROC AUC (c) LOS prediction\nFig.D4:Pretrainingablations.Errorbarsindicatingthe95%confidenceinterval.(a)Larger\nmodels trained on more clinical data has better performance, and mixing web texts does not\nhurt much. (b,c) Similar patterns are observed for insurance denial and LOS prediction.\n24\nAppendix E Calibration Plot\nCalibration curves are calculated using sklearn package [69] with n=15 bins. Expected\ncalibration error (ECE) is calculated with n=15 bins using the torchmetrics library [70].\n(a) Calibration plots for single-task models\n(b) Calibration plots for joint model\nFig. E5: Calibration plot shows that both single task finetuned Lang1 and joint finetuned\nLang1 are well calibrated.\n25\nAppendix F Asymmetry of Transfer between Mortality and\nLOS\nOurmedicalcollaboratorsprovidedanexplanationfortheasymmetrictransferbetweenMor-\ntality and LOS (Length of Stay) we observe for bothLang1-1BandLlama-3.2-1B. If a\npatientdied,theyeitherstayedforashorttime(verysickanddiedimmediately)oralongtime\n(doctorsfailedtosavethemafteralongtime).Ontheotherhand,ifapatientstayedforalong\ntime,theyeithersurvived,ortheydiedafterdoctors\u2019attempts.Thisasymmetryinconditional\nprobability could help explain why the mortality transfer to LOS, but not vice versa.\nThis explanation is corroborated by analysis of the conditional probabilities. Figure F6a\nshows that patient who died are ore likely to stay for 0-2 days or >5 days compared to patient\nwho survived.F6b showsthat patientwho stay fora longtime havesimilar mortalityrisks as\npatient who stayed for 0 days.\nDied Survived\nIn-Hospital Mortality0.00.10.20.30.4Proportion of T otalDistribution of Length of Stay Given Mortality\nLOS bucket\n0-2\n3\n4-5\n>5\n(a) Probabilities of LOS given in-hospital mortal-\nity\n0 10 20 30 40 50 60\nLength of Stay0.000.050.100.150.200.25Probability of In-Hospital MortalityConditional Probability of In-Hospital Mortality Given Length of Stay(b) Probabilities of in-hospital mortality given\nLOS\nFig.F6:AnalysisofconditionalprobabilitiesofLOS(lengthofstay)andin-hospitalmortality\ncould help explain the assymetry of transfer.\n26\nAppendix G Detailed statistics of NYU Notes+\nWe analyzed the top five clinical departments, diagnosis and borough for pathology notes,\nradiology notes and hospital notes. See Figure G7.\n(a)PathologyNotes.Amongpathologynoteswithspecifieddepartments,OB/GYN\n(obstetrics and gynecology) has the highest percentage (9.8%). The most common\nspecifieddiagnosisisgynecologicalexams(6%).Nearlyhalf(48.6%)ofthepathol-\nogy notes are from Manhattan borough.\n(b)Radiology Notes.Most of the radiology notes are from diagnostic radiology\n(82.2%),withscreeningmammogramsbeingthemostcommonspecifieddiagnosis\n(5%).ThetwomostcommonboroughsareManhattan(41%)andQueens(32.6%).\n(c)HospitalNotes.Amongnoteswithspecifieddepartments,mostarefrominternal\nmedicinedepartment(10.1%),withcommondiagnosesincludingsepsis(1.1%)and\nhypertension (1%). The majority of notes are from Brooklyn (27.9%) and Queens\n(24.7%) borough.\nFig. G7: Top five department, diagnosis and borough of NYU Notes+\n27\nAppendix H Prompts of ReMedE Tasks\nWeconstructedpromptstocreatetask-specificquestionsandansweroptionsfromthelabeled\nfinetuning notes:\n\u2022ReadmissionQuestion: Given the above discharge note of the patient, will the patient be\nreadmitted to the hospital within 30 days of discharge?\\n A. no\\n B. yes\\n Answer:\n\u2022In-Hospital MortalityQuestion: Given the above admission note of the patient, will the\npatient die during the hospital admission?\\n A. no\\n B. yes\\n Answer:\n\u2022Charlson Comorbidity IndexQuestion: Given the above admission note of the patient,\nwhat\u2019s the Charlson Comorbidity Index of the patient?\\n A. score 0\\n B. score 1 to 2\\n\nC. score 3 to 4\\n D. score 4 to 7\\n E. score more than 7\\n Answer:\n\u2022InsuranceDenialQuestion:Giventheabovedischargenoteofthepatient,willtheinsurance\nclaim of the patient be denied?\\n A. no\\n B. yes\\n Answer:\n\u2022LengthofStayQuestion:Giventheaboveadmissionnoteofthepatient,howlongwillthe\npatient stay at the hospital?\\n A. 0 to 2 days\\n B. 3 days\\n C. 4 to 5 days\\n D. more\nthan 5 days\\n Answer:\nAppendix I LoRa Finetuning for Llama-3-70b\nToefficientlytrainDeepseek-R1-distill-Llama-3-70bon1nodeof8H100s,weusedLow-Rank\nApproximation(LoRA)finetuning.LoRAreducestrainableparametersbyinsertingtrainable\nrank decomposition matrices into transformer layers while freezing the pretrained weights.\nInourconfiguration,weenabledLoRAadaptersonthequeryandvalueprojectionsofthe\nattention mechanism, with rankr= 8. We set the LoRA scaling factor\u03b1= 16and applied\na dropout rate of 0.05. Other components, such as the key, MLP, and projection layers, were\nleft frozen.\nAppendix J Token probability approximation for models\nwithout logprobs\nUnlike other generalist models, GPT-4o does not provide logprobs to prevent privacy attack.\nHowever, we need probabilities to reliably evaluate classification tasks. To approximate its\nprobabilities,wesample10generationsfromGPTusingtemperatureof1(tonotreweightthe\nLM\u2019s distribution), and count the number of occurrences for each multiple choice options.\nThen we normalize the counts to be probabilities for each option. For cost reasons, we limit\nthe number of examples to be 1000, except for CCI (comorbidity imputation).\nCCIisaspecialcasebecauseitslabeldistributionisveryskewed,sowegreedilyevaluate\non 10,000 samples instead. We binarize the greedy choice to be probability of 0 or 1.\n28\nAppendix K Stratified Evaluation\nWe performed stratified evaluation on readmission to evaluation the performance variation\nacross different groups (age, first race, borough, ethnicity, sex, and whether the patients are\nchildren).Somegroupsareomittedbecausetheyhaveonlyoneclassduetosmallsamplesize.If\nwelookatmeansonly,forage(FigureK8a),patientsbetween10to15hasthebestperformance,\nandpatientsbetween85and90hastheworstperformance.Forrace,MiddleEasternorNorth\nAfricanhasthebestperformance,andAsianIndianhastheworstperformance.Forborough,\nBrooklyn has the best performance and Queens has the worst performance. For ethnicity,\nSpanishHispanicOriginisworst.Forsex,femalehasbetterperformancethanmale.Children\u2019s\nperformance is better than adult and they also have a lower readmission rate.\n(a) Age.\n (b) First Race.\n (c) Borough.\n(d) Ethnicity.\n (e) Sex.\n (f) IsChild.\nFig. K8: Stratified performance analysis on 2024 readmission temporal test set.\n29\nAppendix L Control for Patient Overlap\nWechosetoconstructthe2024temporaltestsetwithoutexplicitpatientsplit,becausepatients\ndo come back to the health system at deployment. While this choice is supported by our\nclinicalcollaborators,wedidanablationwherethetestsetexcludesseenpatients.Keepingthe\nfinetunedmodelfixedandvaryingthetestdatatoeitherincludeorexcludeseenpatients,wesee\nsimilartestperformanceonreadmissionprediction,inhospitalmortalityprediction,and\nlengthofstayprediction.FigureL9ashowsthatonreadmissionprediction,removingpatients\nseen from pretraining and finetuning slightly increases the performance from 76.5% (purple\nbar)to77%(greybar).Similarly,theperformanceincreaseis0.39%formortality(FigureL9b)\nand 0.70% for LOS (Figure L9c). While surprising, the slight increase could be attributed\ntorepeated patients being both older and more likely to be a minority. For readmission\nprediction, repeated patients are on average 13 years older than non repeated patients (55\nv.s. 42 years). For mortality and LOS prediction, the age gap widens to 16 years (57 v.s. 41\nyears). In addition, repeated patients include a larger proportion of non-white patients (41%\nv.s.36%).Thesefindingsshowthatourchoiceoftemporalsplitdoesnotover-estimatemodel\nperformance.\n0.7600.7650.7700.7750.7800.7852024 NYU Readmission ROC AUCControl for Patient Split\nFinetune Split\nTime Split Only (Reported)\nTime Split (Remove Finetune + Pretrain Patient)\n(a) Readmission\n0.9500.9550.9600.9650.9700.9750.9802024 NYU Mortality ROC AUCControl for Patient Split\nFinetune Split\nTime Split Only (Reported)\nTime Split (Remove Finetune Patient)\n(b) Mortality\n0.750.760.770.780.790.802024 NYU LOS ROC AUCControl for Patient Split\nFinetune Split\nTime Split Only (Reported)\nTime Split (Remove Finetune Patient) (c) LOS\nFig. L9:Lang1-1B\u2019s finetuned performance on readmission, mortality and LOS 2024 test\nset, with or without patient split.\n30\nAppendix M External Validation: MIMIC v.s. NYU\nTo check how wellLang1generalizes to a different health system, we compare finetuning\nLang1-1BandLlama-3.2-1Bon three classification tasks (readmission, mortality, length of\nstay)fromMIMICIIIandNYU,andtestonMIMICIII.MIMICIII[59]isadatabaseofelec-\ntronic health records from ICU patients at the Beth Israel Hospital in Boston Massachusetts,\nwhereasNYU+datasetsarecollectedfromNewYorkCity.See5.2.3fordetailsaboutMIMIC\ndatasetsandMethods5.2.2forNYUdatasets.FigureM10showstheheatmapsofMIMICIII\ntest performance for finetuningLang1-1BandLlama-3.2-1Bon NYU or MIMIC-III, with\nzero-shotperformanceasbaselines.Thexaxisisthepretrainedmodel(purpleLang1-1Band\nblueLlama-3.2-1B).Theyaxisisthefinetuningsetting(purpleindicatesfinetuningonNYU\ndata; green indicates finetuning on MIMIC III data; and black indicates zero-shot). Darker\nyellow cells indicate higher test AUROC on MIMIC III.\nLang1-1B Llama-3.2-1B\nPretrained modelzero-shotNYU\nReadmissionMIMIC\nReadmissionFinetune setting51.6%\n\u00b11.5%45.5%\n\u00b11.5%\n66.4%\n\u00b11.4%60.8%\n\u00b11.4%\n67.7%\n\u00b11.4%58.3%\n\u00b11.5%External Validation of Finetuning Lang1\nfor MIMIC III Readmission\n0.500.550.600.650.700.750.80\nROC AUC on MIMIC III Readmission\n(a) Readmission prediction.\nLang1-1B Llama-3.2-1B\nPretrained modelzero-shotNYU\nMortalityMIMIC\nMortalityFinetune setting44.3%\n\u00b13.8%42.7%\n\u00b13.9%\n83.7%\n\u00b12.8%83.1%\n\u00b12.7%\n85.5%\n\u00b12.9%77.3%\n\u00b13.1%External Validation of Finetuning Lang1\nfor MIMIC III Mortality\n0.700.720.740.760.780.800.820.84\nROC AUC on MIMIC III Mortality\n(b) In-hospital Mortality.\nLang1-1B Llama-3.2-1B\nPretrained modelzero-shotNYU\nLOSMIMIC\nLOSFinetune setting50.1%\n\u00b12.1%54.8%\n\u00b12.0%\n65.2%\n\u00b12.1%61.6%\n\u00b12.1%\n64.8%\n\u00b12.1%55.2%\n\u00b12.1%External Validation of Finetuning Lang1\nfor MIMIC III LOS\n0.500.520.540.560.580.600.620.64\nROC AUC on MIMIC III LOS\n (c) LOS.\nFig. M10: Finetuning on NYU transfers well to MIMIC III for readmission, mortality and\nLOS prediction. Overall performance is better on finetuningLang1-1Bcompared toLlama\n3.2 1B.\n31\nFinetuningLang1-1BonNYUReadmissiontransferstoMIMIC-IIIReadmission.The\ndifference in mean AUROC between finetuningLang1-1Bon MIMIC III versus NYU range\nfrom 0.4%-1.8% AUROC and are roughly within standard error, showing that finetuning\nLang1-1Bat NYU transfers well to MIMIC.\nLang1-1Bachieves better performance thanLlama-3.2-1B.The difference in mean\nAUROCbetweenfinetuningLang1-1BandLlama-3.2-1Bonthesamedatarangefrom0.6%-\n9.6%, showing that the clinically pretrainedLang1-1Bis a preferred base model for these\nthree clinical predictive tasks.\nFinetuningonNYU,whichisslightlyout-of-distribution,couldoutperformfinetuningon\nMIMIC forLlama-3.2-1B.It is reasonable to expect that in-distribution finetuning leads\ntobestperformance.WhilethisistrueforLang1-1B(purple),itisnotthecaseforLlama-3.2-\n1B(blue), which sees 2.5% - 6.4% AUROC increase from finetuning on NYU compared to\nfinetuning on MIMIC. We hypothesize that this is because NYU data has more labeled pairs,\nand thatnon clinically pretrained models would benefit more from a larger number of\nslightly out-of-distribution pairs. We test this hypothesis on readmission prediction, where\nNYUReadmission(36,2259examples)is8.6timesthesizeofMIMICIIIreadmission(42,180\nexamples). Figure M11a shows that test performance on MIMIC III is similar whenLlama-\n3.2-1BisfinetunedonNYUReadmissionthatisdownsampledtobethesamesizeasMIMIC\nIII(42,180).Thispatterndoesnotholdwhenthepretrainedmodelischangedtotheclinically\npretrainedLang1-1B: Figure M11b shows that finetuning on MIMIC-III readmission yields\nthe best test performance on MIMIC-III, despite fewer number of finetuning examples.\n42180 362259\nNumber of Finetuning Examples for Llama-3.2-1B0.5000.5250.5500.5750.6000.6250.6500.6750.700MIMIC III Readmission ROC AUCFinetune Data\nNYU\nMIMIC\n(a) Finetuning on full NYU dataset leads\nto best performance onLlama-3.2-1B, even\nthough NYU Readmission is not directly in-\ndistribution for MIMIC Readmission.\n42180 362259\nNumber of Finetuning Examples for Lang1-1B0.5000.5250.5500.5750.6000.6250.6500.6750.700MIMIC III Readmission ROC AUCFinetune Data\nNYU\nMIMIC(b) Finetuning on MIMIC III leads to best\nperformance forLang1-1B, even though\nNYU Readmission has more labeled pairs.\nFig. M11: Clinical models such asLang1-1Bbenefit more from in-domain data, whereas\ngeneralist models such as Llama-3.2-1B could benefit from more slightly OOD examples\ncompared to fewer in-distribution examples.\n32\nReferences\n[1] Sinsky, C., Colligan, L., Li, L., Prgomet, M., Reynolds, S., Goeders, L., Westbrook, J.,\nTutty, M., Blike, G.: Allocation of physician time in ambulatory practice: A time and\nmotion study in 4 specialties. Ann. Intern. Med.165(11), 753\u2013760 (2016)\n[2] Hingle, S.: Electronic health records: An unfulfilled promise and a call to action. Ann.\nIntern. Med.165(11), 818\u2013819 (2016)\n[3] Murphy,D.R.,Meyer,A.N.D.,Russo,E.,Sittig,D.F.,Wei,L.,Singh,H.:Theburdenof\ninboxnotificationsincommercialelectronichealthrecords.JAMAIntern.Med.176(4),\n559\u2013560 (2016)\n[4] Yao,S.,Zhao,J.,Yu,D.,Du,N.,Shafran,I.,Narasimhan,K.,Cao,Y.:ReAct:Synergizing\nreasoning and acting in language models. arXiv [cs.CL] (2022) [cs.CL]\n[5] Hoffmann,J.,Borgeaud,S.,Mensch,A.,Buchatskaya,E.,Cai,T.,Rutherford,E.,Casas,\nD.,Hendricks,L.A.,Welbl,J.,Clark,A.,Hennigan,T.,Noland,E.,Millican,K.,Driess-\nche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Vinyals, O., Rae,\nJ.W.,Sifre,L.:Anempiricalanalysisofcompute-optimallargelanguagemodeltraining.\nIn:Oh,A.H.,Agarwal,A.,Belgrave,D.,Cho,K.(eds.)AdvancesinNeuralInformation\nProcessing Systems (2022).https://openreview.net/forum?id=iBBcRUlOAPR\n[6] DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma,\nS.,Wang,P.,Bi,X.,Zhang,X.,Yu,X.,Wu,Y.,Wu,Z.F.,Gou,Z.,Shao,Z.,Li,Z.,Gao,\nZ.,Liu,A.,Xue,B.,Wang,B.,Wu,B.,Feng,B.,Lu,C.,Zhao,C.,Deng,C.,Zhang,C.,\nRuan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G.,\nLi, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li,\nH., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J.L., Ni, J., Liang,\nJ., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang,\nL., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M.,\nLi, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q.,\nGe, R., Zhang, R., Pan, R., Wang, R., Chen, R.J., Jin, R.L., Chen, R., Lu, S., Zhou, S.,\nChen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S.S., Zhou, S., Wu, S., Ye, S.,\nYun,T.,Pei,T.,Sun,T.,Wang,T.,Zeng,W.,Zhao,W.,Liu,W.,Liang,W.,Gao,W.,Yu,\nW., Zhang, W., Xiao, W.L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X.,\nLiu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X.Q., Jin, X., Shen, X.,\nChen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y.K., Wang,\nY.Q., Wei, Y.X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang,\nY., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou,\nY., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y.,\nZhu,Y.X.,Xu,Y.,Huang,Y.,Li,Y.,Zheng,Y.,Zhu,Y.,Ma,Y.,Tang,Y.,Zha,Y.,Yan,\nY., Ren, Z.Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan,\nZ.,Wu,Z.,Gu,Z.,Zhu,Z.,Liu,Z.,Li,Z.,Xie,Z.,Song,Z.,Pan,Z.,Huang,Z.,Xu,Z.,\nZhang, Z., Zhang, Z.: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\nReinforcement Learning (2025). http://arxiv.org/abs/2501.12948\n33\n[7] Trinh, T.H., Wu, Y., Le, Q.V., He, H., Luong, T.: Solving olympiad geometry without\nhuman demonstrations. Nature625(7995), 476\u2013482 (2024)\n[8] Gottweis, J., Weng, W.-H., Daryin, A., Tu, T., Palepu, A., Sirkovic, P., Myaskovsky,\nA., Weissenberger, F., Rong, K., Tanno, R., Saab, K., Popovici, D., Blum, J., Zhang,\nF., Chou, K., Hassidim, A., Gokturk, B., Vahdat, A., Kohli, P., Matias, Y., Carroll, A.,\nKulkarni,K.,Tomasev,N.,Guan,Y.,Dhillon,V.,Vaishnav,E.D.,Lee,B.,Costa,T.R.D.,\nPenad\u00c3\u013es, J.R., Peltz, G., Xu, Y., Pawlosky, A., Karthikesalingam, A., Natarajan, V.:\nTowards an AI co-scientist (2025). http://arxiv.org/abs/2502.18864\n[9] Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur, P.,\nRosenberg,D.,Mann,G.:BloombergGPT:ALargeLanguageModelforFinance(2023).\nhttp://arxiv.org/abs/2303.17564\n[10] Chen, J., Li, D., Chen, Q., Zhou, W., Liu, X.: Diaformer: Automatic diagnosis via\nsymptomssequencegeneration.Proc.Conf.AAAIArtif.Intell.36(4),4432\u20134440(2022)\n[11] Chen, W., Zhong, C., Peng, J., Wei, Z.: DxFormer: a decoupled automatic diagnostic\nsystem based on decoder-encoder transformer with dense symptom representations.\nBioinformatics39(1), 744 (2023)\n[12] Panagoulias, D.P., Palamidas, F.A., Virvou, M., Tsihrintzis, G.A.: Evaluating the\npotential of LLMs and ChatGPT on medical diagnosis and treatment. In: 2023 14th\nInternational Conference on Information, Intelligence, Systems & Applications (IISA),\npp. 1\u20139. IEEE, ??? (2023)\n[13] Amin, K., Khosla, P., Doshi, R., Chheang, S., Forman, H.P.: Artificial intelligence to\nimprove patient understanding of radiology reports. Yale J. Biol. Med.96(3), 407\u2013417\n(2023)\n[14] Ellershaw, S., Tomlinson, C., Burton, O.E., Frost, T., Hanrahan, J.G., Khan, D.Z.,\nHorsfall, H.L., Little, M., Malgapo, E., Starup-Hansen, J., Ross, J., Noor, K., Vella-\nBaldacchino, M., Shah, A.D., Dobson, R.: Automated generation of hospital discharge\nsummariesusingclinicalguidelinesandlargelanguagemodels.In:AAAI2024SSSon\nClinical FMs, pp. 1\u20138. Stanford University, Stanford, California, USA (2024)\n[15] Zhou,S.,Xu,Z.,Zhang,M.,Xu,C.,Guo,Y.,Zhan,Z.,Ding,S.,Wang,J.,Xu,K.,Fang,\nY.,Xia,L.,Yeung,J.,Zha,D.,Melton,G.B.,Lin,M.,Zhang,R.:Largelanguagemodels\nfor disease diagnosis: A scoping review. arXiv [cs.CL] (2024) [cs.CL]\n[16] Zhang, L., Liu, M., Wang, L., Zhang, Y., Xu, X., Pan, Z., Feng, Y., Zhao, J., Zhang, L.,\nYao,G.,Chen,X.,Xie,X.:Constructingalargelanguagemodeltogenerateimpressions\nfrom findings in radiology reports. Radiology312(3), 240885 (2024)\n[17] He, Z., Bhasuran, B., Jin, Q., Tian, S., Hanna, K., Shavor, C., Arguello, L.G., Murray,\nP., Lu, Z.: Quality of answers of generative large language models versus peer users for\ninterpreting laboratory test results for lay patients: Evaluation study. J. Med. Internet\n34\nRes.26(1), 56655 (2024)\n[18] Kweon, S., Kim, J., Kwak, H., Cha, D., Yoon, H., Kim, K., Yang, J., Won, S., Choi,\nE.: EHRNoteQA: An LLM benchmark for real-world clinical practice using discharge\nsummaries. Neural Inf Process Syst37, 124575\u2013124611 (2024)\n[19] Glicksberg, B.S., Timsina, P., Patel, D., Sawant, A., Vaid, A., Raut, G., Charney, A.W.,\nApakama,D.,Carr,B.G.,Freeman,R.,Nadkarni,G.N.,Klang,E.:Evaluatingtheaccu-\nracy of a state-of-the-art large language model for prediction of admissions from the\nemergency room. J. Am. Med. Inform. Assoc.31(9), 1921\u20131928 (2024)\n[20] Nazyrova, N., Chahed, S., Chausalet, T., Dwek, M.: Leveraging large language models\nfor medical text classification: a hospital readmission prediction case. In: 2024 14th\nInternational Conference on Pattern Recognition Systems (ICPRS), pp. 1\u20137. IEEE, ???\n(2024)\n[21] Ben Shoham, O., Rappoport, N.: Cpllm: Clinical prediction with large language mod-\nels. PLOS Digital Health3(12), 0000680 (2024) https://doi.org/10.1371/journal.pdig.\n0000680 . Published December 6, 2024\n[22] Scarlat, A., Campion, F.X.: Predicting 30-day mortality and readmission using hos-\npital discharge summaries: A comparative analysis of machine learning models, large\nlanguage models, and physicians. medRxiv, 2025\u2013032625324714 (2025)\n[23] Bhasuran, B., Jin, Q., Xie, Y., Yang, C., Hanna, K., Costa, J., Shavor, C., Han, W., Lu,\nZ., He, Z.: Preliminary analysis of the impact of lab results on large language model\ngenerated differential diagnoses. NPJ Digit. Med.8(1), 166 (2025)\n[24] McDuff, D., Schaekermann, M., Tu, T., Palepu, A., Wang, A., Garrison, J., Singhal, K.,\nSharma,Y.,Azizi,S.,Kulkarni,K.,Hou,L.,Cheng,Y.,Liu,Y.,Mahdavi,S.S.,Prakash,\nS.,Pathak,A.,Semturs,C.,Patel,S.,Webster,D.R.,Dominowska,E.,Gottweis,J.,Barral,\nJ.,Chou,K.,Corrado,G.S.,Matias,Y.,Sunshine,J.,Karthikesalingam,A.,Natarajan,V.:\nTowardsaccuratedifferentialdiagnosiswithlargelanguagemodels.Nature,1\u20137(2025)\n[25] Alyakin,A.,Stryker,J.,Alber,D.A.,Sangwon,K.L.,Lee,J.V.,Duderstadt,B.,Save,A.,\nKurland, D., Frome, S., Singh, S., Zhang, J., Yang, E., Park, K.Y., Orillac, C., Valliani,\nA.A.,Neifert,S.,Liu,A.,Patel,A.,Livia,C.,Lau,D.,Laufer,I.,Rozman,P.A.,Hidalgo,\nE.T., Riina, H., Feng, R., Hollon, T., Aphinyanaphongs, Y., Golfinos, J.G., Snyder, L.,\nLeuthardt,E.,Kondziolka,D.,Oermann,E.K.:CNS-Obsidian:ANeurosurgicalVision-\nLanguage Model Built From Scientific Publications (2025). https://arxiv.org/abs/2502.\n19546\n[26] Nori, H., Lee, Y.T., Zhang, S., Carignan, D., Edgar, R., Fusi, N., King, N., Larson, J.,\nLi, Y., Liu, W., Luo, R., McKinney, S.M., Ness, R.O., Poon, H., Qin, T., Usuyama, N.,\nWhite, C., Horvitz, E.: Can generalist foundation models outcompete special-purpose\ntuning? case study in medicine. ArXivabs/2311.16452(2023)\n35\n[27] Zhou,H.-Y.,Adithan,S.,Acosta,J.N.,Topol,E.J.,Rajpurkar,P.:Ageneralistlearnerfor\nmultifaceted medical image interpretation. arXiv [cs.CV] (2024) [cs.CV]\n[28] Lehman, E., Hernandez, E., Mahajan, D., Wulff, J., Smith, M.J., Ziegler, Z., Nadler,\nD., Szolovits, P., Johnson, A., Alsentzer, E.: Do we still need clinical language models?\narXiv [cs.CL] (2023) [cs.CL]\n[29] Johri,S.,Jeong,J.,Tran,B.A.,Schlessinger,D.I.,Wongvibulsin,S.,Barnes,L.A.,Zhou,\nH.-Y.,Cai,Z.R.,VanAllen,E.M.,Kim,D.,Daneshjou,R.,Rajpurkar,P.:Anevaluation\nframework for clinical use of large language models in patient interaction tasks. Nature\nmedicine31(1), 77\u201386 (2025)\n[30] Jiang,Y.,Black,K.C.,Geng,G.,Park,D.,Zou,J.,Ng,A.Y.,Chen,J.H.:MedAgentBench:\nArealisticvirtualEHRenvironmenttobenchmarkmedicalLLMagents.arXiv[cs.LG]\n(2025) [cs.LG]\n[31] Bean,A.M.,Payne,R.,Parsons,G.,Kirk,H.R.,Ciro,J.,Mosquera,R.,Monsalve,S.H.,\nEkanayaka,A.S.,Tarassenko,L.,Rocher,L.,Mahdi,A.:Clinicalknowledgeinllmsdoes\nnot translate to human interactions. ArXivabs/2504.18919(2025)\n[32] Vishwanath, K., Alyakin, A., Alber, D.A., Lee, J.V., Kondziolka, D., Oermann, E.K.:\nMedical large language models are easily distracted (2025). https://arxiv.org/abs/2504.\n01201\n[33] Yan,C.,Fu,X.,Xiong,Y.,Wang,T.,Hui,S.C.,Wu,J.,Liu,X.:Llmsensitivityevaluation\nframework for clinical diagnosis. ArXivabs/2504.13475(2025)\n[34] Hager, P., Jungmann, F., Holland, R., Bhagat, K., Hubrecht, I., Knauer, M., Vielhauer,\nJ., Makowski, M., Braren, R., Kaissis, G., Rueckert, D.: Evaluation and mitigation of\nthe limitations of large language models in clinical decision-making. Nat. Med.30(9),\n2613\u20132622 (2024)\n[35] Arora,R.K.,Wei,J.,Hicks,R.S.,Bowman,P.,Qui\u00c3\u015bonero-Candela,J.,Tsimpourlas,F.,\nSharman,M.,Shah,M.,Vallone,A.,Beutel,A.,Heidecke,J.,Singhal,K.:HealthBench:\nEvaluatinglargelanguagemodelstowardsimprovedhumanhealth.arXiv[cs.CL](2025)\n[cs.CL]\n[36] Wornow,M.,Xu,Y.,Thapa,R.,Patel,B.,Steinberg,E.,Fleming,S.,Pfeffer,M.A.,Fries,\nJ., Shah, N.H.: The shaky foundations of large language models and foundation models\nfor electronic health records. NPJ Digit. Med.6(1), 135 (2023)\n[37] Johnson,A.E.W.,Bulgarelli,L.,Shen,L.,Gayles,A.,Shammout,A.,Horng,S.,Pollard,\nT.J., Hao, S., Moody, B., Gow, B., Lehman, L.-w.H., Celi, L.A., Mark, R.G.: Mimic-\niv, a freely accessible electronic health record dataset. Scientific Data10(1) (2023)\nhttps://doi.org/10.1038/s41597-022-01899-x\n[38] pubmed.gov: Download PubMed Data. NCBI Literature Resources\n36\n[39] Sushil, M., Ludwig, D., Butte, A.J., Rudrapatna, V.A.: Developing a general-purpose\nclinical language inference model from a large corpus of clinical notes. arXiv [cs.CL]\n(2022) [cs.CL]\n[40] Yang, X., Chen, A., PourNejatian, N., Shin, H.C., Smith, K.E., Parisien, C., Compas,\nC.,Martin,C.,Costa,A.B.,Flores,M.G.,Zhang,Y.,Magoc,T.,Harle,C.A.,Lipori,G.,\nMitchell,D.A.,Hogan,W.R.,Shenkman,E.A.,Bian,J.,Wu,Y.:Alargelanguagemodel\nfor electronic health records. NPJ Digit. Med.5(1), 194 (2022)\n[41] Peng, C., Yang, X., Chen, A., Smith, K.E., PourNejatian, N., Costa, A.B., Martin, C.,\nFlores, M.G., Zhang, Y., Magoc, T., Lipori, G., Mitchell, D.A., Ospina, N.S., Ahmed,\nM.M., Hogan, W.R., Shenkman, E.A., Guo, Y., Bian, J., Wu, Y.: A study of generative\nlarge language model for medical research and healthcare. NPJ Digit. Med.6(1), 210\n(2023)\n[42] Jiang,L.Y.,Liu,X.C.,Nejatian,N.P.,Nasir-Moin,M.,Wang,D.,Abidin,A.,Eaton,K.,\nRiina, H.A., Laufer, I., Punjabi, P., Miceli, M., Kim, N.C., Orillac, C., Schnurman, Z.,\nLivia,C.,Weiss,H.,Kurland,D.,Neifert,S.,Dastagirzada,Y.,Kondziolka,D.,Cheung,\nA.T.M., Yang, G., Cao, M., Flores, M., Costa, A.B., Aphinyanaphongs, Y., Cho, K.,\nOermann,E.K.:Healthsystem-scalelanguagemodelsareall-purposepredictionengines.\nNature (2023)\n[43] Goh, E., Gallo, R., Hom, J., Strong, E., Weng, Y., Kerman, H., Cool, J.A., Kanjee, Z.,\nParsons, A.S., Ahuja, N., Horvitz, E., Yang, D., Milstein, A., Olson, A.P.J., Rodman,\nA.,Chen,J.H.:Largelanguagemodelinfluenceondiagnosticreasoning:Arandomized\nclinical trial: A randomized clinical trial. JAMA Netw. Open7(10), 2440969 (2024)\n[44] Nori, H., Daswani, M., Kelly, C., Lundberg, S., Ribeiro, M.T., Wilson, M., Liu, X.,\nSounderajah,V., Carlson, J.,Lungren, M.P.,Gross, B.,Hames, P.,Suleyman, M.,King,\nD., Horvitz, E.: Sequential diagnosis with language models. arXiv [cs.CL] (2025)\n[cs.CL]\n[45] Tu, T., Schaekermann, M., Palepu, A., Saab, K., Freyberg, J., Tanno, R., Wang, A.,\nLi, B., Amin, M., Cheng, Y., Vedadi, E., Tomasev, N., Azizi, S., Singhal, K., Hou, L.,\nWebson,A.,Kulkarni,K.,Mahdavi,S.S.,Semturs,C.,Gottweis,J.,Barral,J.,Chou,K.,\nCorrado, G.S., Matias, Y., Karthikesalingam, A., Natarajan, V.: Towards conversational\ndiagnostic artificial intelligence. Nature, 1\u20139 (2025)\n[46] Luccioni,A.S.,Jernite,Y.,Strubell,E.:Powerhungryprocessing:Wattsdrivingthecost\nof AI deployment? arXiv [cs.LG] (2023) [cs.LG]\n[47] Cottier, B., Rahman, R., Fattorini, L., Maslej, N., Besiroglu, T., Owen, D.: The rising\ncosts of training frontier AI models. arXiv [cs.CY] (2024) [cs.CY]\n[48] Feng, S., Ding, W., Liu, A., Wang, Z., Shi, W., Wang, Y., Shen, Z., Han, X., Lang,\nH., Lee, C.-Y., Pfister, T., Choi, Y., Tsvetkov, Y.: When one LLM drools, multi-LLM\ncollaboration rules. arXiv [cs.CL] (2025) [cs.CL]\n37\n[49] Belcak, P., Heinrich, G., Diao, S., Fu, Y., Dong, X., Muralidharan, S., Lin, Y.C.,\nMolchanov,P.:SmalllanguagemodelsarethefutureofagenticAI.arXiv[cs.AI](2025)\n[cs.AI]\n[50] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin, P., Zhang, C.,\nAgarwal,S.,Slama,K.,Ray,A.,Schulman,J.,Hilton,J.,Kelton,F.,Miller,L.,Simens,\nM.,Askell,A.,Welinder,P.,Christiano,P.,Leike,J.,Lowe,R.:Traininglanguagemodels\nto follow instructions with human feedback. arXiv [cs.CL] (2022) [cs.CL]\n[51] Rocklin, M.: Dask: Parallel computation with blocked algorithms and task scheduling.\nIn: Proceedings of the Python in Science Conference, pp. 126\u2013132. SciPy, ??? (2015)\n[52] Soboleva,D.,Al-Khateeb,F.,Myers,R.,Steeves,J.R.,Hestness,J.,Dey,N.:SlimPajama:\nA 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.\nnet/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama\n(2023). https://huggingface.co/datasets/cerebras/SlimPajama-627B\n[53] Common Crawl Foundation: Common Crawl Dataset. https://commoncrawl.org.\nAccessed: 2025-11-14 (2024)\n[54] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li,\nW.,Liu, P.J.:C4: ColossalCleanCrawled Corpus.https://www.tensorflow.org/datasets/\ncatalog/c4. Accessed: YYYY-MM-DD (2020)\n[55] Wenzek, G., Lacroix, T., Lavergne, T., et al.: BookCorpus2. https://github.com/\nfacebookresearch/cc_net. Included in The Pile and SlimPajama datasets. (2020)\n[56] Rae, J.W., Potapenko, A., Jayakumar, S.M., Lillicrap, T.: Compressing large-scale lan-\nguagemodels.In:InternationalConferenceonMachineLearning(ICML)(2020).PG-19\nlong-book subset from Project Gutenberg.\n[57] Charlson,M.E.,Pompei,P.,Ales,K.L.,MacKenzie,C.R.:Anewmethodofclassifying\nprognostic comorbidity in longitudinal studies: Development and validation. Jour-\nnal of Chronic Diseases40(5), 373\u2013383 (1987) https://doi.org/10.1016/0021-9681(87)\n90171-8\n[58] Charlson Comorbidity Index (CCI). https://www.mdcalc.com/calc/3917/\ncharlson-comorbidity-index-cci. Accessed: 2025-10-12\n[59] Johnson, A., Pollard, T., Mark, R.: MIMIC-III clinical database. PhysioNet (2023)\n[60] Yang, G., Cao, M., Jiang, L.Y., Liu, X.C., Cheung, A.T.M., Weiss, H., Kurland, D.,\nCho, K., Oermann, E.K.: Language model classifier aligns better with physician word\nsensitivity than XGBoost on readmission prediction. arXiv [cs.CL] (2022) [cs.CL]\n[61] Welbl, J., Liu, N.F., Gardner, M.: Crowdsourcing multiple choice science questions.\narXiv [cs.HC] (2017) [cs.HC]\n38\n[62] Jin,Q.,Dhingra,B.,Liu,Z.,Cohen,W.W.,Lu,X.:PubMedQA:Adatasetforbiomedical\nresearch question answering. arXiv [cs.CL] (2019) [cs.CL]\n[63] Lightning AI: LitGPT. https://github.com/Lightning-AI/litgpt (2023)\n[64] Zhao,Y.,Gu,A.,Varma,R.,Luo,L.,Huang,C.-C.,Xu,M.,Wright,L.,Shojanazeri,H.,\nOtt, M., Shleifer, S., Desmaison, A., Balioglu, C., Damania, P., Nguyen, B., Chauhan,\nG., Hao, Y., Mathews, A., Li, S.: PyTorch FSDP: Experiences on scaling fully sharded\ndata parallel. arXiv [cs.DC] (2023) [cs.DC]\n[65] Bengio,Y.:Practicalrecommendationsforgradient-basedtrainingofdeeparchitectures.\narXiv [cs.LG] (2012) [cs.LG]\n[66] Hu,E.J.,Shen,Y.,Wallis,P.,Allen-Zhu,Z.,Li,Y.,Wang,S.,Wang,L.,Chen,W.:LoRA:\nLow-rank adaptation of large language models. arXiv [cs.CL] (2021) [cs.CL]\n[67] Biderman, S., Schoelkopf, H., Sutawika, L., Gao, L., Tow, J., Abbasi, B., Aji, A.F.,\nAmmanamanchi, P.S., Black, S., Clive, J., DiPofi, A., Etxaniz, J., Fattori, B., Forde,\nJ.Z., Foster, C., Hsu, J., Jaiswal, M., Lee, W.Y., Li, H., Lovering, C., Muennighoff, N.,\nPavlick, E., Phang, J., Skowron, A., Tan, S., Tang, X., Wang, K.A., Winata, G.I., Yvon,\nF., Zou, A.: Lessons from the trenches on reproducible evaluation of language models.\narXiv [cs.CL] (2024) [cs.CL]\n[68] Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C.H., Gonzalez, J.E., Zhang,\nH., Stoica, I.: Efficient memory management for large language model serving with\nPagedAttention. arXiv [cs.LG] (2023) [cs.LG]\n[69] Pedregosa,F.,Varoquaux,G.,Gramfort,A.,Michel,V.,Thirion,B.,Grisel,O.,Blondel,\nM., M\u00c3\u0133ller, A., Nothman, J., Louppe, G., Prettenhofer, P., Weiss, R., Dubourg, V.,\nVanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, \u00c3.:\nScikit-learn: Machine learning in python. arXiv [cs.LG] (2012) [cs.LG]\n[70] Detlefsen,N.,Borovec,J.,Schock,J.,Jha,A.,Koker,T.,DiLiello,L.,Stancl,D.,Quan,\nC., Grechkin, M., Falcon, W.: TorchMetrics - measuring reproducibility in PyTorch. J.\nOpen Source Softw.7(70), 4101 (2022)\n39\n",
    "title": "Generalist Foundation Models Are Not Clinical Enough for Hospital Operations",
    "authors": [
      "Lavender Y. Jiang",
      "Angelica Chen",
      "Xu Han",
      "Xujin Chris Liu",
      "Radhika Dua",
      "Kevin Eaton",
      "Frederick Wolff",
      "Robert Steele",
      "Jeff Zhang",
      "Anton Alyakin",
      "Qingkai Pan",
      "Yanbing Chen",
      "Karl L. Sangwon",
      "Daniel A. Alber",
      "Jaden Stryker",
      "Jin Vivian Lee",
      "Yindalon Aphinyanaphongs",
      "Kyunghyun Cho",
      "Eric Karl Oermann"
    ],
    "published": "2025-11-17",
    "arxiv_id": "2511.13703v1",
    "num_pages": 39,
    "num_chars": 91121
  },
  {
    "text": "ST-ProC: A Graph-Prototypical Framework for\nRobust Semi-Supervised Travel Mode Identification\nLuyao Niu\nPeking University\nShenzhen, China\nluyao0160@stu.pku.edu.cnNuoxian Huang\nImperial College London\nLondon, UK\nn.huang25@imperial.ac.uk\nAbstract\u2014Travel mode identification (TMI) from GPS tra-\njectories is critical for urban intelligence, but is hampered by\nthe high cost of annotation, leading to severe label scarcity.\nPrevailing semi-supervised learning (SSL) methods are ill-suited\nfor this task, as they suffer from catastrophic confirmation bias\nand ignore the intrinsic data manifold. We propose ST-ProC,\na novel graph-prototypical multi-objective SSL framework to\naddress these limitations. Our framework synergizes a graph-\nprototypical core with foundational SSL Support. The core\nexploits the data manifold via graph regularization, prototypical\nanchoring, and a novel, margin-aware pseudo-labeling strategy\nto actively reject noise. This core is supported and stabilized by\nfoundational contrastive and teacher-student consistency losses,\nensuring high-quality representations and robust optimization.\nST-ProC outperforms all baselines by a significant margin,\ndemonstrating its efficacy in real-world sparse-label settings, with\na performance boost of 21.5% over state-of-the-art methods like\nFixMatch.\nIndex Terms\u2014Travel Mode Identification, Semi-Supervised\nLearning, Graph Regularization, Prototypical Networks, Con-\ntrastive Learning\nI. INTRODUCTION\nThe proliferation of location-aware devices has led to an\nexplosive growth in human trajectory data. A fundamental\ntask in mobility analysis is Travel Mode Identification (TMI),\nwhich aims to classify travel modes (e.g., walk, bike, bus)\nfrom raw GPS data [1]. TMI serves as a cornerstone for\nhigh-impact applications, including intelligent transportation\nsystems, carbon footprint estimation, personalized location-\nbased services, and smart city planning.\nEarly TMI methods relied on traditional machine learning\n(e.g., SVMs, Random Forests) with extensive hand-crafted\nfeatures [2]. While insightful, these methods suffer from\nlimited representation power. This led to the adoption of deep\nlearning models, such as LSTMs, GRUs [3], and Transformers,\nwhich can automatically learn from sequential patterns.\nDespite these advances, a dominant challenge remains.\nMost deep learning models require massive, accurately labeled\ndatasets to perform well. However, acquiring such ground-\ntruth labels is expensive and labor-intensive, typically requir-\ning manual user annotation. This makes purely supervised\napproaches impractical for real-world, large-scale deployment\n[1].\nThis label scarcity strongly motivates a shift towards SSL.\nHowever, prevailing general-purpose SSL frameworks (e.g.,FixMatch [4]) are ill-suited for the sparse TMI task. First,\nthey suffer from catastrophic confirmation bias in extreme\nlabel scarcity [5]. When labels are scarce, these models\nprogressively reinforce their own noisy pseudo-labels, espe-\ncially on ambiguous modes with subtle motion differences,\nleading to degenerated solutions. Second, they treat samples\nas independent and identically distributed (i.i.d.) and ignored\nthe latent graph structure that governs the data. This i.i.d.\nassumption fails to leverage the fact that trajectories are not\nindependent (e.g., segments on the same rail line share a label),\nthus missing a critical source of regularization [6].\nFurthermore, many methods attempt to compensate for poor\nperformance by introducing external data [7], such as road\nnetworks or POIs. This reliance compromises generalizability\nand introduces privacy risks.\nTo address these specific limitations, we propose ST-ProC,\na novel, context-free, graph-enhanced SSL framework. Our\napproach directly tackles the two SSL challenges. To combat\nconfirmation bias, we introduce a graph-prototypical core. This\ncore uses semantic prototypes, derived from labeled data, to\nact as stable anchors. It then employs a graph-based, margin-\naware filter to actively reject noisy pseudo-labels, rather than\npassively accepting them. To exploit the latent graph structure,\nthis core explicitly builds an endogenous semantic graph\nto enforce manifold consistency through regularization. We\nstabilize this entire process with foundational SSL Support\n(contrastive learning and teacher-student consistency) in a\nmulti-objective design. This structure allows the model to\nlearn robust representations while simultaneously respecting\nthe data\u2019s intrinsic geometry and mitigating label noise.\nIn summary, our contributions are as follows:\n1) Propose ST-ProC, a novel multi-objective SSL framework\nfor sparse-label TMI that integrates graph regularization\nand prototypical anchoring to explicitly model the under-\nlying topological structure of trajectory data.\n2) Develop a robust, dual-filtered pseudo-labeling strategy that\nfuses graph and prototype predictions to actively mitigate\nconfirmation bias in sparse regimes..\n3) Achieve state-of-the-art performance on the GeoLife\nbenchmark, significantly outperforming strong baselines.\nThe remainder of this paper is organized as follows. Section\nII details our proposed framework. Section III presents thearXiv:2511.13702v1  [cs.LG]  17 Nov 2025\nexperimental setup and results, followed by a conclusion in\nSection IV.\nII. METHODOLOGY\nA. Problem Formulation\nWe formulate TMI as a semi-supervised classification task\non trajectory data. The complete datasetDis composed of a\nlabeled subsetD L={(T i, yi)}NL\ni=1(wherey i\u2208 {1, . . . , K}\nis the ground-truth mode) and a significantly larger unla-\nbeled subsetD U={T i}NU\ni=1. Each raw trajectoryT i=\n{(tj, latj, lon j)}Li\nj=1is a temporal sequence of GPS coordi-\nnates. Our objective is to learn a robust mapping function\n\u03a6 :T \u2192 {1, . . . , K}that accurately identifies transport\nmodes. The core challenge is effectively generalizing from\nthe sparse supervision inD Lby mining the latent structural\ninformation withinD U, without relying on external geographic\ncontext.\nB. Framework Overview\nTo tackle the inherent catastrophic confirmation bias and\nmanifold ignorance in sparse-label TMI, we propose ST-ProC,\na graph-prototypical multi-objective framework. As illustrated\nin Fig 1, the pipeline consists of three synergistic stages:\n1)Dual-View Representation Encoding:Raw trajectories\nare synthesized into complementary statistical and sequen-\ntial views. An adaptive dual-stream encoder then fuses\nthese heterogeneous features into a unified latent embed-\ndingz.\n2)Dynamic Semantic Graph Construction:The data mani-\nfold is explicitly modeled by constructing a global semantic\ngraphGover the embedded samples, capturing intrinsic\nrelationships among data points.\n3)Synergistic Multi-Objective Learning:The encoder is\noptimized using a cohesive set of five objectives that\nstrategically integrate (a) foundational contrastive learning,\n(b) prototypical anchoring, (c) manifold graph regulariza-\ntion, (d) confidence-aware pseudo-labeling, and (e) teacher-\nstudent consistency.\nCollectively, ST-ProC innovates by tightly coupling manifold-\naware graph regularization and prototypical semantic anchor-\ning within a stabilized multi-objective loss, enabling the model\nto effectively utilize the intrinsic structure of unlabeled data\nwhile actively suppressing catastrophic confirmation bias.\nC. Trajectory Representation and Encoder\nTo generate high-quality representations suitable for graph\npropagation, we engineer two complementary views from the\nraw trajectoryT i(projected to a 2D planar system). The first is\na holistic statistical vector (f), which is a dense feature summa-\nrizing global kinematics, geometric tortuosity, and stop-and-\ndwell characteristics. The second is a multi-channel motion\nsequence ( \u02dcX), a 4-channel time-series[x, y, v x, vy]capturing\nfine-grained, step-by-step dynamic patterns.\nThese views are processed by an adaptive dual-stream\nencoder. A spatio-temporal stream (Transformer coupled with\nBiGRU) extracts sequential dependencies from \u02dcX, while astatistical stream (MLP) encodes global properties fromf. A\nlearnable gating mechanism adaptively fuses these heteroge-\nneous representations into a comprehensive hidden stateh.\nLast, a projection head mapshto a normalizedD-dimensional\nembeddingz(||z||= 1), which serves as the canonical input\nfor our downstream semi-supervised framework.\nD. Graph-Prototypical Multi-Objective SSL Framework\nThe efficacy of ST-ProC stems from the synergistic opti-\nmization of five objectives within a shared embedding space\nz. This design ensures the learned representations are not only\ndiscriminative but also compliant with the underlying mobility\nmanifold of GPS trajectories.\n1) Dynamic Semantic Graph Construction:Standard i.i.d.\nassumptions fail in TMI because trajectories are governed by\nphysical infrastructure: tracks on the same road segment or\nrail line typically belong to the same transport mode (e.g.,\nsubway) regardless of individual speed variations. To capture\nthis topological dependency, we explicitly model the data\nmanifold. We periodically reconstruct a globalk-NN graph\nGglobal = (V,E)over the embeddingsZ, using cosine simi-\nlarity to link kinetically and spatially similar trajectories. To\ninject this stable, global manifold structure into local training,\nwe derive a batch-wise adjacency matrixA bvia subgraph\nclipping. This ensures that local optimization respects the\nglobal continuity of the transportation network. In the initial\nwarmup phase, we fallback to an on-the-fly dynamick-NN\ngraph computed within the batch.\n2) Foundational Contrastive Learning:Raw GPS data is\ninherently susceptible to sensor noise, sampling rate irregular-\nities, and signal drift. To establish a latent space robust to these\nperturbations, we employ a foundational contrastive objective.\nLetz 1andz 2be embeddings of two augmented views of the\nsame trajectory, where augmentations simulate realistic GPS\nanomalies (e.g., point masking or jittering). We minimize the\nNT-Xent loss:\nLctr=\u2212E i\"\nlogexp(z 1,i\u00b7z2,i/\u03c4c)P2B\nk=1\u22aek\u0338=iexp(z 1,i\u00b7zk/\u03c4c)#\n(1)\nThis objective pulls together views of the same underlying\nmovement pattern, forcing the model to learn representations\ninvariant to sensor-induced variations rather than overfitting to\nspecific coordinate sequences.\n3) Prototypical Anchoring:In sparse-label TMI, distin-\nguishing modes with similar kinematics is challenging. To mit-\nigate semantic drift, we establish stable class representations\nby maintaining a set of learnable prototypesP={p k}K\nk=1,\nwhere each prototype represents the canonical motion signa-\nture of a transport mode (e.g., the stop-and-go pattern of a\nbus). The probability of samplez ibelonging to classkis\nmodeled via:\np(y=k|z i) =exp(z\u22a4\nipk/\u03c4p)PK\nj=1exp(z\u22a4\nipj/\u03c4p)(2)\nFig. 1.Schematic illustration of the ST-ProC framework.\nFor labeled samples, we minimizeL proto to anchor proto-\ntypes to true class semantics:\nLproto=\u2212E (zi,yi)\u2208DL[logp(y i|zi)](3)\nPrototypes are updated via Exponential Moving Average\n(EMA) using high-confidence samples, ensuring they evolve\nstably even when labeled data covers only a fraction of the\nroad network.\n4) Graph Regularization:To enforce the manifold hy-\npothesis\u2014that trajectories traversing the same physical path\nshould share semantic labels\u2014we introduce a dual graph\nregularization mechanism based onA b:\n1)Laplacian Smoothness (L graph smooth ):We utilize the\ngraph LaplacianL Lapto penalize abrupt semantic shifts be-tween connected nodes (e.g., ensuring a trajectory segment\non a railway is not isolatedly classified as car):\nLgraph smooth =1\n|B|Tr(Z\u22a4LLapZ)(4)\n2)Neighbor Contrast (L nbr ctr):We further refine the local\nstructure by treating neighborsN(i)(trajectories with\nsimilar motion contexts) as positive pairs:\nLnbrctr=\u2212E zi\"\nlogP\nzj\u2208N(i)exp(z i\u00b7zj/\u03c4n)\nP\nzk\u0338=ziexp(z i\u00b7zk/\u03c4n)#\n(5)\nThese regularizers ensure the learned embedding space mirrors\nthe topological continuity of the real-world transportation\nnetwork.\n5) Robust Graph-based Pseudo-Labeling:Ambiguous mo-\ntion patterns (e.g., walking vs. waiting for a bus) often\ngenerate noisy predictions in TMI. This component serves as\nour primary defense against confirmation bias by expanding\nsupervision toD Uvia a rigorous filter.\nPseudo-labels\u02c6y iand confidence scoresc iare generated by\nfusing predictions from (a) direct prototype similarity and (b)\nlabel propagation across the global graphG global, effectively\nsmoothing out local noise.\nA pseudo-label is accepted only if it satisfies both ahigh-\nconfidence threshold(c i> \u03c4 conf) and ahigh-margin criterion\n(mi=c(1)\ni\u2212c(2)\ni> \u03c4 margin ), rejecting ambiguous samples\nthat sit on decision boundaries (e.g., transition points between\nmodes).\nThe loss is formulated as a confidence-weighted cross-\nentropy loss on the filtered subsetD filtered :\nLpseudo =\u2212E zi\u2208D filtered[ci\u00b7logp(\u02c6y i|zi)](6)\n6) Teacher-Student Consistency:Last, to stabilize training\nagainst the stochastic nature of GPS augmentations, we incor-\nporate a teacher-student consistency mechanism. The Teacher\nmodel (EMA-updated weights\u03b8 t) provides stable targets, mit-\nigating the variance caused by aggressive data augmentation\n(e.g., large-scale masking):\nLcons=E z\u0002\n||z1\u2212zt||2\n2\u0003\n(7)\nThis ensures that the model yields consistent predictions for\na trajectory regardless of partial signal loss or noise injection.\n7) Optimization Strategy:The model is trained end-to-end\nby minimizing a total objective that balances the five loss\ncomponents. To prevent early overfitting to potentially noisy\npseudo-labels, we employ time-dependent ramp-up functions,\nwp(t)andw c(t), for the semi-supervised terms:\nLtotal=L ctr+\u03bbpLproto\n+\u03bbsLgraph smooth +\u03bbnLnbrctr\n+wp(t)\u00b7 L pseudo +wc(t)\u00b7 L cons(8)\nThis curriculum-based optimization allows the embeddings\nand prototypes to mature before the model relies heavily on\npropagated labels and consistency constraints.\nIII. EXPERIMENTS\nWe first detail the experimental setup and training process,\nthen provide a comparison experiment and detailed class\nanalysis.\nA. Experimental Setup\nWe evaluate our method on theGeoLifeTrajectories dataset\nfocusing on 5 main classes: walk, bike, bus, car (incl. taxi),\nand subway. We simulate the SSL environment by masking\ntraining labels across varied ratios, enforcing a minimum of\n15 labeled samples per class for low ratios. A WeightedRan-\ndomSampler is used to balance the sampling of labeled and\nunlabeled data.\nOur dual-stream encoder ingests 4-channel sequences and\nF= 48statistical features. We use the AdamW optimizer with\na cosine annealing schedule (5-epoch warmup), early stopping,TABLE I\nOVERALL PERFORMANCE COMPARISON ON THETMICLASSIFICATION\nTASK(F1-SCORE). BEST:BOLD; SECOND: UNDERLINE .\nModel 5% 20% 50% 100%\nTrajClus [8] 0.363 0.363 0.363 0.363\nDeepConvLSTM [9] 0.431 0.606 0.573 0.593\nMultiScaleAttn [11] 0.430 0.427 0.594 0.580\nFixMatch [4] 0.618 0.700 0.780 0.769\nST-ProC 0.635 0.775 0.842 0.934\nand radient clipping. The total loss weights are:\u03bb p= 1.0,\n\u03bbc= 0.2,\u03bb s= 0.05,\u03bb n= 0.10, and\u03bb pseudo = 0.5. Key\nhyperparameters include\u03c4= 0.1(contrastive),m= 0.15\n(proto margin),\u03b1= 0.999(EMA), andk= 10(graphk-NN).\nB. Baselines\nWe compare ST-ProC against unsupervised, semi-\nSupervised, and supervised strong baselines. To ensure a\nfair comparison, all deep learning baselines utilize our dual-\nstream adaptive encoder as the backbone, only modifying the\nlearning paradigm and final objective function.\n\u2022TrajClus[8]: A geometric unsupervised method employing\ntrajectory partitioning and density-based clustering.\n\u2022FixMatch[4]: A general-purpose SSL framework for com-\nparison of core pseudo-labeling strategy.\n\u2022DeepConvLSTM[9]: A widely adopted hybrid architecture\nfor spatiotemporal sequence classification.\n\u2022Transformer[10]: The standard transformer encoder for\nsequence-based modeling.\n\u2022MultiScaleAttention[11]: A specialized attention network\ndesigned to capture multi-scale temporal features in trajec-\ntories.\nC. Overall Performance\nTable I presents the comparison across varying labeled\nratios. ST-ProC consistently outperforms all baselines across\nall label regimes. In the label-scarce 5% setting, our model\n(0.635) establishes a commanding 47.3% lead over the best\nsupervised method (DeepConvLSTM, 0.431), and significantly\nsurpasses the unsupervised TrajClus (0.363). This dominance\nhighlights the effectiveness of our framework in leveraging\nunlabeled data. Notably, ST-ProC at 5% labels already exceeds\nthe performance of all supervised models trained on 100% of\nthe data.\nWhen compared to the strong semi-supervised baseline\nFixMatch, ST-ProC maintains a 1.7% gain in the 5% setting,\nvalidating the efficiency of our graph-prototypical SSL strat-\negy. More critically, while FixMatch suffers from performance\nsaturation, plateauing around 0.769 F1 (likely due to accumu-\nlated pseudo-label noise), our model scales unimpeded to a\npeak F1 of 0.934 with 100% labels. This 21.5% performance\ngap between ST-ProC (0.934) and FixMatch (0.769) confirms\nthat our design is not only a better SSL approach for sparse\nsupervision but also a fundamentally superior architecture\ncapable of exploiting the full supervisory signal for complex\nspatiotemporal tasks.\nFig. 2. Confusion Matrix of ST-ProC\u2019s performance at the 20% labeled data\nratio.\nD. Detailed Per-Class Analysis\nA granular analysis of per-class performance is visualized\nin the confusion matrix (Fig. 2) for the 20% label rate. The\nmatrix reveals that ST-ProC reliably classifies modes with dis-\ntinct kinematic profiles. Strong diagonal values are observed\nfor Car, Bike, and the minority class, Subway, suggesting\nthe graph-prototypical core successfully extracts their unique\nmotion signatures.\nConversely, the matrix shows that confusions primarily arise\nin modes with ambiguous or overlapping motion patterns.\nThe Bus class, for instance, is frequently misclassified as Car\n(13.08%) or Walk (15.19%). This indicates difficulty in sep-\narating bus-specific stop-and-go road travel from continuous\ndriving or slow, stop-like segments. Similarly, Walk segments\nshow notable misclassification into Bike (14.44%) and Bus\n(15.19%), highlighting local ambiguities where slow walking\nactivity is confused with cycling or vehicle dwelling. These\nmixed-mode confusions illustrate the intrinsic difficulty of\nTMI, which our manifold-aware approach addresses while\nproviding transparent error diagnosis.\nIV. CONCLUSION\nIn this work, we addressed the critical challenge of con-\nfirmation bias and failure to capture trajectory topological\ndependencies in Semi-Supervised TMI under extreme label\nscarcity. We proposed ST-ProC, a novel, context-free, graph-\nprototypical multi-objective SSL framework. Our core inno-\nvation lies in the synergistic use of dynamic graph regular-\nization and prototypical anchoring to model the underlying\ntransport network structure. This core is complemented by\na robust, dual-filtered pseudo-labeling strategy that actively\nsuppresses label noise, providing a primary defense against\nbias. Experiments on the GeoLife benchmark showed ST-ProC\nachieves state-of-the-art performance, with the model using\nonly 5% of labels outperforming all fully supervised baselines,\ndemonstrating its ability to exploit intrinsic data structure and\nscale effectively with limited supervision. This confirms the\nhigh structural quality of our learned representations. Future\nwork includes extending ST-ProC to online TMI for streaming\ndata and exploring its applicability in other sequence-based\ntasks with limited labeled data.REFERENCES\n[1] J. Zeng, Z. Cai, Y . Huang, M. Zhang, M. Sun, S. Jin, and D. Wang,\n\u201cAdvancing fine-grained travel mode identification in real mobile phone\nsignaling data: A deep learning approach,\u201dIEEE Transactions on\nIntelligent Transportation Systems, 2025.\n[2] P. A. Gonzalez, J. S. Weinstein, S. J. Barbeau, M. A. Labrador, P. L.\nWinters, N. L. Georggi, and R. Perez, \u201cAutomating mode detection for\ntravel behaviour analysis by using global positioning systems-enabled\nmobile phones and neural networks,\u201dIET Intelligent Transport Systems,\nvol. 4, no. 1, pp. 37\u201349, 2010.\n[3] J. James, \u201cTravel mode identification with gps trajectories using wavelet\ntransform and deep learning,\u201dIEEE Transactions on Intelligent Trans-\nportation Systems, vol. 22, no. 2, pp. 1093\u20131103, 2020.\n[4] K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raffel,\nE. D. Cubuk, A. Kurakin, and C.-L. Li, \u201cFixmatch: Simplifying semi-\nsupervised learning with consistency and confidence,\u201dAdvances in\nneural information processing systems, vol. 33, pp. 596\u2013608, 2020.\n[5] E. Arazo, D. Ortego, P. Albert, N. E. O\u2019Connor, and K. McGuinness,\n\u201cPseudo-labeling and confirmation bias in deep semi-supervised learn-\ning,\u201d in2020 International joint conference on neural networks (IJCNN).\nIEEE, 2020, pp. 1\u20138.\n[6] W. Yu and G. Wang, \u201cGraph based embedding learning of trajectory data\nfor transportation mode recognition by fusing sequence and dependency\nrelations,\u201dInternational Journal of Geographical Information Science,\nvol. 37, no. 12, pp. 2514\u20132537, 2023.\n[7] T. Bantis and J. Haworth, \u201cWho you are is how you travel: A framework\nfor transportation mode detection using individual and environmental\ncharacteristics,\u201dTransportation Research Part C: Emerging Technolo-\ngies, vol. 80, pp. 286\u2013309, 2017.\n[8] J.-G. Lee, J. Han, and K.-Y . Whang, \u201cTrajectory clustering: a partition-\nand-group framework,\u201d inProceedings of the 2007 ACM SIGMOD\ninternational conference on Management of data, 2007, pp. 593\u2013604.\n[9] S. P. Singh, M. K. Sharma, A. Lay-Ekuakille, D. Gangwar, and S. Gupta,\n\u201cDeep convlstm with self-attention for human activity decoding using\nwearable sensors,\u201dIEEE Sensors Journal, vol. 21, no. 6, pp. 8575\u20138582,\n2020.\n[10] Y . Liang, K. Ouyang, Y . Wang, X. Liu, H. Chen, J. Zhang, Y . Zheng,\nand R. Zimmermann, \u201cTrajformer: Efficient trajectory classification with\ntransformers,\u201d inProceedings of the 31st ACM International Conference\non Information & Knowledge Management, 2022, pp. 1229\u20131237.\n[11] G. Jiang, S.-K. Lam, P. He, C. Ou, and D. Ai, \u201cA multi-scale attributes\nattention model for transport mode identification,\u201dIEEE transactions on\nintelligent transportation systems, vol. 23, no. 1, pp. 152\u2013164, 2020.\n",
    "title": "ST-ProC: A Graph-Prototypical Framework for Robust Semi-Supervised Travel Mode Identification",
    "authors": [
      "Luyao Niu",
      "Nuoxian Huang"
    ],
    "published": "2025-11-17",
    "arxiv_id": "2511.13702v1",
    "num_pages": 5,
    "num_chars": 22195
  },
  {
    "text": "Learning stochasticity: a nonparametric\nframework for intrinsic noise estimation\nG. Pillonetto1, A. Giaretta2, and M. Bisiacco3\n1Department of Information Engineering, University of Padova, Padova (Italy)\n2Department of Pathology, Cambridge University, Cambridge (UK)\n3Department of Information Engineering, University of Padova, Padova (Italy)\nAbstract\nUnderstanding the principles that govern dynamical systems is a cen-\ntral challenge across many scientific domains, including biology and ecol-\nogy . Incomplete knowledge of nonlinear interactions and stochastic effects\noften renders bottom-up modeling approaches ineffective, motivating the\ndevelopment of methods that can discover governing equations directly\nfrom data. In such contexts, parametric models often struggle with-\nout strong prior knowledge, especially when estimating intrinsic noise.\nNonetheless, incorporating stochastic effects is often essential for under-\nstanding the dynamic behavior of complex systems such as gene regula-\ntory networks and signaling pathways. T o address these challenges, we\nintroduce T rine (Three-phase Regression for INtrinsic noisE), a nonpara-\nmetric, kernel-based framework that infers state-dependent intrinsic noise\nfrom time-series data. T rine features a three-stage algorithm that com-\nbines analytically solvable subproblems with a structured kernel archi-\ntecture that captures both abrupt noise-driven fluctuations and smooth,\nstate-dependent changes in variance. W e validate T rine on biological and\necological systems, demonstrating its ability to uncover hidden dynam-\nics without relying on predefined parametric assumptions. Across several\nbenchmark problems, T rine achieves performance comparable to that of\nan oracle. Biologically , this oracle can be viewed as an idealized observer\ncapable of directly tracking the random fluctuations in molecular concen-\ntrations or reaction events within a cell. The T rine framework thus opens\nnew avenues for understanding how intrinsic noise affects the behavior of\ncomplex systems.\nKeywords: nonlinear dynamic systems |stochastic noise |system identification\n|machine learning |system biology\n1arXiv:2511.13701v1  [cs.LG]  17 Nov 2025\nIntroduction\nProcesses that occur within cells are intrinsically stochastic due to the ran-\ndom nature of biochemical reactions. These intrinsic fluctuations often play\nan important role in cellular functions such as differentiation and drug resis-\ntance [22, 62, 59, 20]. Stochasticity in gene expression generates phenotypic\ndiversity among genetically identical cells [23, 64]. This variability plays a func-\ntional role in processes such as stem cell differentiation and tissue homeostasis\n[15, 18], immune activation [25], and microbial survival strategies [1]. Despite\nits ubiquity , intrinsic noise is notoriously di\ufb00icult to quantify experimentally\nand computationally .\nStochastic models of biochemical networks often take the form of continuous-\ntime discrete Markov processes and are governed by the Chemical Master Equa-\ntion [31]. These models have provided important insights into stochastic effects\nin gene regulation and cell signaling. However, they require knowledge of re-\naction pathways and rate constants that are often di\ufb00icult to attain experi-\nmentally . Consequently , data-driven system identification methods [45, 63, 55]\nhave been developed to reconstruct governing equations directly from time-series\nobservations. These approaches generally rely on parametric assumptions and\nstruggle to capture complex nonlinear stochastic dynamics\u2014especially when the\nunderlying statistics depend on the system state\u2014without strong prior knowl-\nedge of the model structure. This challenge is especially pronounced in biologi-\ncal systems, where stochasticity is often a critical component of function rather\nthan a mere nuisance. The combination of ill-posedness, limited data, and the\nneed for nonparametric methods renders model inference di\ufb00icult\u2014yet all the\nmore important.\nThe problem is closely related to heteroskedastic noise modeling in machine\nlearning [54, 39]. In this field common noise inference strategies assume that\nthe log-variance of stochastic effects is an unknown function of the system state.\nThis function is then estimated by parameterizing it with a neural network or by\nmodeling its smooth profile as realizations of a Gaussian process [57, 61, 32]. The\nlatter approach, related to kernel-based methods, typically employs approximate\ninference techniques to achieve computational tractability based on stochastic\nsimulation techniques and iterative methods [43, 41, 9, 37, 65]. However, stan-\ndard heteroskedastic models often overlook measurement noise\u2014that is, errors\nintroduced through experimental procedures used to observe the system\u2014which\nis distinct from intrinsic stochasticity generated by the system\u2019s internal dy-\nnamics. Using inappropriate priors\u2014such as overly flexible or heavy-tailed log-\nnormal distributions\u2014can lead to variance profiles that are highly sensitive to\nobservational noise, as we also illustrate in this work. This necessitates a method\nthat leverages the flexibility of nonparametric inference, while simultaneously\nensuring robust and interpretable estimates of intrinsic noise.\nHere, we present T rine (Three-phase Regression for INtrinsic noisE), the\nfirst framework capable of nonparametrically recovering state-dependent intrin-\nsic noise from biological time-series data. T rine overcomes the core limitations\nof existing methods through a modular, three-stage regression algorithm based\n2\non structured kernel estimators. Each stage uses linear estimators with ana-\nlytical solutions, favoring parsimonious functional forms in cases where unique\nsolutions do not exist [67, 56]. A key innovation is a custom-designed kernel\nthat captures the dependence of noise statistics on the system state, as well\nas the irregularities and discontinuities characteristic of intrinsic fluctuations in\nmolecular systems.\nW e demonstrate T rine on several case studies\u2014including ecological pop-\nulation dynamics [58], gene regulatory networks [8, 20], and spiking neurons\nmodeled by the FitzHugh\u2013Nagumo system [26]\u2014showing that it accurately re-\nconstructs state-dependent stochastic structure without requiring parametric\nassumptions or prior system knowledge. Crucially , we show that T rine not\nonly outperforms state-of-the-art learning approaches, but also achieves perfor-\nmance comparable to oracle estimators that have access to the true realizations\nof intrinsic noise. Such information is fundamentally unavailable in realistic\nexperimental settings. This highlights the method\u2019s ability to recover latent\nstochastic mechanisms directly from observational data, effectively bridging the\ngap between theoretical stochastic modeling and experimental measurability .\nIntrinsic Noise estimation\nStochastic state-space models. W e consider stochastic dynamical systems\nsubject to both intrinsic and experimental noise. In the continuous-time setting,\nthe system is described by the following state-space model:\n\u02d9x(t) =f(x(t)) +g(x(t))\u00b7w(t), (1)\nyk=x(tk) +ek, k = 1, . . . , N, (2)\nwhere x(t)\u2208Rndenotes the system state at time t, and yk\u2208Rnare noisy\nobservations collected at discrete time points tk. The function fdescribes the\ndeterministic drift, while the noise term g\u00b7w represents the intrinsic noise, re-\nflecting internal stochasticity dependent on the system state. This term includes\na matrix-valued function g(x)and a random vector w(t), whose components are\nindependent Gaussian white noise processes. A diagonal g(x)corresponds to\nthe case where the noise inputs driving the components of x(t)are uncorre-\nlated. Finally , the term ekaccounts for measurement noise. This formulation\nthus captures the two primary sources of uncertainty present in many biological\nand ecological systems. The first is intrinsic noise, arising from internal stochas-\ntic mechanisms such as gene expression bursts or random molecular interactions\nwithin a cell; the second is measurement noise, caused by imperfections in the\nobservation process, such as sensor inaccuracies.\nThe two central functions to be estimated from a finite set of noisy data\nare the deterministic drift fand the noise strength g. This estimation task\nis challenging due to the nonlinear structure of these functions. Additionally ,\ngappears within a stochastic term in the dynamics, further complicating the\nproblem. As a result, the problem is fundamentally ill-posed: infinitely many\n3\n\u02d9x=f(x) +g(x)w(t)\ny(t) =x(t) +e(t)\nTimeNoisy states\nStep 1: Intrinsic Noise Signs Estimation\nSmooth kernel\nfor the deterministic system part fSmooth Kernel\n+\u2212++\u2212\nEstimates of Intrinsic Noise Signs\nStep 2: Intrinsic Noise Estimation\nStructured and discontinuous kernel\nfor intrinsic noise realizations\nDiscontinuous Kernel\nEstimates of Intrinsic Noise\nStep 3: Intrinsic Noise Variance Estimation\nSmooth kernel\nfor intrinsic noise profile gSmooth Kernel\nsystem stateIntrinsic Noise VarianceIntrinsic Noise Model Estimator\nFigure 1: Structured Intrinsic Noise Estimation via T rine . The proposed method\ndecomposes the estimation of structured intrinsic noise into three sequential phases using\nGaussian Process Regression (GPR) with customized kernels: Step 1\u2014Sign Estimation: A\nGPR model with a smooth kernel is used to describe the deterministic component fof the\nsystem dynamics. The residuals from this model are then used to estimate the signs of\nthe intrinsic noise realizations, capturing directional information. Step 2\u2014Intrinsic Noise\nRealization: Based on the estimated signs, a second GPR model is employed, featuring a\nstructured kernel that captures both the discontinuities in the intrinsic noise realizations and\nthe smooth variation of the variance profile with respect to the system state. This model is\nused to recover the realizations of the intrinsic noise. Step 3\u2014Noise V ariance Profiling: The\nabsolute residuals from Step 2, appropriately scaled, are modeled using a third GPR with a\nsmooth kernel to estimate the state-dependent noise standard deviation profile. Overall, this\nmodular approach allows for flexible modeling of nonhomogeneous, state-dependent noise in\ncomplex dynamical systems by decoupling directionality , structure, and variance.4\ncombinations of f andgcan explain the observed data equally well, partic-\nularly in noisy settings. A common strategy for regularization is to adopt a\nparametric formulation, reducing the problem to estimating a finite number of\nparameters within predefined functional families. While this improves tractabil-\nity , it imposes strong prior assumptions on the structure of fandg, which are\noften di\ufb00icult to justify in settings with poorly understood or highly nonlinear\nmechanisms.\nThe TRINE algorithm T o estimate the components of this system from\ndata, we adopt a nonparametric approach based on kernel methods\u2014a flexible\nand principled tool that regularizes the estimation problem without imposing\nexplicit functional forms [61]. These methods encode prior assumptions via a\nsymmetric positive semidefinite function K:Rn\u00d7Rn\u2192R, where K(xa, xb)\nquantifies the similarity between states xaandxb. This means that pairs of\nstates that are \u201cclose\u201d according to some metric tend to have similar values of\nthe unknown function, i.e., f(xa)\u2248f(xb). F or example, if K is continuous,\nany function drawn from the associated model will also be continuous. Defin-\ning a kernel implicitly determines a potentially infinite set of basis functions\u2014\nspecifically , the kernel sections\u2014that span the function space used for estima-\ntion. This construction not only provides great flexibility in approximating\ncomplex functions but also induces an implicit regularization on the expansion\ncoe\ufb00icients. This can be interpreted probabilistically: the kernel defines the co-\nvariance of a Gaussian process prior over functions, favoring those with higher\nprior probability (e.g., smoother if the kernel is regular) [57]. This naturally\npromotes simpler functional forms when multiple solutions fit the data equally\nwell, thus helping to avoid overfitting without explicitly restricting the model\nclass.\nA widely used kernel to encode smoothness is the Gaussian kernel , defined\nas\nK(xa, xb) =\u03bbexp\u0012\n\u2212\u2225xa\u2212xb\u22252\n2\u2113\u0013\n,\nwhere \u03bb > 0and\u2113 > 0are hyperparameters controlling the amplitude and\nsmoothness of the estimate. These parameters are typically learned from data.\nGaussian kernels are universal, in the sense that they can approximate any con-\ntinuous function arbitrarily well [50], and are particularly effective when the\ntarget function is expected to be smooth. In the T rine method, the Gaussian\nkernel is employed directly in some stages and also serves as a foundation for\ndesigning custom kernels in others, as described below.\nT rine is a three-stage estimator designed to reconstruct not only the drift\nfand the standard deviation profile g, but also the intrinsic noise realizations\ng\u00b7w, which are recovered in the middle stage of the algorithm. Crucially , this\nintermediate quantity\u2014the intrinsic noise\u2014is not merely a byproduct of the\nmodel: it is a central component of the algorithm. F rom a modeling perspective,\nit provides direct insight into the stochastic mechanisms driving the system,\n5\nwhich is key in domains where noise itself plays a functional role. F rom a\ncomputational perspective, estimating g\u00b7w is instrumental, as it enables the\nsubsequent inference of the state-dependent variance defined by g.\nThe algorithm is graphically depicted in Fig. 1, which illustrates how this\nthree-phase framework enables the separate and interpretable modeling of the\nstructure and scale of the intrinsic noise. The modular steps of T rine can be\nbriefly described as follows. First, the drift function fis estimated using Gaus-\nsian Process Regression (GPR) with a Gaussian kernel (see the upper blue box\nand smooth kernel curve in Fig. 1). A simplified intrinsic noise model with con-\nstant variance is assumed at this stage, and subsequently refined in the following\nsteps. F rom the residuals of this step, the signs of the intrinsic noise realiza-\ntions are inferred, capturing their directional component (see the red/blue sign\nestimates below the upper blue box in Fig. 1). Second, a specialized GPR\nmodel with a newly introduced structured, discontinuous kernel reconstructs\nthe intrinsic noise realizations (see the middle orange box and discontinuous\nkernel curve in Fig. 1). This step leverages the sign estimates while preserv-\ning the smooth structure of the variance. Third, the absolute residuals from\nthe previous step are appropriately scaled and used as input to a final GPR\nwith a Gaussian kernel to estimate the state-dependent intrinsic noise standard\ndeviation (SD) (see the bottom green box and smooth kernel curve above the\nfinal noise variance profile in Fig. 1). This last step can be further adapted\nto enforce structural constraints such as monotonicity or concavity , embedding\nbiologically motivated priors on the variance profile.\nThe pseudocode of T rine is provided in Appendix, together with the theoret-\nical motivations underlying its structure\u2014including the derivation of the novel\nkernel used in the second phase.\nResults\nPopulation dynamics: Ricker Model with Allee Effect\nThe Ricker model is a classic discrete-time population model developed for\nfish stock dynamics [58]. Like the logistic map, it describes density-dependent\ngrowth, but with an exponentially decreasing per-capita rate, making it suit-\nable for species with strong overcompensation, where populations drop sharply\nafter exceeding carrying capacity . Its rich nonlinear behavior has made it a\nstandard tool for studying chaos, bifurcations, and noise effects in ecological\nsystems [48]. Recent developments include stochastic competition models [19],\nstock\u2013recruitment models with environmental covariates [27], and theoretical\nanalyses [21].\nIn our example, the deterministic drift also incorporates an Allee effect:\nf(xk) =x2\nker(1\u2212xk),\nwhich introduces a positive relationship between population size and growth at\nlow densities. Populations below a critical threshold then face negative growth,\n6\nincreasing extinction risk [20]. Incorporating the Allee effect enables the study\nof bistability between extinction and survival, especially under stochastic influ-\nences.\nAs discussed in [20], in stochastic population models the type of random\nfluctuations determines the appropriate diffusion approximation: demographic\nnoise may produce variance proportional to the population size x, whereas en-\nvironmental noise leads to variance proportional to x2. W e tested the T rine\napproach under both noise types, successfully recovering the system dynamics\nin each case. Here, we present results for the system\nxk+1=f(xk) +g(xk)wk\nwith fgiven above and demographic noise characterized by\ng(x) =p\n0.32+ 0.052x,\nas in [6].\nSimulations were performed with r= 2.5. In this and all subsequent ex-\namples, the standard deviation of the output noise ekis set to a fixed fraction\nof the true state xk, such that the norm of the output noise realizations is\napproximately 30\u201340% of the norm of the intrinsic noise. The top panel of\nFig. 2 presents a simulated trajectory whose dynamics display irregular oscilla-\ntions. The bottom-left panel shows that the T rine estimate of g(x)(blue) closely\nmatches the true profile (red), while the drift estimate (bottom-right panel) is\nalso accurate, with only slight degradation near the upper boundary of the state\nspace\u2014an area less frequently visited by the system.\nFitzHugh\u2013Nagumo Model with Stochastic Noise\nThe FitzHugh\u2013Nagumo (FHN) model is a paradigmatic reduction of the Hodgkin\u2013\nHuxley equations [36], designed to capture the essential dynamical features of\nexcitable systems such as neurons and cardiac cells. By simplifying the high-\ndimensional biophysical description into a two-dimensional system, the FHN\nmodel provides a tractable framework for studying excitability , oscillations,\nand wave propagation in both single cells and extended media [26, 52]. Its\nminimal formulation, consisting of a fast activator variable V(t)and a slower\nrecovery variable W(t), has made it one of the most widely used models for the-\noretical and computational investigations of excitable dynamics. In a neuronal\ncontext, V corresponds to the membrane potential, which exhibits rapid, non-\nlinear excursions during action potentials, while W accounts for slower recovery\nmechanisms, such as sodium channel inactivation and potassium activation, that\nreturn the system to rest.\nT o capture the intrinsic variability present in real biological systems\u2014arising\nfrom ion channel stochasticity , synaptic inputs, or environmental fluctuations\u2014\na stochastic extension of the FHN model is considered, in which both variables\n7\nare subject to multiplicative, state-dependent noise [44, 60]:\n\u02d9V=V\u2212V3\n3\u2212W+Iext+\u03c3V|V|\u03b1\u03b7V(t), (3)\n\u02d9W=\u03f5(V+a\u2212bW) +\u03c3W|W|\u03b2\u03b7W(t), (4)\nwhere \u03b7V(t)and\u03b7W(t)are independent Gaussian white noise processes. The\ndeterministic dynamics are governed by the parameters \u03f5,a,b, and Iext , which\ncontrol timescale separation, excitability , and external input. The stochastic\ncomponents are characterized by \u03c3V,\u03c3W,\u03b1, and \u03b2, determining the noise in-\ntensity and its state dependence. This stochastic formulation enables the inves-\ntigation of phenomena such as noise-induced oscillations, coherence resonance,\nand threshold modulation [66, 68, 69, 4].\nSimulations of the stochastic FHN system were performed using the Euler\u2013\nMaruyama method with a time step of 0.01. The system was placed in an\nexcitable regime and a total of 2000 data points for V andW were sampled at\nintervals of \u2206t= 0.1, see the top panel of Fig. 3. T o emulate realistic experi-\nmental conditions, measurement noise was added to both the system states. As\nin the previous case studies its norm turns out to be approximately one third of\nthat of the intrinsic process noise. The T rine algorithm was used to estimate the\nstochastic structure from this noisy time series. W e set \u03c3V= 0.1and\u03c3W= 0.05.\nThen, to assess its robustness, the method was applied across a range of values\nfor the noise exponents \u03b1and\u03b2, consistently achieving high performance. As a\nrepresentative example, we focus here on the non-polynomial case \u03b1=\u03b2= 0.8.\nAlthough this form of state-dependent noise lies outside the class of polynomial\nfunctions, T rine successfully recovers it in a fully nonparametric fashion. This\nis illustrated in Fig. 3: the bottom-left panel shows the true state-dependent\nstandard deviation driving \u02d9V, while the bottom-right panel displays the esti-\nmate inferred by T rine. The close match between them confirms the method\u2019s\nability to accurately identify complex, nonlinear noise structures directly from\ndata. Moreover, the nonparametric estimate offers practical advantages for ex-\nperimentalists. F rom the estimated standard deviation function, a biologist can\nreadily infer that the stochastic forcing in \u02d9V depends primarily on the state\nvariable V, with negligible or no dependence on W . In addition, the strong\nsymmetry of the recovered profile suggests that the noise intensity is governed\nmainly by |V|, supporting a hypothesis of modulus-based dependence. Such\nqualitative insights, directly extracted from data without imposing a predefined\nfunctional form, can guide the formulation of more refined mechanistic models\nor inspire targeted experimental investigations.\nGene regulatory networks\nSelf promoter The self-promoter is a canonical stochastic gene regulatory\nmodel in systems biology , where a gene activates its own transcription through\npositive feedback. This simple architecture can generate bistability , hysteresis,\nand noise-induced switching, providing a minimal framework to study how in-\ntrinsic noise shapes phenotypic variability [38, 40, 51]. Positive autoregulation\n8\nis widespread: it drives the lysis\u2013lysogeny decision in bacteriophage \u03bb[49], has\nbeen engineered in synthetic circuits to demonstrate robust bistable switching\nin both E. coli [8] and yeast [7], and is a recurrent strategy in virology . F or\ninstance, the HIV T at loop produces bimodal dynamics that control latency\nversus replication [53], while HPV early genes sustain their own promoter ac-\ntivity to stabilize early genes programs [30, 29]. Beyond viruses, self-promoting\nloops also appear in mammalian regulation, e.g. in c-Myc and p53 feedback, re-\ninforcing proliferation or stabilizing cell-fate decisions. Thus, the self-promoter\nmodel provides a minimal yet generalizable framework to explore how feedback\nand noise jointly govern cellular decision-making.\nW e consider a self-promoter system under a fast promoter fluctuation regi-\nmen, which leads to a non-switching SDE formulation [40]. The drift term\nf(x) =ba0+x2\nb+x2\u2212x\u22122xb(a0\u22121)\u0002\n((a0\u22122) +x)x2+b(x\u2212a0)\u0003\n\u03ba(b+x2)4. (5)\ndescribes the deterministic balance of the system. Here, xdenotes the mRNA\nor protein concentration, a0controls the basal expression level, brepresents the\nbinding a\ufb00inity of the activator to the promoter, and \u03bais the promoter switching\nrate. The diffusion term\ng(x) =s\n1\nm0\u0012b(a0+x) +x2(1 +x)\nb+x2\u0013\n+bx2(a0\u22121)2\n\u03ba(b+x2)3. (6)\nquantifies intrinsic noise. Its amplitude is controlled by m0, which scales fluc-\ntuations from synthesis and degradation, and by \u03ba, which governs noise from\nstochastic promoter switching, with smaller values indicating stronger noise.\nAs a result, noise is suppressed at low and high expression levels but peaks\nat intermediate concentrations, where promoter on/off transitions occur most\nfrequently .\nThe self-promoter model described by (5) and (6) thus depends on four key\nparameters with clear biological interpretation [40]. The scalar a0defines the\nbasal activity level, bthe strength of positive feedback, m0the typical steady-\nstate protein copy number, which sets the scale of the dynamics. Finally , \u03ba\nquantifies stochastic fluctuations. Different parameter sets yield distinct dy-\nnamical regimens, ranging from monostability to bistability or noise-dominated\nbehavior. Parameters used in the simulations are reported in Fig. 4.\nIdentification data consist of 1000 points collected from four independent\nexperiments. The stochastic differential equation is simulated using the Euler\u2013\nMaruyama method with a time step of 0.01 min. Data are sampled at intervals\n\u2206t= 0.01, though other values of \u2206twithin the range [0.01,0.1] have also been\ntested, yielding results consistent with those presented below. The rationale\nbehind the choice of measurement noise statistics is the same as in the previous\nexample, ensuring that the output noise norm is approximately 30\u201340% of the\nintrinsic noise norm. The top panel of Fig. 4 shows a representative simulated\ntrajectory , characterized by pronounced burst-like stochastic behavior. The\n9\nbottom panel illustrates how T rine\u2019s estimate of g(x)(blue) closely matches the\ntrue diffusion profile (red). As expected, the profile is low at very small and very\nlarge protein concentrations, and reaches a maximum at intermediate levels\u2014\nprecisely where promoter switching between active and inactive states occurs\nmost frequently . Most data points are concentrated near zero expression lev-\nels, while intermediate states are relatively underrepresented. Nevertheless, the\nkernel-based estimation accurately reconstructs the non-monotonic shape of the\ndiffusion profile. Even without assuming a parametric model, T rine thus pro-\nvides an intuitive signature of stochastic promoter dynamics, offering modelers\na direct handle on how intrinsic noise shapes cell-to-cell variability .\nMutual repressor: T oggle switch The mutual repressor, or genetic tog-\ngle switch, is a minimal circuit where two genes inhibit each other, generating\nbistability and mutually exclusive expression states. Its first synthetic imple-\nmentation in E. coli [28] was a milestone in synthetic biology , later formalized\nin the stochastic setting to show how intrinsic noise drives switching [40]. Since\nthen, the toggle switch has become a paradigmatic systems biology motif for\nstudying stochastic switching, hysteresis, and nonlinear regulation [16, 38]. Nat-\nural analogues include the PU.1\u2013GA T A1 antagonism in hematopoietic lineage\nchoice [17] and the miR-200/ZEB feedback in EMT [46]. Recent studies have\nextended this picture: synthetic toggles have been used to probe spatiotempo-\nral dynamics in E. coli [5], while Jagged\u2013Delta asymmetry in Notch signaling\ngenerates hybrid sender/receiver states through toggle-like dynamics [12]. More\nbroadly , mutual repression motifs are increasingly viewed as modular building\nblocks for complex architectures, capable of producing oscillations, multistabil-\nity , and spatial patterning [34, 3]. This modular perspective emphasizes the\ncentral role of the toggle switch as both a theoretical archetype and a practical\ndesign principle across natural and engineered systems.\nIn our example, we model the toggle switch using a two-dimensional stochas-\ntic differential equation (SDE), written in the compact vector form given in (1),\nwhere x\u2208R2contains the normalized concentrations of two proteins. The\ndiffusion matrix g(x)\u2208R2\u00d72captures stochastic dynamics through two inde-\npendent intrinsic noise sources, each associated with the expression of one of the\nmutually repressive proteins. The state-dependent standard deviations, repre-\nsented by its diagonal entries, quantify fluctuations specific to each protein.\nImportantly , g(x)has a highly intricate parametric structure, reflecting com-\nplex, state-dependent interactions that make its analytic form nontrivial, as can\nbe seen from the full modeling details provided in Appendix.\nThe simulation setup, including the measurement noise characteristics, fol-\nlows that of the previous case study . W e focus here on estimating the standard\ndeviation of the intrinsic noise component affecting the dynamics of the first\nprotein, using 1000 noisy measurements of both state variables. The results are\nrepresentative of those obtained from extensive Monte Carlo simulations with\nindependent noise realizations and varying sampling intervals \u2206t. The top panel\nof Fig. 5 displays a representative trajectory of the two protein concentrations\n10\nin the 2D state space, clearly revealing two basins of attraction\u2014a hallmark\nof bistable systems. The bottom panels show, respectively , the true (left) and\nestimated (right) magnitudes of the intrinsic noise and their corresponding stan-\ndard deviation profiles, computed using \u2206t= 0.01. The left panel shows the\nground truth noise realizations and state-dependent SD, while the right panel\ndemonstrates that the T rine estimate closely matches the true profile. Despite\nthe simulated trajectory not uniformly exploring the state space\u2014most samples\nare concentrated in the two stable basins, with fewer in the transition region\u2014\nT rine still reconstructs the noise SD surface accurately . The nonparametric\nestimate of the diffusion offers a direct view of how fluctuations vary across the\nspace, highlighting regions that are most sensitive to stochastic fluctuations in\nstate switching and cell-fate decisions.\nComparison with other approaches\nT o benchmark T rine, we compared it with three alternative estimators using\nextensive Monte Carlo simulations across four systems presented earlier.\nThe first comparator is an Oracle estimator. In statistics, an \u201coracle\u201d of-\nten refers to a method that has access to privileged information unavailable in\npractice, providing a theoretical performance upper bound. Here, we define the\noracle as having access to the true realizations of intrinsic noise. Biologically ,\nthis corresponds to an idealized observer capable of directly tracking random\nfluctuations in molecular concentrations or reaction events\u2014information hidden\nin real experiments. The oracle thus obtains noiseless measurements of g(x)w(t)\nin (1) and estimates the noise profile using T rine\u2019s third stage, providing a strong\nupper bound on the performance achievable by any data-driven method.\nThe second comparator is a classical literature method based on a log-normal\nGaussian process prior, commonly used to model state-dependent variance [32].\nSpecifically , we consider Most Likely Heteroscedastic Gaussian Process regres-\nsion (MLH-GP ) [41], which iteratively estimates the mean function (deter-\nministic system part) and the state-dependent variance profile. This serves\nas a well-established baseline for nonparametric modeling of state-dependent\nnoise, as demonstrated by its continued study and applications in recent years\n[13, 11, 10, 24]. However, like other heteroscedastic noise methods, it does not\nexplicitly account for observational noise. W e therefore implemented an ex-\ntended version incorporating known measurement noise variances during mean\nand variance estimation.\nThe third comparator is denoted by T rineu, a T rine variant where the third\nstage\u2014estimating the intrinsic noise variance\u2014relies directly on intrinsic noise\nestimates from the GP model in the first stage, bypassing the structured kernel\nof the second stage (the superscript ustands for unstructured ). This comparison\nhighlights the value of the custom-designed kernel and quantifies the resulting\nperformance gains.\nF or each of the four systems, we performed a Monte Carlo study with 2000\nruns. In each run, the measurement noise ein (2) varied, covering scenarios\nfrom very low to high noise. Accuracy of the estimated profile \u02c6grelative to the\n11\ntrue profile gwas quantified using the Fit measure:\nFit= 100 \n1\u2212\u2225g\u2212\u02c6g\u2225\n\u2225g\u2225!\n,\nwhere 100 indicates perfect reconstruction.\nT able 1 reports the average fits returned by Oracle, T rine, T rineuand MLH-\nGP . Detailed results for T oggle Switch and FitzHugh\u2013Nagumo, the two most\ncomplex systems, are shown in Fig. 6, which presents boxplots of the 2000 Fit\nvalues for each estimator. T rine clearly outperforms state-of-the-art approaches\nsuch as MLH-GP , demonstrating that modeling intrinsic noise with a smooth\nGaussian prior is more effective than log-normal formulations, which are heavy-\ntailed, sensitive to measurement noise, and prone to overfitting. Additionally ,\nthe computational cost of T rine is much lower than that of MLH-GP , because\nthe latter is an iterative method, whereas T rine provides solutions by solving\nonly three subproblems. Even more importantly , T rine\u2019s performance is very\nclose to that of the Oracle. As shown in T able 1, the average Fit of the Oracle\nacross the four systems was around 91% . In comparison, T rine achieved average\nFits that were only a few percentage points below the Oracle, with at most a\nthree percentage point difference across all systems. Comparison with T rineu\nfurther emphasizes that achieving performance close to the Oracle critically\ndepends on the structured kernel in T rine\u2019s second stage. This kernel allows\nregularization and reliable estimation of intrinsic noise by capturing both the\nwhite-noise characteristics and the smooth state-dependent variance.\nT able 1: A verage Fit (%) obtained by each estimator across the four examples\nEstimator Ricker Self promoter T oggle Switch FitzHugh\u2013Nagumo\nOracle 93.6 88.1 92.2 91.6\nT rine 91.1 87.2 89.1 90.3\nT rineu82.2 78.1 72.6 79.3\nMLH-GP 78.7 61.8 67.1 61.4\nConclusions\nThe T rine algorithm introduces a flexible yet rigorously designed framework\nfor disentangling intrinsic noise in stochastic dynamical systems. By integrat-\ning nonparametric kernel methods within a modular three-phase design, T rine\naddresses the core challenge of estimating state-dependent noise profiles with-\nout restrictive parametric assumptions and remains robust even under limited\nor noisy data. Beyond accurate estimation, T rine provides a clear and inter-\npretable view of the stochastic mechanisms shaping system dynamics, particu-\nlarly in biological and ecological contexts, where noise is a meaningful functional\ncomponent rather than a mere perturbation. This interpretability makes T rine\n12\nnot only a predictive tool but also a valuable instrument for discovery , revealing\nwhich molecular or dynamical variables modulate stochasticity and uncovering\nfunctional symmetries or nonlinear dependencies often obscured by experimen-\ntal noise.\nEstimating state-dependent variance is fundamental because it links statis-\ntical inference to biological meaning. Intrinsic noise is unavoidable in cellular\nsystems. Quantifying how its variability depends on the system state provides\na direct measure of robustness and sensitivity and allows both modelers and\nbiologists to understand which regions of the state space are most influenced by\nstochastic fluctuations. Notably , recent control-theoretic advances in synthetic\nbiology have shown that stochastic variability can itself be a design target, with\nfeedback architectures capable of modulating or even reducing the stationary\nvariance of biochemical networks (e.g., antithetic integral feedback [14]). In this\ncontext, reconstructing the full state-dependent diffusion profile reveals natural\ntargets for noise-aware control or intervention strategies.\nT o assess T rine\u2019s contribution to the field, we conducted an extensive bench-\nmarking analysis against other state-of-the-art approaches, in particular [41],\nwhich serves as a strong baseline in this area. These tests demonstrated T rine\u2019s\nsuperiority not only in reconstruction accuracy but also in robustness to mea-\nsurement noise and uncertainty in system states. T o further quantify this ad-\nvancement, we introduced an \u201coracle\u201d comparator\u2014a theoretical benchmark\nrepresenting the ideal upper bound of performance. In statistics, an oracle\nestimator has access to privileged information typically hidden in real exper-\niments. Our oracle is envisioned as an idealized observer that can measure\nintrinsic noise realizations without error, obtaining information inaccessible in\nactual experiments. Remarkably , despite being entirely data-driven and lack-\ning any privileged information, T rine achieves performance close to the oracle,\nnarrowing the divide between theoretical stochastic modeling and experimen-\ntal observability . The structured kernel introduced in T rine\u2019s second phase is\nessential for this performance, simultaneously enforcing smooth dependence of\nintrinsic noise variance on the state and modeling sharp discontinuities in its\nrealizations.\nAcross benchmark systems, T rine consistently uncovered biologically mean-\ningful stochastic structures. In the genetic toggle switch, it identified regions of\nthe state space most sensitive to noise-driven transitions, illustrating how intrin-\nsic fluctuations shape the landscape of cell-fate decisions. Similar insights were\nobserved in the self-promoter, FitzHugh\u2013Nagumo, and Ricker models, where\nT rine successfully recovered characteristic variance profiles despite strong non-\nlinearities and uneven sampling. Collectively , these four paradigmatic systems\nrepresent key classes of noisy biological dynamics\u2014feedback regulation, bista-\nbility/excitability , oscillations, and stochastic population growth\u2014highlighting\nboth the biological significance and the broad applicability of T rine.\nLooking ahead, several avenues exist for further development of the frame-\nwork. One promising direction is integrating T rine with experimental design\nstrategies to optimize data collection for noise estimation, enhancing both ac-\ncuracy and e\ufb00iciency . Additionally , coupling T rine with real-time inference\n13\nor adaptive control methods could open new possibilities for monitoring and\nsteering stochastic systems in both experimental and applied settings. More-\nover, the principles underlying T rine extend far beyond biological and ecological\nmodeling. Any domain where state-dependent noise is relevant\u2014including neu-\nroscience, financial modeling, and complex engineering systems\u2014can benefit\nfrom this framework.\nAppendix\nT rine algorithm\nT o introduce T rine, it is first useful to establish some preliminary notation. W e\ndefine a delayed version of the measured states as\nzk:=yk+1, k = 1, . . . , N \u22121.\nThis allows the dataset to be reformulated as a collection of N\u22121training pairs\n{(yk, zk)}N\u22121\nk=1, where each pair corresponds to a fixed time shift. W e assume\nthat the time series {yk}is uniformly sampled, which ensures consistency in\nthe temporal spacing \u2206tacross all training examples. This regularity is crucial\nfor the algorithm to properly exploit the temporal structure of the data. In\ncontinuous time, the following approximation, related to the Euler\u2013Maruyama\nmethod [42], holds:\nxk+1\u2248xk+ \u2206 tf(xk) +p\n\u2206tg(xk)wk\nwhere wkare independent random variables with zero mean and unit variance.\nIt follows that, after training, T rine provides estimates for the deterministic\npart F(xk) :=xk+ \u2206 tf(xk)and the stochastic part G(xk) :=\u221a\u2206tg(xk). Once\nestimates of F andG are obtained, the corresponding fandgcan be directly\nrecovered. The notation F andG is adopted in the pseudo-code reported below,\nbut was not used in Fig. 1, in order to avoid overloading the notation and keep\nthe earlier presentation lighter.\nIt is also convenient to define the vector\nz= [z1, . . . , z N\u22121]\u22a4.\nIn addition, given a kernel function K, we define the associated kernel matrix\nK\u2208R(N\u22121)\u00d7(N\u22121)as\nKij=K(yi, yj).\nThe kernel can also be enriched by including a so-called bias space [55], for\nexample by adding constant or linear terms to capture trends in the data. These\ncomponents are omitted here for notational simplicity , but their inclusion in the\nprocedure is straightforward.\nThe pseudo-code of T rine is reported below.\n14\nT rine pseudocode\nInput: noisy states ykand covariance matrix \u03a3eof the output noise.\nInput hyperparameters used in the step:\nStep 1: \u03bbf, \u2113f, \u03c1n\nStep 2: \u03bbf, \u2113f, \u03bbw, \u2113w\nStep 3: \u03bbg, \u2113g, \u03c1g\n1: Step 1: Estimate intrinsic noise signs using a smooth kernel for\nthe deterministic part F and assuming intrinsic noise of constant\nvariance\n2: Introduce the Gaussian kernel as\nKf(x, x\u2032) =\u03bbfexp\u0012\n\u2212\u2225x\u2212x\u2032\u22252\n2\u2113f\u0013\n3: Compute kernel matrix Kfusing Kfevaluated on noisy states yk\nKf\u2208R(N\u22121)\u00d7(N\u22121),(Kf)ij=Kf(yi, yj)\n4: Compute estimates of intrinsic noise signs:\ns= sign\u0010\n(Kf+ \u03a3 e+\u03c1nI)\u22121z\u0011\n5: Build the sign matrix:\nS= diag( s)\n6: Step 2: Estimate the intrinsic noise realizations using sign-informed\nkernel\n7: Introduce the following Gaussian kernel matrix:\nG\u2208R(N\u22121)\u00d7(N\u22121),Gij=\u03bbwexp\u0012\n\u2212\u2225yi\u2212yj\u22252\n2\u2113w\u0013\n8: Define the following matrix that will serve as a correction factor for G:\nQ=\u03b2211\u22a4+\u0000\n1\u2212\u03b22\u0001\nI\nwhere 1\u2208RN\u22121is the column vector of ones, Iis the identity matrix and\n\u03b2=q\n2\n\u03c0for the intrinsic Gaussian noise case.\n9: Define the structured kernel matrix modeling intrinsic noise realizations as\ncombination of G,Q and the sign matrix S:\nKgw=S\u00b7(G\u25e6Q)\u00b7S\nwhere \u25e6denotes the Hadamard (element-wise) product.\n10: Compute the estimates of the intrinsic noise realizations:\n\u02c6n=Kgw\u00b7(Kf+ \u03a3 e+Kgw)\u22121\u00b7z\n15\n11: Step 3: Estimate the intrinsic noise standard deviation\n12: Define the following Gaussian kernel matrix to model G:\nKg\u2208R(N\u22121)\u00d7(N\u22121),(Kg)ij=\u03bbgexp\u0012\n\u2212\u2225yi\u2212yj\u22252\n2\u2113g\u0013\n13: Define the weights\n\u02c6c= (Kg+\u03c1gI)\u22121\u00b7|\u02c6n|\n\u03b2\n14: Compute the SD profile as:\nbG=Kg\u02c6c\nOutput: estimated intrinsic noise SD vector bG computed over the ykand\nweight vector \u02c6cwhich, for any x, permits to calculate the SD estimate as\nfollows\n\u02c6G(x) =N\u22121X\nk=1\u02c6ck\u03bbgexp\u0012\n\u2212\u2225x\u2212yk\u22252\n2\u2113g\u0013\n.\nAs explained in detail in Appendix, each stage of T rine estimates the un-\nknown functions f,g, and g\u00b7w within the subspace spanned by the kernel\nsections K(yi,\u00b7), where yiare the observed (noisy) states. This construction,\ngrounded in the representer theorem [67], reflects the nonparametric nature of\nthe method: the number of basis functions is not fixed in advance, but in-\ncreases with the amount of data and can become infinite in the limit of dense\nsampling. The associated expansion coe\ufb00icients are computed by solving regu-\nlarized quadratic optimization problems, where the regularization is implicitly\ndefined by the choice of kernel [61].\nThe initial part of the pseudo-code specifies the hyperparameters adopted at\neach stage of the algorithm. These parameters are essential for regularization,\nas they govern the trade-off between fidelity to the empirical data and adher-\nence to the structural priors encoded in the kernel. A distinctive feature of the\nT rine methodology is its systematic formulation of linear estimation subprob-\nlems. This formulation permits the closed-form evaluation of model selection\ncriteria such as the Prediction Error Sum of Squares (PRESS), the Generalized\nCross-V alidation (GCV) score [33] or empirical Bayes methods [55], thereby\nenabling computationally e\ufb00icient and theoretically principled hyperparameter\noptimization. Specifically , in the present work, hyperparameters are consis-\ntently estimated via maximization of the marginal likelihood\u2014also referred to\nas Bayesian evidence\u2014a widely adopted approach in probabilistic modeling.\nThis criterion quantifies the explanatory adequacy of the model with respect\nto the observed data while inherently incorporating the Occam\u2019s razor princi-\nple [47].\nThe parameter \u03b2is used in specific steps of the algorithm to define certain\nmatrices and vectors. The valuep\n2/\u03c0 is calibrated for the case of Gaussian\nintrinsic noise. As detailed in Appendix, the constant can be modified if the\n16\nnoise is known to follow different statistical distributions, such as Laplacian or\nStudent\u2019s t-distributions.\nThe final stage of the algorithm produces an unconstrained estimate of the\ndiffusion component g. In our experiments, we did not impose any additional\nconstraints. However, since the optimization problem in Step 3, which esti-\nmates the intrinsic noise variance, is quadratic, the framework can easily be\nextended to include structural constraints on g, such as nonnegativity , concav-\nity , or monotonicity . These properties can be enforced by applying inequality\nconstraints to discrete-time derivatives of any order, resulting in convex opti-\nmization problems that standard numerical solvers can e\ufb00iciently handle.\nThe next section provides a detailed step-by-step derivation of the algorithm,\nalong with further theoretical insights into its performance and practical aspects.\nMathematical Derivation of the T rine Algorithm\nF or completeness, the pseudocode for T rine is reproduced below. It matches the\nversion provided in Materials and Methods.\nT rine algorithm\nInput: noisy states ykand covariance matrix \u03a3eof the output noise\nInput hyperparameters used in the step:\nStep 1: \u03bbf, \u2113f, \u03c1n\nStep 2: \u03bbf, \u2113f, \u03bbw, \u2113w\nStep 3: \u03bbg, \u2113g, \u03c1g\n1: Step 1: Estimate intrinsic noise signs using a smooth kernel for\nthe deterministic part F and assuming intrinsic noise of constant\nvariance\n2: Introduce the Gaussian kernel as\nKf(x, x\u2032) =\u03bbfexp\u0012\n\u2212\u2225x\u2212x\u2032\u22252\n2\u2113f\u0013\n3: Compute kernel matrix Kfusing Kfevaluated on noisy states yk\nKf\u2208R(N\u22121)\u00d7(N\u22121),(Kf)ij=Kf(yi, yj)\n4: Compute estimates of intrinsic noise signs:\ns= sign\u0010\n(Kf+ \u03a3 e+\u03c1nI)\u22121z\u0011\n5: Build the sign matrix:\nS= diag( s)\n6: Step 2: Estimate the intrinsic noise realizations using sign-informed\nkernel\n7: Introduce the following Gaussian kernel matrix:\nG\u2208R(N\u22121)\u00d7(N\u22121),Gij=\u03bbwexp\u0012\n\u2212\u2225yi\u2212yj\u22252\n2\u2113w\u0013\n17\n8: Define the following matrix that will serve as a correction factor for G:\nQ=\u03b2211\u22a4+\u0000\n1\u2212\u03b22\u0001\nI\nwhere 1\u2208RN\u22121is the column vector of ones, \u03b2=E[|wk|]andIis the\nidentity matrix.\n9: Define the structured kernel matrix modeling intrinsic noise realizations as\ncombination of G,Q and the sign matrix S:\nKgw=S\u00b7(G\u25e6Q)\u00b7S\nwhere \u25e6denotes the Hadamard (element-wise) product.\n10: Compute the estimates of the intrinsic noise realizations:\n\u02c6n=Kgw\u00b7(Kf+ \u03a3 e+Kgw)\u22121\u00b7z\n11: Step 3: Estimate the intrinsic noise standard deviation\n12: Define the following Gaussian kernel matrix to model G:\nKg\u2208R(N\u22121)\u00d7(N\u22121),(Kg)ij=\u03bbgexp\u0012\n\u2212\u2225yi\u2212yj\u22252\n2\u2113g\u0013\n13: Define the weights\n\u02c6c= (Kg+\u03c1gI)\u22121\u00b7|\u02c6n|\n\u03b2\n14: Compute the SD profile as:\nbG=Kg\u02c6c\nOutput: estimated intrinsic noise SD vector bG computed over the ykand\nweight vector \u02c6cwhich, for any x, permits to calculate\n\u02c6G(x) =N\u22121X\nk=1\u02c6ck\u03bbgexp\u0012\n\u2212\u2225x\u2212yk\u22252\n2\u2113g\u0013\n.\nThe following sections are devoted to deriving the three stages of T rine\nthrough regularization theory in Reproducing Kernel Hilbert Spaces (RKHS),\nalongside its Bayesian interpretation.\nStage 1: Sign Estimation of the Intrinsic Noise\nW e begin by reformulating the problem in a supervised learning framework.\nThe available input\u2013output training data consist of delayed state pairs:\nzk:=yk+1, with training pairs (yk, zk), k = 1, . . . , N \u22121.\nThis formulation allows us to learn a map from the observed inputs yk(which\napproximate the true states xk) to the corresponding outputs zk.\n18\nIn particular, we consider the continuous-time system dynamics and approx-\nimate them in discrete time over the measurement sampling interval \u2206t:\nxk+1\u2248xk+ \u2206 tf(xk) +p\n\u2206tg(xk)wk,\nwhere \u2206tdenotes the sampling period at which the noisy states zkare observed.\nW e then define the functions\nF(x) :=x+ \u2206 tf(x), G (x) :=p\n\u2206tg(x),\nso that the model can be written as\nzk\u2248F(xk) +G(xk)wk+ek. (7)\nT o estimate the signs of the intrinsic noise components wk, which enter\nadditively in the dynamics, we introduce two kernel models for F andG, and\nperform regularized regression in Reproducing Kernel Hilbert Spaces (RKHSs).\nW e assume only that F is smooth, and associate to it a Gaussian kernel:\nKf(x, x\u2032) =\u03bbfexp \n\u2212\u2225x\u2212x\u2032\u22252\n2\u21132\nf!\n.\nGiven that we only observe noisy approximations of the true states, the entries\nxkofF, G in (7) are replaced with yk. Using regularization in RKHS, inspired\nby the representer theorem [67] we assume that F lies in the subspace of the\nRKHS H spanned by the kernel sections K(yi,\u00b7). Specifically , we write\nF(\u00b7) =N\u22121X\ni=1\u03b1iKf(yi,\u00b7),\nwhich implies that the squared RKHS norm of F, which serves as a regulariza-\ntion term, is given by\n\u2225F\u22252\nH=\u03b1\u22a4Kf\u03b1,\nwhere Kfis the kernel matrix defined as\n(Kf)ij=Kf(yi, yj),Kf\u2208R(N\u22121)\u00d7(N\u22121),\ne.g., see [61]. As for the stochastic component, at this stage of the algorithm\nwe lack su\ufb00icient information to define a structured kernel. Since our goal\nhere is only to estimate the intrinsic noise signs, the Bayesian interpretation\nof regularization in RKHSs\u2014where kernels represent the covariance functions\nof Gaussian processes\u2014suggests using a diagonal kernel with constant variance\n\u03c1n. This choice simply reflects the white noise nature of the intrinsic noise; it\nwill be the task of the subsequent stages to refine \u03c1nby deriving a functional\nrelationship between the variance and the state x.\nBy the representer theorem, the estimate of the intrinsic noise within the RKHS\ncan also be written as a linear combination of kernel functions centered at the\n19\ndata points. In this specific case, since the kernel is diagonal with constant\nvalue \u03c1n, the estimate reduces simply to the product between the scalar \u03c1nand\nthe unknown coe\ufb00icient vector c\u2208RN\u22121. Hence, using (7), we can express the\nmeasurement model as\nz=Kf\u03b1+\u03c1nc+E, (8)\nwhere the (N\u22121)-dimensional random vector E collects the noise terms ekand\nhas covariance \u03a3e. The vectors \u03b1andcare unknown coe\ufb00icients, whose estima-\ntion enables the recovery of the function F and the intrinsic noise, respectively .\nTheir regularized estimates are obtained by solving the optimization problem\n(\u02c6\u03b1,\u02c6c) = arg min\n\u03b1,c(z\u2212Kf\u03b1\u2212\u03c1nc)\u22a4\u03a3\u22121\ne(z\u2212Kf\u03b1\u2212\u03c1nc) +\u03b1\u22a4Kf\u03b1+\u03c1nc\u22a4c.\nOne has\n\u02c6\u03b1= \u02c6c= (Kf+ \u03a3 e+\u03c1nI)\u22121z,\nwhich correspond to the weights of a regularization network [61]. In fact, the\noptimality conditions are\nKf\u03a3\u22121\ne(z\u2212Kf\u02c6\u03b1\u2212\u03c1n\u02c6c) =Kf\u02c6\u03b1,\n\u03c1n\u03a3\u22121\ne(z\u2212Kf\u02c6\u03b1\u2212\u03c1n\u02c6c) =\u03c1n\u02c6c.\nSubstituting \u02c6\u03b1= \u02c6c, both reduce to\n(Kf+\u03c1nI)\u03a3\u22121\ne(z\u2212(Kf+\u03c1nI)\u02c6c) = (Kf+\u03c1nI)\u02c6c.\nand multiplying both sides by (Kf+\u03c1nI)\u22121and then by \u03a3e, we obtain\n(Kf+ \u03a3 e+\u03c1nI)\u02c6c=z,\nwhich is satisfied by the definition of \u02c6c.\nThe estimates of the intrinsic noise realizations are thus given by\n\u03c1n(Kf+ \u03a3 e+\u03c1nI)\u22121z, (9)\nwhich explains the structure of the sign estimator appearing on line 4 of the\nT rine pseudocode.\nStage 2: Intrinsic Noise Estimation using a structured kernel\nThe sign estimates obtained in Stage 1 are now employed to construct a struc-\ntured kernel that models the realizations of the intrinsic noise. The incorpora-\ntion of sign information is crucial in order to obtain a non-diagonal kernel to\ndescribe the intrinsic noise standard deviation, allowing us to capture its smooth\ndependence on the state value.\nRecall that G(x) =\u221a\u2206tg(x), so that the transition noise between time steps k\nandk+ 1 can be expressed as:\nsign( wk)G(xk)|wk|,\n20\nwhere sign( wk)is set to the estimate obtained in the previous stage.\nT o capture the smooth variability of the function G, we assign it a Gaussian\nkernel:\nKg(xa, xb) =\u03bbwexp\u0012\n\u2212\u2225xa\u2212xb\u22252\n2\u2113w\u0013\n.\nT o obtain the overall kernel for the intrinsic noise, we exploit the Bayesian\ninterpretation of regularization in RKHS. The function G is seen as a zero-\nmean Gaussian random field of covariance Kg, independent of wk. Hence, the\ncovariance of the random process sign( w)G(x)|w|, under the assumption of\nknown signs, is\nCov (sign( wa)G(xa)|wa|,sign( wb)G(xb)|wb|) =(\nKg(xa, xa) ifa=b\nsign( wa)sign( wb)Kg(xa, xb)\u03b22ifa\u0338=b\nHere, \u03b2=E[|wk|]captures the expected magnitude of the intrinsic noise.\nNote that for standard normal noise, \u03b2=p\n2/\u03c0 .\nF ollowing the same principle as in Stage 1, we assume that the intrinsic noise\nrealizations lie in the subspace spanned by the kernel sections centered at the\nobserved noisy states yk. In practice, this means that the input locations used\nfor learning are the noisy estimates yk, rather than the true but unobservable\nstates xk. F rom the covariance expression above, we recover the structured\nkernel matrix Kgw used in the pseudocode (line 9), which incorporates both\nthe Gaussian kernel and the sign information. The deterministic part F is still\nassigned the Gaussian kernel Kf. This completes our kernel model and, recalling\n(7), it follows that:\nz=Kf\u03b1+Kgwc+E,\nwhere E is the vector of output noise.\nW e estimate \u03b1andcby solving the regularized least squares problem:\n(\u02c6\u03b1,\u02c6c) = arg min\n\u03b1,c(z\u2212Kf\u03b1\u2212Kgwc)\u22a4\u03a3\u22121\ne(z\u2212Kf\u03b1\u2212Kgwc) +\u03b1\u22a4Kf\u03b1+c\u22a4Kgwc,\nwhich has the closed-form solution:\n\u02c6\u03b1= \u02c6c= (Kf+ \u03a3 e+Kgw)\u22121z.\nThis leads to the estimate of the intrinsic noise realizations:\n\u02c6n=Kgw\u00b7(Kf+ \u03a3 e+Kgw)\u22121\u00b7z, (10)\nas given in line 10 of the T rine pseudocode.\nFinally , one may optionally include a constant mean term \u00b5in the model\nofG(x). Since sign( wk)is assumed known, the process sign( wk)G(xk)|wk|has\na mean vector given by s\u25e6(\u00b5\u03b2), where s= [sign( w1), . . . , sign( wN\u22121)]\u22a4, the\nsymbol \u25e6denotes the Hadamard (element-wise) product and \u03b2=E[|wk|]. The\nkernel matrix Kgw is corrected by an additional diagonal term \u00b52(1\u2212\u03b22)Ito\naccount for the increased variance. The estimate then becomes:\n\u02c6n=s\u25e6(\u00b5\u03b2)+\u0000\nKgw+\u00b52(1\u2212\u03b22)I\u0001\u0000\nKf+ \u03a3 e+Kgw+\u00b52(1\u2212\u03b22)I\u0001\u22121(z\u2212s\u25e6(\u00b5\u03b2)).\n21\nStage 3: Intrinsic Noise V ariance Estimation\nAt this stage, we aim to estimate the profile of the intrinsic noise standard\ndeviation function, denoted G(x). T o motivate the estimation strategy , con-\nsider that for a fixed value of x(t), the random variable G(x)|wt|represents the\nabsolute value of the intrinsic noise component at that point. Since the noise\nvariable wthas zero mean and unit variance, its absolute value has expected\nvalue \u03b2=E[|wt|]. Therefore, the expectation of G(x)|wt|is simply G(x)\u03b2.\nF rom Stage 2 of the algorithm, we obtain the vector \u02c6n reported in (10),\nwhich estimates the realizations of the intrinsic noise. By taking its absolute\nvalue and dividing by \u03b2, we construct the vector |\u02c6n|/\u03b2 , which can be interpreted\nas a pointwise estimate of the standard deviation profile G at the noisy states\nyk.\nT o recover a smooth function G(x), we perform a regularized kernel regres-\nsion. W e associate to Gthe Gaussian kernel Kgand use the corresponding kernel\nmatrix Kg\u2208R(N\u22121)\u00d7(N\u22121), as defined in line 12 of the T rine pseudocode. Given\nthe regularization parameter \u03c1g, we solve the following regularized least squares\nproblem:\n\u02c6c= arg min\nc\r\r\r\r|\u02c6n|\n\u03b2\u2212Kgc\r\r\r\r2\n+\u03c1gc\u22a4Kgc\nThis yields the closed-form solution:\n\u02c6c= (Kg+\u03c1gI)\u22121\u00b7|\u02c6n|\n\u03b2\nand the estimates of the standard deviations at the ykare contained in the\nvector\nbG=Kg\u02c6c. (11)\nThe estimate at a generic xis then given by the sum of kernel sections centred\nat the yk:\n\u02c6G(x) =N\u22121X\nk=1\u02c6ck\u03bbgexp\u0012\n\u2212\u2225x\u2212yk\u22252\n2\u2113g\u0013\nThe three expressions above correspond to those reported in lines 13 and 14 of\nthe T rine pseudocode.\nSince the entries of |\u02c6n|/\u03b2 are nonnegative by construction, the components\nofbG are typically nonnegative. Indeed, in all simulations presented in the main\ntext, we observed that such estimated profile did not require explicit constraints.\nNevertheless, since the regression problem is a convex quadratic program, it is\nstraightforward to incorporate additional linear constraints if desired. In par-\nticular, any linear constraint on the vector q=Kg\u02c6c\u2014which corresponds to the\nestimated values of G at the training points yk\u2014can be added while preserving\nconvexity . F or example, enforcing the positivity of first-order discrete differences\nofqpromotes monotonicity , while requiring the negativity of second-order dif-\nferences enforces concavity . These constraints can encode soft, high-level prior\nknowledge provided by the user, and can be seamlessly incorporated into the\n22\nestimation process using standard convex optimization techniques. Constraints\ncan also be imposed on locations different from the training points ykby ex-\ntending the definition of the estimated profile qto an arbitrary evaluation grid.\nIn this setting, qis expressed as q=K gridc, where K grid is the kernel matrix\nevaluating the Gaussian kernel between the evaluation grid points and the orig-\ninal data points. T o ensure consistency with the observed data, a selection\nmatrix M is introduced, which extracts the components of qcorresponding to\nthe training points yk. Linear constraints can then be applied directly to the\nextended vector q, while the resulting optimization problem remains a convex\nquadratic program that can be e\ufb00iciently solved using standard methods.\nFinally , recall that in real applications the hyperparameters \u03bbf, \u2113f, \u03c1nin the\nfirst stage, \u03bbf, \u2113f, \u03bbw, \u2113w in the second stage, and \u03bbg, \u2113g, \u03c1gin the third stage\nare unknown. Since at each step of T rine the problem reduces to regularized\nleast squares in RKHS, standard criteria such as GCV, PRESS, or marginal\nlikelihood optimization can be employed for their estimation [35, 55]. In all\nthe experiments, the latter approach has been adopted, as implemented in the\nStatistics and Machine Learning Toolbox for Matlab.\nIntrinsic noise estimation: role of sign knowledge\nIn this section, we investigate how intrinsic noise estimation may benefit from\nknowing the signs of the intrinsic noise, as estimated in the first stage of T rine.\nTheoretical findings will also help interpret more effectively the results from a\nMonte Carlo study of the toggle switch example, presented in the next section.\nConsider a simplified setting where the drift fis either absent or assumed\nto be known. The intrinsic noise standard deviation is modeled as the discrete-\ntime stochastic process gk+\u00b5, indexed by k= 1,2, . . . . Here, \u00b5is a positive\nscalar, and first- and second-order moments of gkfollows those of the first-order\nautoregressive process (AR(1)):\ngk+1=agk+vk, k = 1,2, . . .\nwhere:\n\u2022vkis white noise (all the random variables are mutually independent) with\nzero mean and variance \u21132;\n\u20220< a < 1, so that for large k, the process gkreaches stationarity with\nzero mean and variance\nVar(gk) =\u03b32:=\u21132\n1\u2212a2,Cov( gk, gj) =\u03b32a|k\u2212j|.\nThe AR(1) process is simple yet significant. Its parameter aplays a role similar\nto the kernel width in a Gaussian kernel, as it determines the correlation be-\ntween samples, thus controlling the smoothness of the intrinsic noise SD profile.\n23\nAs shown below, this structure allows for the derivation of closed-form expres-\nsions for the estimation performance.\nThe intrinsic noise is then given by (gk+\u00b5)wk, where wkis a white noise\nprocess, independent of gk, with zero mean and unit variance:\nE[wk] = 0,E[w2\nk] = 1,E[|wk|] :=\u03b2\u22650.\nW e consider two possible scenarios for the estimation task. In the first\nscenario, the signs of the intrinsic noise are unknown, and we define:\nzu\nk= (gk+\u00b5)wk, k = 1,2, . . . .\nThe measured output process yu\nkis:\nyu\nk=zu\nk+ek, k = 1,2, . . .\nwhere ekis white noise with variance \u03c32.\nIn the second scenario, the signs are assumed to be known:\nzk= (gk+\u00b5)|wk|, k = 1,2, . . .\nand\nyk=zk+ek, k = 1,2, . . .\nThe processes gk,wk, and ekare all mutually independent.\nOur goal is to compute and compare the mean square error (MSE) of the op-\ntimal linear estimators of zu\nkandzk, based on all output measurements available\nup to time k, in the steady-state regime (i.e., for large k). Note that the case\nwith known signs corresponds to the second stage of T rine, and that accurate\nestimates of zuorz(here representing the intrinsic noise) are crucial for the\nsuccessful implementation of T rine third stage (which estimates the variance\nprofile exploiting the intrinsic noise estimates).\nEstimation of zu\nk\nF or the stochastic processes\nzu\nk= (gk+\u00b5)wk, yu\nk=zu\nk+ek,\nfor large k, simple considerations exploiting the independence among g,w, and\ne, as well as the whiteness of w ande, lead to:\nE[zu\nk] = 0,\nCov( zu\nk, yu\nk) = Var( zu\nk) =\u03b32+\u00b52,\nVar(yu\nk) =\u03b32+\u00b52+\u03c32,\n24\nand\nCov( zu\nk, yu\nj) = Cov( zu\nk, zu\nj) = 0 , fork\u0338=j.\nIt follows that the optimal linear estimator of zu\nkdepends solely on yu\nk, since\nall previous measurements {yu\ni}k\u22121\ni=1 are uncorrelated with zu\nk[2]. This implies\nthat the estimator cannot exploit any correlation or smoothness structure in the\nstandard deviation profile.\nThe optimal estimator is therefore:\n\u02c6zu\nk=Cov( zu\nk, yu\nk)\nVar(yu\nk)yu\nk=\u03b32+\u00b52\n\u03b32+\u00b52+\u03c32yu\nk.\nThe MSE of this estimator is\nMSE 1= Var( zu\nk\u2212\u02c6zu\nk) = Var( zu\nk)\u2212Cov( zu\nk, yu\nk)2\nVar(yu\nk)= (\u03b32+\u00b52)\u2212(\u03b32+\u00b52)2\n\u03b32+\u00b52+\u03c32.\nThis expression can be equivalently rewritten as:\nMSE 1=(\u03b32+\u00b52)\u03c32\n\u03b32+\u00b52+\u03c32.\nEstimation of zk\nThe derivation of the MSE for the linear estimator of zk, based on the set\n{yi}k\ni=1 , is significantly more involved. Some preliminary steps are necessary .\nThe key idea is to construct a state-space model that shares the same first- and\nsecond-order moments as zkandyk. W e then apply Kalman filtering techniques\nto derive a closed-form expression for the asymptotic filtered covariance, which\ncorresponds to the desired MSE [2].\nFirst- and second-order moments of zk\nSimple calculations yield the following expressions for the mean and variance of\nzk:\nE[zk] = E[(gk+\u00b5)|wk|]\n=E[gk]\u00b7E[|wk|] +\u00b5\u00b7E[|wk|] ( independence )\n=\u00b5\u03b2,\nVar(zk) = E[z2\nk]\u2212(E[zk])2\n=E[(gk+\u00b5)2]\u00b7E[w2\nk]\u2212\u00b52\u03b22\n= ( \u03b32+\u00b52)\u2212\u00b52\u03b22\n=\u03b32+\u00b52(1\u2212\u03b22).\n25\nThe covariance between zkandzjfork\u0338=jis given by:\nCov( zk, zj) = E[zkzj]\u2212E[zk]E[zj]\n= (E[gkgj] +\u00b52)\u00b7\u03b22\u2212\u00b52\u03b22\n= (Cov( gk, gj) +\u00b52)\u03b22\u2212\u00b52\u03b22\n=\u03b22\u03b32a|k\u2212j|.\nT wo stochastic processes with the same moments as zkand yk\nConsider the process\n\u02dczk=m+qk+\u03b5k, (12)\nwhere:\n\u2022m=\u00b5\u03b2 is a constant mean term;\n\u2022qkis an AR(1) process:\nqk+1=aqk+\u03b7k,\nwith innovation variance\nVar(\u03b7k) =\u03c32\n\u03b7=\u03b22\u21132,\nso that the stationary variance of qkis\nVar(qk) =\u03b22\u03b32;\n\u2022\u03b5kis white noise, independent of qk, with variance\n\u03c32\n\u03b5= (\u03b32+\u00b52)(1\u2212\u03b22).\nW e now show that \u02dczkmatches the first- and second-order moments of zk.\nSince both qkand\u03b5kare zero-mean, we have:\nE[\u02dczk] =m=\u00b5\u03b2=E[zk].\nF or the covariance between distinct time indices k\u0338=j:\nCov(\u02dc zk,\u02dczj) = Cov( qk, qj) =\u03b22\u03b32a|k\u2212j|= Cov( zk, zj).\nFinally , for the variance:\nVar(\u02dczk) = Var( qk) + Var( \u03b5k)\n=\u03b22\u03b32+ (\u03b32+\u00b52)(1\u2212\u03b22)\n=\u03b32+\u00b52(1\u2212\u03b22)\n= Var( zk).\nW e also define the corresponding output process as\n\u02dcyk=m+qk+nk,\n26\nwhere\nm=\u00b5\u03b2, n k=\u03b5k+ek.\nThe process nkis white and independent of qk, with variance\nVar(nk) =\u03c32\nn\n= Var( \u03b5k) + Var( ek)\n= (\u03b32+\u00b52)(1\u2212\u03b22) +\u03c32.\nIt follows that the process \u02dcykhas the same first- and second-order moments\nas the original process ykdefined from zk. Moreover, the cross-covariances\nbetween the stochastic processes \u02dczkand \u02dcykexactly match those between zkand\nyk. Therefore, the processes \u02dczkand \u02dcykcan thus be used as surrogate processes\nfor MSE computation via linear filtering techniques [2].\nThe MSE of the optimal linear estimator of zk\nFirst, it is useful to consider the asymptotic prediction error of qk. In particular,\nwe now compute the stationary solution P of the Algebraic Riccati Equation\n(ARE), which gives the asymptotic prediction error variance of the Kalman\nfilter applied to the state-space model describing \u02dczkand \u02dcyk[2]. The ARE reads:\nP=a2P\u2212a2P2\nP+\u03c32n+\u03c32\n\u03b7\n=a2P\u2212a2P2\nP+ (\u03b32+\u00b52)(1\u2212\u03b22) +\u03c32+\u03b22\u21132.\nThe closed-form solution to this equation is:\nP=Q\u2212(1\u2212a2)R+p\n[(1\u2212a2)R\u2212Q]2+ 4QR\n2,\nwhere\nQ=\u03b22\u21132, R = (\u03b32+\u00b52)(1\u2212\u03b22) +\u03c32.\nThis can be written more explicitly as:\nP=1\n2[\n\u03b22\u21132\u2212(1\u2212a2)(\n(\u03b32+\u00b52)(1\u2212\u03b22) +\u03c32)\n+\u221a\n((1\u2212a2) ((\u03b32+\u00b52)(1\u2212\u03b22) +\u03c32)\u2212\u03b22\u21132)2+ 4\u03b22\u21132((\u03b32+\u00b52)(1\u2212\u03b22) +\u03c32)]\n(13)\nThis quantity P represents the steady-state prediction error variance affect-\ningqkasymptotically .\nConsider now the optimal linear estimator of zk= (gk+\u00b5)|wk|based on\nall the measurements yi=zi+eicollected up to instant k. Such estimator\nis unbiased and its MSE (variance) coincides with that of the optimal linear\nestimator of \u02dczk=m+qk+\u03b5kbased on \u02dcyi=m+qi+ni, i= 1, . . . , k , since\n27\nthese processes share the same first- and second-order moments. T o compute\nsuch MSE, recall that, at steady state, before observing the measurement \u02dcyk,\nthe prediction error variance of qk, which defines \u02dczkthrough (12), was denoted\nbyP. It follows that the variance of the prediction error of \u02dczkis:\nP+ Var( \u03b5k) =P+ (\u03b32+\u00b52)(1\u2212\u03b22).\nAfter receiving the new measurement\n\u02dcyk=m+qk+\u03b5k+ek= \u02dczk+ek,\nthe asymptotic variance of the estimation error of \u02dczkis\nMSE 2=\u00121\nP+ (\u03b32+\u00b52)(1\u2212\u03b22)+1\n\u03c32\u0013\u22121\n,\nwith P given in (13). Thus, this is also the MSE affecting asymptotically zkin\nterms of the original parameters: \u00b5(the asymptotic mean of gk),\u21132(variance of\nthe noise driving gk),\u03b2(the mean of |wk|),a(the AR(1) parameter regulating\nthe correlation among the gk),\u03c32(output noise variance), and the steady-state\nvariance of gk:\n\u03b32:=\u21132\n1\u2212a2.\nMSE comparison: role of signs knowledge\nW e are now in a position to understand how much improvement in intrinsic noise\nestimation can be obtained by exploiting knowledge of its signs. According to\nthe analysis performed in the previous sections, we need to investigate the ratio:\nr:=MSE 2\nMSE 1\nwhere\nMSE 1=(\u00b52+\u03b32)\u03c32\n\u00b52+\u03b32+\u03c32.\nand\nMSE 2=[P+ (\u00b52+\u03b32)(1\u2212\u03b22)]\u03c32\nP+ (\u00b52+\u03b32)(1\u2212\u03b22) +\u03c32\nwith P given in (13). Small values of rmean that signs knowledge has an\nimportant effect in reducing the estimation error of the intrinsic noise. Recalling\nthat \u03b2:=E[|wk|], the following result then holds.\nProposition 1 One has\nr\u2208(1\u2212\u03b22,1]\nand every value in this semi-open interval can be achieved by appropriately\nselecting the system parameters.\n28\nBefore proving this result, some comments are in order. In the case where\nwkis Gaussian, it holds that \u03b22=2\n\u03c0. Therefore, the maximum possible MSE\nreduction obtained by using the second estimator instead of the first one is\nconsiderable since\ninfr= 1\u22122\n\u03c0\u22480.37 ( Gaussian case ),\nsee also Fig. 7 which displays some profiles of ras a function of the correlation\nparameter a.\nIfwkfollows a Laplacian distribution, it holds that \u03b22=1\n2so that\ninfr=1\n2(Laplacian case ).\nAn additional example is a symmetric Bernoulli process where wk=\u00b11with\nequal probability and one thus has \u03b22= 1 . Therefore, the infimum of ris zero\nin the Bernoulli case,\ninfr= 0 ( Bernoulli case ),\nbecause MSE 2can be made arbitrarily small by increasing the correlation among\nthegk. Such improvements are possible because the second estimator (corre-\nsponding to the second stage of T rine) can exploit correlations among all output\nmeasurements, whereas the first one cannot.\nProof of Proposition 1: W e begin by proving that r\u22641, which is equiv-\nalent to:\n[P+ (\u00b52+\u03b32)(1\u2212\u03b22)]\u03c32\nP+ (\u00b52+\u03b32)(1\u2212\u03b22) +\u03c32\u2264(\u00b52+\u03b32)\u03c32\n\u00b52+\u03b32+\u03c32.\nThis simplifies to:\nP\u2264\u03b22(\u00b52+\u03b32) =:P0.\nW e can rewrite the ARE as:\nf(P) := (1 \u2212a2)(P\u2212\u03b22\u03b32)(P\u2212P0+\u03b32+\u00b52+\u03c32) +a2P2= 0.\nSince the coe\ufb00icient of P2is 1 and the ARE has exactly one positive solution\n(and one negative), we conclude that P\u2264P0if and only if f(P0)\u22650, which\nyields:\nf(P0) =\u03b22(1\u2212a2)\u00b52(\u03b32+\u00b52+\u03c32) +a2P2\n0\u22650,\nwhich is always true. The equality P=P0(and thus r= 1 ) occurs only when\na= 0 and\u00b5= 0 (i.e., gkis white noise and has zero mean).\nW e now prove that r >1\u2212\u03b22and that all values in the interval (1\u2212\u03b22,1)\nare attainable. Define:\nP0=\u03b4\u03c32, P =\u03f5\u03c32, \u03b32=\u03b22\u03d5\u03c32,\nwhich implies \u03f5\u2264\u03b4from P\u2264P0. Then, the ARE becomes:\na2\u03f52+ (1\u2212a2)\b\n[(\u03b22\u22121)\u03b4+ 1\u2212\u03d5]\u03f5\u2212\u03d5[(\u03b22\u22121)\u03b4\u22121]\t\n= 0\n29\nwhich has to be solved in the unknown \u03f5. Note that \u03b4and\u03d5depend only on\n\u00b52, \u03b32, and \u03c32, and can be selected independently of a. Although \u03b32=\u21132\n1\u2212a2\nseems to depend on a, for any fixed \u03b32we can always define \u21132(a, \u03b32) :=\u03b32(1\u2212\na2). As a\u21921\u2212, it follows that \u03f5\u21920+.\nNow, the MSEs can be rewritten as:\nMSE 1=P0\u03c32\nP0+\u03b22\u03c32=\u03b4\n\u03b4+\u03b22\u03c32,\nMSE 2=[P+P0(1/\u03b22\u22121)]\u03c32\nP+P0(1/\u03b22\u22121) +\u03c32=\u03f5+ (1/\u03b22\u22121)\u03b4\n\u03f5+ (1/\u03b22\u22121)\u03b4+ 1\u03c32.\nSince MSE 2is an increasing function of \u03f5andMSE 1does not depend on it, the\ninfimum of the ratio is attained as \u03f5\u21920+. Define:\ninf\n\u03f5\u2208(0,\u03b4]r(\u03f5, \u03b4) :=r(\u03b4) =\u03b4+\u03b22\n\u03b4+\u03b22\n1\u2212\u03b22.\nThis function is increasing in \u03b4, so its infimum is obtained as \u03b4\u21920+, while its\nsupremum is obtained letting \u03b4go to infinity , yielding:\ninf\n\u03b4r(\u03b4) = 1 \u2212\u03b22,\nsup\n\u03b4r(\u03b4) = 1 .\nTherefore, for any r\u2208(1\u2212\u03b22,1), we can select \u03b4 > 0and let \u03f5\u21920+(i.e.,\na\u21921\u2212) to achieve it. T o confirm this, observe that:\nr(\u03f5, \u03b4)\u22651\u2212\u03b22\nfor all 0< \u03f5\u2264\u03b4. Finally , recall also that r= 1 is obtained with a=\u00b5= 0 and\nthis completes the proof.\nIn conclusion, the maximum performance improvement is achieved as a\u2192\n1\u2212and\u03b32, \u00b52\u226a\u03c32, whereas the worst-case occurs when a=\u00b5= 0 .\nInterpretation of the result in the T rine setting\nIn the T rine setting, the results obtained here support this conclusion. As shown\nin the derivation of the T rine algorithm in the first part of this Supplementary\nMaterial, and according to the Bayesian interpretation of regularization, if the\npostulated covariance for the SD profile gis correct, the kernel obtained in the\nsecond stage possesses theoretical optimality . In fact, it enables the construc-\ntion of the minimum-variance linear estimator of the intrinsic noise under the\nassumption of known signs. Therefore, if the sign estimates produced in the\nfirst stage are su\ufb00iciently accurate, the structured kernel can fully exploit the\ninformation encoded in the regularity of the variance profile, thereby improving\nthe intrinsic noise estimates. The more regular the variance profile is, the lower\nthe estimator\u2019s MSE will be, as quantified in this section and illustrated in the\nexamples in Fig. 7.\n30\nMonte Carlo study\nW e present an additional study concerning the T oggle switch example. This\nanalysis further illustrates T rine\u2019s robustness and the meaning of the theoreti-\ncal findings outlined in the previous section.\nW e perform a Monte Carlo study using the same T oggle switch model introduced\nin the main text, in which the system describes the evolution of two proteins, ex-\npressed from two mutually inhibiting promoters and modeled through stochastic\ndifferential equations [40]. In each run, we use 1000 data points and generate\nindependent realizations of both the intrinsic noise and the output noise. The\nstandard deviation of the output Gaussian noise is set as a varying fraction of\nthe true state values, so that each run corresponds to a different coe\ufb00icient of\nvariation (CV%).\nW e repeat the procedure for 10000 independent runs. F or each run, we\ncompute the norm of the realization of the output noise and compare it to the\nnorm of the intrinsic noise realization. Based on the ratio of these norms, we\ncategorize the runs into different bins. Within each bin, we report the average\npercentage fit, over all runs that fall into the corresponding bin, between the\nestimatedbG reported in (11) and true profile G of the intrinsic noise standard\ndeviation. Recall that the fit is defined by\n100\u0010\n1\u2212\u2225G\u2212bG\u2225\n\u2225G\u2225\u0011\n,\nso that higher fit percentage indicates better estimator performance.\nIn addition to the full T rine algorithm, we compare two alternative versions:\n\u2022 T rineu: a version in which the third stage (estimation of the intrinsic\nnoise variance profile) uses directly the intrinsic noise estimates in (9)\ncoming from the first stage, i.e., obtained without the structured kernel\n(the superscript uindeed stands for unstructured ).\n\u2022 Oracle : a baseline version in which the third stage uses the true intrinsic\nnoise realizations. This serves as a benchmark to evaluate the performance\nceiling of the estimator.\nW e summarize the performance of the estimators in T able 2. Each row corre-\nsponds to a bin defined by the ratio between the norm of the realizations of the\noutput and intrinsic noise. Within each bin, we report the average percentage\nfit for T rine, T rineuand Oracle. Each average is computed over at least 1000\nMonte Carlo runs.\nIt is remarkable that when the output noise does not dominate over the\nintrinsic noise, T rine achieves a performance very close to that of the oracle.\nMoreover, it largely outperforms T rineubecause the intrinsic noise signs are\naccurately estimated, so that the theoretical results presented in the previous\nsection suggest the structured kernel in the second stage can be effectively ex-\nploited.\n31\nT able 2: Monte Carlo results across bins of output-to-intrinsic noise\nratio. Each bin corresponds to a range of values for the ratio \u2225e\u2225/\u2225n\u2225, where\neis the output noise realization and nis the intrinsic noise realization. In each\nbin, the reported values are the average percentage fit between the estimated\nand true intrinsic noise standard deviation profile. A higher fit indicates better\nperformance.\nBin range (\u2225e\u2225/\u2225n\u2225) T rine T rineuOracle\n[0.0,0.1) 91.4 73.5 92.2\n[0.1,0.2) 90.4 71.6 92.1\n[0.2,0.3) 87.4 70.8 92.2\n[0.3,0.4) 82.1 67.9 92.2\n[0.4,0.5) 74.8 67.3 92.2\n[0.5,0.6) 65.2 68.3 92.1\n[0.6,1.0] 42.2 61.5 91.8\nNote, however, that when the output noise increases and effectively masks\nthe intrinsic noise, it may be preferable to use T rineu. In this regime, the sign\nestimates become too imprecise, and the theoretical properties of the structured\nestimator discussed earlier can no longer be exploited. In fact, the structured\nkernel becomes too complex to be estimated reliably , leading to a degradation\nin T rine\u2019s performance. T o this regard, it is interesting to note that the first\nstep of T rine can also be used to estimate the relative strength between output\nand intrinsic noise. Indeed, the first phase returns the parameter \u03c1n, which\ncan be interpreted as an estimate of the average variance of the intrinsic noise.\nThis can be compared with the average \u03c1eof the diagonal elements of \u03a3e,\nwhich represents the mean variance of the output noise. A large ratio\u221a\u03c1e/\u221a\u03c1n\nsuggests that the results from T rineushould be considered.\nT oggle Switch Simulation Details\nThe two-dimensional state vector contains the two proteins x1andx2coming\nfrom two different promoters mutually inhibiting each other. Their dynamics\nare regulated by the following system of stochastic differential equations [40]:\n\u02d9x(t) =f(x(t))dt+g(x(t))w(t),\n32\nwhere the drift f(x)contains the two deterministic functions\nf1(x1, x2) =b+x2\n1\nb+x2\n1+x2\n2\u2212x1\n\u22121\n\u03ba2x1x2(x1+x2)[\n(x1\u22121)(b+x2\n1)(2b+x2\n1) +x1x2\n2(\n3b+x1(2x1\u22121))\n+x1x4\n2]\nb(\nb+x2\n1+x2\n2)4,\nf2(x1, x2) =b+x2\n2\nb+x2\n1+x2\n2\u2212x2\n\u22121\n\u03ba2x1x2(x1+x2)[\n(x2\u22121)(b+x2\n2)(2b+x2\n2) +x2x2\n1(\n3b+x2(2x2\u22121))\n+x2x4\n1]\nb(\nb+x2\n1+x2\n2)4.\nThe components w1, w2ofw are independent white Gaussian noises with\nunit variance. Finally , the state-dependent covariance of the intrinsic noise\ng(x(t))w(t)is\nQ(x1, x2) :=g(x)g(x)\u22a4=[\nQ1(x1, x2)Q12(x1, x2)\nQ12(x1, x2)Q2(x1, x2)]\n,\nwith entries\nQ1(x1, x2) =1\nm0(b+x2\n1\nb+x2\n1+x2\n2+x1)\n+1\n\u03bax2\n2(\nb2+ 2b x2\n1+x2\n1x2\n2+x4\n1)\nb(\nb+x2\n1+x2\n2)3,\nQ2(x1, x2) =1\nm0(b+x2\n2\nb+x2\n1+x2\n2+x2)\n+1\n\u03bax2\n1(\nb2+ 2b x2\n2+x2\n1x2\n2+x4\n2)\nb(\nb+x2\n1+x2\n2)3,\nQ12(x1, x2) =1\n\u03bax2\n1x2\n2(\n2b+x2\n1+x2\n2)\nb(\nb+x2\n1+x2\n2)3.\nFinally , model parameters used to simulate the data are reported in T able\n3.\nT able 3: T oggle switch \u2014 model parameters\nParameter V alue Description\n\u03b8 1\u00d7104molec /s Active production rate\n\u03b1 1000 s\u22121Degradation rate\n\u03b2 50 Cooperativity\n\u03b4 0.75 s\u22121Switch ON rate\n\u03b11 \u03b1 Degradation rate [min\u22121]\n\u03b41 \u03b4 Switch ON rate [min\u22121]\nK 0.01 Parameter K\nb\u03b2 \u03b8 \u03b42\n\u03b12\n1Dimensionless parameter b\n\u03baK \u03b12\n1\n\u03b8 \u03b43Dimensionless parameter \u03ba\nm0 1000 Molecular scale\n33\nReferences\n[1] M. Ackermann. A functional perspective on phenotypic heterogeneity in\nmicroorganisms. Nature Reviews Microbiology , 13(8):497\u2013508, 2015.\n[2] B Anderson, J Moore, Optimal Filtering . (Prentice-Hall, Englewood Cliffs,\nN.J., USA), (1979).\n[3] G. Bal\u00e1zsi, A. van Oudenaarden, and J.J. Collins. Cellular decision making\nand biological noise: F rom microbes to mammals. Cel l , 144:910\u2013925, 2011.\n[4] B. Bao, L. Chen, H. Bao, M. Chen, and Q. Xu. Bifurcations to bursting\noscillations in memristor-based fitzhugh\u2013nagumo circuit. Chaos, Solitons\n& F ractals , 181:114608, 2024.\n[5] I. Barbier, R. Perez-Carrasco, and Y. Schaerli. Controlling spatiotempo-\nral pattern formation in a concentration gradient with a synthetic toggle\nswitch. Mol Syst Biol. , 16, 2020.\n[6] I. Bashkirtseva and L. Ryashko. Analysis of the noise-induced regimes in\nricker population model with allee effect via confidence domains technique.\nBioMed Research International , 2014:1\u20137, 2014.\n[7] A. Becskei, B. Serphin, and L. Serrano. Positive feedback in eukaryotic gene\nnetworks: cell differentiation by graded to binary response conversion. The\nEMBO Journal , 20:2528\u20132535, 2001.\n[8] A. Becskei and L. Serrano. Engineering stability in gene networks by au-\ntoregulation. Nature , 405:590\u2013593, 2000.\n[9] M. Binois, R. Gramacy , and M. Ludkovski. Practical heteroscedastic Gaus-\nsian process modeling for large simulation experiments. Journal of Com-\nputational and Graphical Statistics , 27(4):808\u2013821, 2018.\n[10] M. Binois, R. B. Gramacy , J. M. Bardsley , D. J. Moquin, A. P . Smith,\nand A. M. Smith. Parameter and uncertainty estimation for dynamical\nsystems using surrogate stochastic processes. SIAM Journal on Scientific\nComputing , 41(4):A2212\u2013A2238, 2019.\n[11] M. Binois, J. Huang, R. B. Gramacy , and M. Ludkovski. Replication or\nexploration? Sequential design for stochastic simulation experiments. T ech-\nnometrics , 61(1):7\u201323, 2019.\n[12] M. Boareto, M.K. Jolly , M. Lu, J.N. Onuchic, C. Clementi, and E. Ben-\nJacob. Jagged-delta asymmetry in notch signaling can give rise to a\nsender/receiver hybrid phenotype. Proc Natl Acad Sci USA , 112:E402\u2013\n9, 2015.\n[13] A. Boukouvalas, D. Cornford, and M. Stehl\u00edk. Optimal design for correlated\nprocesses with input-dependent noise. Computational Statistics and Data\nAnalysis , 71:1088\u20131102, 2014.\n34\n[14] C. Briat, A. Gupta, and M. Khammash. Antithetic proportional\u2011integral\nfeedback for reduced variance and improved control performance of\nstochastic reaction networks. Journal of the Royal Society Interface ,\n15(143):20180079, 2018.\n[15] Howard H. Chang, Martin Hemberg, Mauricio Barahona, Donald E. Ing-\nber, and Sui Huang. T ranscriptome-wide noise controls lineage choice in\nmammalian progenitor cells. Nature , 453:544\u2013547, 2008.\n[16] J. L. Cherry and F. R. Adler. How to make a biological switch. J. there.\nBiol. , 203:117\u2013133, 2000.\n[17] V. Chickarmane and C. Peterson T. Enver. Computational modeling of the\nhematopoietic erythroid-myeloid switch reveals insights into cooperativity ,\npriming, and irreversibility . PLoS Comput Biol. , 5, 2009.\n[18] E. Clayton, D.P . Doup\u00e9, A.M. Klein, D.J. Winton, B.D. Simons, and P .H.\nJones. A single type of progenitor cell maintains normal epidermis. Nature ,\n446(7132):185\u2013189, 2007.\n[19] T. Dallas, B. A. Melbourne, G. Legault, and A. Hastings. Initial abundance\nand stochasticity influence competitive outcome in communities. Journal\nof Animal Ecology , 90(5):1185\u20131197, 2021.\n[20] B. Dennis. Allee effects in stochastic populations. Oikos , 96(3):389\u2013401,\n2002.\n[21] S. Elaydi, Y. Kang, and R. Luis. Global asymptotic stability of evolutionary\nperiodic ricker competition models. Journal of Difference Equations and\nApplications , 2024.\n[22] M. Elowitz and S. Leibler. A synthetic oscillatory network of transcriptional\nregulators. Nature , 403:335\u2013338, 2000.\n[23] Michael B. Elowitz, Arnold J. Levine, Eric D. Siggia, and Peter S. Swain.\nStochastic gene expression in a single cell. Science , 297(5584):1183\u20131186,\n2002.\n[24] G.A. F aza, N. Loka, K. Shariatmadar, H. Hallez, and D. Moens. Most likely\nheteroscedastic Gaussian process via kernel smoothing. Knowledge-Based\nSystems , 328, 2025.\n[25] O. F einerman, J. V eiga, J.R. Dorfman, R.N. Germain, and G. Altan-\nBonnet. V ariability and robustness in t cell activation from regulated het-\nerogeneity in protein levels. Science , 321(5892):1081\u20131084, 2008.\n[26] R. FitzHugh. Impulses and physiological states in theoretical models of\nnerve membrane. Biophysical Journal , 1(6):445\u2013466, 1961.\n35\n[27] X. Gao and Y. W ang. Dynamic prediction of the ricker-type model of por-\ntunus trituberculatus on the basis of marine environmental factors. F ron-\ntiers in Marine Science , 9, 2022.\n[28] T.S. Gardner, C.R. Cantor, and J.J. Collins. Construction of a genetic\ntoggle switch in escherichia coli. Nature , 403(6767):339\u2013342, 2000.\n[29] A. Giaretta. Human papillomavirus early promoter regulatory core as a\nbistable switch. Annu Int Conf IEEE Eng Med Biol Soc. , pages 2925\u20132928,\n2019.\n[30] A. Giaretta, G.M. T offolo, and T.C. Elston. Stochastic modeling of hu-\nman papillomavirusearly promoter gene regulation. Journal of Theoretical\nBiology , 486:110057, 2020.\n[31] D. T. Gillespie. Exact stochastic simulation of coupled chemical reactions.\nThe Journal of Physical Chemistry , 81(25):2340\u20132361, 1977.\n[32] P . W. Goldberg, C. K. I. Williams, and C. M. Bishop. Regression with\ninput-dependent noise: A Gaussian process treatment. In Advances in\nNeural Information Processing Systems 10 (NIPS 1997) , pages 493\u2013499.\nMIT Press, 1998.\n[33] G. H. Golub, M. Heath, and G. W ahba. Generalized cross-validation as a\nmethod for choosing a good ridge parameter. T echnometrics , 21(2):215\u2013\n223, May 1979.\n[34] R. Guantes and J.F. Poyatos. Multistable decision switches for flexible\ncontrol of epigenetic differentiation. PLoS Comput Biol. , 4, 2008.\n[35] T. Hastie, R. Tibshirani, and J.R. F riedman. The Elements of Statistical\nLearning . Springer, 2001.\n[36] A. L. Hodgkin and A. F. Huxley . A quantitative description of membrane\ncurrent and its application to conduction and excitation in nerve. Journal\nof Physiology , 117:500\u2013544, 1952.\n[37] X. Hong, Y. Ding, L. Ren, L. Chen, and B. Huang. A weighted het-\neroscedastic Gaussian process modelling via particle swarm optimization.\nChemometrics and Intel ligent Laboratory Systems , 172:129\u2013138, 2018.\n[38] M. Kaern, T.C. Elston, W.J. Blake, and J.J. Collins. Stochasticity in\ngene expression: from theories to phenotypes. Nature Review Genetics ,\n6(6):451\u2013464, 2005.\n[39] A. Kendall and Y. Gal. What uncertainties do we need in Bayesian deep\nlearning for computer vision? In Proceedings of the Conference on Neural\nInformation Processing Systems (Long Beach, CA) , 2017.\n36\n[40] T.B. Kepler and T.C. Elston. Stochasticity in transcriptional regula-\ntion: origins, consequences, and mathematical representations. Biophys\nJ., 81:3116\u20133136, 2001.\n[41] K. Kersting, C. Plagemann, P . Pfaff, and W. Burgard. Most likely het-\neroscedastic Gaussian process regression. In Proceedings of the 24th Inter-\nnational Conference on Machine Learning (ICML) , pages 393\u2013400. ACM,\n2007.\n[42] P .E. Kloeden and E. Platen. Numerical Solution of Stochastic Differential\nEquations , volume 23 of Applications of Mathematics (New Y ork) . Springer,\n1992.\n[43] M. L\u00e1zaro-Gredilla and M.K. Titsias. V ariational heteroscedastic Gaussian\nprocess regression. In Proceedings of the 28th International Conference on\nMachine Learning (ICML) , pages 841\u2013848, Bellevue, W ashington, USA,\n2011. Omnipress.\n[44] B. Lindner, J. Garc\u00eda-Ojalvo, A. Neiman, and L. Schimansky-Geier. Effects\nof noise in excitable systems. Physics Reports , 392(6):321\u2013424, 2004.\n[45] L. Ljung. System Identification - Theory for the User . Prentice-Hall, Upper\nSaddle River, N.J., 2nd edition, 1999.\n[46] M. Lu, M.L. Jolly , H. Levine, J.N. Onuchic, and E. Ben-Jacob. Microrna-\nbased regulation of epithelial-hybrid-mesenchymal fate determination. Proc\nNatl Acad Sci USA , 110:18144\u201318149, 2013.\n[47] D.J.C. MacKay . Bayesian interpolation. Neural Computation , 4:415\u2013447,\n1992.\n[48] R.M. May . Simple mathematical models with very complicated dynamics.\nNature , 261:459\u2013467, 1976.\n[49] H.H. McAdams and A. Stochastic mechanisms in gene expression. Proc\nNatl Acad Sci U S A. , 7:2651\u20132667, 2006.\n[50] C.A. Micchelli, Y. Xu, and H. Zhang. Universal kernels. J. Mach. Learn.\nRes. , 94:814\u2013819, 1997.\n[51] A.Y. Mitrophanov and E.A. Groisman. Positive feedback in cellular control\nsystems. Bioessays , 30(6):542\u2013555, 2008.\n[52] J. Nagumo, S. Arimoto, and S. Y oshizawa. An active pulse transmission\nline simulating nerve axon. Proceedings of the IRE , 50(10):2061\u20132070, 1962.\n[53] L.S. W einberger nd J.C. Burnett, J.E. T oettcher, a.P . Arkin, and D. V.\nSchaffer. Stochastic gene expression in a lentiviral positive-feedback loop:\nHiv-1 tat fluctuations drive phenotypic diversity . Cel l , 122:169 \u2013 182, 2005.\n37\n[54] D. A. Nix and A. S. W eigend. Estimating the mean and variance of the tar-\nget probability distribution. In Proceedings of the 1994 IEEE International\nConference on Neural Networks , pages 55\u201360, Orlando, FL, 1994.\n[55] G. Pillonetto, T. Chen, A. Chiuso, G. De Nicolao, and L. Ljung. Regularized\nSystem Identification . Springer, 2022.\n[56] G. Pillonetto and L. Ljung. F ull Bayesian identification of linear dynamic\nsystems using stable kernels. Proceedings of the National Academy of Sci-\nences USA , 120, 2023.\n[57] C. Rasmussen and C. Williams. Gaussian Processes for Machine Learning .\nThe MIT Press, 2006.\n[58] W. E. Ricker. Stock and recruitment. Journal of the Fisheries Research\nBoard of Canada , 11(5):559\u2013623, 1954.\n[59] J. Ripa, P . Lundberg, and V. Kaitala. A general theory of environmental\nnoise in ecological food webs. The American Naturalist , 151:256\u2013263, 1998.\n[60] G. Schmid, I. Goychuk, and P . H\u00e4nggi. Stochastic resonance as a collective\nproperty of ion channel assemblies. Europhysics Letters , 56(1):22\u201328, 2001.\n[61] B. Scholkopf and A. J. Smola. Learning with Kernels: Support V ector Ma-\nchines, Regularization, Optimization, and Beyond . Adaptive Computation\nand Machine Learning. The MIT Press, 2001.\n[62] S. M. Shaffer, M. C. Dunagin, et al. Rare cell variability and drug-induced\nreprogramming as a mode of cancer drug resistance. Nature , 546(7658):431\u2013\n435, 2017.\n[63] T. Soderstrom and P . Stoica. System Identification . Prentice-Hall, 1989.\n[64] Peter S. Swain, Michael B. Elowitz, and Eric D. Siggia. Intrinsic and\nextrinsic contributions to stochasticity in gene expression. Proceedings of\nthe National Academy of Sciences , 99(20):12795\u201312800, 2002.\n[65] V. T olvanen, P . Jyl\u00e4nki, and A. V ehtari. Expectation propagation for non-\nstationary heteroscedastic Gaussian process regression. In 2014 IEEE In-\nternational W orkshop on Machine Learning for Signal Processing (MLSP) ,\npages 1\u20136. IEEE, 2014.\n[66] H. C. T uckwell. Introduction to Theoretical Neurobiology: V olume 2, Non-\nlinear and Stochastic Theories . Cambridge University Press, Cambridge,\nUK, 2005.\n[67] G. W ahba. Spline models for observational data . SIAM, Philadelphia, 1990.\n[68] M. E. Y amakou, T. D. T ran, L. H. Duc, and J. Jost. The stochastic\nfitzhugh\u2013nagumo neuron model in the excitable regime embeds a leaky\nintegrate-and-fire model. Journal of Mathematical Biology , 79(2):509\u2013532,\n2019.\n38\n[69] H. Zhang, T. Y ang, Y. Xu, and W. Xu. Limiting dynamics for stochastic\nfitzhugh\u2013nagumo lattice systems in weighted spaces. Journal of Dynamics\nand Differential Equations , 36:321\u2013352, 2024.\n39\nRicker model\nFigure 2: Results for Ricker model. The top panel show the simulations of\nthe system using r= 2.5and intrinsic noise SD g(x) =\u221a\n0.32+ 0.052x[6].\nThe bottom left panel shows the estimated absolute values of the intrinsic noise\nrealizations returned by the second stage of T rine (grey dots), the true gfunction\n(red) and the estimated one returned by T rine (blue). The bottom right panel\ndisplays the true deterministic part f(red) and the estimate by T rine (blue).\n40\nV(activator)-3 -2 -1 0 1 2W(recovery)\n-0.500.511.522.5\nFigure 3: Results for FitzHugh\u2013Nagumo model. The top panel shows a rep-\nresentative simulated trajectory in the 2D plane (V, W )obtained using pa-\nrameters \u03f5= 0.08,a= 0.7,b= 0.8,Iext= 0.5,\u03c3V= 0.1,\u03c3W= 0.05 and\n\u03b1=\u03b2= 0.8. Stochastic dynamics qualitatively show two basins of attrac-\ntion. The left-bottom panel shows the true state-dependent standard deviation\ndriving \u02d9V, while the right-bottom panel displays its T rine estimate.\n41\nAutoregulation \u2013self promoterFigure 4: Results for Self-Promoter model. The top left panel shows the self-\npromoter gene regulatory network. The top right panel displays a representative\nsimulated trajectory , whose dynamics exhibit pronounced burst-like stochastic\nbehavior. Data are generated using the parameters a0= 0.05 (basal activity),\nb= 10 (feedback strength), m0= 25 (copy-number scale), \u03ba= 1 (promoter-\nswitching noise) [40]. The bottom panel reports the true SD profile (red) and\nthe T rine estimate (blue).\n42\nMutual repressor \u2013toggle switch\nFigure 5: Results for T oggle switch model. The top left panel shows the gene\nregulatory network, while the top right panel shows a representative simulated\ntrajectory of the two proteins in the 2D plane. Their stochastic dynamics show\ntwo basins of attraction, typical of bistable systems. The left-bottom panel\nshows the true intrinsic noise and state-dependent standard deviation while\nT rine estimates are in the right-bottom panel. Data are generated using the\nparameters b= 0.28,m0= 1000 ,\u03ba= 2.37 (see Appendix for further details).\n43\nFigure 6: Boxplots of the Fit values (percentage accuracy) obtained from 2000\nMonte Carlo runs for each estimator across the T oggle Switch (left block) and the\nFitzHugh\u2013Nagumo system (right block). The experiment is designed such that,\nin each run, the observed noise level can vary . Specifically , the ratio between the\nnorm of measurement and intrinsic noise may vary uniformly over [0,0.4]. The\nfour estimators compared are Oracle, T rine, T rineu, and MLH-GP . The figure\nhighlights that T rine performs very close to the Oracle, an ideal procedure which\ncan directly measure intrinsic noise realizations, and substantially outperforms\nthe classical MLH-GP approach [41], which uses a log-normal prior to model\nthe state-dependent variance. Comparison with T rineu, a variant in which the\nthird stage of T rine relies directly on the intrinsic noise estimates from the first\nstage, emphasizes the importance of the structured kernel in regularizing and\naccurately estimating the intrinsic noise profile.\n44\na0.7 0.75 0.8 0.85 0.9 0.95 1r= MSE 2/MSE1\n0.30.40.50.60.70.80.9Ratio of MSEs\n\u00b5= 0,\u03c3= 1\n\u00b5= 1,\u03c3= 10Figure 7: Comparison of the ratio r=MSE 2\nMSE 1as a function of the correlation\nparameter afor two parameter settings using Gaussian intrinsic noise. Smaller\nvalues correspond to larger performance gains for the estimator that leverages\nthe signs of intrinsic noise realizations. Setting \u03b3= 1, \u03b22=2\n\u03c0(Gaussian intrinsic\nnoise case), the blue curve then corresponds to \u00b5= 0, \u03c3= 1 while the orange\ncurve corresponds to \u00b5= 1, \u03c3= 10 .\n45\n",
    "title": "Learning stochasticity: a nonparametric framework for intrinsic noise estimation",
    "authors": [
      "Gianluigi Pillonetto",
      "Alberto Giaretta",
      "Mauro Bisiacco"
    ],
    "published": "2025-11-17",
    "arxiv_id": "2511.13701v1",
    "num_pages": 45,
    "num_chars": 89432
  },
  {
    "text": "Efficient Calibration for Decision Making\nParikshit Gopalan\nAppleKonstantinos Stavropoulos\u2217\nUT AustinKunal Talwar\nApplePranay Tankala\u2217\nHarvard\nAbstract\nA decision-theoretic characterization of perfect calibration is that an agent seeking to minimize\naproperlossinexpectationcannotimprovetheiroutcomebypost-processingaperfectlycalibrated\npredictor. Hu and Wu (FOCS\u201924) use this to define an approximate calibration measure called\ncalibration decision loss ( CDL), which measures the maximal improvement achievable by any\npost-processing over any proper loss. Unfortunately, CDLturns out to be intractable to even\nweakly approximate in the offline setting, given black-box access to the predictions and labels.\nWe suggest circumventing this by restricting attention to structured families of post-processing\nfunctionsK. We define the calibration decision loss relative to K, denoted CDLKwhere we\nconsider all proper losses but restrict post-processings to a structured family K. We develop a\ncomprehensive theory of when CDLKis information-theoretically and computationally tractable:\n\u2022Complexity characterization.The sample complexity of estimating CDLKis determined\nby the VC dimension of thr(K), the concept class consisting of thresholds applied to\nany\u03ba\u2208K. Computationally, estimating CDLKreduces to agnostically learning thr(K).\nThis implies that estimating CDLrelative to1-Lipschitz post-processings is information-\ntheoretically hard.\n\u2022Quantitative characterization.Augmenting thr(K)with indicators of intervals of the\nform[0,a]yields a family of weight functions K\u2032such that CDLKis characterized, up to\na quadratic factor, by the weighted calibration error restricted to K\u2032. This significantly\ngeneralizes prior bounds that were for specific choices ofK.\n\u2022Omniprediction.If thr(K)is efficiently learnable there exists asinglepost-processing\nthat performs competitively with the best post-processing in Kforeveryproper loss.\nClassical recalibration algorithms including the Pool Adjacent Violators (PAV) algorithm\nand Uniform-mass binning give similaromnipredictionguarantees for natural classes of\npost-processings with monotonic structure.\nIn addition to introducing new definitions and algorithmic techniques to the theory of\ncalibration for decision making, our results give rigorous guarantees for some widely used\nrecalibration procedures in machine learning.\n\u2217Work done during an internship at Apple.arXiv:2511.13699v1  [cs.LG]  17 Nov 2025\nContents\n1 Introduction 1\n1.1 Approximate Calibration From Indistinguishability. . . . . . . . . . . . . . . . . . . . 2\n1.2 Approximate Calibration From No Regret. . . . . . . . . . . . . . . . . . . . . . . . . 2\n2 Overview of Our Results 5\n2.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2 Testing and Auditing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.3 Calibration Decision Loss and Weight-Restricted Calibration . . . . . . . . . . . . . 8\n2.4 Post-Processing and Omniprediction . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3 Preliminaries 13\n4 Sample Complexity of Testing and Auditing 15\n4.1 A Characterization via VC dimension . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n4.2 Other Families of Loss Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n5 Relation to Weight-Restricted Calibration 19\n5.1 The Characterization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.2 Translation Invariance Alone Is Insufficient . . . . . . . . . . . . . . . . . . . . . . . 26\n6 Computationally Efficient Testing and Auditing 27\n7 Omniprediction 29\n7.1 Omniprediction From Agnostic Learning . . . . . . . . . . . . . . . . . . . . . . . . . 29\n7.2 Pool Adjacent Violators Is an Omnipredictor . . . . . . . . . . . . . . . . . . . . . . 31\n7.3 Omniprediction Through Uniform-Mass Binning and Recalibration . . . . . . . . . . 33\nA Why Generalized Monotone Functions 41\nB Additional Proofs 42\nB.1 Additional Proofs From Section 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\nB.2 Additional Proofs From Section 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\nC Tightness of the Weight-Restricted Calibration Characterization 45\nD Properties of Generalized Monotone Post-Processings 46\n1 Introduction\nConsider the setting of binary classification, where we see examples( x,y)drawn from a distribution\nDonX\u00d7{ 0,1}. A predictor P:X\u2192 [0,1]estimates the probability that the label y= 1for a\ngivenx. While predictions take values in the interval[0 ,1], the labels are binary. What does it\nmean for a predictor to be good in such a setting?\nThe notion of (perfect) calibration, which originates in the forecasting literature [ Daw85] requires\nthat every predicted value pin the range of P, we have E[y|P(x) =p] =p. The Bayes optimal\npredictor, defined as p\u2217(x) =E[y|x]is calibrated, but calibration is strictly weaker than Bayes\noptimality. Despite this, calibration gives important guarantees for downstream decision makers\nwho make decisions based on the predictions, where they can trust a calibrated predictor and actas\nthough it were indeed the Bayes optimal.\nConsider an agent who uses the predictionsP(x)to choose an actiona\u2208A, so as to minimize\ntheir expected loss E[\u2113(a,y)]for some loss function \u2113:A\u00d7{ 0,1}\u2192R. Such an agent can respond\naccording to the best-response function \u03ba\u2113: [0,1]\u2192A, where if we denote by Ber(p)the Bernoulli\ndistribution with parameterp, then\n\u03ba\u2113(p) = arg min\na\u2208AE\n\u02dcy\u223cBer(p)[\u2113(a,\u02dcy)].\nIf we wished to minimize loss for the labels \u02dcy\u223cBer (p), wherep=P(x), thenPis Bayes optimal.\nSo this corresponds to the agent trusting the predictor Peven for labels y, as though it were the\nBayes optimal predictor.\nCalibration gives two important guarantees to any such agent, for every loss function\u2113:\n\u2022No surprises:The expected loss suffered E[\u2113(\u03ba(p),y)]under the true labels equals the\nexpected lossE[\u2113(\u03ba(p),\u02dcy)]they would expect to suffer ifPwas indeed Bayes optimal.\n\u2022No regrets:The expected loss E[\u2113(\u03ba(p),y)]under the true labels is indeed minimized by\nplaying the best response\u03ba\u2217over any other function\u03ba: [0,1]\u2192A.\nWhile our work focuses on calibration guarantees for decision making, in the the broader\ncontext, recent interest in calibration has been driven by the numerous surprising applications of\ncalibration and its generalizations to algorithmic fairness [ HKRR18 ], learning [ GKR+22], complexity\ntheory [CDV24], pseudorandomness [ DKR+21], cryptography [ HV25] and other areas of theoretical\ncomputer science.\nPerfect calibration is an idealized notion; we cannot realistically expect it from predictors in the\nreal world for computational and information-theoretic reasons. This has motivated the formulation\nof approximate notions of calibration [ FV98,KF08,ZKS+21,BGHN23a ,BGHN23b ,BN24,HW24,\nOKK25,RSB+25]. To be useful, approximate notions of calibration should be computationally\nefficient, and yield some relaxed form of the guarantees above. We refer here to the efficiency\nof auditing for calibration error, where the goal is to estimate a calibration measure to within a\nprescribed additive error, from random samples of the form( p,y). Following common practice in\nthe literature (see e.g. [ BGHN23a ]), we henceforth drop xfrom our notation and consider the joint\ndistribution( p,y). Whilexinfluences how p=P(x)andy=y|xare jointly distributed, we do\nnot mention it explicitly, to emphasize the fact that calibration measures (like loss functions) are\ntypically independent of the feature space.\n1\n1.1 Approximate Calibration From Indistinguishability.\nThis is a view of calibration as a notion of indistinguishability between the real world and a\nsimulation that the predictor Pproposes. It is based on the outcome indistinguishability framework\nof [DKR+21] and developed in [ GKSZ22 ,GH25]. The real world is modeled by the joint distribution\nJ= (p,y)and the simulation by \u02dcJ= (p,\u02dcy). Perfect calibration is equivalent to the two distributions\nbeing identical.\nThis naturally suggests relaxations that only require the distributions to be close, not identical,\nbut also restrict the set of distinguishers tofool, in analogy with cryptography and pseudorandomness.\nThis restriction turns out to be crucial for efficient auditing. This relaxation is captured by the\ndefinition of weight-restricted calibration [ GKSZ22], where we consider a family of functions\nW\u2286{ [0,1]\u221d\u21d5\u22a3\u221a\u222b\u2294\u2240\u2192[\u22121,1]}and require that the distributions( p,y)and(p,\u02dcy)be indistinguishable to all\npredictors of the form {f(p,y) =w(p)y}w\u2208W.1Formally, for a distribution J= (p,y), we define the\nW-restricted calibration error as\nCEW(J) = max\nw\u2208WE[w(p)(y\u2212\u02dcy)] = max\nw\u2208WE[w(p)(y\u2212p)].\nThe No Surprises property, which is referred to as decision OI in the literature [ GHK+23], can be\nseen as a form of indistinguishability between these distributions.\nA sequence of works in recent years [ GKSZ22 ,BGHN23a ,BGHN23b ,BN24,OKK25,RSB+25]\nhave painted a complete picture of weight families for which W-restricted calibration is meaningful\nand tractable; we refer the reader to the recent survey [ GH25]. We highlight a few results that are\nrelevant to us:\n1.Complexity of auditing.The tractability of estimating CEWis tightly captured by the\nlearnability of the classW[GHR24].\n2.Expected calibration Error.The expected calibration error ECE(J)corresponds to\nW=W\u2217being all bounded functions. Since W\u2217has unbounded VC dimension, ECEcannot\nbe audited from finitely many samples [GHR24].\n3.Decision OI.Decision OI is characterized by CEIntwhere Intis the set of indicators of\nintervals in[0,1][OKK25].2\n4.Smooth calibration.Considering the family of1-Lipschitz weight functions gives smooth\ncalibration [ KF08], which is robust under small perturbations of the predictor, and captures\nan intuitive measure of calibration error called the distance to calibration [BGHN23a].\nSince both intervals and Lipschitz functions are efficiently learnable in1dimension, the latter two\nnotions can be audited efficiently (see also [HJTY24]).\n1.2 Approximate Calibration From No Regret.\nAn alternate view of approximate calibration comes from relaxing the no regret property of perfect\ncalibration. We will restrict our attention to (bounded) proper loss functions. Informally, for a loss\nfunction to be proper, a predictor with knowledge of the true outcome distribution should not be\nable to further decrease its loss by dishonestly predicting some alternate distribution. Formally, these\n1All bounded distinguishersf(p,y)can be assumed to have the formw(p)y, see [GH25].\n2This notion is called Proper calibration by [ OKK25] and cutoff calibration by [ RSB+25], we will refer to it as\nInterval-restricted calibrationCE Intin keeping with the weight-restricted calibration nomenclature.\n2\nare loss functions where the action space Ais the space of predictions[0 ,1], and when y\u223cBer (q),\nthe prediction pthat minimizes the expected loss E[\u2113(p,y)]isp=q. Restricting to proper losses is\nwithout loss of generality since an arbitrary loss function\u2113can be converted to a proper loss\u2113\u2032by\ncomposing it with the best response \u2113\u2032=\u2113\u25e6\u03ba\u2217\n\u2113.3We letL\u2217denote the set of all bounded proper\nloss functions, andK\u2217={[0,1]\u221d\u21d5\u22a3\u221a\u222b\u2294\u2240\u2192[0,1]}denote the family of all post-processings.\nThe starting point is the following characterization of calibration due to [ FV98]:J= (p,y)is\nperfectly calibrated iff for every proper loss\u2113\u2208L\u2217and post-processing\u03ba\u2208K\u2217,\nE[\u2113(p,y)\u2264E[\u2113(\u03ba(p),y)].\nThis means that in every miscalibrated predictor, this inequality is violated for some \u2113,\u03bapair. The\nrecent work of Hu and Wu [ HW24], building on [ KLST23], suggests using the magnitude of this\nviolation as a measure of miscalibration, which they call thecalibration decision loss. Formally, for\na family of post-processingsK\u2286K\u2217,4we define thecalibration decision loss relative toKas\nCDLK(J) = max\n\u03ba\u2208KE[\u2113(p,y)\u2212\u2113(\u03ba(p),y)].\nThis leads to a natural family of calibration measures parametrized by K, which measures how\nmuch regret (excess loss) a decision maker could possibly suffer for a lack of calibration, relative to\na baseline set of post-processings K. In this work we propose studying the complexity of CDLKfor\ngeneral familiesKof post-processing functions.\nMotivated by the online prediction setting, [ HW24] were primarily interested in the case K=K\u2217,\nwhere they show a tight connection with theECE:\nECE(J)2\u2272CDLK\u2217(J)\u2272ECE(J).\nIn the online setting, they showed surprisingly that CDLadmits much lower regret rates than\nECE. However, for the auditing problem in the offline setting which is our main focus, this tight\nconnection to ECEmeans that CDLK\u2217cannot be audited from finitely many samples. One might\nhope to circumvent this intractability by suitably restricting the family of post-processings K, in\nanalogy to the weight-restricted calibration setting. The family of monotone post-processingsM +\nwas considered in [ RSB+25], who bound the calibration decision loss relative to M+byCEInt, where\nIntis the family of indicators of intervals, showing thatCDL M+(J)\u22642CE Int(J).5\nIn addition to being a natural question from a theoretical viewpoint, understanding CDLKfor\nvarious classesKis relevant to how calibration errors are measured and remediated in practice\n[GPSW17 ]. Popular methods for recalibration such as Isotonic Regression [ ZE01,ZE02] and the\nPool Adjacent Violators (PAV) algorithm [ ABE+55] and Platt scaling [ Pla00], try to find the best\npost-processing from a family K(see [NC05] for more details about these methods). Isotonic\nregression (which is the problem solved by PAV) considers monotone post-processings, whereas Platt\nscaling considers a parametrized subclass of monotone functions consisting of sigmoids. Implicitly,\nsuch methods consider the predictor calibrated if CDLKis small. Other families of post-processings\nlike recalibration based on Uniform-mass binning [ ZE01,GR21,SSH23] and vector/matrix scaling\nhave been proposed [ GPSW17 ]. Methods that find the best post-processing from a small class have\nalso shown to be effective in practice (see e.g. [ BHJB25]). However, there was a lack of theory to\nguide the choice ofK.\n3In other words, a predictor that knows that an agent will best-respond to their predictions can view the agent as\noptimizing a proper loss.\n4We assumeKcontains the identity function, which ensure the positivity ofCDL K.\n5While not stated in terms ofCDL, Proposition 3.2 in their paper is equivalent to the above inequality.\n3\nOther results in the literature on decision making from calibration correspond to studying\nCDLfor restrictions of post-processings, loss functions or both. For instance, [BGHN23b] consider\nLipschitz post-processings, and show that the maximum improvement to the expected squared loss\n\u21132(p,y) = (y\u2212p )2from such post-processings is quadratically related to the smooth calibration\nerror. But in contrast to weight-restricted calibration, there wasn\u2019t a comprehensive understanding\nof when calibration decision loss relative to post-processings inKis a tractable measure.\nOur work:In this work, we seek to understand the families of post-processings Kfor which CDLK\ngives a tractable calibration measure that has strong guarantees for downstream decision makers.\nThis raises several natural questions:\n1.Complexity characterization.For what families of post-processings Kis estimating CDLK\ntractable in terms of sample and computational complexity? Is there a complexity measure\nforKthat governs tractability?\n2.Specific post-processings.Is CDLestimation tractable for monotone post-processings and\nLipschitz post-processings?6What is the most-expressive family Kfor which we can estimate\nCDLKefficiently?\n3.Efficient post-processing.If a predictor has large calibration decision loss relative to K,\nhow should we post-process it?\n4.Relation to weight-restricted calibration.How does calibration decision loss relative\ntoKrelate to weight-restricted calibration? Does small calibration error in one sense imply\nsmall error in the other?\nThe main contribution of this paper is to develop a comprehensive theory of when calibration\ndecision loss relative to Kis tractable. We use this theory to answer the questions raised above.\nSome highlights from our results include:\n\u2022We show that allowing all Lipschitz post-processings results in a CDLnotion that is information-\ntheoretically hard to estimate, requiring unbounded sample size. This is in stark contrast to\nweighted calibration, where allowing all Lipschitz weight functions yields smooth calibration,\nwhich is not only tractable but captures distance to calibration and anchors the notion of\nconsistent calibration measures as defined by [BGHN23a].\n\u2022We show that restricting post-processings to monotone functions and their generalizations\nyield tractable notions of CDL. Our theoretical results justify the central role such families\nplay in practice [Pla00, ABE+55, ZE02].\n\u2022Weestablishtightconnectionsbetweencalibrationdecisionlossandweightrestrictedcalibration\nfor any valid post-processing class K, unifying and generalizing the previous results of [ KLST23,\nHW24, RSB+25].\n\u2022We introduce new algorithmic techniques from the omniprediction literature to post-processing.\nWe give rigorous new guarantees for some commonly used recalibration procedures in the\nmachine learning literature like Isotonic regression/Pool Adjacent Violators [ ABE+55,ZE02]\nand Uniform mass binning [ZE01, GR21, SSH23].\n6We invite the reader to make a guess about these two specific families before reading further.\n4\n2 Overview of Our Results\nIn this section, we present the key definitions and an overview of our main results, and highlight\nsome of the key ideas behind them, without getting into the technical details.\n2.1 Definitions\nProper loss functions.\nDefinition 2.1(Proper Losses).A loss\u2113: [0,1]\u00d7{0,1}\u2192Risproperif for allp,q\u2208[0,1],\nEy\u223cBer(q)/bracketleftbig\u2113(p,y)\u2212\u2113(q,y)/bracketrightbig\u22650.\nWe letL\u2217be the class of proper losses such that|\u2113(p,1)\u2212\u2113(p,0)|\u22641for allp\u2208[0,1].7\nOf particular importance are theV-shapedlosses studied in [ HSLW23 ,KLST23], which are all\nfunctions\n\u2113v(p,y) =\u2212sign(p\u2212v)(y\u2212v),\nwherev\u2208[0,1]. These are proper losses, and moreover they form a basis for L\u2217; a precise formulation\nwhich builds on [KLST23] is given in Lemma 3.5.\nPost-processing functions.We say that a class of post-processings \u03ba: [0,1]\u2192[0,1]isvalidif it\ncontains the identity function and satisfies a certaintranslation invarianceunder shifts of either axis\n(the precise definition is in Definition 3.9). Most natural classes of post-processings we know that\nhave been considered previously, including K\u2217, Lipschitz, monotone and bounded degree polynomial\npost-processings, are valid classes. For a valid post-processing class,K, let\nthr(K) =/braceleft\uf8ecig\np\u221d\u21d5\u22a3\u221a\u222b\u2294\u2240\u2192sign+/parenleft\uf8ecig\n\u03ba(p)\u22121\n2/parenright\uf8ecig/vextendsingle/vextendsingle/vextendsingle\u03ba\u2208K/braceright\uf8ecig\n.\nWhile the choice of1 /2might seem arbitrary, the translation invariance of Kimplies that any\nconstant in(0,1)will do.\nWe will pay special attention to the class ofmonotonicpost-processing functions, as well as a\nnatural generalization of this class. To define the class, recall that aninterval I\u2286[0,1]with no\nfurther specification may be open, closed, half-open, or a singleton.\nDefinition 2.2(Generalized Monotonicity).Given an integer r\u2208Nand a function \u03ba: [0,1]\u2192[0,1],\nwe say\u03ba\u2208M rif for all valuesv\u2208R, thev-superlevel setof\u03ba,\n\u03ba\u22121/parenleftbig[v,1]/parenrightbig=/braceleftbigp\u2208[0,1]/vextendsingle/vextendsingle\u03ba(p)\u2265v/bracerightbig,\ncan be expressed as the union of at most rdisjoint intervals I1,...,Ir\u2286[0,1]. Then,Mris a valid\npost-processing class. In addition, let M+andM\u2212denote the sets of monotonically nondecreasing\nand nonincreasing functions\u03ba: [0,1]\u2192[0,1], respectively.\nWe callMra class of \u201cgeneralized\u201d monotonic functions because the monotonic functions M+\nandM\u2212are both subsets of M1, and becauseMr\u2286Msfor allr\u2264s. We also note that Mrand\nM+are valid post-processing classes, but M\u2212is not: Although M\u2212is translation invariant, it\ndoes not contain the identity function. Generalized monotone functions are the broadest class of\nweight functions that admit efficient algorithms for CDLby our results. In Section A we discuss why\nthey constitute a natural class of post-processings to consider, both from a theoretical and practical\nviewpoint. In Figure 1 we give an example of a function which is (very) non-monotone, but is a\n3-generalized monotone function.\n7Note that our definition of L\u2217includes all proper loss functions with range[0 ,1]. The exact range ([ \u22121,1]versus\n[0,1]) is not crucial, it will only change the definition ofCDLby a factor of2.\n5\nFigure 1: Function \u03ba: [0,1]\u2192[0,1]that crosses every threshold v\u2208[0,1]of its range at most3\ntimes. The monotonicity of the function \u03ba(p)changes14times as pgrows from0to1. Although \u03ba\nis non-monotone, we have\u03ba\u2208M rforr= 3.\nCalibration Decision Loss.Next we definecalibration decision loss( CDL), introduced by\n[HW24]. We study a version ofCDLthat is parameterized by avalid post-processing classK.\nDefinition 2.3(Calibration Decision Loss relative to K).Given valid post-processing class Kand\na joint distributionJover pairs(p,y)\u2208[0,1]\u00d7{0,1}, thecalibration decision lossis\nCDLK(J) = sup\n\u2113,\u03baE/bracketleftbig\u2113(p,y)\u2212\u2113(\u03ba(p),y)/bracketrightbig,\nwhere the supremum is taken over all \u2113\u2208L\u2217and\u03ba\u2208K. Given a subset L\u2282L\u2217we define the\n(L,K)-calibration decision loss as\nCDLL,K(J) = sup\n\u2113\u2208L,\u03ba\u2208KE/bracketleftbig\u2113(p,y)\u2212\u2113(\u03ba(p),y)/bracketrightbig.\nGiven a particular \u2113\u2208L\u2217, we also define a loss-specific version of CDL, called thecalibration fixed\ndecision loss, as\nCDL\u2113,K(J) = sup\n\u03baE/bracketleftbig\u2113(p,y)\u2212\u2113(\u03ba(p),y)/bracketrightbig.\nFor any validK,CDLKis always non-negative since the identity function is in K. WhenK=K\u2217\nis the family ofallfunctions \u03ba: [0,1]\u2192[0,1], then our notion of CDLcoincides with that of [ HW24].\nIn the remainder of this section, we describe the three main tasks related to calibration decision\nloss that we study in our work: efficient testing/auditing, relationship to weight-restricted calibration\nand efficient omniprediction, and our results for each of them.\n2.2 Testing and Auditing\nThe first problem we study is the characterization of post-processing classes Kthat allow for efficient\ntestingofCDL K, which we define as follows.\nDefinition 2.4(Testing CDL).We say that an algorithm Ais an(\u03b1,\u03b2)-tester for CDLK, if the\nalgorithm, upon receiving a large enough set of i.i.d. examples from some unknown distribution J,\noutputs either AcceptorReject, and satisfies the following properties with probability at least2 /3.\n1. IfCDLK(J)>\u03b1, thenAoutputsReject,\n6\n2. IfCDLK(J)\u2264\u03b2, thenAoutputsAccept.\nAs is typical, the probability here is taken over the sampling process, and the internal randomness\nof the tester. The tester\u2019s output is unconstrained if CDLK(J)\u2208(\u03b2,\u03b1). We also consider a relaxation\nof the testing problem, calledauditing, which we define in terms of theexpected calibration error\nECE(J) =E/vextendsingle/vextendsingleE[y|p]\u2212p/vextendsingle/vextendsingle. Recall that Hu and Wu [ HW24] showed that CDLK(J)\u22642\u00b7ECE (J). The\nfollowing definition weakens the second requirement to accepting predictors with smallECE.\nDefinition 2.5(Auditing for CDL).We say that an algorithm Ais an(\u03b1,\u03b2)-auditor for CDLK, if\nthe algorithm, upon receiving a large enough set of i.i.d. examples from some unknown distribution\nJ, outputs either AcceptorReject, and satisfies the following property, with probability at least2 /3.\n1. IfCDLK(J)>\u03b1, thenAoutputsReject,\n2. IfECE(J)\u2264\u03b2/2, thenAoutputsAccept.\nSample complexity of testing and auditing.Our first result shows that the VC dimension of\nthr(K)governs the sample efficiency of calibration testing and auditing forCDL K.\nTheorem 2.6(Sample Complexity Bounds).Let Kbe a valid post-processing class, and let\nd=VCdim(thr(K)). Then,\n1. For any\u03b1,\u03b5\u2208(0,1), there is an(\u03b1,\u03b1\u2212\u03b5)-tester forCDL Kwith sample complexity \u02dcO(d/\u03b52).\n2. Any(1/8,0)-auditor forCDL Krequires\u2126(\u221a\nd)samples.\nSome implications of this result:\n\u2022Auditing is easier than testing, since any( \u03b1,\u03b2)-tester for CDLKis also an(\u03b1,\u03b2)-auditor. Hence\nour lower bound for auditing also applies to testers. The case \u03b2= 0corresponds to perfect\ncalibration, so our lower bound holds even for algorithms that must only accept perfectly\ncalibrated predictors, and reject predictors with largeCDL K.\n\u2022A key corollary is that auditing CDL Lipis not possible with finitely many samples. This follows\nfrom the observation thatthr(Lip)has unbounded VC dimension (see Corollary 4.4).\n\u2022The lower bound makes use of V-shaped losses. These losses are not strongly convex unlike\ncommon proper losses used in practice, and one may wonder if the lower bound can be\ncircumvented by assuming strong convexity. The answer is No: we prove that essentially the\nsame lower bound carries over even for strongly convex losses (see Corollary 4.7).\n\u2022V-shaped losses are discontinuous in the prediction value p. If we restrict our attention to loss\nfunctions that are Lipschitz continuous as a function of p, and only consider the CDL for such\nlosses and Lipschitz post-processings, then we show that this measure is quadratically related\nto the smooth calibration error, hence it is efficiently auditable (see Theorem 4.9).\nComputationally efficiency from learnability.We show that efficient algorithms for testing\nand auditingCDL Kcan be derived from efficient learning algorithms forthr(K).\nWe will use the standard primitive of agnostic learning, first introduced by [ KSS94]. It is known\nto be equivalent to seemingly weaker primitives like weak agnostic learning [Fel09, KK09].\n7\nDefinition 2.7(Agnostic Learning).Let C\u2286{ [0,1]\u2192{\u00b1 1}}. We say that an algorithm Ais an\n\u03b5-agnostic learner for Cif, upon receiving a large enough set of i.i.d. examples from some unknown\ndistribution Dover[0,1]\u00d7{\u00b1 1},Aoutputs some hypothesis h: [0,1]\u2192{\u00b1 1}such that, with\nprobability at least0.9over the samples and the internal randomness ofA, we have:\nP\n(p,z)\u223cD[h(p)\u0338=z]\u2264min\nf\u2208CP\n(p,z)\u223cD[f(p)\u0338=z] +\u03b5\nMoreover, ifh\u2208Cwe say thatAis proper.\nOur main algorithmic result is a reduction from CDLKtesting to proper agnostic learning for\nthr(K). ForCDL Kauditing, improper agnostic learning suffices.\nTheorem 2.8(Testing and Auditing from Agnostic Learning, Theorem 6.1).Let Kbe a valid\npost-processing class. Let ALbe an\u03b5-agnostic learner for thr(K). Then, for any \u03b1\u2208(0,1), there is\nan(\u03b1,\u03b1\u2212 3\u03b5)-auditor for CDLKthat makes \u02dcO(1/\u03b5)calls to AL. Moreover, if ALis proper, then\nthere an(\u03b1,\u03b1\u22123\u03b5)-tester forCDL Kof similar complexity.\nThis result lets us translate efficient agnostic learning algorithms for classes of Boolean functions\non[0,1]to efficient algorithms forCDL Kestimation for valid post-processing classes.\n\u2022For the classM+of monotone post-processings, thr(M+)is the family of intervals[ a,1], and\nit is a classic result that intervals are efficiently agnostically learnable. The efficient tester\nthat results from Theorem 2.8 strengthens and extends the result of [ RSB+25] who showed\nthat CDLM+can be bounded by ensuring low weight-restricted calibration error for intervals.\n\u2022More generally, for \u03ba\u2208Mr, the sets\u03ba(p)\u2265vcan be expressed as the union of at most r\ndisjoint intervals. We show that this class admits an efficient proper agnostic learner. As a\ncorollary, we obtain an efficient tester forCDL Mr.\n2.3 Calibration Decision Loss and Weight-Restricted Calibration\nThe next problem we study is the relation between CDLKandweight-restrictedcalibration measures,\ndefined below. The standard definition [ GKSZ22,GH25] either takes the absolute value of the\nexpectation or assumes thatWis closed under negation, we intentionally will do neither.\nDefinition 2.9(Weight-Restricted Calibration Error).Given a distribution Jover(p,y)\u2208[0,1]\u00d7\n{0,1}and a class of weight functions W\u2286{ [0,1]\u2192[\u22121,+1]}, theW-restricted calibration erroris\nCEW(J) = sup\nw\u2208WE/bracketleftbigw(p)(y\u2212p)/bracketrightbig.\nIn the case thatWis closed under negations, we haveCE W(J)\u22650.\nSeveral works [ KLST23,HW24,BGHN23b ,RSB+25] have proved results that can be viewed\nas instances of this general question for specific choice of loss families Land post-processings K.\nSuch characterizations are valuable because weight-restricted calibration error measures have been\nwell studied in the literature, and we understand the relation between various measures fairly well\n[BGHN23a ,GH25]. Moreover, we would like notions of approximate calibration to simultaneously\ngive small calibration decision loss and strong indistinguishability, making it natural to ask to what\nextent one implies the other.\nWe significantly extend and complete this line of work with a general characterization that holds\nfor all valid classesK.\n8\nTheorem 2.10.Given a valid post-processing classK, let\nthr\u2032(K) =thr(K)\u222a/braceleft\uf8ecig\np\u221d\u21d5\u22a3\u221a\u222b\u2294\u2240\u2192\u2212sign+(p\u2212v)/vextendsingle/vextendsingle/vextendsinglev\u2208R/braceright\uf8ecig\n.\nThen\nCEthr\u2032(K)(J)2\u2272CDLK(J)\u2272CEthr\u2032(K)(J).\nWhile the class thr(K)contains only theupperthresholds of functions in K, the class thr\u2032(K)\nalso includes alllowerthresholds of the identity function. It also contains upper thresholds of the\nidentity function by the inclusion of thr(K), sinceKis translation invariant and contains the identity.\nSince our definition of weight-restricted calibration error did not involve absolute values, the decision\nforthr\u2032(K)to not include lower thresholds of all functions inKis deliberate and consequential.\nThis change in the definition of the appropriate concept class for characterizing testability and\nfor the characterization in terms of weighted calibration error may at first seem counterintuitive.\nThis arises because CDLis fundamentally an asymmetric notion (meaning we do not assume \u03ba\u2208K\nimplies1\u2212\u03ba\u2208K); e.g. monotone increasing post-processings are significantly more natural than\nmonotone decreasing ones. Our definition of thr\u2032(K)precisely captures how much we need to\nenhance thr(K)to be able to relate it to weighted calibration. Our results on testing, auditing\nand omniprediction are characterized by naturally symmetric notions such as VC Dimension, and\nagnostic learning, and would hold equally well if we used thr\u2032(K)instead of thr(K), or even if we\nadded all lower thresholds of functions in K; this would not change the VC dimension or agnostic\nlearnability. But Theorem 2.10 would no longer hold with thr(K)in place of thr\u2032(K), as explained\nin the proof sketch below.\nThis result generalizes and significantly extends results that were previously known in the\nliterature. For instance, when K=K\u2217, then thr(K)consists of all functions W\u2217={[0,1]\u221d\u21d5\u22a3\u221a\u222b\u2294\u2240\u2192[\u22121,1]}\nand the corresponding weight-restricted calibration notion is just ECE. Thus in this case, we recover\nthe result of [HW24, KLST23] which shows that\nECE(J)2\u2272CDLK\u2217(J)\u2272ECE(J).\nWhenK=M+is all monotone increasing functions, then thr(M+)contains all indicators of\nintervals[a,1]fora\u2208[0,1]. Let Intdenote the collection of these indicators, along with their\ncomplements. In this case, our result shows that\nCEInt(J)2\u2272CDLM+(J)\u2272CE Int(J).\nThe upper bound was shown in the work of [ RSB+25], while the lower bound is new. For the class\nof generalized monotone functions Mr, our theorem shows that CDLMris quadratically related to a\ngeneralized version of CDL Int, in which single intervals are replaced by unions of at most rintervals.\nProof Sketch.Theorem 2.10 is a highly general result, which transforms a bound on the thr\u2032(K)-\nrestricted calibration error into a bound on CDLK, and vice versa. In the language of outcome\nindistinguishability, the key insight underlying the proof is that the two types of functions in the\nweight class thr\u2032(K), namely upper thresholds of Kand lower thresholds of the identity, are precisely\nwhat we need to move, respectively,toandfromthe world of simulated outcomes. This perspective\nbuilds on theloss OIframework of [GHK+23], particularly their study ofdecision OI.\nInslightlymoredetail, wefirstconsideraweightfunctionobtainedbycomposingapost-processing\nfunction\u03ba(p)with a proper loss function\u2019snegated discrete derivative \u2212\u2202\u2113(p) =\u2113(p,0)\u2212\u2113(p,1). In\nthe worst case\u2014that is, the case of a V-shaped loss function\u2014this composite function corresponds\n9\nexactly to someupper thresholdof \u03ba. Calibration with respect to this composite weight function\nensures that moving from real outcomes yto simulated outcomes \u02dcycan onlyimprovethe loss\nattained by\u03ba(p), up to an\u03b5slack factor:\nE/bracketleft\uf8ecig\n\u2113/parenleftbig\u03ba(p),y/parenrightbig/bracketright\uf8ecig\n\u2265E/bracketleft\uf8ecig\n\u2113/parenleftbig\u03ba(p),\u02dcy/parenrightbig/bracketright\uf8ecig\n\u2212\u03b5.\nNext, we observe that in the simulated world, where \u02dcy\u223cBer (p), the predictor pis Bayes optimal by\ndefinition. In particular, it outperforms\u03ba(p):\nE/bracketleft\uf8ecig\n\u2113/parenleftbig\u03ba(p),\u02dcy/parenrightbig/bracketright\uf8ecig\n\u2265E/bracketleftbig\u2113(p,\u02dcy)/bracketrightbig.\nFinally, we will show that weight functions corresponding tolower thresholdsof the identity allow\nus to move back to the world of real outcomes, again up to a small slack factor:\nE/bracketleftbig\u2113(p,\u02dcy)/bracketrightbig\u2265E/bracketleftbig\u2113(p,y)/bracketrightbig\u2212\u03b5.\nCombining this chain of inequalities, we deduce that poutperforms \u03ba(p)in the real world, as well.\nPhrased differently,thr\u2032(K)-restricted calibration ensures thatCDL Kis small.\nThe proof of the converse implication\u2014that small CDLKimplies small thr\u2032(K)-restricted cali-\nbration error (up to a quadratic gap)\u2014relies on similar ideas, but is much subtler. For simplicity,\nconsider the most fundamental post-processing class for which the result was not previously known:\nmonotonically increasing functions K=M+. In this case, we are given that CDLM+\u2264\u03b5and\nare tasked with proving that CEInt\u2272\u221a\u03b5. To do so, we consider an arbitrary interval I\u2286[0,1]\nand break it into m=O(1/\u221a\u03b5)subintervals I1,...,Imwith roughly equal probability mass under\nthe distribution of predictions. We then show that the calibration error restricted to a particular\nsubintervalI j= [a,b]can be bounded by the product of its length and mass, plus an\u03b5slack term.\nSince each subinterval has mass \u22481/mand their lengths sum to at most1, our total bound becomes\nroughly1/m+m\u03b5=O(\u221a\u03b5). We give our full proof of Theorem 2.10 in Section 5.\nWe conclude by observing that some assumption on Klike validity is necessary for the characteri-\nzation to hold: the characterization does not hold for the class M\u2212of non-increasing post-processings.\nThis class is translation invariant, but does not contain the identity, so it is not valid by our definition.\n2.4 Post-Processing and Omniprediction\nHere we ask the question: if the predictor Psuffers large calibration decision loss relative to K, how\nshould we remedy it? We would ideally like to post-process it in an efficient manner that gives\nguarantees for every loss in L\u2217, competitive to baselines from the set K. But the issue is that there\nmight be several losses in \u2113\u2208L\u2217that witness large calibration decision loss, each with its own\npost-processing\u03ba=\u03ba\u2113, which might not be good for a different loss.\nTo circumvent this, we could allow post-processings that need not themselves lie in K. For\ninstance, if we could take \u03ba(p) =E[y|p]to be the perfect recalibration, then we would have\na guarantee for all losses. However, this function will be inefficient to compute (it is as hard\nas estimating ECE). Under what conditions can we efficiently find a post-processing that gives\nguarantees for all losses?\nWe formulate this as a problem of efficientomniprediction[ GKR+22,GHK+23]. The notion\nof omniprediction originating in supervised learning [ GKR+22] asks for a predictor that is simul-\ntaneously competitive with the best hypothesis from a class Cof hypotheses, for any loss from a\nfamilyL. The power of this notion comes from the fact that the best hypothesis in ccan depend\n10\non the loss \u2113\u2208L, whereas our predictor is oblivious to \u2113. In our context, the goal is to learn a\npost-processing function /hatwide\u03bathat outperforms any other in Kwith respect toallpossible decision\ntasks. This is a departure from its standard definition, where the baseline is a hypothesis class Cof\nfunctions on the feature space X, and is similar to its formulation in the recent work of [ HWY25],\nthat shows omniprediction guarantees for smooth calibration.\nDefinition 2.11(Omniprediction).We say that a function /hatwide\u03ba: [0,1]\u2192[0,1]is an(\u03b5,K)-\nomnipredictorfor some distributionJover pairs(p,y)if for all\u2113\u2208L\u2217and\u03ba\u2208K,\nE/bracketleftbig\u2113(/hatwide\u03ba(p),y)/bracketrightbig\u2264E/bracketleftbig\u2113(\u03ba(p),y)/bracketrightbig+\u03b5.\nWe say thatAlearns an( \u03b5,K)-omnipredictor with probability1 \u2212\u03b4if, upon receiving a large\nenough set of i.i.d. samples from some unknown distribution J, the algorithmAoutputs an\n(\u03b5,K)-omnipredictor forJwith probability at least1\u2212\u03b4.\nIn this section, we will suppress the dependence on \u03b5,\u03b4and say that an algorithm learns a\nK-omnipredictor if it returns an( \u03b5,K)-omnipredictor with high probability.8We show a range of\nomniprediction guarantees for various classes K, either by using techniques from the omniprediction\nliterature [ GHK+23], or by a new analysis of well-known recalibration procedures that have been\nproposed in the machine learning literature [ ABE+55,ZE01,GR21,SSH23]. We start with the\nmost general result.\nOmniprediction from agnostic learning.For all valid post-processing classes K, we prove\nthat an omnipredictor can be efficiently learnt, under the assumption that thr(K)is agnostically\nlearnable. Thus the same assumption we require for efficient auditing of CDLKis in fact sufficient\nto ensure omniprediction.\nTheorem 2.12(Omniprediction from Agnostic Learning, Informal Version of Theorem 7.1).For\nevery valid post-processing class K, there is an efficient reduction from learning a K-omnipredictor\nto agnostic learning forthr(K).\nWe follow the loss OI framework of [ GHK+23] which is an indistinguishability-based approach\nto omniprediction. The key difference between their setting and ours is that they compete against\na baseline of hypotheses C={c:X\u2192{ +1,\u22121}}whereXdenotes the feature space. Whereas\nin our calibration setting, we do not have a feature space X, we compete against a baseline of\npost-processing functions \u03ba(p). Nevertheless, we show how one can adapt their techniques to the\nsetting of calibration to learn omnipredictors efficiently.\nPool Adjacent Violators is an omnipredictor.The Pool Adjacent Violators algorithm\n[ABE+55] solves the problem of isotonic regression: given samples from J= (p,y), it finds a\nmonotone post-processing of a predictor pthat minimizes the square loss among all monotone\npost-processings. Given a sample of {( yi,pi)}pairs, it starts from the Bayes optimal predictor on\nthe sample \u03ba(p) =E[y|p], and pools/merges any adjacent pair that violates monotonicity, till it\nreaches a monotone predictor. We present the algorithm formally in Algorithm 1.\nVarious works have observed that it actually gives guarantees for broader classes of loss functions\n(including convex proper losses); see [ ZE02,BdP13] and references therein. We will show that it\nis an omnipredictor for all of L\u2217relative to monotone post-processings. This general statement is\n8The results will typically involve a reduction to some other algorithm, whose parameters will be suitably chosen\nfunctions of\u03b5,\u03b4. The formal theorem statements make this dependence explicit.\n11\nnew to our knowledge. For instance [ BdP13] shows the result for a subset of scoring rules that they\ncall regular proper scoring rules, defined in terms of certain integrals. This does not capture all\nproper losses (indeed it is unclear to us what subset they capture), and they use considerably more\ncomplex arguments. We present a simple argument that relies on thecloser is betterproperty of\nproper losses (see Lemma 3.2) which states for a proper loss, moving the prediction pcloser to the\nBayes optimalE[y|p]can only help. Formally, we establish the following:\nTheorem 2.13(Omniprediction through PAV, Informal Version of Theorem 7.5).Pool Adjacent\nViolators (PAV) with sufficiently many samples learns aM +omnipredictor.\nThis shows that the class of monotone post-processings admits aproper omnipredictor: for every\ndistribution J= (p,y), there is a single post-processing \u03ba\u2217\u2208M +with the guarantee that for every\nproper loss\u2113\u2208L\u2217, and\u03ba\u2208M +,\nE[\u2113(\u03ba\u2217(p),y)]\u2264E[\u2113(\u03ba(p),y)].\nOther thanK\u2217, this is the only natural class of post-processings we know that has this property.\nOmniprediction through Bucketing and Recalibration.We next analyze bucketed recalibra-\ntion through uniform-mass binning. Binning is a long-established technique for measuring calibration\n[Mil62,San63]. The method of uniform-mass binning was introduced by [ ZE01] as the first binning-\nbased approach not only for measuring calibration, but also for obtaining a calibrated predictor.\nRather than choosing equal-width bins, we choose bin boundaries as quantiles, so that every bin has\nroughly the same mass. It remains empirically competitive to this day [ NCH15,GPSW17 ,RCSM22 ],\nand its calibration properties have been theoretically studied as well [GR21, SSH23].\nInformally, uniform-mass binning tries to divide[0 ,1]into bins that each have probability \u03b5\nof containing the prediction p. Ifphas a continuous distribution, we can do this by taking the\n\u03b5-quantiles as bin boundaries. For general distributions, we might need to allowsingletonbuckets\nconsisting of a single point to account for point-masses that might be larger than \u03b5. This gives a\npartition of[0,1]intoO(1/\u03b5)buckets that are either singletons, or have probability bounded by\u03b5.\nWe show that uniform-mass binning followed by recalibration yields omniprediction with respect\nto the class of generalized monotone post-processings.\nTheorem 2.14(Omniprediction from Uniform-mass binning, Informal Version of Theorem 7.9).\nFor allr\u22651, Uniform-mass binning (with O(r2)bins) and recalibration (Algorithm 2) learns an\nMr-omnipredictor.\nThe proof proceeds by comparing the bucket-wise recalibration /hatwide\u03bato any hypothesis in \u03ba\u2208Mr,\nfor a specific V-shaped loss\u2113 v. We rely on two observations:\n1.Ifsign(\u03ba(p)\u2212v)is constant for a bucket Ij, then/hatwide\u03badoes at least as well as \u03bafor the loss \u2113v,\nup to sampling error (which is small by uniform convergence for intervals).\n2.Ifsign(\u03ba(p)\u2212v)is not constant for bucket Ij, this means that the graph of \u03ba(p)crosses the\nvaluevin this bucket, so the bucket is not a singleton.\n\u2022By the definition of Mr, this can happen for only2 rbuckets, which contain the endpoints\nof therintervals that constitute the set\u03ba(p)\u2265v.\n\u2022Since we use Uniform-mass bucketing, the total probability assigned to these buckets is\nsmall, which bounds how much worse /hatwide\u03bais than\u03ba.\n12\nSummary\nOur work presents a comprehensive theory of the complexity of CDLKfor binary classification.\nIt delineates classes of post-processings (e.g. Lipschitz) that are intractable and classes that are\nefficient (e.g. generalized monotone functions). It proves a tight relationship to weight-restricted\ncalibration. It introduces techniques from the literature on omniprediction for post-processing\npredictors, and proves rigorous new guarantees for some well-known algorithms used in practice.\nWe leave open the question of understanding efficient CDLfor the multiclass setting where the\nnumber of labels kgrows, as is the case in image classification. It is known that the problem in the\nweight-restricted setting becomes much harder for large k, indeed notions like smooth calibration\nand distance to calibration require sample complexity exp(k)[GHR24]. It is a challenging question\nto formulate efficient and meaningful notions ofCDLfor this setting.\n3 Preliminaries\nIn this section, we briefly review relevant concepts related to proper losses, calibration error, post-\nprocessing classes, and relevant fundamental concepts from learning theory. Proofs for results in\nthis section appear in Appendix B.1.\nMathematical MiscellanyGiven scalars t,a,b\u2208R witha\u2264b, we write[ t]b\nato denote the\nprojection of tonto the closed interval[ a,b]. When referring to aninterval I\u2286Rwith no further\nspecification, we mean that Imay be open, closed, half-open, or a singleton. Phrased differently, an\nintervalImay have the form[ a,b],[a,b),(a,b], or(a,b)for somea\u2264b. This includes singleton sets\nof the form{a}. We shall sometimes write f(x)\u2272g(x)to mean that f(x) =O(g(x)). Similarly,\nf(x)\u2273g(x)meansf(x) = \u2126(g(x)). To avoid ambiguity, we will only use this notation when both\nfunctionsf(x)andg(x)under consideration are nonnegative.\nProper LossesSince the class L\u2217of proper losses will be our focus throughout this work, it will\nbe instructive to review the following standard characterization of the functions that it contains.\nLemma 3.1(Proper Loss Characterization).If \u03c6: [0,1]\u2192Ris concave and \u03c6\u2032: [0,1]\u2192[\u22121,+1]\nis the derivative of\u03c6(or an arbitrary superderivative if\u03c6is nondifferentiable), thenL\u2217contains\n\u2113(p,y) =\u03c6(p) +\u03c6\u2032(p)(y\u2212p).\nConversely, every\u2113\u2208L\u2217has this form.\nWhen we first defined L\u2217, we insisted that thediscrete partial derivativewith respect to the\nbinary outcome, namely \u2202\u2113(p) =\u2113(p,1)\u2212\u2113(p,0), be bounded in absolute value (see Definition 2.1).\nThe characterization in Lemma 3.1 shows that this corresponds to a bound on \u03c6\u2032(p) =\u2202\u2113(p). The\ncharacterization also leads to the following useful lemma, we call thecloser is betterlemma which\nstates that shifting one\u2019s prediction toward the truth can only improve one\u2019s loss. To state it, recall\nthat\u2113(p,q) =E y\u223cBer(q) [\u2113(p,y)].\nLemma 3.2(Closer is better).If\u2113\u2208L\u2217anda\u2264b\u2264c, then\u2113(b,c)\u2264\u2113(a,c)and\u2113(b,a)\u2264\u2113(c,a).\nThe preceding lemma can be used to show a bound on the range of any\u2113\u2208L\u2217:\nLemma 3.3(Bound on|\u2202\u2113|Implies Bound on|\u2113|).If\u2113\u2208L\u2217, then there existsc\u2208Rsuch that\nc\u22121\u2264\u2113(p,y)\u2264c+ 1\nfor all inputs(p,y)\u2208[0,1]\u00d7{0,1}.\n13\nAnother consequence of the characterization in Lemma 3.1 is the following description of the\n\u201cboundary\u201d of the convex set L\u2217. Specifically, it is roughly the case that any loss in L\u2217is a convex\ncombination of a special type of proper loss functions, which we callV-shapedlosses.\nDefinition 3.4(V-Shaped Loss).Given a scalar s\u2208[\u22121,+1], consider the modified sign function\nsigns(t) =\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3+1ift>0,\nsift= 0,\n\u22121ift<0.\nTheV-shapedlosses are the functions\n\u2113v,s(p,y) =\u2212signs(p\u2212v)(y\u2212v),\nwherev\u2208[0,1]ands\u2208[\u22121,+1]are any values. Each loss \u2113v,sbelongs to the class L\u2217. We pay\nparticular attention to the V-shaped losses \u2113v:=\u2113v,0and\u2113+\nv:=\u2113v,(+1), and\u2113\u2212\nv:=\u2113v,(\u22121). We also\nwritesign+= sign(+1)andsign\u2212= sign(\u22121)for brevity.\nLemma 3.5(Modification from [ KLST23]).LetL0be the set containing the V-shaped losses \u2113+\nv\nand\u2113\u2212\nvfor allv\u2208[0,1], as well as constant functions. Then for all \u2113\u2208L\u2217and\u03b5>0, there exists a\nconvex combination\u2113 0of finitely many functions inL 0such that|\u2113(p)\u2212\u2113 0(p)|\u2264\u03b5at allp\u2208[0,1].\nThe preceding lemma can be used to reduce a search over all \u2113\u2208L\u2217to a search over a much\nsmaller set. For example, in the original definition of CDL, it suffices to take the supremum over\njust the set of \u2113+\nvand\u2113\u2212\nvlosses, rather than all of L\u2217. In fact, the following corollary further shows\nthat it suffices to take the supremum overjustthe set of\u2113+\nvlossesorthe set of\u2113\u2212\nvlosses:\nCorollary 3.6.Given a distributionJover pairs(p,y), post-processingsK, andL\u2286L\u2217, let\nCDLL,K(J) = sup\n\u2113,\u03baE/bracketleftbig\u2113(p,y)\u2212\u2113(\u03ba(p),y)/bracketrightbig,\nwhere the supremum is taken over all\u2113\u2208L(notL\u2217) and\u03ba\u2208K. then\nCDLK(J) =CDLL+,K(J) =CDLL\u2212,K(J),\nwhereL+={\u2113+\nv|v\u2208[0,1]}andL\u2212={\u2113\u2212\nv|v\u2208[0,1]}.\nIn Definition 2.9, we gave a broad definition ofweighted calibration errormeasures, which is\nparameterized by a class of weight functions W. The larger the class W, the larger the corresponding\nweighted calibration error. When Wcomprises all functions with range contained in[ \u22121,+1], we\narrive at the standard notion ofexpected calibration error (ECE).\nDefinition 3.7(Expected Calibration Error).Given a distributionJover(p,y)\u2208[0,1]\u00d7{0,1},\nECE(J) =E/vextendsingle/vextendsingleE[y\u2212p|p]/vextendsingle/vextendsingle.\nAs mentioned previously, [ KLST23,HW24] showed that the calibration decision loss CDLK\u2217\nwith respect to the class ofallpost-processing functions K\u2217={[0,1]\u2192[0,1]}is quadratically\nrelated toECE. In particular, the upper bound also holds forCDL Kfor any subsetK\u2286K\u2217.\nTheorem 3.8([KLST23, HW24]).Given a distributionJover(p,y)\u2208[0,1]\u00d7{0,1},\nECE(J)2\u2264CDLK\u2217(J)\u22642ECE(J).\n14\nPost-ProcessingHere, we briefly discuss some terminology and facts related to post-processing\nclasses that will be useful in subsequent sections. The first thing we need is a definition of what\nconstitutes avalidpost-processing class K. For this, our only two requirements are that the K\ncontains the identity and istranslation invariant.\nDefinition 3.9(Valid Post-Processing Class).We say that a class of functions K\u2286{ [0,1]\u2192[0,1]}\nis avalid post-processing classif the following two conditions hold:\n\u2022(Identity)Kcontains the identity function\u03ba(p) =p,\n\u2022(Translation Invariance)Fix anys,t\u2208R. If\u03babelongs toK, then so does the function\n\u03bas,t(p) =/bracketleft\uf8ecig\n\u03ba/parenleftbig[p+s]1\n0/parenrightbig+t/bracketright\uf8ecig1\n0.\nThe fact thatKcontains the identity function means that CDLKwill always be nonnegative.\nLater, we will show that the complexity of calibration decision loss relative to a valid post-processing\nclassesK, is closely related to the complexity of the classthr(K)of its upper thresholds.\nDefinition 3.10(Upper Thresholds).Given a valid post-processing class K, itsupper thresholds\nare\nthr(K) =/braceleft\uf8ecig\np\u221d\u21d5\u22a3\u221a\u222b\u2294\u2240\u2192sign+/parenleft\uf8ecig\n\u03ba(p)\u22121\n2/parenright\uf8ecig/vextendsingle/vextendsingle/vextendsingle\u03ba\u2208K/braceright\uf8ecig\n.\nAlthough the cutoff value of1 /2in Definition 3.10 may seem arbitrary, its choice is not especially\nimportant when working with valid post-processing classes, which are translation invariant.\nDefinition 3.11(VC Dimension).A class C\u2286{ [0,1]\u2192{\u00b1 1}}shattersS\u2286[0,1]if every function\nfromSto{\u00b11}is the restriction of some function in C. TheVC dimensionof C, denoted VCdim (C),\nis the size of the largest set it shatters. We sayVCdim(C) =\u221eif the dimension is unbounded.\n4 Sample Complexity of Testing and Auditing\nIn this section, we show that the sample complexity of testing/auditing is characterized (up to\na quadratic gap) by the VC dimension of the class thr(K). We use this to derive a lower bound\nforCDL Lipwhere Lipdenotes the family of1-Lipschitz post-processings. We then consider the\npossibility of circumventing this lower bound by consider restricted loss families.\n4.1 A Characterization via VC dimension\nOur main sample complexity characterization is as follows.\nTheorem 4.1(Sample Complexity Bounds).Let Kbe a valid post-processing class, and let\nVCdim(thr(K)) =d. Then,\n1.For any\u03b1,\u03b5\u2208 (0,1), there is an( \u03b1,\u03b1\u2212\u03b5 )-tester for CDLKwhose sample complexity is\nO(dlog(1/\u03b5)/\u03b52).\n2. Any(1/8,0)-auditor forCDL Krequires\u2126(\u221a\nd)samples.\nThe upper bound relies on the following generalization result for CDL.\n15\nLemma 4.2(Uniform Convergence for CDL).Let Kbe a valid post-processing class, and let\nVCdim (thr(K)) =d. For any\u03b5,\u03b4\u2208 (0,1)and any set Sofm=O(dlog(1/\u03b5\u03b4)/\u03b52)i.i.d. examples\nfrom some distributionJover[0,1]\u00d7{0,1}, with probability at least1\u2212\u03b4, we have:\nsup\n\u2113\u2208L\u2217\n\u03ba\u2208K/vextendsingle/vextendsingle/vextendsingleE\n(p,y)\u223cJ[\u2113(p,y)\u2212\u2113(\u03ba(p),y)]\u2212E\n(p,y)\u223cS[\u2113(p,y)\u2212\u2113(\u03ba(p),y)]/vextendsingle/vextendsingle/vextendsingle\u2264\u03b5.\nIn particular, with probability at least1\u2212\u03b4we have|CDL K(J)\u2212CDLK(S)|\u2264\u03b5.\nGiven this lemma, the upper bound is straightforward: we estimate CDLK(S)over a sufficiently\nlarge set of samples S, and decide to accept or reject by thresholding at \u03b1\u2212\u03b5/ 2. We defer both the\nproof of Lemma 4.2 and the derivation of the upper bound in Theorem 4.1 to Appendix B.2.\nWe now show the lower bound for auditing. Let p1,p2,...,pd\u2208[0,1]such that( sign+(\u03ba(pi)\u2212\n1/2))i\u2208[d]takes all the possible values in {\u00b11}dfor different choices of \u03ba\u2208K. By the translation\ninvariance ofK, we can assume that there exist q1,...,qd/2\u2208[1/4,3/4]such that( sign+(\u03ba(qi)\u2212\n1/2))i\u2208[d/2]takes all the possible values in{\u00b11}d/2for different choices of\u03ba\u2208K.9\nWe consider the following distributions.\n1.LetJ0be a distribution over[0 ,1]\u00d7{0,1}whose marginal on[0 ,1]is the uniform distribution\nover the set{q 1,...,qd/2}, andE (p,y)\u223cJ 0[y|p=qi] =qi.\n2.LetJ1be a distribution over distributions that is defined as follows. To sample a distribution\nJ1fromJ1, we drawd/2independent random variables yi\u223cBer (qi)fori= 1,...,d/ 2. The\nmarginal ofJ 1on[0,1]is uniform over{q 1,...,qd/2}, andE (p,y)\u223cJ 1[y|p=qi] =yi.\nIt is easy to show that J0is perfectly calibrated, whereas every distribution J1\u2208J 1hasECE(J1)\u2265\n1/4. This is the basis of a lower bound for estimating the ECE in [ GHR24]. It is not true that\nCDLK(J1)is large for every choice of J1\u2208J 1. Nevertheless, we will show that CDLK(J1)is likely to\nbe large for a randomJ 1, which is sufficient for the lower bound to go through.\nLemma 4.3.We have\nPr\nJ1\u223cJ1/bracketleftbigg\nCDLK(J1)\u22651\n8/bracketrightbigg\n\u22655\n6.\nProof.Consider the proper loss function\n\u2113(p,y) =\u2113+\n1/2(p,y) =\u2212(y\u22121/2) sign+(p\u22121/2).\nWe will lower boundCDL Kby\nCDL\u2113,K(J1) = sup\n\u03ba\u2208K\uf8eb\n\uf8ed2\ndd/2/summationdisplay\ni=1/parenleftbigg\nyi\u22121\n2/parenrightbigg\nsign+/parenleftbigg\n\u03ba(qi)\u22121\n2/parenrightbigg\n\u2212sign+/parenleftbigg\nqi\u22121\n2/parenrightbigg\uf8f6\n\uf8f8\n= sup\n\u03ba\u2208K\uf8eb\n\uf8ed2\ndd/2/summationdisplay\ni=1/parenleftbigg\nyi\u22121\n2/parenrightbigg\nsign+/parenleftbigg\n\u03ba(qi)\u22121\n2/parenrightbigg\uf8f6\n\uf8f8\n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\na1\u22122\ndd/2/summationdisplay\ni=1sign+/parenleftbigg\nqi\u22121\n2/parenrightbigg\n/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\na2(4.1)\nTo lower bounda 1, we choose\u03ba\u2208Ksuch that\n\u2200i,sign+/parenleftbigg\n\u03ba(qi)\u22121\n2/parenrightbigg\n= sign+/parenleftbigg\nyi\u22121\n2/parenrightbigg\n.\n9If at leastd/2points lie in[0 ,1/2], then we consider q= [p+ 1/4]and\u03ba\u2032(q) =\u03ba([q\u22121/4]) =\u03ba(p). Else, at least\nd/2points lie in[1/2,1], so we considerq i= [pi\u22121/4]and\u03ba\u2032(q) =\u03ba([q+ 1/4]) =\u03ba(p i).\n16\nThere exists such a \u03bafor any realization of( yi)idue to the choice of( qi)i.10Sinceyi\u2208{0,1}, this\nensures that\na1\u22652\ndd/2/summationdisplay\ni=1/parenleftbigg\nyi\u22121\n2/parenrightbigg\nsign+/parenleftbigg\n\u03ba(qi)\u22121\n2/parenrightbigg\n=2\ndd/2/summationdisplay\ni=1/vextendsingle/vextendsingle/vextendsingle/vextendsingleyi\u22121\n2/vextendsingle/vextendsingle/vextendsingle/vextendsingle=1\n2.(4.2)\nNext we show an upper bound ona 2. Over the random choice ofy i\u223cBer(qi), sinceE[y i|qi] =qi,\nE[a2] =2\ndd/2/summationdisplay\ni=1/parenleftbigg\nqi\u22121\n2/parenrightbigg\nsign+/parenleftbigg\nqi\u22121\n2/parenrightbigg\n=2\ndd/2/summationdisplay\ni=1/vextendsingle/vextendsingle/vextendsingle/vextendsingleqi\u22121\n2/vextendsingle/vextendsingle/vextendsingle/vextendsingle\n\u22641\n4\nwhere the last step uses the fact that qi\u2208[1/4,3/4]. By the Hoeffding bound, for dlarger than\nsome constant,Pr[a 2\u22653/8]\u22641/6. Therefore, with probability5/6, by Equations (4.1) and (4.2),\nCDLK(J1)\u22651\n2\u22123\n8=1\n8.\nWe now complete the proof of the lower bound in Theorem 4.1, using a birthday paradox\nargument, as in [GHR24].\nProof of Theorem 4.1, Lower bound. Suppose that we have a(1 /8,0)-auditorAforCDLKwith\nsample complexity at mostm. Since the distributionJ 0is perfectly calibrated\nP\nS0\u223cJm\n0[A(S 0) =Accept]\u22652/3.\nOn the other hand, for J1\u2208J 1, Lemma 4.3 implies that CDLK(J1)\u22651/8with probability5 /6.\nConditioned on this event, Abeing a(1/8,0)auditor for CDLK,A(S1)must reject with probability\nat least2/3. This means that\nP\nJ1\u223cJ1\nS1\u223cJm\n1[A(S 1) =Accept]\u22641\n3\u00b75\n6+1\n6<1\n2.(4.3)\nBut this means that the auditorAsatisfies the condition\n/vextendsingle/vextendsingle/vextendsingleP\nS0\u223cJm\n0[A(S 0) =Accept]\u2212P\nJ1\u223cJ1\nS1\u223cJm\n1[A(S 1) =Accept]/vextendsingle/vextendsingle/vextendsingle\u22651/6,(4.4)\nwhere the probabilities are over the random variables S0,J1,S1and any potential randomness of A.\nIf we condition on the events that all the elements of S0and all the elements of S1are distinct, then\nthe corresponding conditional distributions are identical. Therefore, the total variation distance\nbetween the distribution of S0and the distribution of S1is bounded by the collision probability,\nwhich is smaller than1/6unlessm= \u2126(\u221a\nd).\n10This is in fact the\u03bathat maximizesa 1.\n17\nLower bound for Lipschitz post-processings.Theorem 4.1 rules out efficient CDL estimation\nfor the class of Lipschitz post-processings.\nCorollary 4.4.Let Lip\u2282K\u2217denote the class of1-Lipschitz post-processings. There is no(1 /8,0)-\nauditor forCDL Lipwith finite sample complexity.\nThis follows because thr(Lip)has infinite VC dimension, this can be seen taking Sto be a\ngrid on the interval[0 ,1]of multiples of2 \u03b3, and observing that the functions which take values in\n(1/2\u00b1\u03b3)on the grid points and interpolate linearly between them are Lipschitz. The thresholds of\nthese functions shatter the setS. We now take\u03b3\u21920.\nUpper bound for Generalized monotone post-processings.Theorem 4.1 implies a sample-\nefficient algorithm to estimate CDLMrfor the classMrof generalized monotone functions. This\nfollows from Proposition D.1 which bounds their VC dimension.\nCorollary 4.5.For all r\u22651,\u03b1\u2208[0,1]and\u03b5<\u03b1, there is an( \u03b1,\u03b1\u2212\u03b5 )-tester for CDLMrwith\nsample complexityO(rlog(1/\u03b5)/\u03b52).\nThis bound by itself does not guarantee computational efficiency. But it can be made computa-\ntionally efficient, as shown in Corollary 6.2.\n4.2 Other Families of Loss Functions\nTo conclude this section, we briefly discuss the role played by the class L\u2217in our auditing lower\nbound. For example, one might ask whether our auditing lower bound continues to hold if we\nreplace the class L\u2217with natural subclasses that exclude the \u2113+\n1/2loss, which played a key role in\nour proof. Here, we will investigate two such relaxations, corresponding to strongly proper loss\nfunctions, and loss functions that are Lipschitz in the predictionsp. Note that V-shaped losses do\nnot satisfy either of these conditions.\nLower bounds for strongly proper losses.We consider the class L\u2217\n\u00b5\u2212scof losses\u2113\u2208L\u2217whose\nassociated concave function \u03c6(p) =Ey\u223cBer(p)\u2113(p,y)satisfies\u00b5-strong concavityfor some \u00b5>0. To\nstate the definition, recall that we write\u2113(p,q) =E y\u223cBer(q)\u2113(p,y).\nDefinition 4.6.We say a functionf: [0,1]\u2192Ris\u00b5-strongly concaveif for allx,y,\u03bb\u2208[0,1],\nf/parenleftbig\u03bbx+ (1\u2212\u03bb)y/parenrightbig\u2265\u03bbf(x) + (1\u2212\u03bb)f(y) +\u03bb(1\u2212\u03bb)\u00b7\u00b5\n2(x\u2212y)2.\nWe letL\u2217\n\u00b5\u2212scdenote the class ofstrongly properlosses \u2113\u2208L\u2217, meaning that \u03c6(p) =\u2113(p,p)is\n\u00b5-strongly concave inp. We letCDL L\u2217\n\u00b5\u2212sc,Kdenote the similarly restricted version ofCDL K.\nWhile\u03c6(p)is a concave function for any \u2113\u2208L\u2217, Definition 4.6 goes beyond this by requiring\nstrong concavity. For example, \u03c6(p) =p(1\u2212p)is a\u00b5-strongly concave function with \u00b5= 2,\ncorresponding to the squared loss \u2113sq(p,y) = (y\u2212p )2. We now study the testing and auditing of\nCDL with respect to this restricted class of proper losses, showing that the main results of this\nsection continue to hold.\nCorollary 4.7(Sample Complexity for L\u2217\n\u00b5\u2212sc).LetKbe a valid post-processing class, and let\nVCdim(thr(K)) =d. Then, the following are true.\n18\n1.For any\u03b1,\u03b5,\u00b5\u2208 (0,1), there is an( \u03b1,\u03b1\u2212\u03b5 )-tester for CDLL\u2217\n\u00b5\u2212sc,Kwhose sample complexity\nisO(dlog(1/\u03b5)/\u03b52).\n2. For any\u00b5\u2208(0,1/16), any(1/8\u22122\u00b5,0)-auditor forCDL L\u2217\n\u00b5\u2212sc,Krequires\u2126(\u221a\nd)samples.\nWe defer the proof to Appendix B.2.\nLipschitz losses and Smooth Calibration Error.Corollary 4.4 shows that CDLKis intractable\nwhenKconsists of Lipschitz post-processings. If we further restrict the proper losses to be Lipschitz,\nwe will show that CDL is tightly characterized by the smooth calibration error [ KF08,BGHN23a ],\nwhich is known to be both information-theoretically and computationally tractable.\nWe define our families of Lipschitz losses and post-processings.\nDefinition 4.8.Let LLip\u2282L\u2217denote the family of all losses \u2113such that\u2113(p,y)is1-Lipschitz in\nthe first argument. Let Lip(2)denote the family of post-processing functions \u03ba: [0,1]\u2192[0,1]that\nare2-Lipschitz.\nWe consider2-Lipschitz rather than1-Lipschitz post-processings for technical reasons: in order\nto capture functions of the form[ p+w(p)]1\n0, wherewis1-Lipschitz. This is convenient in order to\nprovide a characterization of CDLin terms of smCE. But note that allowing more post-processings\nmakes the positive result we will show more powerful, moreover the lower bound of Corollary 4.4\nalso holds for the class of2-Lipschitz post-processings.\nRecall that thesmooth calibration erroris\nsmCE(J) = sup\nwE[w(p)(y\u2212p)],\nwhere the supremum is taken over allw: [0,1]\u2192[\u22121,1]that are1-Lipschitz.\nTheorem 4.9.For any distributionJover[0,1]\u00d7{0,1}, we have that:\n1\n2(smCE(J))2\u2264CDLLLip,Lip(2) (J)\u22646\u00b7smCE(J)\nWe present the proof in Appendix B.2. The upper bound follows from the loss OI lemma\nof [GHK+23] (Lemma 5.4), while the lower bound follows from arguments in [ BGHN23b ]. Since\nsmCEcan be estimated efficiently from samples [ BGHN23a ], we have an efficient( \u03b1,c\u03b12)-auditor\nforCDLLLip,Lip(2). We note that the restriction to Lipschitz losses is a significant one, which does\nexclude important losses. For instance, consider the \u21131loss:\u21131(p,y) =|p\u2212y|. It is not proper, but\nit is Lipschitz in p. If we convert it to a proper loss by composing it with the best response, we\nget the\u21131/2loss, which is not Lipschitz in p. This happens because the best-response1[ p\u22651/2]\nis non-Lipschitz. This is not uncommon, even when the original loss is Lipschitz, since decision\nmaking in both theory and practice often involves sharp thresholds.\n5 Relation to Weight-Restricted Calibration\nIn this section, we establish a tight relationship between CDLK, the calibration decision loss for a class\nK, and a certain weight-restricted calibration error measure, a notion we defined in Definition 2.9.\nThe particular weight class we consider will involve thresholds ofK.\n19\n5.1 The Characterization\nIn order to state our general characterization, recall from Definition 3.10 that thr(K)denotes the\nclass ofupper thresholdsof a valid post-processing class K. In this section, we will require a slight\nmodification to the classthr(K):\nDefinition 5.1(Modified Thresholds).Given a valid post-processing classK, let\nthr\u2032(K) =thr(K)\u222a/braceleft\uf8ecig\np\u221d\u21d5\u22a3\u221a\u222b\u2294\u2240\u2192\u2212sign+(p\u2212v)/vextendsingle/vextendsingle/vextendsinglev\u2208R/braceright\uf8ecig\n.\nWhile the class thr(K)contains only theupperthresholds of functions in K, the class thr\u2032(K)\nalso includes alllowerthresholds of the identity function. It also contains upper thresholds of the\nidentity function due to its inclusion of thr(K), sinceKis assumed to be translation invariant and\ncontain the identity. The main result of this section is as follows:\nTheorem 5.2.IfKis a valid post-processing class, then\nCEthr\u2032(K)(J)2\u2272CDLK(J)\u2272CEthr\u2032(K)(J).\nBefore discussing the proof of Theorem 5.2, we first present a couple of examples illustrating\nits use. Consider K=M+, the class of monotonically nondecreasing post-processing functions. In\nthis setting, the theorem implies that CDLM+corresponds (up to quadratic equivalence) to the\nInterval-restricted calibration error CEInt[OKK25,RSB+25]\u2014whose weight class consists of all\nupper and lower thresholds of the identity: sign(p\u2212v )and\u2212sign (p\u2212v )for allv\u2208[0,1]. The\nupper bound CDLM+(J)\u2272CE Int(J)recovers a previous result from [ RSB+25]. The lower bound\nCDLM+(J)\u2273CE Int(J)2, however, is new. Moreover, Theorem 5.2 is tight: there exist prediction-\noutcome distributions J1andJ2such that CDLM+(J1)\u2272CE Int(J1)2andCDLM+(J2)\u2273CE Int(J2).\nWe defer discussion of these examples to Appendix C.\nSimilarly, given r\u2208N, we may setK=Mr, whereMris the class of r-wise generalized\nmonotone post-processing functions defined in Definition 2.2. In this case, Theorem 5.2 implies that\nCDLMris quadratically related to a natural r-wise generalization of Interval-restricted calibration\nerror, in which the weight class comprises all indicators for unions of at mostrdisjoint intervals.\nThe proof of Theorem 5.2 will rely on the following few useful lemmas. The first lemma that we\nneed is an immediate consequence of the bounded convergence theorem. It allows us to work with\neither strict or weak inequalities interchangeably, sincelim s\u2192t+1[p\u2265s] =1[p>t], etc.\nLemma 5.3.Fix any distribution Jover pairs(p,y)\u2208[0,1]\u00d7{0,1}. Then, any pointwise convergent\nsequence of weight functionsw 1,w2,...: [0,1]\u2192[\u22121,1]satisfies\nlim\nk\u2192\u221eE/bracketleftbigwk(p)(y\u2212p)/bracketrightbig=E/bracketleftbigg\nlim\nk\u2192\u221ewk(p)(y\u2212p)/bracketrightbigg\n.\nThe second lemma we need relates a gap in loss values to an expression that resembles weight-\nrestricted calibration. We call it thedecision OIlemma for its connection tooutcome indistin-\nguishability (OI)[DKR+21]\u2014specifically, the notion of decision OI from [GHK+23].\nLemma 5.4(Decision OI).For any fixedp,q\u2208[0,1]andy\u2208{0,1},\n\u2113(p,y)\u2212\u2113(q,y)\u2264/parenleftbig\u2202\u2113(p)\u2212\u2202\u2113(q)/parenrightbig(y\u2212p).\n20\nProof.Let\u03c6be the concave function given by Lemma 3.1 such that \u2113(p,y) =\u03c6(p) +\u03c6\u2032(p)(y\u2212p ),\nwhere\u03c6\u2032=\u2202\u2113is some superderivative of\u03c6. Then,\n\u2113(p,y)\u2212\u2113(q,y) =\u03c6(p)\u2212\u03c6(q)\u2212\u03c6\u2032(q)(p\u2212q)/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright\n(\u2217)+/parenleftbig\u03c6\u2032(p)\u2212\u03c6\u2032(q)/parenrightbig(y\u2212p),\nand(\u2217)is nonpositive by concavity.\nThe third and final lemma we require is more subtle, providing a key relationship between\ninterval miscalibration and the ability of an additive post-processing function to reduce loss. We\ncall it thesmall interval lemma, since we will always apply it to intervals Iwith either short length\nb\u2212a, or low massPr[p\u2208I].\nLemma 5.5(Small Interval).If( p,y)\u223cJandKcontains all functions \u03ba(p) = [p+t]1\n0fort\u2208R,\nthen for all0\u2264a\u2264b\u22641and intervalsI\u2208{[a,b],[a,b),(a,b],(a,b)},\n/vextendsingle/vextendsingle/vextendsingleE/bracketleftbig1[p\u2208I](y\u2212p)/bracketrightbig/vextendsingle/vextendsingle/vextendsingle\u2264(b\u2212a) Pr[p\u2208I] +1\n2CDLK(J).\nProof.By the definition of calibration decision loss,\nCDLK(J)\u2265sup\nv,tE[\u2113+\nv(p,y)\u2212\u2113+\nv(\u03bat(p),y)],\nwhere the supremum is taken over allv\u2208[0,1]andt\u2208[\u22121,1]. This expectation equals\nE/bracketleft\uf8ecig/parenleft\uf8ecig\nsign+/parenleftbig[p+t]1\n0\u2212v/parenrightbig\u2212sign+(p\u2212v)/parenright\uf8ecig\n(y\u2212v)/bracketright\uf8ecig\n=\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f32\u00b7E/bracketleftbig1[v\u2212t\u2264p<v](y\u2212v)/bracketrightbigift>0,\n2\u00b7E/bracketleftbig1[v\u2264p<v\u2212t](v\u2212y)/bracketrightbigift<0,\n0ift= 0.\nNow consider any0\u2264a<b\u22641. Takingv=bandt=b\u2212a>0, we see that\nE/bracketleftbig1[a\u2264p<b](y\u2212b)/bracketrightbig\u22641\n2CDLK(J).(5.1)\nSimilarly, takingv=aandt=a\u2212b<0, we see that\nE/bracketleftbig1[a\u2264p<b](a\u2212y)/bracketrightbig\u22641\n2CDLK(J).(5.2)\nWe visualize equations (5.1) and (5.2) in Figure 2. Combining (5.1) and (5.2) yields\na\u00b7Pr[a\u2264p<b]\u22121\n2CDLK(J)\u2264E/bracketleftbigy\u00b71[a\u2264p<b]/bracketrightbig\u2264b\u00b7Pr[a\u2264p<b] +1\n2CDLK(J).\nNext, we subtract the quantity( a+b)/2\u00b7Pr[a\u2264p<b ]from the left, right, and middle of the above\nchain of inequalities. Doing so yields\n/vextendsingle/vextendsingle/vextendsingle/vextendsingleE/bracketleft\uf8ecig\n1[a\u2264p<b]/parenleft\uf8ecig\ny\u2212a+b\n2/parenright\uf8ecig/bracketright\uf8ecig/vextendsingle/vextendsingle/vextendsingle/vextendsingle\u2264b\u2212a\n2Pr[a\u2264p<b] +1\n2CDLK(J).\nObserve that, conditional on the event that a\u2264p<b , the midpoint( a+b)/2of the interval[ a,b)\ndiffers from the predictionpby at most(b\u2212a)/2. Consequently, we have\n/vextendsingle/vextendsingle/vextendsingleE/bracketleftbig1[a\u2264p<b](y\u2212p)/bracketrightbig/vextendsingle/vextendsingle/vextendsingle\u2264(b\u2212a) Pr[a\u2264p<b] +1\n2CDLK(J).\n21\nFigure 2: Visualization of inequalities (5.1)and(5.2). The dashed line outlines the box[ a,b]\u00d7[a,b]\nin the(p,y)-plane. The left and right images correspond to (5.1)and(5.2), respectively. The\ninequalities assert that in each image, the blue area is at least as large as the red, minus a CDLK(J)\nslack term, assuming the marginal distribution ofpis uniform.\nThe left hand side is the absolute value of the calibration error restricted to the indicator function\nfor the half-open interval I= [a,b). Moreover, our argument up to this point works for any\n0\u2264a<b\u22641, so that we have proved the claim for any such interval.\nBy carrying out the preceding argument with \u2113\u2212\nvin place of \u2113+\nv, we can similarly bound the\nabsolute calibration error restricted to half-open intervals of the form I= (a,b], where0\u2264a<b\u2264 1.\nBy varying aandband applying Lemma 5.3, we can also obtain the bound for any interval Iof the\nform(a,b],(a,b), or[a,b], where0\u2264a\u2264b\u2264 1, except for I= [0,1], but this case is trivial because\nthe right hand side is at least1.\nWith these three lemmas in hand, we are ready to prove Theorem 5.2.\nProof of Theorem 5.2, Upper Bound. As a reminder, our goal is to show that if Kis a valid post-\nprocessing class, then\nCDLK(J)\u2272CEthr\u2032(K)(J).\nWe first apply the boundary characterization of L\u2217from Corollary 3.6, which yields the following\nupper bound:\nCDLK(J)\u2264sup\nv,\u03baE[\u2113+\nv(p,y)\u2212\u2113+\nv(\u03ba(p),y)],\nwhere the supremum is over v\u2208[0,1]and\u03ba\u2208K. Next, we relate the gap in loss values between\npand\u03ba(p)to an expression resembling weight-restricted calibration using the decision OI lemma\n(Lemma 5.4):\nCDLK(J)\u2264sup\nv,\u03baE/bracketleft\uf8ecig/parenleftbigsign+(\u03ba(p)\u2212v)\u2212sign+(p\u2212v)/parenrightbig(y\u2212p)/bracketright\uf8ecig\n,\nBy assumption, bothsign+(\u03ba(p)\u2212v)and\u2212sign+(p\u2212v)belong tothr\u2032(K). We conclude that\nCDLK(J)\u22642\u00b7CEthr\u2032(K)(J).\nProof of Theorem 5.2, Lower Bound. As a reminder, our goal is to show that if Kis a valid post-\nprocessing class, then\nCEthr\u2032(K)(J)2\u2272CDLK(J).\nSuppose that CDLK(J) =\u03b5. We will prove that CEthr\u2032(K)(J)\u2272\u221a\u03b5. For this, we first recall that\nthere are two types of weight functions in thr\u2032(K). The first kind of weight functions are functions\nof the form\nw1(p) = sign+(\u03ba(p)\u2212v) =1[\u03ba(p)\u2265v]\u00b72\u22121,\n22\nfor some\u03ba\u2208Kandv\u2208[0,1]. The second kind are functions of the form\nw2(p) =\u2212sign+(p\u2212v) =1[p<v]\u00b72\u22121,\nfor somev\u2208[0,1]. Therefore, it suffices to prove that the (signed) calibration errors with respect to\nall weight functions of the form p\u221d\u21d5\u22a3\u221a\u222b\u2294\u2240\u21921[\u03ba(p)\u2265v]orp\u221d\u21d5\u22a3\u221a\u222b\u2294\u2240\u21921[p<b ]andp\u221d\u21d5\u22a3\u221a\u222b\u2294\u2240\u2192\u2212 1are either negative, or\npositive but small (i.e. O(\u221a\u03b5)). We shall do so in the following two lemmas, Lemmas 5.6 and 5.7.\nWhen combined, these two lemmas complete the proof of Theorem 5.2.\nLemma 5.6.Let Jbe a distribution over pairs( p,y)\u2208[0,1]\u00d7{0,1}, and letKbe a valid\npost-processing class. There exists an absolute constant C > 0such that for all \u03ba\u2208Kandv\u2208[0,1],\nE/bracketleftbig1[\u03ba(p)\u2265v](y\u2212p)/bracketrightbig\u2264C/radical\uf8ecig\nCDLK(J).\n(Note that the expectation can be negative, with arbitrarily large magnitude.)\nLemma 5.7.Let Jbe a distribution over pairs( p,y)\u2208[0,1]\u00d7{0,1}, and letKbe a valid\npost-processing class. There exists an absolute constantC >0such that for all intervalsI\u2286[0,1],\n/vextendsingle/vextendsingle/vextendsingleE/bracketleftbig1[p\u2208I](y\u2212p)/bracketrightbig/vextendsingle/vextendsingle/vextendsingle\u2264C/radical\uf8ecig\nCDLK(J).\nOf the two lemmas, Lemma 5.7 has the slightly simpler proof, so we present it first.\nProof of Lemma 5.7. Let\u03b5=CDLK(J)and letI\u2286[0,1]be an interval, which, as usual, may be\nopen, closed, half-open, or a singleton. Let m\u2208Nbe an integer to be specified later. We select a\nsequence ofcutoffpoints c1<\u00b7\u00b7\u00b7<c msuch thatc1andcmare the left and right endpoints of the\nintervalI, and each open interval( ci\u22121,ci)contains at most a1 /mfraction of the total probability\nmass ofI. That is, we require\nPr[ci\u22121<p<ci]\u22641\nmPr[p\u2208I].\nBecause we consider open intervals( ci\u22121,ci), such a selection is always possible. Note, however,\nthat the distribution of pmay have positive mass on some or all of the cutoff points c1,...,cm.\nNext, we split the associated weight-restricted calibration error at the cutoff pointsc i, as follows:\nE/bracketleftbig1[p\u2208I](y\u2212p)] =m/summationdisplay\ni=1E/bracketleftbig1[p=ci](y\u2212p)/bracketrightbig+m/summationdisplay\ni=2E/bracketleftbig1[ci\u22121<p<ci](y\u2212p)/bracketrightbig.(5.3)\nBy the small interval lemma (Lemma 5.5) each term the first sum in equation (5.3)is bounded\nin absolute value by O(\u03b5), and the ithterm in the second sum is bounded in absolute value by\n(ci\u2212ci\u22121)\u00b71/m+\u03b5. Note that the difference ci\u2212ci\u22121telescopes when we sum over i. Therefore,\napplying the triangle inequality to equation (5.3), we have that\n/vextendsingle/vextendsingle/vextendsingleE/bracketleftbig1[p\u2208I](y\u2212p)]/vextendsingle/vextendsingle/vextendsingle\u2264m\u03b5+ (c m\u2212c1)\u00b71\nm+m\u03b5.\nRecall that cm\u2212c1is the length of I, which is at most1. Setting m= \u0398(1/\u221a\u03b5), the above bound\nreduces toO(\u221a\u03b5), as claimed.\nNext, we present the proof of Lemma 5.6, which has a similar structure to that of Lemma 5.7,\nbut with a few extra steps. This will conclude the proof of Theorem 5.2.\n23\nProof of Lemma 5.6. Once again, let \u03b5=CDLK(J). Instead of considering an interval, we now\nconsider a set of the form S={p\u2208 [0,1]|\u03ba(p)\u2265v}for some post-processing \u03ba\u2208Kand value\nv\u2208[0,1]. We again select cutoff points points c0<\u00b7\u00b7\u00b7< c msuch thatc0= 0andcm= 1and\neach open interval( ci\u22121,ci)contains at most a1 /mfraction of the mass of S. That is, we require\nPr[p\u2208S\u2229 (ci\u22121,ci)]\u2264(1/m)Pr[p\u2208S ]. Since we consider open intervals( ci\u22121,ci), such a selection\nis always possible. Note, however, that the distribution of pmay have positive mass on some or all\nof the cutoff points c0,...,cm. Next, we split the associated weight-restricted calibration error at\nthe cutoff pointsc i, as follows:\nE/bracketleftbig1[p\u2208S](y\u2212p)/bracketrightbig=m/summationdisplay\ni=0E/bracketleftbig1/bracketleftbigp\u2208S\u2229{c i}/bracketrightbig(y\u2212p)/bracketright\uf8ecig\n+m/summationdisplay\ni=1E/bracketleft\uf8ecig\n1/bracketleftbigp\u2208S\u2229(c i\u22121,ci)/bracketrightbig(y\u2212p)/bracketright\uf8ecig\n.(5.4)\nOur goal will be to argue that each term of the two sums is either negative (with unbounded\nmagnitude) or positive but small (i.e. with magnitude at most O(\u221a\u03b5)). First, by the small interval\nlemma (Lemma 5.5), the first sum in equation (5.4)is bounded in absolute value by O(m\u03b5). To\nbound the ithterm in the second sum, let a=ci\u22121andb=ci. We will consider the following\nthree comprehensive cases, showing that the ithterm in the second sum of equation (5.4)is at most\nO((b\u2212a)/m+\u03b5).\n\u2022(Case 1:S\u2229(a,b)is an interval)In this case, Lemma 5.5 tells us that\n/vextendsingle/vextendsingle/vextendsingleE/bracketleft\uf8ecig\n1/bracketleftbigp\u2208S\u2229(a,b)/bracketrightbig(y\u2212p)/bracketright\uf8ecig/vextendsingle/vextendsingle/vextendsingle\u2264(b\u2212a) Pr[p\u2208S\u2229(a,b)] +\u03b5,\nand the right hand side is at most(b\u2212a)/m+\u03b5.\n\u2022(Case 2:S\u2229(a,b)is not an interval and a /\u2208Sandb\u2208S)We first relate the calibration\nerror onS\u2229(a,b)to a quantity in whichy\u2212phas been replaced withy\u2212b, as follows:\nE/bracketleft\uf8ecig\n1/bracketleftbigp\u2208S\u2229(a,b)/bracketrightbig(y\u2212p)/bracketright\uf8ecig\n\u2264E/bracketleft\uf8ecig\n1/bracketleftbigp\u2208S\u2229(a,b)/bracketrightbig(y\u2212b)/bracketright\uf8ecig\n+ (b\u2212a) Pr[p\u2208S\u2229(a,b)],\nand the rightmost term is at most( b\u2212a )/m. To bound the first term on the right, which\ninvolves a factor of y\u2212b, we will relate the entire expectation to the \u2113+\nbloss. To do so, recall\nthatS={p\u2208 [0,1]|\u03ba(p)\u2265v}for some\u03ba\u2208Kandv\u2208[0,1]. By translation invariance, K\nalso contains the function\n\u03ba\u2032(p) =/bracketleft\uf8ecig\n\u03ba/parenleft\uf8ecig\n[p]b\na/parenright\uf8ecig\n+b\u2212v/bracketright\uf8ecig1\n0.\nWe visualize the transformation from \u03bato\u03ba\u2032in Figure 3. Indeed, by shifting the graph of \u03ba\nleft bya(and back again) and right by1 \u2212b(and back again), we can ensure that \u03batakes the\nconstant value \u03ba(a)on[0,a]and the constant value \u03ba(b)on[b,1], which corresponds to the\ntruncation[p]b\nain the formula for\u03ba\u2032.\nNext, we claim that \u03ba\u2032(p)\u2265bif and only if p\u2208(S\u2229(a,b))\u222a[b,1]. To prove this, recall our\nassumption that a /\u2208S, which implies \u03ba\u2032(p)<bfor allp\u2264a. Similarly, since b\u2208S, we have\n\u03ba\u2032(p)\u2265bfor allp\u2265b. Forp\u2208(a,b), we have\u03ba\u2032(p)\u2265bif and only if p\u2208Sas well. In terms\nof\u03ba\u2032, we have\nE/bracketleftbig1[p\u2208S\u2229(a,b)](y\u2212b)/bracketrightbig=E/bracketleftbig(1[\u03ba\u2032(p)\u2265b]\u22121[p\u2265b])(y\u2212b)/bracketrightbig.\nThis can be rewritten in terms of the\u2113+\nvloss as\nE/bracketleftbig\u2113+\nb(p,y)\u2212\u2113+\nb(\u03ba(p),y)/bracketrightbig.\n24\nFigure 3: Transformation from \u03bato\u03ba\u2032. The dashed line outlines the box[ a,b]\u00d7[a,b]in the\n(p,\u03ba(p))-plane. Starting from the set S={p\u2208 [0,1] :\u03ba(p)\u2265v}, the transformation removes from S\nany points to the left of a, adds toSany points to the right of b, and performs a vertical shift so\nthat the threshold coincides withb. The transformation requires\u03ba(a)<vand\u03ba(b)\u2265v.\nBy the definition of CDLK, this quantity is either negative or at most O(\u03b5). We conclude that\nE/bracketleft\uf8ecig\n1/bracketleftbigp\u2208S\u2229(a,b)/bracketrightbig(y\u2212p)/bracketright\uf8ecig\n\u2264b\u2212a\nm+\u03b5.\n\u2022(Case 3:S\u2229(a,b)is not an interval and either a\u2208Sorb /\u2208S)In this case, we will\nperform a simple trick to reduce Case 2, where a /\u2208Sandb\u2208S. Basically, the idea is to shift\nato the right until we hit a point not inS, and shiftbto the left until we hit a point inS.\nMore formally, let a\u2032=inf(a,b)\\Sbe the infimum of points outside Sin the interval, and let\nb\u2032=sup(a,b)\u2229Sbe the supremum of points in Sin the interval. Clearly, a\u2032\u2265aandb\u2032\u2264b.\nAlso, we have the strict inequalitya\u2032<b\u2032sinceS\u2229(a,b)is not an interval.\nNext, take any nonincreasing sequence of points aj/\u2208Sthat approach a, as well as a\nnondecreasing sequence of points bj\u2208Sthat approach b. Suppose also that we have the strict\ninequalityaj<bj. To bound the calibration error restricted to S\u2229(a\u2032,b\u2032), we apply the result\nof Case 2 to each pair(a j,bj)and take the limit asj\u2192\u221e(Lemma 5.3). This yields\nE/bracketleft\uf8ecig\n1/bracketleftbigp\u2208S\u2229(a\u2032,b\u2032)/bracketrightbig(y\u2212p)/bracketright\uf8ecig\n\u2264b\u2032\u2212a\u2032\nm+\u03b5.\nThe calibration errors restricted to S\u2229(a,a\u2032]andS\u2229[b\u2032,b)are similarly bounded by the small\ninterval lemma (Lemma 5.5) , since the former is a contiguous interval and the latter is either\na singleton or empty set. This yields\nE/bracketleft\uf8ecig\n1/bracketleftbigp\u2208S\u2229(a,a\u2032)/bracketrightbig(y\u2212p)/bracketright\uf8ecig\n\u2264a\u2032\u2212a\nm+\u03b5\nand\nE/bracketleft\uf8ecig\n1/bracketleftbigp\u2208S\u2229(b\u2032,b)/bracketrightbig(y\u2212p)/bracketright\uf8ecig\n\u2264b\u2212b\u2032\nm+\u03b5.\nAdding these three inequalities, we see that, in total, over S\u2229(a,a\u2032],S\u2229(a\u2032,b\u2032), andS\u2229[b\u2032,b),\nwe have\nE/bracketleft\uf8ecig\n1/bracketleftbigp\u2208S\u2229(a,b)/bracketrightbig(y\u2212p)/bracketright\uf8ecig\n\u2264b\u2212a\nm+ 3\u03b5.\nAt this point, we have shown that the ithterm of the second summation of equation (5.4)is either\nnegative (with arbitrarily large magnitude) or positive but bounded in magnitude by O((ci\u2212\nci\u22121)/m+\u03b5). Settingm= \u0398(1/\u221a\u03b5), the entire bound telescopes to O(\u221a\u03b5), proving Lemma 5.6.\n25\n5.2 Translation Invariance Alone Is Insufficient\nWe have established that CDLKis polynomially characterized by the thr\u2032(K)-restricted calibration\nerror for any valid post-processing class K. Here, we provide a example that justifies these\nassumptions, by showing that the characterization does not hold for the class of non-increasing\npost-processings. Recall that the class of non-increasing post-processings is translation invariant,\nbut does not contain the identity.\nTheorem 5.8.LetK=M \u2212andW=thr\u2032(M\u2212). There is a distributionJsuch that:\nCEW(J)\u22651\n4, yetCDLK(J)\u22640\nProof.LetJbe the distribution over( p,y)such that the marginal distribution of pis0with\nprobability1/2and1/2with probability1/2. Moreover, we have\np\u2217(p) :=E\n(p,y)\u223cJ[y|p] =/braceleft\uf8ecigg\n0,ifp= 0\n1,ifp= 1/2\nThe setWcontains the constant function w\u2217:p\u221d\u21d5\u22a3\u221a\u222b\u2294\u2240\u21921, corresponding to the nonincreasing constant\npostprocessing\u03ba:p\u221d\u21d5\u22a3\u221a\u222b\u2294\u2240\u21921. Therefore, we have the following:\nCEW(J)\u2265E\n(p,y)\u223cJ[w\u2217(p)(y\u2212p)]\n=E\n(p,y)\u223cJ[y\u2212p]\n=1\n2P\n(p,y)\u223cJ[p= 1/2] =1\n4.\nDue to Corollary 3.6, in order to show that CDLK(J)\u22640, it suffices to show that Q\u22c6\nv,\u03ba\u22640for all\nv\u2208[0,1],\u03ba\u2208Kand\u22c6\u2208{+,\u2212}, whereQ\u22c6\nv,\u03bais defined as follows:\nQ\u22c6\nv,\u03ba:=E\n(p,y)\u223cJ[(y\u2212v)(sign\u22c6(\u03ba(p)\u2212v)\u2212sign\u22c6(p\u2212v))]\n=1\n2(\u2212v)(sign\u22c6(\u03ba(0)\u2212v)\u2212sign\u22c6(\u2212v)) +1\n2(1\u2212v)(sign\u22c6(\u03ba(1/2)\u2212v)\u2212sign\u22c6(1/2\u2212v))\n=1\n2(\u2212v)(sign\u22c6(\u03ba(0)\u2212v) + 1) +1\n2(1\u2212v)(sign\u22c6(\u03ba(1/2)\u2212v)\u2212sign\u22c6(1/2\u2212v))\nThe first term in the above expression is non-positive. Therefore, Q\u22c6\nv,\u03ba\u22640unless the second\nterm is positive. If the second term is positive, then sign\u22c6(\u03ba(1/2)\u2212v) = 1, which implies, due to\nmonotonicity of \u03ba, that sign\u22c6(\u03ba(0)\u2212v) = 1. Assuming that sign\u22c6(\u03ba(1/2)\u2212v) =sign\u22c6(\u03ba(0)\u2212v) = 1,\nwe have the following cases.\nCase I:v\u22651/2.We haveQ\u22c6\nv,\u03ba\u2264\u2212v+ (1\u2212v) = 1\u22122v\u22640.\nCase II:v<1/2.We haveQ\u22c6\nv,\u03ba=\u2212v\u22640.\nRemark5.9.The characterization of Theorem 5.2 fails even if we consider a class Kthat includes\nbothM\u2212and the identity function, since the identity function does not alter the loss value.\nHence, neither translation invariance nor inclusion of the identity function alone suffices for the\ncharacterization of Theorem 5.2.\n26\n6 Computationally Efficient Testing and Auditing\nIn this section, we show that the problem of testing CDLKfor some post-processing class Kcan\nbe reduced to proper agnostic learning of the threshold class thr(K), whereas auditing reduces to\nimproper agnostic learning. Note that the characterization of calibration decision loss in terms of\nthe calibration error with respect to the threshold class we provided in Section 5 already establishes\na connection between agnostic learning and CDLtesting: it says that an agnostic learner for thr(K)\ngives an(\u03b1,c\u03b12\u2212\u03b5)tester for some constant c. However, our testing guarantee here is stronger,\nsince we get\u03b2=\u03b1\u2212\u03b5.\nTheorem 6.1(Testing and Auditing from Agnostic Learning).Let Kbe a valid post-processing\nclass. Let ALbe an agnostic \u03b5-learner for thr(K)with sample complexity m. For any\u03b1\u2208(0,1),\nthere is an( \u03b1,\u03b1\u2212 3\u03b5)-auditor for CDLKthat makes at most O((1/\u03b5)log(1/\u03b5\u03b4))non-adaptive calls to\nAL, usesO((1/\u03b52)log(1/\u03b5\u03b4))additional samples, and performs \u02dcO(log(1/\u03b4)/\u03b53)additional operations.\nMoreover, if ALis proper, then there is an( \u03b1,\u03b1\u2212 3\u03b5)-tester for CDLKwith the same specifications.\nWe obtain the following corollary for the class of generalized monotone post-processings, based\non a folkore result on agnostic learning unions of intervals in one dimension (see Appendix D).\nCorollary 6.2.For any r\u22651,\u03b1,\u03b5\u2208 (0,1), there is an( \u03b1,\u03b1\u2212\u03b5 )-tester for CDLMrwith sample\ncomplexity \u02dcO(r/\u03b52)and runtime \u02dcO(r2/\u03b53).\nThe proof of Theorem 6.1 is based on the characterization of proper losses in terms of the\nV-shaped losses (Lemma 3.5) and an appropriate discretization argument for the parameter v\u2208[0,1]\nassociated with the family of V-shaped losses. For each (discrete) choice of v, we identify a relevant\nagnostic learning problem and solve it using the agnostic learning oracle. First, we state the version\nof the result for a fixed choice ofvseparately, as a lemma.\nLemma 6.3.Let Kbe a valid post-processing class. Given v\u2208[0,1], denote the calibration fixed\ndecision loss with respect to the loss\u2113+\nvby\nCDLv,K(J) = sup\n\u03ba\u2208KE[\u2113+\nv(p,y)\u2212\u2113+\nv(\u03ba(p),y)].\nLetALbe an agnostic( \u03b5,\u03b4)-learner for thr(K)with sample complexity m. Then, for any \u03b1,\u03b3\u2208\n(0,1), there exists an algorithm that calls ALonce, usesO((1/\u03b32)log(1/\u03b4))additional samples and\noperations, and with probability at least1\u2212\u03b4outputs an estimate/hatwidestCDLv,K(J)satisfying\n/hatwidestCDLv,K(J)\u2208/bracketleft\uf8ecig\nCDLv,K(J)\u22122\u03b5\u22122\u03b3,2\u00b7ECE(J) + 2\u03b3/bracketright\uf8ecig\n.\nMoreover, ifALis proper, then we have the stronger guarantee\n/hatwidestCDLv,K(J)\u2208/bracketleft\uf8ecig\nCDLv,K(J)\u22122\u03b5\u22122\u03b3,CDL v,K(J) + 2\u03b3/bracketright\uf8ecig\n.\nProof.Our strategy will be to manipulate the expression for CDLv,Kinto a form amenable to agnostic\nlearning. First, we substitute the definition of the loss function \u2113+\nv(p,y) =\u2212sign+(p\u2212v )(y\u2212v ),\nand use the translation invariance of the class K. This allows us to rewrite the formula for CDLv,K\nas\nCDLv,K(J) =E/bracketleftbig\u2212sign+(p\u2212v)(y\u2212v)/bracketrightbig+ sup\n\u03ba\u2208KE/bracketleft\uf8ecig\nsign+/parenleft\uf8ecig\n\u03ba(p)\u22121\n2/parenright\uf8ecig\n(y\u2212v)/bracketright\uf8ecig\n.\nBy Hoeffding\u2019s inequality, the first expectation in the above equation can be directly estimated up\nto error\u03b3with probability at least1\u2212\u03b4from a sample of(p,y)pairs of sizeO(log(1/\u03b4)/\u03b32).\n27\nWe can similarly get a handle on the second term, which includes a supremum over post-\nprocessing functions \u03ba, using a single call to the agnostic learner with concept class C=thr(K)and\nlabelsz=y\u2212v\u2208 [\u22121,+1]. (For discrete labels z\u2208{\u00b1 1}, simply perform randomized rounding\nthat preserves zin expectation.) If the agnostic learner returns a hypothesis h: [0,1]\u2192{\u00b1 1}, then\nwe have the following guarantee with probability at least1\u2212\u03b4:\nE/bracketleftbigh(p)(y\u2212v)/bracketrightbig\u2265sup\n\u03ba\u2208KE/bracketleft\uf8ecig\nsign+/parenleft\uf8ecig\n\u03ba(p)\u22121\n2/parenright\uf8ecig\n(y\u2212v)/bracketright\uf8ecig\n\u22122\u03b5.\nNote also that h(p) =sign+(\u03ba(p)\u22121/2)for some function \u03ba: [0,1]\u2192[0,1], which we may assume\nbelongs to the classKif the agnostic learner isproper. Consequently, we have\nE/bracketleftbigh(p)(y\u2212v)/bracketrightbig\u2264sup\n\u03baE/bracketleft\uf8ecig\nsign+/parenleft\uf8ecig\n\u03ba(p)\u22121\n2/parenright\uf8ecig\n(y\u2212v)/bracketright\uf8ecig\n,\nwhere the supremum is taken over all \u03ba: [0,1]\u2192[0,1]in case of an improper learner, or all \u03ba\u2208K\nin the case of a proper learner. Of course, given an additional O(log(1/\u03b4)/\u03b32)samples, we can\nestimate the quantityE[h(p)(y\u2212v)]up to error\u03b3with probability at least1\u2212\u03b4.\nAt this point, our algorithm makes a single call to AL, usesO(log(1/\u03b4)/\u03b32)additional samples,\nand with probability at least1\u2212\u03b4outputs a scalar estimate/hatwidestCDLv,K(J)satisfying\nCDLv,K(J)\u22122\u03b5\u22122\u03b3\u2264/hatwidestCDLv,K(J)\u2264CDL v,K\u2217(J) + 2\u03b3,\nwhereK\u2217isthesetofallpost-processings \u03ba: [0,1]\u2192[0,1]. ByTheorem3.8, CDLv,K\u2217(J)\u22642\u00b7ECE (J).\nIn the special case that ALis a proper agnostic learner, we have the following stronger condition,\nwithKreplacingK\u2217in the upper bound:\nCDLv,K(J)\u22122\u03b5\u22122\u03b3\u2264/hatwidestCDLv,K(J)\u2264CDL v,K(J) + 2\u03b3.\nHaving shown that agnostic learning can be used to understand the calibration fixed decision\nloss with respect to a particular \u2113+\nvloss, we now prove Theorem 6.1, which shows that proper and\nimproper agnostic learning similarly imply testing and auditing forCDL K.\nProof of Theorem 6.1. Suppose momentarily that Lemma 6.3 were to holdsimultaneouslyfor all\nvaluesv\u2208[0,1], rather than one fixed value. Then, the supremum of the estimates/hatwidestCDLv,Kwould lie\nbetween CDLK(J)\u22122\u03b5\u22122\u03b3and2\u00b7ECEK(J)+2\u03b3. In the special case that ALis proper, the supremum\nwould also lie below CDLK(J) + 2\u03b3. For\u03b3 <\u03b5/ 4, this would immediately imply( \u03b1,\u03b1\u2212 3\u03b5)-auditing,\nor, in the case of a proper agnostic learner,(\u03b1,\u03b1\u22123\u03b5)-testing.\nWith this in mind, our strategy will be to efficiently obtain these estimates xvfor a sufficiently\nwell-spacednetof points v1,...,vt\u2208[0,1]. By \u201cwell-spaced,\u201d we mean that replacing any v\u2208[0,1]\nwith its nearest neighbor in the net should change the value of CDLv,K(J)by at most O(\u03b3). For\nthis, recall the formula for the calibration fixed decision loss with respect to\u2113+\nv:\nCDLv,K(J) =E/bracketleftbig\u2212sign+(p\u2212v)(y\u2212v)/bracketrightbig+ sup\n\u03ba\u2208KE/bracketleft\uf8ecig\nsign+/parenleft\uf8ecig\n\u03ba(p)\u22121\n2/parenright\uf8ecig\n(y\u2212v)/bracketright\uf8ecig\n.\nBy inspection of this formula, it is clear that it suffices for every v\u2208[0,1]to have a nearest neighbor\nviin the net that is bothclose in lengthandclose in probability:\n\u2022(Close in Length)|v\u2212v i|\u2264\u03b3,\n\u2022(Close in Probability)Pr[sign+(p\u2212v)\u0338= sign+(p\u2212vi)]\u2264\u03b3.\n28\nSuch a net can be constructed by taking the union of equally spaced points0 ,\u03b3,2\u03b3,..., 1with\nseveral independent samples p1,...,psdrawn from the distribution J. The equally spaced points\nclearly guarantee closeness in length. Closeness in probability follows from the fact that we can\npartition the interval[0 ,1]intoO(1/\u03b3)contiguous subintervals, each of which either has probability\nmass\u0398(\u03b3)or is a singleton of mass\u2126( \u03b3). Then, as long as s=O(log(1/\u03b3\u03b4)/\u03b3), with probability at\nleast1\u2212\u03b4, every subinterval contains some sampled pointp i.\nTo conclude, we observe that for \u03b3= \u2126(\u03b5), calling our fixed- valgorithm from Lemma 6.3 as a\nsubroutine for each point in the net leads to a total of O((1/\u03b5)log(1/\u03b5\u03b4))calls. By a union bound over\nthe\u03b4failure probability of each call to the subroutine, we need only O((1/\u03b52)log(1/\u03b5\u03b4))additional\nsamples, in total. Similarly, the total number of additional operations is also \u02dcO(log(1/\u03b4)/\u03b53).\n7 Omniprediction\nRecall the definition of omniprediction (Definition 2.11). In this section, we prove the following\nomniprediction guarantees:\n1.We show that if thr(K)is agnostically learnable, then one can efficiently learn an( \u03b5,K)\nomnipredictor. This result is proved by adapting the loss OI framework of [ GHK+23] to the\ncalibration setting.\n2.We provide a strong omniprediction guarantee for the classical Pool Adjacent Violators\nalgorithm [ ABE+55]: it gives a proper omnipredictor for the class of all monotone post-\nprocessings.\n3.We also provide an analysis of bucketed recalibration through uniform-mass binning, which has\nbeen studied in [ ZE01,GR21,SSH23]. We show that it gives an omnipredictor for generalized\nmonotone functions.\n7.1 Omniprediction From Agnostic Learning\nWe show a general result that reduces omniprediction for Kto agnostic learning for thr(K). Thus\nunder the same computational assumptions that we needed for efficient auditing of CDLK, we can\nget the strong post-processing guarantee of omniprediction.\nThe following theorem reduces omniprediction with respect to all proper losses and some\npost-processing classKto agnostic learning of the classthr(K).\nTheorem 7.1(Omniprediction from Agnostic Learning).Let Kbe a valid post-processing class. Let\nALbe an(\u03b5/3)-agnostic learner for thr(K). There is an algorithm that learns an( \u03b5,K)-omnipredictor\nwith probability1 \u2212\u03b4that calls ALO (log(1/\u03b4)/\u03b52)times and performs poly(1/\u03b5)log(1/\u03b4)additional\noperations.\n[GHK+23] show that omniprediction follows from a condition called calibrated multiaccuracy.\nBelow we define a version of calibrated multiaccuracy that is tailored to our application.\nDefinition 7.2(Calibrated Multiaccuracy).Let /hatwide\u03ba: [0,1]\u2192[0,1]andC\u2286{ [0,1]\u2192{\u00b1 1}}. We say\nthat/hatwide\u03bais\u03b5-calibrated-multiaccurate w.r.t. Cunder the distribution J, or/hatwide\u03ba\u2208calMAC(J,\u03b5), if the\nfollowing properties hold.\n1. (Calibration).ECE(/hatwideJ)\u2264\u03b5, where/hatwideJis the distribution of pairs( /hatwide\u03ba(p),y), where(p,y)\u223cJ.\n2. (Multiaccuracy).|E (p,y)\u223cJ [c(p)(y\u2212/hatwide\u03ba(p))]|\u2264\u03b5for allc\u2208C.\n29\nWe now prove the following lemma, which states that calibrated multiaccuracy implies om-\nniprediction.\nLemma 7.3.Let /hatwide\u03ba\u2208calMAC(J,\u03b5), whereC=thr(K). Then /hatwide\u03bais a(2\u03b5,K)-omnipredictor.\nProof.We will show that for any\u03ba\u2208Kand any\u2113\u2208L\u2217the following holds.\nE\n(p,y)\u223cJ[\u2113(/hatwide\u03ba(p),y)]\u2264E\n(p,y)\u223cJ[\u2113(\u03ba(p),y)] + 2\u03b5(7.1)\nWe define the distribution \u02dcJto be the distribution( p,\u02dcy)wherephas the same as the marginal\ndistribution as underJand\u02dcy\u223cBer( /hatwide\u03ba(p))so thatE[\u02dcy|p] = /hatwide\u03ba(p).\nFor the following, for any \u2113\u2208L\u2217, we let\u2202\u2113: [0,1]\u2192[\u22121,1]denote the function \u2202\u2113(p) =\n\u2113(p,1)\u2212\u2113(p,0). Note that for anyp\u2208[0,1]andy\u2208{0,1}, we have\u2113(p,y) =y\u2202\u2113(p) +\u2113(p,0).\nWe will first bound the quantity\u2206 1:=EJ[\u2113(/hatwide\u03ba(p),y)]\u2212E \u02dcJ[\u2113(/hatwide\u03ba(p),\u02dcy)]for any\u2113\u2208L\u2217as follows.\n\u22061=E\nJ[y\u2202\u2113(/hatwide\u03ba(p)) +\u2113(/hatwide\u03ba(p),0)]\u2212E\n\u02dcJ[\u02dcy\u2202\u2113(/hatwide\u03ba(p)) +\u2113(/hatwide\u03ba(p),0)]\n=E\nJ[y\u2202\u2113(/hatwide\u03ba(p))]\u2212E\n\u02dcJ[\u02dcy\u2202\u2113(/hatwide\u03ba(p))](J, \u02dcJhave the samep-marginal)\n=E\nJ[(y\u2212/hatwide\u03ba(p))\u2202\u2113(/hatwide\u03ba(p))](E[\u02dcy|p] = /hatwide\u03ba(p))\n=E\n(q,y)\u223c/hatwideJ[(y\u2212q)\u2202\u2113(q)](( /hatwide\u03ba(p),y)\u223c/hatwideJ)\n\u2264sup\nw:[0,1]\u2192[\u22121,1]/vextendsingle/vextendsingle/vextendsingleE\n/hatwideJ[(y\u2212q)w(q)]/vextendsingle/vextendsingle/vextendsingle=ECE(/hatwideJ)\u2264\u03b5(ECE(/hatwideJ)\u2264\u03b5)\nOn the other hand, since \u2113is a proper loss and E[\u02dcy|p] =/hatwide\u03ba(p), Definition 2.1 implies that for\nfor anyp,p\u2032\u2208[0,1],E\u02dcJ[\u2113(/hatwide\u03ba(p),\u02dcy)|p]\u2264E \u02dcJ[\u2113(p\u2032,\u02dcy)|p]. It follows that for any \u03ba\u2208K,E\u02dcJ[\u2113(/hatwide\u03ba(p),\u02dcy)]\u2264\nE\u02dcJ[\u2113(\u03ba(p),\u02dcy)]. Combining this with the bound on\u2206 1yields\nE\n(p,y)\u223cJ[\u2113(/hatwide\u03ba(p),y)]\u2264E\n(p,\u02dcy)\u223c \u02dcJ[\u2113(\u03ba(p),\u02dcy)] +\u03b5(7.2)\nWe will now show that the quantity\u2206 2:=E(p,\u02dcy)\u223c \u02dcJ[\u2113(\u03ba(p),\u02dcy)]\u2212E (p,y)\u223cJ [\u2113(\u03ba(p),y)]satisfies\n\u22062\u2264\u03b5for any\u2113\u2208L\u2217and\u03ba\u2208K. Combining the bound on\u2206 2with Eq. (7.2) implies Eq. (7.1).\nBy Lemma 3.5, we can write\n\u2202\u2113(p) =\u2212/integraldisplay\n[0,1]sign+(p\u2212v)d\u00b5+\n\u2113(v)\u2212/integraldisplay\n[0,1]sign\u2212(p\u2212v)d\u00b5\u2212\n\u2113(v),\nwhere\u00b5\u00b1\n\u2113are measures over[0 ,1]such that \u00b5+\n\u2113([0,1]) +\u00b5\u2212\n\u2113([0,1])\u22641. Therefore, using similar\nmanipulations like those for bounding\u2206 1, we obtain the following.\n\u22062=E\n(p,y)\u223cJ[(/hatwide\u03ba(p)\u2212y)\u2202\u2113(\u03ba(p))]\n\u2264sup\nv\u2208[0,1]E\n(p,y)\u223cJ[(y\u2212/hatwide\u03ba(p)) sign+(\u03ba(p)\u2212v)]\nSinceKis closed under translations, there is\u03ba\u2032\u2208Ksuch that\n\u22062\u2264E\n(p,y)\u223cJ[(y\u2212/hatwide\u03ba(p)) sign+(\u03ba\u2032(p)\u22121/2)]\n\u2264sup\nc\u2208C/vextendsingle/vextendsingle/vextendsingleE\n(p,y)\u223cJ[(y\u2212/hatwide\u03ba(p))c(p)]/vextendsingle/vextendsingle/vextendsingle (By the definition ofC=thr(K))\n\u2264\u03b5,( /hatwide\u03ba\u2208calMAC(J,\u03b5))\nwhich concludes the proof.\n30\nThe final ingredient of Theorem 7.1 is the following result from [ GHK+23] which reduces learning\na predictor satisfying calibrated multiaccuracy to agnostic learning.\nTheorem 7.4(Calibrated Multiaccuracy [ GHK+23]).LetC\u2286{ [0,1]\u2192{\u00b1 1}}and let ALbe an\n(\u03b5/3)-agnostic learner for C. There is an algorithm that calls ALO (log(1/\u03b4)/\u03b52)times, performs\npoly(1/\u03b5)log(1/\u03b4)additional operations, and outputs /hatwide\u03ba\u2208calMAC(J,\u03b5/2)with probability at least\n1\u2212\u03b4.\n7.2 Pool Adjacent Violators Is an Omnipredictor\nPool Adjacent Violators is a classical algorithm for recalibration [ ABE+55], which finds a monotone\npost-processing of a predictor. It starts with a sample of {( yi,pi)}pairs. It starts from the Bayes\noptimal predictor \u03ba(pi) =E[yi|pi], and pools/merges any adjacent pair that violates monotonicity\ninto a single interval Iwhere the prediction is the conditional expectation E[y|I]. We present the\nalgorithm formally in Algorithm 1.\nAlgorithm 1:PoolAdjacentViolators(S)\nInput:SetSofmpairs of the form(p,y)wherep\u2208[0,1],y\u2208{0,1}\nOutput: A pair(O,I)whereO= ((p1,y1),..., (pm,ym))is an ordering of the input set S,\nandIis a sequence of disjoint intervals on[0,1]that coverP={p 1,...,pm}.\n1LetO= ((p 1,y1),(p 2,y2),...,(pm,ym))be such thatp 1\u2264p2\u2264\u00b7\u00b7\u00b7\u2264p m;\n2Lett= 0,I(0)= (I(0)\n1,...,I(0)\nm), whereI(0)\ni={pi}fori\u2208[m];\n3Set\u00afy(0)\ni=yifor alli\u2208[m];\n4whilethere isj\u2217\u2208[m\u2212t]such that\u00afy(t)\nj\u2217\u2265\u00afy(t)\nj\u2217+1do\n5Merge the setsI(t)\nj\u2217andI(t)\nj\u2217+1, i.e., setI(t+1)= (I(t+1)\nj)j\u2208[m\u2212t\u22121] , where\nI(t+1)\nj =\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3I(t)\nj,if0\u2264j <j\u2217\nI(t)\nj\u2217\u222aI(t)\nj\u2217+1,ifj=j\u2217\nI(t)\nj+1,ifm\u2212t\u22121\u2265j >j\u2217\uf8fc\n\uf8f4\uf8f4\uf8fd\n\uf8f4\uf8f4\uf8fe;\n6Let\u00afy(t+1)\nj =1\n|I(t+1)\nj\u2229P|/summationtext\ni:pi\u2208I(t+1)\njyifor allj\u2208[m\u2212t];\u25b7Can be implemented inO(1)\n7Updatet\u2190t+ 1;\n8Return(O,I) = (O,I(t));\nWe show the following guarantee for PAV.\nTheorem 7.5(Omniprediction through PAV).For \u03b5,\u03b4\u2208 (0,1), Pool Adjacent Violators (PAV) run\nonO(log(1/(\u03b5\u03b4))/\u03b52)samples is an algorithm that learns an( \u03b5,M +)-omnipredictor with probability\n1\u2212\u03b4that runs in time \u02dcO(1/\u03b52).\nIn order to prove Theorem 7.5, we first consider the empirical version of the omniprediction\nproblem and show the following result.\nTheorem 7.6(PAV guarantees).Let Sbe a set of mpairs of the form( p,y)wherep\u2208[0,1]\nandy\u2208{ 0,1}. There is an O(mlogm )-time algorithm (Algorithm 1) that computes a monotone\npost-processing /hatwide\u03ba\u2208M +such that for any proper loss\u2113\u2208L\u2217we have:\n/summationdisplay\n(p,y)\u2208S\u2113(/hatwide\u03ba(p),y) = min\n\u03ba\u2208M +/summationdisplay\n(p,y)\u2208S\u2113(\u03ba(p),y)\n31\nProof.Let(O= (pi,yi)i\u2208[m],I= (Ij)j\u2208[m\u2032])be the output of Algorithm 1. The monotone post-\nprocessing/hatwide\u03bais obtained by setting for each i\u2208[m]:/hatwide\u03ba(pi) =\u00afyIj, wherejis such that pi\u2208Ij, and\ninterpolating linearly between the pointsp i, in order to preserve monotonicity.\nFor any fixed proper loss \u2113, we will show that /hatwide\u03bais an optimal monotone post-processing. To this\nend, we use an exchange argument to prove the correctness of the greedy approach of Algorithm 1.\nIn particular, we will show that at any step t\u2208{ 0,1,...,m\u2212 1}of the algorithm, and for any\nj\u2217\u2208[m\u2212t ]such that \u00afy(t)\nj\u2217\u2265\u00afy(t)\nj\u2217+1, there is an optimal monotone post-processing \u02dc\u03bathat gives the\nsame value to all the points in the setI(t)\nj\u2217\u222aI(t)\nj\u2217+1. Therefore, it is safe to mergeI(t)\nj\u2217andI(t)\nj\u2217+1.\nClaim.Assume that there is an optimal monotone post-processing that is constant on each of the\nintervalsI 1,I2,...,In. LetIj,Ij+1be two intervals such that\n\u00afyj=E[y|p\u2208I j]\u2265E[y|p\u2208I j+1] = \u00afyj+1.\nThere exists an optimal post-processing\u02dc\u03ba\u2208M +such that\u02dc\u03ba(I j) = \u02dc\u03ba(Ij+1).\nProof.Consider any post-processing function \u03ba\u2208M +that is constant on each of the intervals\nI1,...,In. Our goal is to find \u02dc\u03ba\u2208M +which does as well as \u03bafor any proper loss \u2113\u2208L\u2217, and\nwhere \u02dc\u03ba(Ij) =\u02dc\u03ba(Ij+1). On the other intervals, we will have \u02dc\u03ba=\u03baand they both suffer the same\nloss, so we can ignore those intervals.\nLet us write \u03bai=\u03ba(Ii),\u02dc\u03bai=\u02dc\u03ba(Ii)fori\u2208{j,j + 1}. By monotonicity, \u03baj\u2264\u03baj+1. If they are\nequal, we can take \u02dc\u03ba=\u03ba, so assume the inequality is strict. In this case, we have \u00afyj\u2265\u00afyj+1and\n\u03baj<\u03baj+1. We now consider three collectively exhaustive cases:\n1.\u03baj<\u03baj+1\u2264\u00afyj. We set\u02dc\u03ba j= \u02dc\u03baj+1=\u03baj+1. By Lemma 3.2,\n\u2113(\u02dc\u03baj,\u00afyj) =\u2113(\u03baj+1,\u00afyj)\u2264\u2113(\u03baj,\u00afyj)\nso\u02dc\u03bacan only improve\u03baon intervalI j, while they agree onI j+1.\n2.\u00afyj+1\u2264\u03baj<\u03baj+1. We set\u02dc\u03ba j= \u02dc\u03baj+1=\u03baj. By Lemma 3.2,\n\u2113(\u02dc\u03baj+1,\u00afyj+1) =\u2113(\u03baj,\u00afyj+1)\u2264\u2113(\u03baj+1,\u00afyj+1)\nso\u02dc\u03bacan only improve\u03baon intervalI j+1, while they agree onI j.\n3.\u03baj<\u00afyj+1\u2264\u00afyj<\u03baj+1. Pick anya\u2208[\u00afyj+1,\u00afyj]and let \u02dc\u03baj=\u02dc\u03baj+1=a. Since\u03baj<a\u2264\u00afyj+1, by\nLemma 3.2, \u2113(a,\u00afyj)\u2264\u2113(\u03baj,\u00afyj). Similarly, since \u00afyj+1\u2264a<\u03baj+1,\u2113(a,\u00afyj+1)\u2264\u2113(\u03baj+1,\u00afyj+1).\nHence\u02dc\u03bacan only improve\u03baon both intervalsI j,Ij+1.\nThis concludes the proof of the exchange argument.\nFor a given loss \u2113, let\u03ba\u2217be an optimal monotone post-processing for the distribution S. Since\nI(0)is composed of singletons, \u03ba\u2217is trivially constant on its intervals. The claim then implies\nthat each iteration of PAV inductively preserves the property that there is an optimal monotone\n32\npostprocessing \u03bathat is constant on intervals in I(t). Considering a \u03bathat is optimal for I, and let\n\u03bajbe its value forp\u2208I j. Denoting by\u00afy jthe average ofyonI j, we have:\nE\n(p,y)\u223cS[\u2113(\u03ba(p),y)] =1\nm/summationdisplay\nj\u2208[m\u2032]/summationdisplay\ni:p\u2208Ij\u2113(\u03ba(p),yi)\n=1\nm/summationdisplay\nj\u2208[m\u2032]/summationdisplay\ni:p\u2208Ij\u2113(\u03baj,yi)\n\u22651\nm/summationdisplay\nj\u2208[m\u2032]/summationdisplay\ni:p\u2208Ij\u2113(\u00afyj,yi)\n=E\n(p,y)\u223cS[\u2113(/hatwide\u03ba(p),y)],\nwhere the inequality follows from the fact that \u2113is proper (Definition 2.1) and \u00afyjis the average of y\nonIj. This implies that /hatwide\u03bais an optimal post-processing.\nFinally, since the output of Algorithm 1 satisfies \u00afy1<\u00afy2<\u00b7\u00b7\u00b7<\u00afy m\u2032, it follows that /hatwide\u03baitself is\nmonotone. The claim follows.\nRemark7.7.Even though Theorem 7.6 is stated for L\u2217, it actually holds for all proper loss functions;\nin the case of PAV, the boundedness assumption is only needed for generalization.\nWe are now ready to prove Theorem 7.5.\nProof of Theorem 7.5. It suffices to show that whenever |S|\u2265Clog(1/\u03b5\u03b4)/\u03b52(for some sufficiently\nlarge constant C\u2265 1, whereSconsists of i.i.d. samples from some distribution Jover[0,1]\u00d7{0,1},\nthen, with probability at least1\u2212\u03b4, the following is true uniformly over all\u03ba, /hatwide\u03ba\u2208M +,\u2113\u2208L\u2217:\n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleE\n(p,y)\u223cJ[\u2113(/hatwide\u03ba(p),y)\u2212\u2113(\u03ba(p),y)]\u22121\nm/summationdisplay\n(p,y)\u2208S(\u2113(/hatwide\u03ba(p),y)\u2212\u2113(\u03ba(p),y))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle\u2264\u03b5\nThe above inequality follows by combining the fact that VCdim (thr(M+)) = 1with Lemma 4.2.\n7.3 Omniprediction Through Uniform-Mass Binning and Recalibration\nWe next analyze recalibration through uniform-mass binning. Binning is a long-established technique\nfor measuring calibration [Mil62, San63]. The method of uniform-mass binning was introduced by\n[ZE01] as the first binning-based approach not only for measuring calibration, but also for obtaining\na calibrated predictor. We show that this natural algorithm yields omniprediction with respect to\nthe class of generalized monotone post-processings. In other words, recalibration via uniform-mass\nbinning improves the performance of the input predictor under every proper loss, with improvement\nat least as large as that achieved by the best generalized monotone post-processing. While previous\nwork focused on the calibration properties of uniform-mass binning, we show that this method\npreserves the information encoded in the predictor\u2014when measured by proper losses\u2014at least as\nwell as any generalized monotone post-processing.\nWe note that the omniprediction guarantee achieved by this method is stronger than the one we\nestablished for the PAV algorithm in Theorem 7.5, since it applies to the larger classM r.\nWe define uniform-mass binning as follows.\n33\nDefinition 7.8(Uniform-Mass Binning).Let P= (p1,p2,...,pm)be a collection of mpoints in\n[0,1], with potential repetitions, and let \u03b5\u2208(0,1). We say that a partition( Ij)j\u2208[t]of[0,1]is an\n\u03b5-uniform-mass partition with respect to Pift\u22642/\u03b5and eachIjin the partition is an interval\n(open, closed, or half-open) for which at least one of the following holds:\n\u2022(Small buckets)|{i\u2208[m] :p i\u2208Ij}|\u2264\u03b5m.\n\u2022(Overflow buckets) The intervalI j= [a,a]for somea\u2208[0,1].\nOverflow buckets are necessary since it could be the case that pi=afor say1/2the samples, in\nwhich case we create a separate bucket for it. We show that recalibration with uniform-mass binning\nachieves omniprediction with respect to the class of r-generalized monotone post-processings, with\na number of buckets that is linear in r. While the resulting predictor /hatwide\u03baneed not lie inMr, it is\npiecewise constant onr\u2032=O(r/\u03b5)intervals, hence it belongs toM r\u2032.\nTheorem 7.9.Let \u03b5\u2208(0,1)andr\u22651. Then, Algorithm 2, run with parameter \u03b5\u2032=\u03b5/8ron\nO(r2log(1/\u03b4)/\u03b54)samples learns an( \u03b5,Mr)-omnipredictor with probability1 \u2212\u03b4and has time\ncomplexityO(r2log(1/\u03b4)/\u03b54).\nAlgorithm 2:UMB-Recalibration(S,\u03b5\u2032)\nInput:SetSofmpairs of the form(p,y)wherep\u2208[0,1],y\u2208{0,1}, parameter\u03b5\u2032\u2208(0,1)\nOutput:Post-processing function /hatwide\u03ba: [0,1]\u2192[0,1]\n1Create an\u03b5\u2032-uniform-mass partition(I j)j\u2208[t]w.r.t.(p 1,...,pm)greedily, wheret\u22642/\u03b5\u2032;\n2For allj\u2208[t], let\n/hatwide\u03baj=/summationtext\ni\u2208[m]yi1[pi\u2208Ij]\n/summationtext\ni\u2032\u2208[m]1[pi\u2032\u2208Ij].\n3Return the function /hatwide\u03badefined as\n/hatwide\u03ba(p) =t/summationdisplay\nj=1/hatwide\u03baj1[p\u2208Ij].\nIn order to prove Theorem 7.9, we will use the following result regarding the sampling errors. The\nfirst inequality here is a standard uniform convergence bound. The second one captures the intuition\nthat the per-bucket average is likely to be close to accurate for all buckets that are reasonably large.\nThe somewhat unusual formalization of this condition that we use below better fits our application\nlater, and allows for a tighter bound on the sample complexity.\nLemma 7.10.Let \u03b3\u2208(0,1), letSbe a set of mi.i.d. samples from some distribution Jover\n[0,1]\u00d7{0,1}, and let(Ij,/hatwide\u03baj)j\u2208[t]be as defined in Algorithm 2. If m\u2265Clog (1/\u03b4)/\u03b32for some\nsufficiently large constantC\u22651, then the following hold with probability at least1\u2212\u03b4:\n/vextendsingle/vextendsingle/vextendsingleP\nJ[p\u2208Ij]\u22121\nm/summationdisplay\ni\u2208[m]1[pi\u2208Ij]/vextendsingle/vextendsingle/vextendsingle\u2264\u03b3,for allj\u2208[t].\n|(/hatwide\u03baj\u2212E\nJ[y|p\u2208Ij])\u00b7Pr\nJ[p\u2208Ij]|\u2264\u03b3,for anyj\u2208[t];\n34\nProof.The intervals( Ij)j\u2208[t]in the output of the algorithm depend on the input examples( pi,yi)i\u2208[m]\nwhich are drawn i.i.d. from some distribution J. Using standard uniform convergence arguments\n(as for proving Lemma 4.2), and the fact that the VC dimension of intervals over[0 ,1]is2, we have\nthat with probability at least1 \u2212\u03b4, the following guarantees hold as long as m\u2265Clog (1/\u03b4)/\u03b32for\nsome sufficiently large constantC\u22651.\n/vextendsingle/vextendsingle/vextendsingle1\nm/summationdisplay\ni\u2208[m]yi1[pi\u2208Ij]\u2212E\n(p,y)\u223cJ[y1[p\u2208I j]]/vextendsingle/vextendsingle/vextendsingle\u2264\u03b3\n2,for allj\u2208[t](7.3)\n/vextendsingle/vextendsingle/vextendsingle1\nm/summationdisplay\ni\u2208[m]1[pi\u2208Ij]\u2212P\n(p,y)\u223cJ[p\u2208Ij]/vextendsingle/vextendsingle/vextendsingle\u2264\u03b3\n2,for allj\u2208[t].(7.4)\nFixing aj\u2208[t], we write:\n|(/hatwide\u03baj\u2212E\nJ[y|p\u2208Ij])\u00b7Pr\nJ[p\u2208Ij]|=|/hatwide\u03baj\u00b7Pr\nJ[p\u2208Ij]\u2212E\nJ[y\u00b71[p\u2208I j]]|\n=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtext\ni\u2208[m]yi1[pi\u2208Ij]\n/summationtext\ni\u2208[m]1[pi\u2208Ij]\u00b7Pr\nJ[p\u2208Ij]\u2212E\nJ[y\u00b71[p\u2208I j]]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle\n\u2264/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtext\ni\u2208[m]yi1[pi\u2208Ij]\n/summationtext\ni\u2208[m]1[pi\u2208Ij]\u00b7Pr\nJ[p\u2208Ij]\u22121\nm/summationdisplay\ni\u2208[m]yi1[pi\u2208Ij]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle\n+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1\nm/summationdisplay\ni\u2208[m]yi1[pi\u2208Ij]\u2212E\nJ[y\u00b71[p\u2208I j]]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle\n=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtext\ni\u2208[m]yi1[pi\u2208Ij]\n/summationtext\ni\u2208[m]1[pi\u2208Ij]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle\u00b7/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglePr\nJ[p\u2208Ij]\u22121\nm/summationdisplay\ni\u2208[m]1[pi\u2208Ij]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle\n+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1\nm/summationdisplay\ni\u2208[m]yi1[pi\u2208Ij]\u2212E\nJ[y\u00b71[p\u2208I j]]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle\nHere in the first step, we have used the definition of /hatwide\u03baj, and used triangle inequality in the second\nstep. The second term in the final expression is easily seen to be at most \u03b3/2by Eq. (7.3). Finally\nnote that /vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtext\ni\u2208[m]yi1[pi\u2208Ij]\n/summationtext\ni\u2208[m]1[pi\u2208Ij]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle\u22641\nfor every instantiation of the samples, so that Eq. (7.4) implies a bound of \u03b3/2on the first term as\nwell. This concludes the proof of Lemma 7.10.\nWe are now ready to prove Theorem 7.9. At a high level, we will split the decision loss for\na loss\u2113vand a post-processing function \u03baacross the buckets. We argue that except for a small\nnumber (O(r)) of buckets, the sign of( \u03ba(p)\u2212v)is constant within the bucket, using the fact\nthat\u03ba\u2208Mr. We separately handle these buckets using the fact that each such bucket (which\ncannot be an overflow bucket) has small probability mass. For the typical buckets that don\u2019t have\na disagreement, the recalibration, if population-exact, would immediately ensure no decision loss.\nWe will use Lemma 7.10 to control the error that arises due to the potential inaccuracy of the\nsampling-based recalibration.\n35\nProof of Theorem 7.9.With foresight, we set\n\u03b5\u2032=\u03b5\n8r, \u03b3=\u03b5\u03b5\u2032\n4=\u03b52\n32r.\nOur goal is to show the following inequality for all\u03ba\u2208M rand\u2113\u2208L\u2217:\nE\n(p,y)\u223cJ[\u2113(/hatwide\u03ba(p),y)]\u2264E\n(p,y)\u223cJ[\u2113(\u03ba(p),y)] +\u03b5(7.5)\nBy Lemma 3.5, any proper loss\u2113\u2208L\u2217can be written as follows:\n\u2113(p,y) =/integraldisplay\n[0,1]\u2113+\nv(p,y)d\u00b5+\n\u2113(v) +/integraldisplay\n[0,1]\u2113\u2212\nv(p,y)d\u00b5\u2212\n\u2113(v),\nwhere\u00b5\u2113issomemeasureover[0 ,1]with\u00b5+\n\u2113([0,1])+\u00b5\u2212\n\u2113([0,1])\u22641and\u2113\u00b1\nv(p,y) =\u2212(y\u2212v)sign\u00b1(p\u2212v).\nTherefore, it suffices to show (7.5)for the losses( \u2113\u00b1\nv)v\u2208[0,1]. Below, we will focus on the losses\n(\u2113+\nv)v\u2208[0,1], since the proof for(\u2113\u2212\nv)v\u2208[0,1]will be analogous.\nControlling Buckets of Disagreement.We fix an arbitrary \u03ba\u2208Mrandv\u2208[0,1], and split\n[t]into two parts as follows. We letH=H \u03ba,v\u2286[t]be as follows.\nH=/vextendsingle/vextendsingle/vextendsingle/braceleft\uf8ecig\nj\u2208[t] :\u2203p,q\u2208I jsuch thatsign+(\u03ba(p)\u2212v)\u0338= sign+(\u03ba(q)\u2212v)/braceright\uf8ecig/vextendsingle/vextendsingle/vextendsingle (7.6)\nA key observation is that |H|\u2264 2r, due to the definition of Mr(Definition 2.2). In particular,\nsince{p:\u03ba(p)\u2265v}can be expressed as a union of at most rdisjoint intervals I, the number of\nsign changes of \u03ba(p)\u2212vaspincreases from0to1is at most2 r(at the endpoints of the intervals).\nMoreover, any j\u2208Hcorresponds to at least one distinct point of sign change for \u03ba(p)\u2212v, and,\nhence,|H|\u22642r.\nNote that whenever |Ij|= 1, we have j\u0338\u2208H. Recall that, due to the construction of( Ij)j\u2208[t]\n(Algorithm 2), for any jsuch that|Ij|>1we have|Ij\u2229{pi:i\u2208[m]}|\u2264\u03b5\u2032m. Due to the uniform\nconvergence bound of Lemma 7.10 we overall have:\nP\nJ[p\u2208Ij]\u2264\u03b5\u2032+\u03b3for allj\u2208H(7.7)\nHandling typical buckets.For an interval Ijnot inH, the sign+(\u03ba(p)\u2212v)is constant throughout\nthe interval, and the same is true for /hatwide\u03ba, by construction. Let sj=sign+(\u03ba(p)\u2212v)forp\u2208Ij,\nj\u2208[t]\\Hand similarly /hatwidesj= sign+(/hatwide\u03ba(p)\u2212v). For such a bucket, we can write\nE[\u2113+\nv(/hatwide\u03ba(p),y)\u00b71[p\u2208I j]]\u2212E[\u2113+\nv(\u03ba(p),y)\u00b71[p\u2208I j]]\n=E[\u2212(E[y|1[p\u2208I j]\u2212v)\u00b7(/hatwidesj\u2212sj)\u00b71[p\u2208I j]]\n=E[\u2212(/hatwide\u03baj\u2212v)\u00b7(/hatwidesj\u2212sj)\u00b71[p\u2208I j]] +E[\u2212(E[y|1[p\u2208I j]\u2212/hatwide\u03baj)\u00b7(/hatwidesj\u2212sj)\u00b71[p\u2208I j]].\nObserve that\u2212(/hatwide\u03baj\u2212v)/hatwidesj=\u2212|/hatwide\u03baj\u2212v|\u2264\u2212 (/hatwide\u03baj\u2212v)sjso that the first term is bounded above by\nzero. On the other hand, the second term is controlled by Lemma 7.10:\nE[\u2212(E[y|1[p\u2208I j]\u2212/hatwide\u03baj)\u00b7(/hatwidesj\u2212sj)\u00b71[p\u2208I j]]\u22642\u00b7E[(E[y|1[p\u2208I j]\u2212/hatwide\u03baj)\u00b71[p\u2208I j]]\n\u22642\u03b3.\nThus we have shown that for any intervalI j\u0338\u2208H:\nE[\u2113+\nv(/hatwide\u03ba(p),y)\u00b71[p\u2208I j]]\u2212E[\u2113+\nv(\u03ba(p),y)\u00b71[p\u2208I j]]\u22642\u03b3(7.8)\n36\nPutting it Together.We are now ready to prove the theorem. We will split the buckets into the\nbuckets of disagreement and the rest, and use the bounds above to control the total error. We write\nE\n(p,y)\u223cJ[\u2113(/hatwide\u03ba(p),y)]\u2212E\n(p,y)\u223cJ[\u2113(\u03ba(p),y)]\n=/summationdisplay\nj\u2208[t]E\n(p,y)\u223cJ[(\u2113(/hatwide\u03ba(p),y)\u2212\u2113(\u03ba(p),y))\u00b71[p\u2208I j]]\n=/summationdisplay\nj\u2208HE\n(p,y)\u223cJ[(\u2113(/hatwide\u03ba(p),y)\u2212\u2113(\u03ba(p),y))\u00b71[p\u2208I j]]\n+/summationdisplay\nj\u2208[t]\\HE\n(p,y)\u223cJ[(\u2113(/hatwide\u03ba(p),y)\u2212\u2113(\u03ba(p),y))\u00b71[p\u2208I j]]\n\u22642r\u00b7(\u03b5\u2032+\u03b3) + (2/\u03b5\u2032)\u00b7(2\u03b3)\n\u22644r\u03b5\u2032+ 4\u03b3/\u03b5\u2032\n\u2264\u03b5.\nHere we have bounded the sum over Hand[t]\\Husing Eq. (7.7) and Eq. (7.8) respectively. The\nclaim follows.\n37\nReferences\n[ABE+55]MiriamAyer, HDanielBrunk, GeorgeMEwing, WilliamTReid, andEdwardSilverman.\nAn empirical distribution function for sampling with incomplete information.The\nannals of mathematical statistics, pages 641\u2013647, 1955. 3, 4, 11, 29, 31\n[BdP13] Niko Br\u00fcmmer and Johan A. du Preez. The PAV algorithm optimizes binary proper\nscoring rules.ArXiv, abs/1304.2331, 2013. 11, 12\n[BGHN23a] Jaroslaw Blasiok, Parikshit Gopalan, Lunjia Hu, and Preetum Nakkiran. A unifying\ntheory of distance from calibration. InProceedings of the 55th Annual ACM Symposium\non Theory of Computing, STOC 2023, pages 1727\u20131740. ACM, 2023. 1, 2, 4, 8, 19\n[BGHN23b] Jaroslaw Blasiok, Parikshit Gopalan, Lunjia Hu, and Preetum Nakkiran. When\ndoes optimizing a proper loss yield calibration? InAdvances in Neural Information\nProcessing Systems, volume 36, pages 72071\u201372095, 2023. 1, 2, 4, 8, 19\n[BHJB25] Eug\u00e8ne Berta, David Holzm\u00fcller, Michael I. Jordan, and Francis Bach. Rethinking\nearly stopping: Refine, then calibrate, 2025. 3\n[BN24]Jaroslaw Blasiok and Preetum Nakkiran. Smooth ECE: principled reliability dia-\ngrams via kernel smoothing. InThe Twelfth International Conference on Learning\nRepresentations, ICLR 2024, 2024. 1, 2\n[CDV24] S\u00edlvia Casacuberta, Cynthia Dwork, and Salil Vadhan. Complexity-theoretic impli-\ncations of multicalibration. InProceedings of the 56th Annual ACM Symposium on\nTheory of Computing, STOC 2024, page 1071\u20131082, 2024. 1\n[Daw85] A Philip Dawid. Calibration-based empirical probability.The Annals of Statistics,\npages 1251\u20131274, 1985. 1\n[DKR+21]Cynthia Dwork, Michael P. Kim, Omer Reingold, Guy N. Rothblum, and Gal Yona.\nOutcome indistinguishability. InACM Symposium on Theory of Computing (STOC\u201921),\n2021. 1, 2, 20\n[Fel09]VitalyFeldman. Distribution-specificagnosticboosting.arXiv preprint arXiv:0909.2927,\n2009. 7\n[FV98]Dean P. Foster and Rakesh V. Vohra. Asymptotic calibration.Biometrika, 85(2):379\u2013\n390, 1998. 1, 3\n[GH25]Parikshit Gopalan and Lunjia Hu. Calibration through the lens of indistinguishability.\nACM SIGecom Exchanges, 23(1), July 2025. 2, 8\n[GHK+23]Parikshit Gopalan, Lunjia Hu, Michael P Kim, Omer Reingold, and Udi Wieder. Loss\nminimization through the lens of outcome indistinguishability. In14th Innovations in\nTheoretical Computer Science Conference (ITCS 2023), 2023. 2, 9, 10, 11, 19, 20, 29,\n31\n[GHR24] Parikshit Gopalan, Lunjia Hu, and Guy N. Rothblum. On computationally efficient\nmulti-class calibration. InThe Thirty Seventh Annual Conference on Learning Theory,\nvolume 247 ofProceedings of Machine Learning Research, pages 1983\u20132026. PMLR,\n2024. 2, 13, 16, 17\n38\n[GKR+22]Parikshit Gopalan, Adam Tauman Kalai, Omer Reingold, Vatsal Sharan, and Udi\nWieder. Omnipredictors. InInnovations in Theoretical Computer Science (ITCS\u20192022),\n2022. 1, 10\n[GKSZ22] Parikshit Gopalan, Michael P. Kim, Mihir Singhal, and Shengjia Zhao. Low-degree\nmulticalibration. InConference on Learning Theory, 2-5 July 2022, London, UK,\nvolume 178 ofProceedings of Machine Learning Research, pages 3193\u20133234. PMLR,\n2022. 2, 8\n[GPSW17] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern\nneural networks. InInternational Conference on Machine Learning, pages 1321\u20131330.\nPMLR, 2017. 3, 12\n[GR21]Chirag Gupta and Aaditya Ramdas. Distribution-free calibration guarantees for\nhistogram binning without sample splitting. InInternational conference on machine\nlearning, pages 3942\u20133952. PMLR, 2021. 3, 4, 11, 12, 29\n[HJTY24] Lunjia Hu, Arun Jambulapati, Kevin Tian, and Chutong Yang. Testing calibration in\nnearly-linear time. InThe Thirty-eighth Annual Conference on Neural Information\nProcessing Systems, 2024. 2\n[HKRR18] \u00darsula H\u00e9bert-Johnson, Michael P. Kim, Omer Reingold, and Guy N. Rothblum. Mul-\nticalibration: Calibration for the (computationally-identifiable) masses. InProceedings\nof the 35th International Conference on Machine Learning, ICML, 2018. 1\n[HSLW23] Jason D. Hartline, Liren Shan, Yingkai Li, and Yifan Wu. Optimal scoring rules for\nmulti-dimensional effort. InThe Thirty Sixth Annual Conference on Learning Theory,\nCOLT 2023, volume 195 ofProceedings of Machine Learning Research, pages 2624\u20132650.\nPMLR, 2023. 5\n[HV25]Lunjia Hu and Salil P. Vadhan. Generalized and unified equivalences between hardness\nand pseudoentropy.CoRR, abs/2507.05972, 2025. 1\n[HW24] Lunjia Hu and Yifan Wu. Calibration error for decision making. InProceedings of the\n65th Annual IEEE Symposium on Foundations of Computer Science (FOCS). IEEE,\n2024. 1, 3, 4, 6, 7, 8, 9, 14, 45\n[HWY25] Jason D. Hartline, Yifan Wu, and Yunran Yang. Smooth calibration and decision\nmaking. In11th Symposium on Foundations of Responsible Computing (FORC 2025),\nvolume 329 ofLIPIcs, pages 16:1\u201316:22. Schloss Dagstuhl \u2013 Leibniz-Zentrum f\u00fcr\nInformatik, 2025. 11\n[KF08]Sham Kakade and Dean Foster. Deterministic calibration and Nash equilibrium.\nJournal of Computer and System Sciences, 74(1):115\u2013130, 2008. 1, 2, 19\n[KK09]Adam Kalai and Varun Kanade. Potential-based agnostic boosting. InAdvances in\nNeural Information Processing Systems, volume 22, 2009. 7\n[KLST23] Bobby Kleinberg, Renato Paes Leme, Jon Schneider, and Yifeng Teng. U-calibration:\nForecasting for an unknown agent. InThe Thirty Sixth Annual Conference on Learning\nTheory, pages 5143\u20135145. PMLR, 2023. 3, 4, 5, 8, 9, 14\n39\n[KSS94] Michael J. Kearns, Robert E. Schapire, and Linda Sellie. Toward efficient agnostic\nlearning.Mach. Learn., 17(2-3):115\u2013141, 1994. 7, 46\n[Mil62]Robert G Miller. Statistical prediction by discriminant analysis. InStatistical prediction\nby discriminant analysis, pages 1\u201354. Springer, 1962. 12, 33\n[MRT18] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.Foundations of machine\nlearning. MIT press, 2018. 43\n[NC05]Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with\nsupervised learning. In Luc De Raedt and Stefan Wrobel, editors,Machine Learning,\nProceedings of the Twenty-Second International Conference (ICML 2005), volume 119\nofACM International Conference Proceeding Series, pages 625\u2013632. ACM, 2005. 3\n[NCH15] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well\ncalibrated probabilities using bayesian binning. InProceedings of the AAAI conference\non artificial intelligence, volume 29, 2015. 12\n[OKK25] Princewill Okoroafor, Robert Kleinberg, and Michael P. Kim. Near-optimal algorithms\nfor omniprediction.CoRR, abs/2501.17205, 2025. 1, 2, 20\n[Pla00]John Platt. Probabilistic outputs for support vector machines and comparisons to\nregularized likelihood methods.Adv. Large Margin Classif., 10, 06 2000. 3, 4\n[RCSM22] RebeccaRoelofs, NicholasCain, JonathonShlens, andMichaelCMozer. Mitigatingbias\nin calibration error estimation. InInternational Conference on Artificial Intelligence\nand Statistics, pages 4036\u20134054. PMLR, 2022. 12\n[RSB+25]Raphael Rossellini, Jake A. Soloff, Rina Foygel Barber, Zhimei Ren, and Rebecca\nWillett. Can a calibration metric be both testable and actionable? InThe Thirty\nEighth Annual Conference on Learning Theory, volume 291 ofProceedings of Machine\nLearning Research, pages 4937\u20134972. PMLR, 2025. 1, 2, 3, 4, 8, 9, 20\n[San63]FrederickSanders. Onsubjectiveprobabilityforecasting.Journal of Applied Meteorology\nand Climatology, 2(2):191\u2013201, 1963. 12, 33\n[SSH23] Zeyu Sun, Dogyoon Song, and Alfred Hero. Minimum-risk recalibration of classifiers.\nAdvances in Neural Information Processing Systems, 36:69505\u201369531, 2023. 3, 4, 11,\n12, 29\n[ZE01]Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from\ndecision trees and naive bayesian classifiers. InIcml, volume 1, pages 609\u2013616. Citeseer,\n2001. 3, 4, 11, 12, 29, 33\n[ZE02]Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multi-\nclass probability estimates. InProceedings of the eighth ACM SIGKDD international\nconference on Knowledge discovery and data mining, pages 694\u2013699. ACM, 2002. 3, 4,\n11\n[ZKS+21]Shengjia Zhao, Michael P. Kim, Roshni Sahoo, Tengyu Ma, and Stefano Ermon.\nCalibrating predictions to decisions: A novel approach to multi-class calibration. In\nAnnual Conference on Neural Information Processing Systems 2021, NeurIPS 2021,\npages 22313\u201322324, 2021. 1\n40\nA Why Generalized Monotone Functions\nHere we discuss why generalized monotone functions are a natural class of post-processing functions.\nIntuitively, monotonicity is a natural constraint on post-processing functions to apply to predictors\nif we believe that the predictors are reasonably good to begin with: if p= 0.7, that ought to mean\nthat the probability of the label1is higher than if p= 0.3. However, this might not be true for\ntwo close-by values like0 .7and0.6999. Thus it is natural to relax the condition and allow some\nviolations of monotonicity (see Figure 1 for an example).\nA first attempt might be through allowing small total variation. An increasing sequence Inof\nlengthnin[0,1]is(pi)i\u2208[n]such that0\u2264p1<p2\u00b7\u00b7\u00b7<pn\u22641. For a function \u03ba: [0,1]\u2192[0,1], we\ndefine its total variation as\ntv(\u03ba) = limn\u2192\u221esup\nInn\u22121/summationdisplay\ni=1|\u03ba(pi+1)\u2212\u03ba(pi)|\nwhere the supremum is over all increasing sequences Inof lengthn. The class tv(f)\u22641contains\nboth monotone and Lipschitz functions. But we have seen that VCdim (thr(Lip)) =\u221ewhereas\nVCdim(thr(M +)) = 1, so they are very different fromCDLviewpoint.\nIs there a strengthening of tvthat rules out arbitrary Lipschitz function but extends monotone\nfunctions? We show thatM ris such a class.\nDefinition A.1.For v\u2208(0,1), let thecrossing numberat vdenoted crv(\u03ba)denote the largest n\nfor which there is a strictly increasing sequence( pi)i\u2208[n+1]such that sign(\u03ba(pi)\u2212v)alternates. Let\ncr(\u03ba) = sup\nv\u2208[0,1]crv(\u03ba),\ncr(K) = sup\n\u03ba\u2208Kcr(\u03ba)\nIt is easy to show thatcr(f)enjoys the following properties:\n\u2022cr(\u03ba)is within a constant factor of the smallestrfor which\u03ba\u2208M r.\n\u2022We have VCdim (thr(K))\u2264cr(K), since the alternating sequence of signs on cr(K) + 1points is\nnot realizable withinthr(K).\n\u2022tv (\u03ba)\u2264cr(\u03ba). We can see tv(\u03ba)as a bound on Ev[crv(\u03ba)]over uniformly random v\u2208[0,1],\nwhereascr(\u03ba)boundscr v(\u03ba)in the worst case. For more detail, see the following lemma.\nLemma A.2.tv(\u03ba)\u2264cr(\u03ba)for all\u03ba: [0,1]\u2192[0,1].\nProof.Fix any increasing sequence Inof points0\u2264p 1< p 2<\u00b7\u00b7\u00b7< p n\u22641. For any particular\nindex2\u2264i\u2264n, the probability over a uniformly random value v\u223c[0,1]that sign(\u03ba(pi\u22121)\u2212v)\u0338=\nsign(\u03ba(pi)\u2212v)is precisely|\u03ba(pi)\u2212\u03ba(pi\u22121)|. Let crv(\u03ba,p)denote the number of consecutive indices\nfor which this sign change occurs. Then, by linearity of expectation,\nEv[crv(\u03ba,p)] =n/summationdisplay\ni=2Prv/bracketleftbigsign(\u03ba(pi\u22121)\u2212v)\u0338= sign(\u03ba(p i)\u2212v)/bracketrightbig=n/summationdisplay\ni=2|\u03ba(pi\u22121)\u2212\u03ba(pi)|.\nThe supremum of the right side over all sequences Inis precisely tv(\u03ba), by definition. The supremum\nof the left hand side over all sequencesI nis\nsup\nInEv[crv(\u03ba,p)]\u2264sup\nvsup\nIncrv(\u03ba,p) = sup\nvcrv(\u03ba) = cr(\u03ba).\n41\nThusMr={\u03ba:cr(\u03ba)\u2264O(r)}corresponds to a class of functions that relaxes monotonicity,\nbut excludes functions whose thresholds have high VC dimension, like Lipschitz functions.\nB Additional Proofs\nB.1 Additional Proofs From Section 3\nProof of Lemma 3.2.Take\u03c6,\u03c6\u2032as in the characterization in Lemma 3.1, so that\n\u2113(p,q) =\u03c6(p) +\u03c6\u2032(p)(q\u2212p).\nThen we have\n\u2113(a,c)\u2212\u2113(b,c) =\u03c6(a) +\u03c6\u2032(a)(c\u2212a)\u2212/parenleft\uf8ecig\n\u03c6(b) +\u03c6\u2032(b)(c\u2212b)/parenright\uf8ecig\n=\u03c6(a) +\u03c6\u2032(a)(b\u2212a)\u2212\u03c6(b)/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright\n(\u2217)+/parenleft\uf8ecig\n\u03c6\u2032(a)\u2212\u03c6\u2032(b)/parenright\uf8ecig\n/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\n(\u2217\u2217)(c\u2212b),\nwhere nonnegativity of( \u2217)and(\u2217\u2217)follow from concavity of \u03c6. The other inequality is analogous.\nProof of Lemma 3.3. By Lemma 3.2, the function \u2113(p,0)is nondecreasing in p, and the function\n\u2113(p,1)is nonincreasing in p. We also have|\u2113(p,1)\u2212\u2113(p,0)|\u22641for any\u2113\u2208L\u2217. Thus, for any\n0\u2264p\u2264q\u22641,\n\u2113(p,0)\u2264\u2113(q,0)\u2264\u2113(q,1) + 1.\nConsidering more cases like this, we see that |\u2113(p,0)\u2212\u2113(q,1)|\u22641for allp,q\u2208 [0,1]. Combining\nthis inequality with |\u2113(p,1)\u2212\u2113(p,0)|\u22641, we see that any two outputs of \u2113on distinct inputs differ\nby at most2. Equivalently, we conclude that the range of any \u2113\u2208L\u2217is contained in an interval of\nlength at most2.\nProof of Corollary 3.6. First, by Lemma 3.5, we know that any loss \u2113\u2208L\u2217that is not of the form\n\u2113+\nvor\u2113\u2212\nvor a constant function is redundant and need not be considered in the supremum. Next, for\nany fixed(p,y)\u2208[0,1]\u00d7{0,1}andv\u2208[0,1), we have limw\u2192v+\u2113+\nw(p,y) =\u2113\u2212\nv(p,y). Therefore, by\nthe bounded convergence theorem, the losses \u2113\u2212\nvforv\u2208[0,1)are redundant if we take the supremum\nover all\u2113+\nwforw\u2208[0,1]. Note that \u2113\u2212\n1(p,y) =y\u22121is also redundant because it does not depend\nonp. Similarly, if v\u2208(0,1], then limw\u2192v\u2212\u2113\u2212\nw(p,y) =\u2113+\nv(p,y). Therefore, the losses \u2113+\nvforv\u2208(0,1]\nare redundant if we take the supremum over \u2113\u2212\nwlosses forw\u2208[0,1]. Note that \u2113+\n0(p,y) =\u2212yis also\nredundant because it does not depend onp. The same is true of constant functions.\nB.2 Additional Proofs From Section 4\nProof of Lemma 4.2.We define the following quantities forv\u2208[0,1]and\u03ba\u2208K.\nQ+\nv,\u03ba=E\n(p,y)\u223cJ[\u2113+\nv(p,y)\u2212\u2113+\nv(\u03ba(p),y)]andQ\u2212\nv,\u03ba=E\n(p,y)\u223cJ[\u2113\u2212\nv(p,y)\u2212\u2113\u2212\nv(\u03ba(p),y)]\n/hatwideQ+\nv,\u03ba=E\n(p,y)\u223cS[\u2113+\nv(p,y)\u2212\u2113+\nv(\u03ba(p),y)]and/hatwideQ\u2212\nv,\u03ba=E\n(p,y)\u223cS[\u2113\u2212\nv(p,y)\u2212\u2113\u2212\nv(\u03ba(p),y)]\nDue to Corollary 3.6, it suffices to show that, with probability at least1 \u2212\u03b4over the choice of S,\nwe have the following:\n|Q\u22c6\nv,\u03ba\u2212/hatwideQ\u22c6\nv,\u03ba|\u2264\u03b5,for allv\u2208[0,1],\u03ba\u2208K,\u22c6\u2208{+,\u2212}(B.1)\n42\nFor the remainder of the proof, we will focus on |Q+\nv,\u03ba\u2212/hatwideQ+\nv,\u03ba|, since the other case follows identically,\nwith the additional observation that, for translation-invariant classes K, we have VCdim (thr(K)) =\nVCdim(thr\u2212(K)), wherethr \u2212(K) ={p\u221d\u21d5\u22a3\u221a\u222b\u2294\u2240\u2192sign\u2212(\u03ba(p)\u22121/2) :\u03ba\u2208K}.\nWe will show that condition (B.1)holds with probability at least1 \u2212\u03b4as long as the size of\nSism\u2265Cd\n\u03b52log(1\n\u03b5\u03b4)for some large enough constant C\u2265 1. To this end, recall that \u2113+\nv(p,y) =\n\u2212(y\u2212v) sign+(p\u2212v). Therefore:\nQ+\nv,\u03ba=E\n(p,y)\u223cJ[(y\u2212v)(sign+(\u03ba(p)\u2212v)\u2212sign+(p\u2212v))]\n/hatwideQ+\nv,\u03ba=E\n(p,y)\u223cS[(y\u2212v)(sign+(\u03ba(p)\u2212v)\u2212sign+(p\u2212v))]\nFor the following, we use signto denote sign+andQv,\u03ba,/hatwideQv,\u03bato denoteQ+\nv,\u03ba,/hatwideQ+\nv,\u03ba. SinceKis\ntranslation invariant, for any \u03ba\u2208Kandv\u2208[0,1], there is \u03ba\u2032\u2208Ksuch that\u03ba\u2032(p) = [\u03ba(p) +1\n2\u2212v]1\n0.\nThen, we have sign(\u03ba(p)\u2212v) =sign(\u03ba(p) + 1/2\u2212v\u2212 1/2) = sign(\u03ba\u2032(p)\u22121/2), for allp\u2208[0,1], and,\ntherefore:\nQv,\u03ba=E\n(p,y)\u223cJ[(y\u2212v)(sign(\u03ba\u2032(p)\u22121/2)\u2212sign(p\u2212v))]\n/hatwideQv,\u03ba=E\n(p,y)\u223cS[(y\u2212v)(sign(\u03ba\u2032(p)\u22121/2)\u2212sign(p\u2212v))]\nLetV={i\u03f5\n16:i= 0,1,...,\u230a16\n\u03b5\u230b}\u222a{ 1}, and\u03c0\u03b5(v) =arg minv\u2032\u2208V|v\u2212v\u2032|. We define the following\nquantities.\nQV\nv,\u03ba=E\n(p,y)\u223cJ[(y\u2212\u03c0\u03b5(v))(sign(\u03ba\u2032(p)\u22121/2)\u2212sign(p\u2212v))]\n/hatwideQV\nv,\u03ba=E\n(p,y)\u223cS[(y\u2212\u03c0\u03b5(v))(sign(\u03ba\u2032(p)\u22121/2)\u2212sign(p\u2212v))]\nNote that, by the choice of V, we have|QV\nv,\u03ba\u2212Qv,\u03ba|\u2264\u03b5/ 8, and, similarly, |/hatwideQV\nv,\u03ba\u2212/hatwideQv,\u03ba|\u2264\u03b5/ 8, for\nallv\u2208[0,1]and\u03ba\u2208K. Therefore, it suffices to show that with probability at least1 \u2212\u03b4, the\nfollowing holds for anyv\u2208[0,1], anyv\u2032\u2208V, and any\u03ba\u2032\u2208K:\n/vextendsingle/vextendsingle/vextendsingleE\n(p,y)\u223cJ[(y\u2212v\u2032)(sign(\u03ba\u2032(p)\u22121/2)\u2212sign(p\u2212v))]\u2212E\n(p,y)\u223cS[(y\u2212v\u2032)(sign(\u03ba\u2032(p)\u22121/2)\u2212sign(p\u2212v))]/vextendsingle/vextendsingle/vextendsingle\u2264\u03b5\n4.\nSince the size of VisO(1/\u03b5), it suffices to prove that the above bound holds for each individual\nv\u2032\u2208V(but uniformly over v\u2208 [0,1]and\u03ba\u2208 K) with probability1 \u2212O(\u03b5\u03b4). The desired\nresult would then follow by a union bound. For each v\u2032\u2208V, the desired bound is true due to\nstandard uniform convergence arguments, combined with the fact that the VC dimension of the class\n{p\u221d\u21d5\u22a3\u221a\u222b\u2294\u2240\u2192sign (\u03ba(p)\u22121/2) :\u03ba\u2208K}isd, and the VC dimension of the class {p\u221d\u21d5\u22a3\u221a\u222b\u2294\u2240\u2192sign (p\u2212v ) :v\u2208[0,1]}\nis1. In particular, we may combine the following results from [ MRT18]: Corollary 3.8 (which\ngives a bound on the Rademacher complexity of a binary class in terms of the associated growth\nfunction), Theorem 3.17 (Sauer\u2019s Lemma, which bounds the growth function in terms of the VC\ndimension), and Theorem 11.3 (which gives a generalization bound for regression with respect to\nbounded and Lipschitz losses, in terms of the Rademacher complexity of the underlying function\nclass). The choice of O(\u03b5\u03b4)for the failure probability only incurs an additive term of O(log(1/\u03b5)),\nand, therefore, our choice for the number of samplesmsuffices to achieve the desired result.\nProof of Theorem 4.1 Upper Bound. LetA1be the algorithm that receives a set Sofmi.i.d.\nexamples from some unknown distribution Jover[0,1]\u00d7{0,1}, wherem\u2265C\u00b7dlog (1/\u03b5)/\u03b52, for\nsome sufficiently large universal constantC\u22651, and does the following:\n43\n1. Compute the following quantity:\nCDLK(S) = sup\n\u03ba\u2208K,\u2113\u2208L\u22171\nm/summationdisplay\n(p,y)\u2208S(\u2113(p,y)\u2212\u2113(\u03ba(p),y))\n2. IfCDLK(S)\u2264\u03b1\u2212\u03f5/2, then outputAccept. Otherwise, outputReject.\nBy Lemma 4.2, with probability at least2 /3, we have|CDLK(J)\u2212CDLK(S)|\u2264\u03b5/ 2. Under this\nevent, if CDLK(J)\u2264\u03b1\u2212\u03b5, then algorithm A1will accept. On the other hand, if CDLK(J)> \u03b1,\nthenA 1will reject.\nProof of Corollary 4.7. The proof of the upper bound is essentially the same as in Theorem 4.1. The\nkey ingredient of that proof was the uniform convergence bound stated in Lemma 4.2, which holds\nfor a supremum over all \u2113\u2208L\u2217. In particular, it continues to hold if we only take the supremum\nover\u2113\u2208L\u2217\n\u00b5\u2212sc, so the proof still goes through. For the lower bound, we can no longer rely on our\nargument based on the \u2113+\n1/2loss, since its associated function \u03c61/2(p) =\u2212|p\u2212 1/2|is not strongly\nconcave. To circumvent this issue, consider the following version of the squared loss:\n\u2113sq(p,y) = (y\u2212p)2.\nThen\u2202\u2113sq(p) = 1\u22122p\u2208[\u22121,+1], so the function \u2113sqindeed belongs to the class L\u2217. Moreover,\n\u03c6sq(p) =\u2113sq(p,p) =p(1\u2212p)is2-strongly concave, so \u2113sq\u2208L\u2217\n2\u2212sc. Next, given any \u2113\u2208L\u2217and\u00b5>0,\nwe define the following convex combination, which belongs toL\u2217\n\u00b5\u2212sc:\n\u2113\u00b5=\u00b5\n2\u2113sq+/parenleft\uf8ecig\n1\u2212\u00b5\n2/parenright\uf8ecig\n\u2113.\nWe will use \u2113\u00b5to study the effect of restricting to \u00b5-strongly convex proper losses. First, since we\nare considering a restricted class of loss functions, we clearly have CDLL\u2217\n\u00b5\u2212sc,K\u2264CDLK. Conversely,\nlet\u03babe a post-processing function that improves\u2113by at least\u03b1, meaning that\nE/bracketleftbig\u2113sq(\u03ba(p),y)/bracketrightbig\u2264E/bracketleftbig\u2113sq(p,y)/bracketrightbig\u2212\u03b1.\nSince the convex combination \u2113\u00b5puts1\u2212\u00b5/ 2weight on \u2113, the post-processing \u03bamust improve the\nloss on this part of \u2113\u00b5by at least(1\u2212\u00b5/ 2)\u03b1. Although \u03bamay worsen the loss on \u2113sqarbitrarily, the\nconvex combination \u2113\u00b5puts only\u00b5/2weight on \u2113sq, so it must worsen the loss on this part of \u2113\u00b5by\nat most\u00b52\u00b72(recall that all losses in L\u2217, such as\u2113sqhave range bounded in an interval of length2,\nby Lemma 3.3). In total,\u03bamust improve the loss on\u2113 \u00b5by at least\n/parenleft\uf8ecig\n1\u2212\u00b5\n2/parenright\uf8ecig\n\u03b1\u2212\u00b5\u2265\u03b1\u22122\u00b5.\nIt follows immediately that an( \u03b1,\u03b2)-auditor for CDLKis implied by an( \u03b1\u22122\u00b5,\u03b2)-auditor for\nCDLL\u2217\n\u00b5\u2212sc,K. Thus, setting \u03b1= 1/8and\u03b2= 0, our lower bound for auditing carries over to the case\nofL\u2217\n\u00b5-sc, as claimed.\nProof of Theorem 4.9.We will prove the upper and lower bounds separately.\nUpper Bound.From the loss OI lemma (Lemma 5.4), we have that \u2113(p,y)\u2212\u2113(\u03ba(p),y)\u2264\n(\u2202\u2113(\u03ba(p))\u2212\u2202\u2113(p))(y\u2212p ). The function w\u2032(p) =\u2202\u2113(\u03ba(p))\u2212\u2202\u2113(p)is6-Lipschitz, because \u03bais\n2-Lipschitz and\u2202\u2113is2-Lipschitz. Therefore, we have\nsmCDL(J)\u2264sup\nw\u2032: 6-LipschitzE/bracketleft\uf8ecig\nw\u2032(p)(y\u2212p)/bracketright\uf8ecig\n\u22646\u00b7smCE(J)\n44\nLower Bound.Suppose that there is a1-Lipschitz functionw: [0,1]\u2192[\u22121,1]such that\nsmCE(J) =E\n(p,y)\u223cJ[(y\u2212p)\u00b7w(p)] =\u03b1.\nThen, the post-processing function\u03ba(p) = [p+\u03b1w(p)]1\n0is2-Lipschitz and satisfies:\nE\n(p,y)\u223cJ/bracketleft\uf8ecig\n(y\u2212\u03ba(p))2/bracketright\uf8ecig\n\u2264E\n(p,y)\u223cJ/bracketleft\uf8ecig\n(y\u2212p)2/bracketright\uf8ecig\n\u2212\u03b12\nThe squared loss \u2113sq(p,y) = (p\u2212y )2/2is1-Lipschitz and proper. Therefore, smCDL (J)\u2265\u03b12/2.\nCTightness of the Weight-Restricted Calibration Characterization\nIn this section, we show that the quadratic gap in Theorem 5.2 is essentially tight, using the example\nof the class of monotonically nondecreasing post-processingsK=M +.\nTheorem C.1.There exist distributionsJ 1,J2over pairs(p,y)\u2208[0,1]\u00d7{0,1}such that\nCDLM+(J1)\u2273CE Int(J1)\nand\nCDLM+(J2)\u2272CE Int(J2)2.\nProof.Our examples are essentially the same ones used by [ HW24] to establish the tightness of\ntheir relationship betweenCDL K\u2217andECE, which also has a quadratic gap.\nForJ 1, suppose thatp= 1\u2212\u03b5andy= 1deterministically. Then,\nCEInt(J1)\u2264ECE(J 1) =\u03b5,\nbut if we setv= 1\u2212\u03b5/2and consider the monotonic post-processing\u03ba(p) =p+\u03b5, then\nCDLM+(J1)\u2265E[\u2113+\nv(p,y)]\u2212E[\u2113+\nv(p+\u03b5,y)] =\u03b5\n2\u2212/parenleft\uf8ecig\n\u2212\u03b5\n2/parenright\uf8ecig\n=\u03b5.\nTherefore,CDL M+(J1)\u2273CE Int(J1).\nForJ 2, consider a uniformp\u223c[0,1\u2212\u03b5], and suppose thaty|p\u223cBer(p+\u03b5). Then,\nCEInt(J2)\u2265E[y\u2212p] =\u03b5,\nbut the characterization ofCDLin Corollary 3.6 implies\nCDLM+(J2) = sup\nv\u2208[0,1],\n\u03ba\u2208M +E[\u2113+\nv(p,y)]\u2212E[\u2113+\nv(p,y)]\n= sup\nv\u2208[0,1]E[\u2113+\nv(p,y)]\u2212E[\u2113+\nv(p+\u03b5,y)]\n= sup\nv\u2208[0,1]E[(sign+(p+\u03b5\u2212v)\u2212sign+(p\u2212v))(p+\u03b5\u2212v)]\n= sup\nv\u2208[0,1]2\u00b7E[1[v\u2212\u03b5\u2264p<v](p+\u03b5\u2212v)]\n\u2264sup\nv\u2208[0,1]2\u00b7E[1[v\u2212\u03b5\u2264p<v](v+\u03b5\u2212v)]\n= 2\u00b7\u03b5\n1\u2212\u03b5\u00b7\u03b5\n\u2272\u03b52.\nThus,CDLM+(J2)\u2272CE Int(J2)2.\n45\nD Properties of Generalized Monotone Post-Processings\nWe provide here some classical results related to generalized monotone post-processings, which, in\nparticular, imply Corollary 6.2. We begin with the following result on their VC dimension.\nProposition D.1.For anyr\u2208N,VCdim(thr(M r)) = 2r.\nProof.Consider the pointsp i=i\n2rwherei\u2208[2r]. We will show the following:\n{v\u2208{\u00b11}2r:v= (sign+(\u03ba(pi)\u22121/2)) i\u2208[2r]for some\u03ba\u2208M r}={\u00b11}2r\nIn particular, for anyv \u2208{\u00b1 1}2r, we form the intervals I1,I2,...,Iras follows. Let I1be an interval\nof the form[ pi,pj], wherei\u2264jandv(i) =v(i+1) =\u00b7\u00b7\u00b7=v(j) = 1andv( i\u2032) =\u22121for alli\u2032<iand\ni\u2032=j+ 1. We then proceed recursively to form I2,...,Irafter removing the points p1,...,pj,pj+1.\nNote that for each interval we form, we remove at least2points. One for pjand one for pj+1. We\nlet\u03ba v(p) =1[p\u2208\u222a i\u2208[r]Ii], where we have\u03ba v\u2208Mrand(sign+(\u03ba(pi)\u22121/2)) i\u2208[2r] =v.\nOn the other hand, for any set of2 r+ 1distinct points on[0 ,1], no function in Mrcan\ngenerate the labelingv\u2032\u2208{\u00b1 1}2r+1, wherev\u2032(2i\u22121) = 1andv\u2032(2i) =\u22121, fori= 1,2,...,r,\nandv\u2032(2r+ 1) = 1, because the set {p:\u03ba(p)\u22651/2}would then have at least r+ 1disjoint\ncomponents.\nOur testing result in Corollary 6.2, which pertains to the specific class Mr, is achieved by\ninstantiating our more general Theorem 6.1 with the following standard agnostic learner for unions\nofrintervals, based on dynamic programming (see Section 4.2 in [ KSS94]). Indeed, the tester of\nTheorem 6.1 makes O(log(1/\u03b5\u03b4)/\u03b5)non-adaptive calls to the agnostic learner, which has sample\ncomplexity O((r+log1/\u03b4)/\u03b52)and runtime \u02dcO(r2+rlog 1/\u03b4)/\u03b52). Therefore, a union bound over the\nfailure probability of each call implies a tester with total sample complexity \u02dcO(r/\u03b52)and runtime\n\u02dcO(r2/\u03b53). For completeness, we describe and analyze the proper agnostic learner here.\nTheorem D.2([ KSS94]).Letr\u22651andC=thr(Mr). For any\u03b5,\u03b4\u2208 (0,1), there is a proper agnos-\ntic(\u03b5,\u03b4)-learner forCwith sample complexity O((r+ log 1/\u03b4 )/\u03b52)and runtime \u02dcO((r2+rlog 1/\u03b4)/\u03b52).\nProof of Theorem D.2. We will show that there is an algorithm ERM (C)that takes as input a set S\nof labeled examples of the form( p,z)wherep\u2208[0,1]andz\u2208{\u00b1 1}, runs in time O(|S|r+|S|log|S| ),\nand outputs someh\u2208Csuch that the following holds:\nP\n(p,z)\u223cS[h(p)\u0338=z]\u2264min\nf\u2208CP\n(p,z)\u223cS[f(p)\u0338=z]\nIn Algorithm 3 we present a version of ERM (C)that does not return h, but only returns an estimate of\nits error. The algorithm ERM (C)can be implemented based on Algorithm 3 by using some additional\nspace. The agnostic learning result then follows by standard uniform convergence arguments, due to\nthe fact that VCdim (C) = 2r(Proposition D.1), as long as m:=|S|\u2265C (r+log(1/\u03b4))/\u03b52for some\nsufficiently large constantC\u22651.\nWe first observe that1[ z\u0338=f(p)] = ( 1\u2212z\u00b7f(p) )/2for anyf\u2208C,z\u2208{\u00b1 1}. Moreover, any\nf\u2208Ccan be expressed as follows forrdisjoint intervalsI 1,I2,...,Ir\u2286[0,1]:\nf(p) = 2r/summationdisplay\ni=11[p\u2208Ii]\u22121\n46\nAlgorithm 3:EstimateMinimumRisk(S,r)\nInput:SetSofmpairs of the form(p,z)wherep\u2208[0,1],z\u2208{\u00b11}andr\u22651.\nOutput:A valueR\u2208[0,1].\n/* Sorting the Samples, in time:O(mlog(m))*/\n1LetO= ((p 1,z1),(p 2,z2),...,(pm,zm))be an ordering such thatp 1\u2264p2\u2264\u00b7\u00b7\u00b7\u2264p m;\n2If there are any duplicates pi=pi+1=\u00b7\u00b7\u00b7=pi+k, then merge them and set z\u2032=/summationtextk\nj=0zi+j,\nso that we obtainO\u2032= ((p\u2032\n1,z\u2032\n1),(p\u2032\n2,z\u2032\n2),...,(p\u2032\nm\u2032,z\u2032\nm\u2032))withp\u2032\n1<p\u2032\n2<\u00b7\u00b7\u00b7<p\u2032\nm\u2032;\n/* Initializations, in time:O(m)*/\n3SetZ(i) =/summationtext\nj\u2264iz\u2032\nj,Z(0) = 0;\n4SetQ(0,t) =Q(s,1) = 0for allt\u2208[m\u2032+ 1]ands\u2208[r];\n5SetM(0,j) = max 1\u2264i\u2264j{\u2212Z(i\u22121)}for allj\u2208[m\u2032+ 1]andM(s,1) = 0for alls\u2208[r];\n6SetB(0,t) = max 1\u2264j<t{Z(j) +M(0,j)}for allt\u2208[m\u2032+ 1]andB(s,1) = 0for alls\u2208[r];\n/* Dynamic Programming, in time:O(mr)*/\n7fors= 1,2,...,rdo\n8fort= 2,3,...,m\u2032+ 1do\n9Q(s,t) = max{Q(s\u22121,t),B(s\u22121,t)};\n10forj= 2,3,...,m\u2032+ 1do\n11M(s,j) = max{M(s,j\u22121),Q(s\u22121,j)\u2212Z(j\u22121)};\n12B(s,j) = max{B(s,j\u22121),Z(j)\u2212M(s,j)};\n13LetR=Q(r,m\u2032+ 1)/m;\nWe therefore have the following:\nmin\nf\u2208CP\n(p,z)\u223cS[f(p)\u0338=z] =1\n2\u22121\n2max\nf\u2208CE\n(p,z)\u223cS[zf(p)]\n=1\n2+1\n2E\n(p,z)\u223cS[z]\u22121\n2max\nI1,...,Irr/summationdisplay\ni=1E\n(p,z)\u223cS[z1[p\u2208I i])]\nIt suffices to find the endpoints of the intervals(I i)ithat maximize the following quantity:\nQ:= max\nI1,...,Irr/summationdisplay\ni=1/summationdisplay\n(p,z)\u2208Sz1[p\u2208Ii]\nLetS\u2032be the set of pairs of the form( p,z\u2032), where each p\u2208[0,1]appears in Sat least once and z\u2032\nis the sum of all zsuch that(p,z)\u2208S. Eachp\u2208[0,1]appears in S\u2032at most once. Let Pbe the set\nofp\u2208[0,1]appearing inS\u2032. Then, we can writeQas follows:\nQ= max\nk\u2208[r]max\nai,bi\u2208P\nai\u2264bi<ai+1k/summationdisplay\ni=1/summationdisplay\n(p,z\u2032)\u2208S\u2032z\u20321[p\u2208[ai,bi]].\nLetP(q) =P\u2229[0,q)and letQ(k,q)be defined as follows:\nQ(k,q) := max\nk\u2032\u2264kmax\nai,bi\u2208P(q)\nai\u2264bi<ai+1k\u2032/summationdisplay\ni=1/summationdisplay\n(p,z\u2032)\u2208S\u2032z\u20321[p\u2208[ai,bi]].\n47\nThen,Q(k,q)satisfies the following recurrence for allk\u2208[r]andq\u2208P\u222a{\u221e}:\nQ(k,q) = max/braceleft\uf8ecigg\nQ(k\u22121,q),max\na,b\u2208P(q)\na\u2264b/braceleft\uf8ecigg\nQ(k\u22121,a) +/summationdisplay\n(p,z\u2032)\u2208S\u2032z\u20321[p\u2208[a,b]]/braceright\uf8ecigg/braceright\uf8ecigg\n,\nas long as the initial conditions are the following for allq\u2208P\u222a{\u221e}andk\u2208[r]:\nQ(0,q) =Q(k,p min) = 0,wherep min= min\np\u2208Pp\nOur goal is to estimate the quantity Q=Q(r,\u221e), and retrieve the points( ai,bi)ithat achieve the\nmaximum. Due to the structure of Q(k,q), the estimation of Qcan be achieved in time poly(r,|P| )\nvia dynamic programming. For the indices( ai,bi)i, we associate each pair( k,q)with a set of\nintervalsI(k,q)of the form[a,b], so that:\nI(k,q) =/braceleft\uf8ecigg\nI(k\u22121,q),ifQ(k,q) =Q(k\u22121,q)\nI(k\u22121,a)\u222a{[a,b]},ifQ(k,q) =Q(k\u22121,a) +/summationtext\n(p,z\u2032)\u2208S\u2032z\u20321[p\u2208[a,b]]\nIn order to obtain an improved time complexity, we may use some additional memory to store\nintermediate auxiliary quantities as described in Algorithm 3. In particular, we define the following\nquantities for alla\u2208P:\nZ(a) =/summationdisplay\n(p,z\u2032)\u2208S\u2032z\u20321[p\u2264a],andprev(a) = max\na\u2032\u2208P(a)a\u2032\nand we equivalently formulateQ(k,q)as follows:\nQ(k,q) = max/braceleft\uf8ecigg\nQ(k\u22121,q),max\na,b\u2208P(q)\na\u2264b/braceleft\uf8ecigg\nQ(k\u22121,a) +Z(b)\u2212Z(prev(a))/braceright\uf8ecigg/braceright\uf8ecigg\n= max/braceleft\uf8ecigg\nQ(k\u22121,q),max\nb\u2208P(q)/braceleft\uf8ecigg\nZ(b) + max\na\u2264b/braceleft\uf8ecigg\nQ(k\u22121,a)\u2212Z(prev(a))/braceright\uf8ecigg/braceright\uf8ecigg/braceright\uf8ecigg\nwhere we may define the following quantities:\nM(k\u22121,b) = max\na\u2264b/braceleft\uf8ecigg\nQ(k\u22121,a)\u2212Z(prev(a))/braceright\uf8ecigg\nB(k\u22121,q) = max\nb\u2208P(q)/braceleft\uf8ecigg\nZ(b) +M(k\u22121,b)/braceright\uf8ecigg\nDue to the definitions of the auxiliary quantities, they satisfy the following recurrence relations.\nQ(k,q) = max{Q(k\u22121,q),B(k\u22121,q)}\nB(k,q) = max{B(k,prev(q)),Z(q) +M(k,q)}\nM(k,q) = max{M(k,prev(q)),Q(k\u22121,q)\u2212Z(prev(q))}\nEach step of the above recurrence relations can be computed in O(1), which implies the desired\nbound on the runtime.\n48\n",
    "title": "Efficient Calibration for Decision Making",
    "authors": [
      "Parikshit Gopalan",
      "Konstantinos Stavropoulos",
      "Kunal Talwar",
      "Pranay Tankala"
    ],
    "published": "2025-11-17",
    "arxiv_id": "2511.13699v1",
    "num_pages": 50,
    "num_chars": 135366
  },
  {
    "text": "Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and\nImage Generation\nSofia Jamil1, Kotla Sai Charan1, Sriparna Saha1, Koustava Goswami2, Joseph K J2\n1Department of Computer Science and Engineering, Indian Institute of Technology Patna, India\n2Adobe Research\nAbstract\nIndian poetry, known for its linguistic complexity and deep\ncultural resonance, has a rich and varied heritage spanning\nthousands of years. However, its layered meanings, cultural\nallusions, and sophisticated grammatical constructions often\npose challenges for comprehension, especially for non-native\nspeakers or readers unfamiliar with its context and language.\nDespite its cultural significance, existing works on poetry\nhavelargelyoverlookedIndianlanguagepoems.Inthispaper,\nwe propose theTranslation and Image Generation (TAI)\nframework, leveraging Large Language Models (LLMs) and\nLatent Diffusion Models through appropriate prompt tuning.\nOur framework supports the United Nations Sustainable De-\nvelopmentGoalsofQualityEducation(SDG4)andReduced\nInequalities (SDG 10) by enhancing the accessibility of cul-\nturally rich Indian-language poetry to a global audience. It\nincludes (1) a translation module that uses an Odds Ratio\nPreference Alignment Algorithm to accurately translate mor-\nphologically rich poetry into English; (2) an image genera-\ntionmodulethatemploysasemanticgraphtocapturetokens,\ndependencies,andsemanticrelationshipsbetweenmetaphors\nandtheirmeanings,tocreatevisuallymeaningfulrepresenta-\ntionsofIndianpoems.Ourcomprehensiveexperimentaleval-\nuation, including both human and quantitative assessments,\ndemonstrates the superiority ofTAIDiffusion in poem im-\nage generation tasks, outperforming strong baselines. To fur-\ntheraddressthescarcityofresourcesforIndian-languagepo-\netry, we introduce theMorphologically Rich Indian Lan-\nguage PoemsMorphoVerseDataset, comprising 1,570 po-\nems across 21 low-resource Indian languages. By addressing\nthe gap in poetry translation and visual comprehension, this\nworkaimstobroadenaccessibilityandenrichthereader\u2019sex-\nperience.\nCode\u2014 https://github.com/SofeeyaJ/Crossing-Borders---\nAAAI26\nIntroduction\nIndianpoetryandcultureisarichanddiverseformoflitera-\nture that includes several languages and countless dialects,\neach with its own distinct style, rhythm, and cultural sig-\nnificance. FromKalidasa\u2019sclassicalSanskritverses to the\nlyrical compositions ofTamil Sangampoetry,Urdughazals\nCopyright\u00a92026, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.toBengaliPoems on Social Causes, Indian poetry reflects\na diverse range of emotions, philosophies, and artistic ex-\npressions. The morphological richness of Indian languages,\nwith their complex word structures and intricate grammat-\nical rules, makes poetry deeply expressive but challenging\nto comprehend across languages (Maji et al. 2025a,b). They\nsometimes create a barrier to perception, making it diffi-\ncult for modern readers to fully appreciate and recognize\nthe works of great poets, often leading to unintended ne-\nglect of these literary treasures. This necessitates the need\nfor a generalized framework capable of interpreting poetry\nin a way that is accessible to a general reader unfamiliar\nwith Indian languages. Motivated by this, and in alignment\nwith the United Nations Sustainable Development Goals of\nQuality Education (SDG 4) and Reduced Inequalities (SDG\n10),weproposeTranslationandImageGeneration(TAI),\na two-step framework that translates morphologically rich,\nlow-resource Indian poetry and generates images that visu-\nally convey their meanings.\nFigure1:AnexampleofourTranslationandImageGener-\nationTAIframework, which first translates a Punjabi poem\n(alow-resourceIndianlanguage)intoEnglishandthengen-\nerates a visual representation, capturing the poem\u2019s key vi-\nsual elements (A field with farmers dressed in traditional\nPunjabi attire).\nTo date, previous research has explored machine transla-\ntion for poetry in an excellent way, but most studies have\nfocused on structured poetic forms with clear format or\nrhyme constraints (Ghazvininejad, Choi, and Knight 2018;\nChakrabarty,Saakyan,andMuresan2021;Songetal.2023),\nwhichdiffersignificantlyfromthemorphologicallyrichandarXiv:2511.13689v1  [cs.CL]  17 Nov 2025\nstructurally diverse nature of Indic poetry. A comparative\nstudyonmachinevs.humantranslationofArabicpoetryinto\nEnglish ((Alowedi and Al-Ahdal 2023)) concluded that ma-\nchinetranslationstrugglestocapturethesocio-culturalcon-\ntext and nuanced poetic elements, making it unsuitable for\nliterary translation.\nOur research question is:How can we interpret Indian\npoetry without losing its poetic meanings embedded in the\nverses?Withtheintroductionoftransformer-basedarchitec-\nturesandLargeLanguageModels(LLMs),theyhaveexhib-\nitedremarkablecapabilitiesinNaturalLanguageProcessing\n(NLP)tasksinvolvingtranslationsandbeyond(Naveedetal.\n2023). However, while LLMs have demonstrated improved\nperformanceinEnglishtranslation,theireffectivenessinIn-\ndic languages remains limited (Bhat, Varma, and Pedanekar\n2023), and their performance in poetry translation is often\ninadequate. Consequently, in our research, we incorporated\nanalignmentalgorithmtoguideLLMsingeneratingtransla-\ntionsthataremorealignedwithhuman-writtenpoetictrans-\nlationsandprovideanaccuraterepresentationofthepoem\u2019s\nmeaning.\nRecentadvancementsinthemodelingcapabilitiesoflarge\nscale models contributed to significant changes in text-to-\nimage synthesis (TIS) (Jamil et al. 2025a). However, the\nqualityofimagesgeneratedbythesemodelsheavilyrelieson\nproperly crafted, keyword-based text prompts (Zhong et al.\n2023;Jamiletal.2025b).Thedependencearisesfromtrain-\ning data limitations, which require detailed prompts to pro-\nduce high-quality visuals (Betker et al. 2023). Building on\nthis idea, we have integrated semantic graph knowledge for\nthe prompt formulation, which serves as input to the im-\nage generation framework. By mapping tokens, dependen-\ncies, and metaphorical relationships, semantic graphs facil-\nitate the creation of well-structured prompts that generate\ncontextually rich and meaningful images, ensuring that the\ngenerated visuals capture the main idea of the poem rather\nthanjustitsliteralwords.Figure1illustratestherepresenta-\ntion of poems generated by ourTAItwo-step framework. In\nsummary, the key contributions of this study are as follows:\n1. An Indian poem translation module incorporating the\nOdds Ratio Preference Optimization algorithm to pro-\nduce translations that closely align with human-written\ntranslations.\n2. Asemanticgraph-basedapproachforimagepromptcon-\nstruction that captures the themes, metaphorical mean-\nings, and visual elements present in poetry.\n3. A well-curatedMorphologically Rich Indian Language\nPoems(MorphoVerse)datasetwith1,570poemsin21di-\nverse Indian languages.\n4. Culturally informed human evaluations, where experts\nfrom diverse linguistic backgrounds confirmed that our\nframeworkgeneratesimagesthatbestrepresentpoemsin\nterms of meaning, cultural themes, and visual elements.\nBackground and Related Works\nMultiLingual Poetry Translation:Early research in po-\netrymachinetranslationfocusedonphrase-basedtranslationsystems, with initial efforts aimed at translating French po-\netryintometricalEnglishverse(Genzel,Uszkoreit,andOch\n2010).Later,(Greene,Bodrumlu,andKnight2010)applied\nstatistical methods to translate rhymed poetry, successfully\nconverting Italian poetry into English. (Dubey 2019) ex-\nploredautomaticsensedisambiguationfortranslatingHindi\npoetryintoDogri,leveragingthen-gramapproachforaccu-\nrate word mapping. (Nair, Krishnan, and Deetha 2016) de-\nveloped a rule-based machine translation system that used\ndeclension rules to convert English sentences into Hindi,\nwithgrammaticalstructureverificationthroughtheStanford\nPOStagger.Morerecently,(Chakrawarti,Bansal,andBansal\n2022) introduced a Hybrid Machine Translation (HBMT)\nmodelforconvertingHindipoetryintoEnglish,significantly\nimprovingsemanticandsyntacticaccuracyinpoetrytransla-\ntion.Whilepreviousresearchhasmademajorimprovements\nin multilingual poetry translation, limited work has been\ndone on low-resource poetry in various Indian languages.\nTonarrowthisgap,wefocusontranslatinglow-resourceIn-\ndian poetry into English from a curated poetry dataset that\nincludes poems from 21 different Indian languages.\nImage Generation via Diffusion Models:Recent ad-\nvancements in diffusion models (Sohl-Dickstein et al. 2015;\nDhariwal and Nichol 2021; Song et al. 2020) have signifi-\ncantlyenhancedthequalityoftext-to-imagegeneration,with\nmodelssuchasDreambooth(Ruizetal.2023)andDALLE3\n(Betker et al. 2023) achieving impressive results. Addition-\nally,severalrecentstudies(Xuetal.2024;Jiangetal.2023b;\nHuangetal.2023;Jamiletal.2025c,b)haveexploredimage\nunderstanding feedback, leveraging reward-based models to\nrefine diffusion techniques for better text-image alignment.\nDespite these advancements, current models still struggle\nwith complex prompts, often generating images that lack\ncore elements, leading to semantic confusion (Feng et al.\n2022;Lianetal.2023;Bar-Taletal.2023).Toaddressthese\nissues, we incorporated a semantic graph for prompt con-\nstructiontocapturethesemanticmeaningsofthepoemtext.\nMorphoVerseDataset\nIntheexistingliterature,datasetsavailableforpoetrygener-\nationinIndianlanguagessuchasHindiPoems(Shah2024),\nSukhan (Aggarwal, Ghosh, and Mamidi 2020) and Devan-\ngriPoemdataset(Acharyaetal.2020)arepredominantlyin\nHindi, with no author translations. To address this gap and\nachieve our research objectives, we curated theMorpholog-\nicallyRichIndianLanguagePoems(MorphoVerse)Dataset,\ncomprising1,570poemsin21diverseIndianlanguages,and\nprovided comparative statistics in Table 1. We outline the\nsteps below taken to construct our dataset.\n1.DataCollection:WecompiledtheMorphoVersedataset\nfrom various open online sources, focusing specifically on\nIndian languages. To ensure the reliability and accuracy of\nthe data collection process, we selected a group of three\nEnglish-proficient final-year undergraduate students. These\nindividuals were chosen based on their technical expertise\nand ability to verify the authenticity of the poems sourced\nfromdifferentonlineplatforms.Forlow-resourcelanguages,\nwe conducted an extensive search for corresponding author\ntranslations across multiple online sources to enhance the\ndataset\u2019s quality and completeness.\n2.DataCleaning:Giventhediversesourcesofthecollected\npoems, we carried out extensive data cleaning to maintain\nconsistency. This process involved, removing duplicate en-\ntries,correctinginaccuracies,eliminatingextraneouswhites-\npaceandHTMLtags,andaddressingotherformattingerrors.\nAfter cleaning the data, we carefully reviewed the dataset,\nand resolved any remaining discrepancies using Cohen\u2019s\nKappa score to ensure consistency and reliability.\nDatasetCount Languages Translations\nHindi_Poems2500 Hindi%\nSUKHAN845 Hindi%\nDevanagri Poem dataset1500 Devanagri%\nMorphoVerse (OURS)1570 21!\nTable 1: Statistics of our Proposed DatasetMorphoVersein\nComparison to other Hindi Poems Dataset\nMethodology\nProblem Statement\nWeformalizethepoem-to-imagegenerationtaskasatextto\nimagesynthesistask,wherealowresourceIndianpoem\ud835\udc43is\ntransformed into a generated image\ud835\udc3c. This process consists\nof three components: (1) A translation module that converts\n\ud835\udc43intoanEnglishlanguagePoemwhilepreservingthemor-\nphologicalessenceofthepoems(2)Asemanticgraph,which\ncaptures key tokens, dependencies, and metaphorical rela-\ntionships between poem texts and (3) Creating an appropri-\nate image prompt for generating an image by incorporating\nboth linguistic and semantic knowledge. Our overall frame-\nwork is illustrated in Figure 2. This section provides a de-\ntailed explanation of the Translation Module, the Semantic\nGraph Construction utilized for Image prompt construction,\nand the Image Prompt Creation for image generation.\nTranslation Module\nPoetry translations must preserve the original text\u2019s poetic\nessence, rhythm, and stylistic nuances (Skerratt 2013). To\nachieve this, we employ Large Language Models (LLMs).\nHowever, prior research has shown that widely used models\nlike GPT-3.5 and GPT-4 often produce literal translations,\nfailingtocapturethepoeticstructureandmetaphoricaldepth\nof the source text (Wang et al. 2024). To improve the trans-\nlation process, we implement Odds Ratio Preference Opti-\nmization (ORPO) (Hong, Lee, and Thorne 2024) to refine\nthe translation process and ensure that the model prioritizes\npoeticallymeaningfuloutputsoverliteraltranslations.Asil-\nlustratedinFigure2(b),thealignmentalgorithmintroduces\nanoddsratio-basedpenaltytotheconventionalnegativelog-\nlikelihood loss, enabling the model to distinguish between\npreferredandunpreferredtranslationstyles.Theoddsofgen-\nerating a translated poem\ud835\udc66given an input poem\ud835\udc65under\nmodel\ud835\udf03is defined as:odds\ud835\udf03(\ud835\udc66\u2223\ud835\udc65) =\ud835\udc43\ud835\udf03(\ud835\udc66\u2223\ud835\udc65)\n1 \u2212\ud835\udc43\ud835\udf03(\ud835\udc66\u2223\ud835\udc65)(1)\nIf odds\ud835\udf03(\ud835\udc66\u2223\ud835\udc65) =\ud835\udc58, it means that the model\ud835\udf03is k times\nmore likely to generate the output poem\ud835\udc66than to not gen-\nerate it. To reinforce alignment, we introduce the odds ratio\n(OR) to compare the preferred response (\ud835\udc66\ud835\udc64) with the non-\npreferred response (\ud835\udc66\ud835\udc59).\nOR\ud835\udf03(\ud835\udc66\ud835\udc64,\ud835\udc66\ud835\udc59) =odds\ud835\udf03(\ud835\udc66\ud835\udc64\u2223\ud835\udc65)\nodds\ud835\udf03(\ud835\udc66\ud835\udc59\u2223\ud835\udc65)(2)\nTheORPOobjectivefunctioncombinesthestandardSuper-\nvised Fine-Tuning (SFT) loss with an additional term that\npenalizes the model if it fails to differentiate sufficiently\nbetween favored and disfavored responses as expressed in\nEquation 3. The hyperparameter\ud835\udf06regulates the intensity of\nthis penalty.\n\ud835\udc3fORPO =\ud835\udd3c(\ud835\udc65,\ud835\udc66\ud835\udc64,\ud835\udc66\ud835\udc59)[\ud835\udc3fSFT+\ud835\udf06\u22c5\ud835\udc3fOR].(3)\nThe penalty term\ud835\udc3fORis based on the log odds ratio, re-\nfined by a sigmoid function to smooth the gradient.\n\ud835\udc3fOR= \u2212 log\ud835\udf0e(\nlogodds\ud835\udf03(\ud835\udc66\ud835\udc64\u2223\ud835\udc65)\nodds\ud835\udf03(\ud835\udc66\ud835\udc59\u2223\ud835\udc65))\n(4)\nSemantic Graph Generation\nPoetryfrequentlyconveysabstractandhiddenmeaningsbe-\nyondtheliteralmeaningsofthewords.Itisessentialtocap-\nture these subtle details to ensure an accurate visual repre-\nsentation.Toapproachthisissue,weintroduceamodulede-\nsignedforthegenerationofsemanticgraphsgeneratedfrom\ntranslatedpoems,thussimplifyingtheextractionofconcep-\ntualrelationshipsandsyntacticstructures.Givenatranslated\npoem\ud835\udc47, we define a directed semantic graph\ud835\udc3a= (\ud835\udc49,\ud835\udc38),\nwhere\ud835\udc49representsthesetofnodesand\ud835\udc38representstheset\nof edges. Each node is defined as a pair,\ud835\udc63\ud835\udc56= (\ud835\udcc1\ud835\udc56,\ud835\udc60\ud835\udc56), where\n\ud835\udcc1\ud835\udc56denotesthelemma(baseform)oftoken\ud835\udc61\ud835\udc56and\ud835\udc60\ud835\udc56represents\nitswordnetidentifiertoresolvecontextualmeaning(synset).\nA node is added to the graph for each token:\n\ud835\udc49= {\ud835\udc63\ud835\udc56\u2223\ud835\udc63\ud835\udc56= (\ud835\udcc1\ud835\udc56,\ud835\udc60\ud835\udc56),\ud835\udc61\ud835\udc56\u2208\ud835\udc47}(5)\nEdges in the graph encode syntactic dependencies and\nhypernym relations. For dependency relations, an edge is\nformed between two nodes\ud835\udc63\ud835\udc56and\ud835\udc63\ud835\udc57if token\ud835\udc61\ud835\udc56depends on\ntoken\ud835\udc61\ud835\udc57,\ud835\udc38dep= {(\ud835\udc63\ud835\udc56,\ud835\udc63\ud835\udc57) \u2223\ud835\udc61\ud835\udc56depends on\ud835\udc61\ud835\udc57}and hypernym\nedgeslinkanode\ud835\udc63\ud835\udc56toanothernode\ud835\udc63\ud835\udc58ifthesynsetof\ud835\udc63\ud835\udc56hasa\nhypernym\ud835\udc60\ud835\udc58,\ud835\udc38hypernym = {(\ud835\udc63\ud835\udc56,\ud835\udc63\ud835\udc58) \u2223\ud835\udc60\ud835\udc56\u2208Hypernyms(\ud835\udc60\ud835\udc58)}.\nIn other words,\ud835\udc38depcaptures direct grammatical relation-\nships between words, while\ud835\udc38hypernymconnects words to\ntheir more general meanings. Our approach further imple-\nments a greedy modularity optimization (Al-Mukhtar and\nAl-Shamery2018)toidentifysemanticallycoherentclusters\nwithinthegraph.Let\ud835\udc3a\u2032= (\ud835\udc49\u2032,\ud835\udc38\u2032)betheundirectedversion\nof the graph\ud835\udc3a. Community detection is formulated as par-\ntitioning\ud835\udc49\u2032into clusters\ud835\udc36= {\ud835\udc361,\ud835\udc362,\u2026,\ud835\udc36\ud835\udc5a}, optimizing\nthe modularity\ud835\udc44:\nFigure 2: Framework for Translation and Image GenerationTAIFramework\n\ud835\udc44=1\n2\ud835\udc5a\u2211\n\ud835\udc56,\ud835\udc57(\n\ud835\udc34\ud835\udc56\ud835\udc57\u2212\ud835\udc58\ud835\udc56\ud835\udc58\ud835\udc57\n2\ud835\udc5a)\n\ud835\udeff(\ud835\udc50\ud835\udc56,\ud835\udc50\ud835\udc57)(6)\nwhere\ud835\udc34\ud835\udc56\ud835\udc57istheadjacencymatrixof\ud835\udc3a\u2032,\ud835\udc58\ud835\udc56and\ud835\udc58\ud835\udc57arethe\ndegrees of nodes\ud835\udc56and\ud835\udc57,\ud835\udc5arepresents the total number of\nedges, and\ud835\udeff(\ud835\udc50\ud835\udc56,\ud835\udc50\ud835\udc57)is 1 if nodes\ud835\udc56and\ud835\udc57belong to the same\ncommunity and 0 otherwise.\nImage Prompt Construction\nThe semantic graph constructed from poems is utilized as\ninputforaninstructiongenerationmodule,whichguidesthe\nimage creation process. This module employs GPT-4o mini\nforformulatingtext-basedpromptsandtheStable-Diffusion-\n3.5-Medium (Esser et al. 2024) for efficient, cost-effective\nimage generation. As illustrated in Figure 2 (c), the ini-\ntial prompt given to GPT is:\u201cIdentify the central theme or\nmetaphorfromthegraphrepresentingasemanticanalysisof\napoem\u2019stext,dividedintosmaller\"communities\"orclusters\nof related words, with their dependencies and relationships\nillustrated through nodes and edges. Integrate all key ele-\nmentsintoacohesivevisualscenethatrepresentsthepoem\u2019s\nessence. Ensure the generated prompt is coherent, detailed,\nand suitable for understanding of stable diffusion model. \u201d\nThis initial prompt guides the GPT in creating a detailed\nimageinstructionbasedonthesemanticcontentofthepoem.\nForexample,theImageInstructionPromptgeneratedforthe\npoem shown in Figure 2(a) is,\u201cA serene garden scene at\ndawn, featuring blooming roses surrounded by lush green-\nery. The air carries a fragrant bouquet, evoking memories\nof home. A gentle morning light casts a fresh glow over the\nlandscape,symbolizingtheessenceofnostalgiaandwarmth.\nInclude elements of freshness and calm, illustrating the\nbeautyandsimplicityofhomeinnature.\u201dThispromptisthen\nfedintotheimagegenerationmodeltoproducetheimage.To\nevaluatethequalityandauthenticityofthegeneratedimages,\nweincorporatedhumanfeedback.Wecollaboratedwithfour\ndomain experts from the Indian Poetry Society to evaluate\nthe generated images. We sampled 5% of poems from the\nMorphoVersedataset, with experts rating each image on ascale of 1 to 5. The average score was calculated after each\niteration,withhigherscoresindicatingbetteralignmentwith\nthe poem, and lower scores reflecting misalignment. Based\non the expert feedback, as shown in Figure 2(c), we itera-\ntivelyrefinedtheGPTprompts,improvingthegeneratedim-\nage instructions and producing more accurate and aligned\nimages. This iterative refinement process resulted in a grad-\nualimprovementinthegeneratedimagesandtheiralignment\nwith poems. After five rounds of feedback, the scores be-\ngan to decline in the sixth round. At this point, we finalized\nthe fifth prompt as\u201cFrom the graph representing a poem\u2019s\nsemantic analysis, extract key metaphors and thematic ele-\nments. Develop a detailed image generation prompt that in-\ncorporates all significant nodes and their relationships into\na unified visual scene. The instruction should be imagina-\ntive, coherent, and tailored for Stable Diffusion\u2019s interpre-\ntation of metaphorical imagery.\u201dBased on this prompt, we\nuse GPT to generate detailed image instruction\ud835\udc3c. The final\nimageisgeneratedbyaStableDiffusionModel,whichtakes\nthe generated instruction\ud835\udc3cas input. Thus, for each poem in\nthe dataset\ue230= {\ud835\udc431,\ud835\udc432,...,\ud835\udc43\ud835\udc5b}, we obtain a corresponding\nset of generated images\ue235= {\ud835\udc3c1,\ud835\udc3c2,...,\ud835\udc3c\ud835\udc5b}. Based on this\nprompt, we use GPT to generate detailed image instructions\n\ud835\udc3c,where\ud835\udc3cisastructuredpromptcontainingdescriptiveel-\nements extracted from semantic clusters in\ud835\udc3a. The final im-\nage is generated by a Stable Diffusion Model, which takes\nthe generated instruction\ud835\udc3cas input. Thus, for each poem in\nthe dataset\ue230= {\ud835\udc431,\ud835\udc432,...,\ud835\udc43\ud835\udc5b}, we obtain a corresponding\nset of generated images\ue235= {\ud835\udc3c1,\ud835\udc3c2,...,\ud835\udc3c\ud835\udc5b}.\nExperiments\nTraining SettingsWe fine-tuned the baseline models on\n1,570 samples from theMorphoVerseDataset, with a 70:30\nsplit between training and validation. Fine-tuning was per-\nformedusingLoRA(Huetal.2021)modelswithalearning\nrateof1e-04forthreeepochs,aweightdecayof1e-2,abatch\nsizeof32,and88,000trainingsteps.TheLoRArankwasset\nat 32, with alpha = 32. Additionally, Direct Preference Op-\ntimization (DPO) (Rafailov et al. 2024) was applied on the\nsupervised fine-tuned model with a learning rate of 1e-04,\nand ORPO (Hong, Lee, and Thorne 2024) was used with a\nlearningrateof5e-05forfourepochsonpre-trainedmodels.\nAll experiments were conducted using PyTorch on a single\nserver with NVIDIA RTX 100 GPUs.\nBaselinesFor the translation task, we selected base-\nline models trained with Indian languages, including\nMistral-7B-Instruct-v0.3 (Jiang et al. 2023a), Qwen 2.5\n(Team 2024b), gemma-2-9b-it (Team 2024a), Llama-3.1-\n8B-Instruct (AI@Meta 2024), and sarvam-11. For the text-\nto-image generation task, we employed pretrained diffusion\nmodels,includingStable-Diffusion-3.5-Medium(Esseretal.\n2024), Playground-V2.5-1024px-Aesthetic (Li et al. 2024),\nand Sana_1600M_1024px (Xie et al. 2024).\nEvaluation ProtocolsTo evaluate translation quality, we\nusedROUGE(Lin2004),BLEU(Papinenietal.2002),ME-\nTEOR (Banerjee and Lavie 2005), and COMET (Rei et al.\n2020) scores. To evaluate alignment between the generated\nimage, poem, and prompt, we used BLIP (Li et al. 2022)\nScore, where captions were generated for images and their\nsimilaritytotheoriginalimage-generationpromptwascom-\nputed.Additionally,Long-CLIP(Zhangetal.2024)wasused\nto measure the cosine similarity between the poem and the\ngenerated image. We also utilize the Image Rewards metric\n(Xuetal.2023)tofurtherassessthequalityofthegenerated\nimages. This metric evaluates human preferences in text-to-\nimage synthesis through extensive analysis and providing a\nquantitative score for the generated images.\nFigure 3: Comparative Qualitative Analysis of Translated\nPoemsUsingtheoddsratiopreferenceoptimization(ORPO)\nApproach Across Different LLMs.\nFigure 4: Comparative Qualitative Evaluation of Translated\nPoemsUsingDifferentApproacheswiththeGemmaModel.\nResults\nQualitative Evaluation\nStep 1: Translation Task\n1https://huggingface.co/sarvamai/sarvam-1We conducted qualitative comparisons of the translated\npoems, evaluating them based on three key criteria: struc-\ntural accuracy (whether the number of lines remained con-\nsistent), semantic accuracy (whether the meaning was pre-\nserved or altered), and syntactic accuracy (whether words\nwere replaced with synonyms or altered in a way that\nchanged the original meaning). As shown in Figure 3, we\nestablished a comparative analysis across multiple models,\nincluding Gemma, Mistral, Qwen, and LLama and sarvam,\nall integrated with the Odds Ratio Preference Optimization\n(ORPO) approach. Among these, the Gemma model with\nORPO demonstrated the highest structural, semantic, and\nsyntactic accuracy when compared to the original English\ntranslations. On the other hand, for performing qualitative\nanalysisofthetranslatedpoemswithouttheORPOapproach,\nwe show the results of the Gemma model in Figure 4. It is\nobserved that the poem generated by the Gemma pretrained\nmodel failed to retain structural, semantic, or syntactic ac-\ncuracy in the pretrained version. After applying the fine-\ntuning approach, structural accuracy improved, but seman-\nticand syntacticaccuracy showedno improvements.Onthe\notherhand,theDirectPreferenceOptimizationtechniqueim-\nproved both structural and semantic accuracy but failed to\nimprovesyntacticaccuracy.Alltheresultsarehighlightedin\nthe table for clearer understanding.\nStep 2: Image generation taskWe conducted a quali-\ntative analysis of the generated images based on three key\naspects:a) Meaning Capture:Since poetry is a complex\nart form, it is crucial that the generated images effectively\nconvey the meaning embedded in the text. As observed in\nFigure5,Poem3evokesnostalgia,serenity,andadeepcon-\nnection to home, reminiscing about peaceful mornings that\nbring comfort. The images generated using semantic graph\nknowledgesuccessfullydepictsahomeatdawn,surrounded\nby a rose garden, precisely aligning with the poet\u2019s intent of\nportraying home as a cherished memory. Similarly, inPoem\n2,whichconveysthemesofhopeandcompanionship,theim-\nagegeneratedthroughsemanticgraphknowledgeaccurately\nrepresentstheideaofmovingforwardtofacetheworldwith\nhopeandcompanionship.Asimilarportrayalofcelebrations\nis observed inPoem 1, demonstrating that the poetic mean-\ning is effectively captured.b) Visual Element Representa-\ntion:As shown in Figure 5, images generated using seman-\ntic knowledge successfully incorporate key visual elements,\nsuchastrees,themoon,andhumanfiguresrepresentingcom-\npanionshipinPoem2.ForPoem3,theimagesaccuratelyde-\npictahomeandarosegardeninthemorning.Allthevisual\nelements mentioned in the poem, essential for representing\ntheimage,areeffectivelycaptured.Similarly,forPoem1,we\nobservethatdoorsandcelebratorypropsareaccuratelypor-\ntrayed.c)CulturalRepresentation:Amongallapproaches,\nsemanticgraphknowledgedemonstratedthestrongestability\nto capture cultural elements, as seen inPoem 1of Figure 5.\nThis poem, written in Urdu, celebrates the culture ofSindh.\nThe generated image accurately portrays this by depicting a\nmandressedintraditionalSindhiattireengagedinacelebra-\ntorymoment.Thislevelofculturalrepresentationwasabsent\nin images produced through other approaches.\nFigure 5: Qualitative Evaluation of the Images Generated using constructed prompts with Semantic graph knowledge\nAutomated Evaluation\nStep 1: Poem TranslationFor the automatic evaluation of\npoem translation tasks, we tested four different approaches\nacrossallLLMs:a)zero-shottranslation,b)supervisedfine-\ntuning, and alignment techniques such as c) Direct Prefer-\nence Optimization (DPO) and d) Odd Ratio Preference Op-\ntimization (ORPO). As shown in Table 2, Gemma 2 consis-\ntentlyoutperformsallothermodelsacrossallevaluationmet-\nrics, both in zero-shot and optimized settings. Moreover, as\nobserved from Table 2, fine-tuning the model significantly\nenhancestranslationqualityforeachmodelwhencompared\nto the zero-shot setting, demonstrating the model\u2019s ability\nto effectively learn from additional training data. Further-\nmore, ORPO consistently improves translation performance\nacrossallmodels,resultinginsignificantincreasesinBLEU,\nMETEOR,andCOMETscores.Additionaly,wealsoevalu-\nated the SARVAM model, which is specifically optimized\nfor Indian languages. However, it underperforms compared\ntoothermodelsinbothzero-shotandfine-tunedsettings,in-\ndicating limitations in its translation capabilities for poetry.\nStep 2: Image GenerationOurTAIapproach consis-\ntentlyoutperformsotherapproachesacrossallbaselinesand\nscores, as demonstrated in Table 3. Given that the Long-\nCLIP score reflects semantic consistency between text and\nimage, it is evident that sending the constructed prompts\nwith semantic graph knowledge to Stable Diffusion yields\nthehighestscorefortheconstructedprompt(finalizedviahu-\nman feedback in Section Image Prompt Construction). Fur-\nthermore, for the Image Rewards metric, we evaluated the\nalignment between prompts used for image generation andthe resulting images using the reward model, where ourTAI\napproach demonstrated superior performance. Additionally,\nweassessedimagerewardsbetweenthetranslatedpoemand\nthegeneratedimage,findingthatthegeneratedimageclosely\nalignswiththepoems.OurTAIapproachisbroadlyapplica-\nbletoallSD-stylemodels.TheexperimentalresultsfromSD\n3.5 Medium and Playground v2.5 confirm the flexibility of\nourdesignedpromptstootherdiffusionbasedTexttoImage\nmodels.\nAblation Study\nTo assess the effectiveness of our proposed approach, we\nevaluatedthepoem-to-imagegenerationtaskunderdifferent\nsettings.\nAblation Study 1: Generating Images Using Direct\nPromptsInthisapproach,weprovidedthetranslatedpoem\nto GPT and prompted it to generate descriptive prompts es-\nsential for image generation. The images were generated\nusing the GPT-generated prompts. However, these images\nfailed to accurately capture the meaning and cultural diver-\nsity of the poems, as evident from Figure 5 forPoem 1. The\ngenerated image forPoem 1does not effectively capture the\nsindh culture. ForPoem 2, the generated image depictsa\nlonelymanwalkingthroughaforest,whichisdifferentfrom\nthepoem\u2019sintendedmeaning.Moreover,forPoem3,ahouse\nwith a rose garden is generated, but it lacks the freshness\nof the morning, which the poet intended to convey through\nthe lines:There\u2019s the freshness of home, Memories of home\nadorn the mind.\nAblation Study 2: Generating Images Directly from the\nModelsR1 R2 RL BLEU1 BLEU2 BLEU3 BLEU4 METEOR COMET\nQwenZero Shot0.453 0.2225 0.397 0.3062 0.1988 0.1311 0.0828 0.3639 -0.1964\nFine Tuned0.5951 0.3169 0.5264 0.4509 0.3196 0.234 0.1674 0.4561 0.2223\nDPO0.5975 0.3253 0.5302 0.4564 0.3287 0.2449 0.1776 0.4621 0.2327\nORPO0.6163 0.3584 0.5548 0.4831 0.3626 0.2767 0.2099 0.4866 0.2564\nLlama 3.1Zero Shot0.496 0.242 0.4285 0.3429 0.2257 0.1519 0.0971 0.3601 -0.0842\nFine Tuned0.6356 0.3735 0.5712 0.4918 0.3681 0.2855 0.2189 0.4948 0.3141\nDPO0.6363 0.3803 0.5731 0.4421 0.3241 0.2441 0.1784 0.4535 0.2946\nORPO0.6421 0.3834 0.5791 0.5038 0.3802 0.296 0.2277 0.5094 0.3168\nGemma 2Zero Shot0.6701 0.4106 0.6101 0.5193 0.3976 0.3109 0.2389 0.5344 0.3579\nFine Tuned0.676 0.4229 0.6165 0.5353 0.4148 0.3289 0.2576 0.5452 0.3687\nDPO0.6677 0.4106 0.606 0.5227 0.4018 0.3161 0.2458 0.5343 0.3515\nORPO0.6922 0.4451 0.634 0.5589 0.4421 0.3586 0.2864 0.5693 0.4034\nMistralZero Shot0.5056 0.238 0.4438 0.3664 0.2335 0.1518 0.1004 0.3621 -0.0886\nFine Tuned0.6076 0.3467 0.5445 0.4776 0.353 0.2683 0.2036 0.4807 0.2134\nDPO0.6056 0.3443 0.5434 0.4769 0.3526 0.2697 0.2031 0.4801 0.2264\nORPO0.58 0.3249 0.5251 0.4545 0.3318 0.2493 0.1858 0.4521 0.131\nSarvam 1Zero Shot0.2848 0.1111 0.2329 0.1533 0.0869 0.0427 0.017 0.1707 -0.7911\nFine Tuned0.5928 0.3242 0.5159 0.4435 0.3276 0.2411 0.1815 0.4514 0.2265\nDPO0.5934 0.3331 0.5429 0.4672 0.3345 0.2509 0.1819 0.4505 0.2321\nORPO0.6011 0.3348 0.5732 0.4723 0.3546 0.2566 0.2079 0.4625 0.2557\nTable2:ResultsforthetranslationtaskontheMorphoVersedatasetusingourTAIFramework\u2019stranslationmodule.Theresults\nare in terms of ROUGE-1 (R1), ROUGE-2(R2), ROUGE-L(RL), BLEU scores, METEOR, and COMET scores.\nModelsLong-CLIP BLIP IR\nSD 3.5 mediumCP0.2436 0.4613 0.5342\nAS 10.2329 0.4510 0.3842\nAS 20.2341 0.3813 -0.2471\nSana 1.6BCP0.2211 0.4372 0.3638\nAS 10.2222 0.4154 0.3010\nAS 20.2045 0.2648 -0.3867\nPlayground v2.5CP0.2332 0.4606 0.4684\nAS 10.2201 0.4436 0.4014\nAS 20.2204 0.3198 -0.2278\nTable 3: Results for the image generation task for poems\nin theMorphoVersedataset under different settings : Con-\nstructed Prompt(CP), Ablation Study 1(AS 1), Ablation\nStudy 2(AS 2).\nPoemTheimagesgeneratedusingthisapproachdidnotcap-\nture the poems\u2019 meaning, visual elements, or cultural diver-\nsity. As evident from Figure 5 forPoem 1, and2, the gen-\nerated images simply display some poem\u2019s text on a back-\nground, lacking any meaningful visual representation. For\nPoem 3, onlyroseis generated, which does not effectively\nconvey the intended message of the poem. Table 3 further\ndemonstrates that this approach resulted in a decrease in\nLong-CLIP and BLIP scores, indicating poor alignment be-\ntweenthegeneratedimagesandpoemverses.Moreover,the\nimage reward score is significantly low, indicating that gen-\nerating images directly from the poem text produces inade-quateandunreliableresults.Thissuggeststhattexttoimage\ngeneration models lack an inherent understanding of poetic\nimagery.\nConclusion\nIn this paper, we introduce a two-stageTranslation and Im-\nage Generation (TAI)framework designed to generate im-\nagesforlow-resourceMorphologicallyrichIndianlanguage\npoems. Our approach significantly improves both the trans-\nlation quality of language models and the accuracy of im-\nagegeneration forpoetictexts.To achievethis,weintegrate\nsemantic graph knowledge to construct prompts for precise\nvisualrepresentations ofpoeticverses. Additionally,weim-\nplementpromptalignmentthroughhumanfeedback,refining\nasinglepromptgiventoLLMstoincorporatesemanticgraph\nknowledgeintoimagepromptconstruction.Ourmethodim-\nproves image quality while simplifying prompt creation for\ntext-to-image models, reducing user trial and error. It effec-\ntively balances semantic meaning, visual elements, and cul-\nturaldiversitytoensurethatthegeneratedimagesaccurately\ncapture the main idea of the poem. Furthermore, we intro-\nduceMorphoVerse,adatasetcomprising1,570poemsacross\n21 diverse Indian languages, designed to facilitate research\nin poetry translation and image generation. Extensive hu-\nman and quantitative evaluations validate the effectiveness\nof ourTAIframework, demonstrating its superiority in cre-\natinghigh-quality,contextuallyrichimagesforpoetictexts.2\n2All the experimental research and analysis were conducted in\nan academic setting at IIT Patna.\nReferences\nAcharya, P.; Pathak, A. K.; Balabantaray, R. C.; and Singh,\nA. K. 2020. Language identification of devanagari poems.\narXiv preprint arXiv:2012.15023.\nAggarwal, S.; Ghosh, A.; and Mamidi, R. 2020. SUKHAN:\nCorpusofHindiShayarisannotatedwithSentimentPolarity\nInformation. InProceedings of the 17th International Con-\nference on Natural Language Processing (ICON), 228\u2013233.\nAI@Meta. 2024. Llama 3 Model Card.\nAl-Mukhtar, A. F.; and Al-Shamery, E. S. 2018. Greedy\nmodularity graph clustering for community detection of\nlarge co-authorship network.Int. J. Eng. Technol, 7(4.19):\n857.\nAlowedi, N. A.; and Al-Ahdal, A. A. M. H. 2023. Artificial\nIntelligencebasedArabic-to-Englishmachineversushuman\ntranslationofpoetry:Ananalyticalstudyofoutcomes.Jour-\nnalofNamibianStudies:HistoryPoliticsCulture,33:1523\u2013\n1538.\nBanerjee, S.; and Lavie, A. 2005. METEOR: An automatic\nmetricforMTevaluationwithimprovedcorrelationwithhu-\nman judgments. InProceedings of the acl workshop on in-\ntrinsicandextrinsicevaluationmeasuresformachinetrans-\nlation and/or summarization, 65\u201372.\nBar-Tal,O.;Yariv,L.;Lipman,Y.;andDekel,T.2023. Mul-\ntidiffusion: Fusing diffusion paths for controlled image gen-\neration.\nBetker, J.; Goh, G.; Jing, L.; Brooks, T.; Wang, J.; Li, L.;\nOuyang, L.; Zhuang, J.; Lee, J.; Guo, Y.; et al. 2023. Im-\nproving image generation with better captions.Computer\nScience. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):\n8.\nBhat, S.; Varma, V.; and Pedanekar, N. 2023. Generative\nModels For Indic Languages: Evaluating Content Genera-\ntion Capabilities. In Mitkov, R.; and Angelova, G., eds.,\nProceedingsofthe14thInternationalConferenceonRecent\nAdvancesinNaturalLanguageProcessing,187\u2013195.Varna,\nBulgaria: INCOMA Ltd., Shoumen, Bulgaria.\nChakrabarty, T.; Saakyan, A.; and Muresan, S. 2021. Don\u2019t\nGo Far Off: An Empirical Study on Neural Poetry Transla-\ntion.arXiv preprint arXiv:2109.02972.\nChakrawarti,R.K.;Bansal,J.;andBansal,P.2022. Machine\ntranslation model for effective translation of Hindi poetries\ninto English.Journal of Experimental & Theoretical Artifi-\ncial Intelligence, 34(1): 95\u2013109.\nDhariwal, P.; and Nichol, A. 2021. Diffusion models beat\ngans on image synthesis.Advances in neural information\nprocessing systems, 34: 8780\u20138794.\nDubey,P.2019. TheHinditoDogrimachinetranslationsys-\ntem: grammatical perspective.International Journal of In-\nformation Technology, 11(1): 171\u2013182.\nEsser, P.; Kulal, S.; Blattmann, A.; Entezari, R.; M\u00fcller,\nJ.; Saini, H.; Levi, Y.; Lorenz, D.; Sauer, A.; Boesel, F.;\nPodell, D.; Dockhorn, T.; English, Z.; Lacey, K.; Goodwin,\nA.; Marek, Y.; and Rombach, R. 2024. Scaling Rectified\nFlow Transformers for High-Resolution Image Synthesis.\narXiv:2403.03206.Feng, W.; He, X.; Fu, T.-J.; Jampani, V.; Akula, A.;\nNarayana, P.; Basu, S.; Wang, X. E.; and Wang, W. Y.\n2022. Training-free structured diffusion guidance for\ncompositional text-to-image synthesis.arXiv preprint\narXiv:2212.05032.\nGenzel, D.; Uszkoreit, J.; and Och, F. J. 2010. \u201cPoetic\u201d sta-\ntistical machine translation: rhyme and meter. InProceed-\ningsofthe2010ConferenceonEmpiricalMethodsinNatu-\nral Language Processing, 158\u2013166.\nGhazvininejad, M.; Choi, Y.; and Knight, K. 2018. Neural\npoetrytranslation. InProceedingsofthe2018Conferenceof\ntheNorthAmericanChapteroftheAssociationforComputa-\ntional Linguistics: Human Language Technologies, Volume\n2 (Short Papers), 67\u201371.\nGreene, E.; Bodrumlu, T.; and Knight, K. 2010. Automatic\nanalysis of rhythmic poetry with applications to generation\nand translation. InProceedings of the 2010 conference on\nempiricalmethodsinnaturallanguageprocessing,524\u2013533.\nHong, J.; Lee, N.; and Thorne, J. 2024. ORPO: Monolithic\nPreference Optimization without Reference Model. In Al-\nOnaizan,Y.;Bansal,M.;andChen,Y.-N.,eds.,Proceedings\nof the 2024 Conference on Empirical Methods in Natural\nLanguageProcessing,11170\u201311189.Miami,Florida,USA:\nAssociation for Computational Linguistics.\nHu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang,\nS.; Wang, L.; and Chen, W. 2021. LoRA: Low-Rank Adap-\ntation of Large Language Models. arXiv:2106.09685.\nHuang, L.; Chen, D.; Liu, Y.; Shen, Y.; Zhao, D.; and\nZhou, J. 2023. Composer: Creative and controllable im-\nage synthesis with composable conditions.arXiv preprint\narXiv:2302.09778.\nJamil, S.; Charan, K. S.; Saha, S.; Goswami, K.; and J, J. K.\n2025a.Do It Yourself (DIY): Modifying Images for Poems\nin a Zero-Shot Setting Using Weighted Prompt Manipula-\ntion. In Christodoulopoulos, C.; Chakraborty, T.; Rose, C.;\nand Peng, V., eds.,Proceedings of the 2025 Conference on\nEmpiricalMethodsinNaturalLanguageProcessing,19668\u2013\n19676. Suzhou, China: Association for Computational Lin-\nguistics. ISBN 979-8-89176-332-6.\nJamil, S.; Reddy, B. A.; Kumar, R.; Saha, S.; Goswami, K.;\nand Joseph, K. J. 2025b. PoemTale Diffusion: Minimising\nInformation Loss in Poem to Image Generation with Multi-\nStage Prompt Refinement. arXiv:2507.13708.\nJamil, S.; Reddy, B. A.; Kumar, R.; Saha, S.; Joseph, K.;\nand Goswami, K. 2025c. Poetry in Pixels: Prompt Tuning\nfor Poem Image Generation via Diffusion Models.arXiv\npreprint arXiv:2501.05839.\nJiang, A. Q.; Sablayrolles, A.; Mensch, A.; Bamford, C.;\nChaplot, D. S.; de las Casas, D.; Bressand, F.; Lengyel, G.;\nLample, G.; Saulnier, L.; Lavaud, L. R.; Lachaux, M.-A.;\nStock, P.; Scao, T. L.; Lavril, T.; Wang, T.; Lacroix, T.; and\nSayed, W. E. 2023a. Mistral 7B. arXiv:2310.06825.\nJiang,Z.;Fang,G.;Han,J.;Lu,G.;Xu,H.;Liao,S.;Chang,\nX.; and Liang, X. 2023b. RealignDiff: Boosting Text-to-\nImage Diffusion Model with Coarse-to-fine Semantic Re-\nalignment.arXiv preprint arXiv:2305.19599.\nLi, D.; Kamko, A.; Akhgari, E.; Sabet, A.; Xu, L.; and\nDoshi, S. 2024. Playground v2.5: Three Insights towards\nEnhancing Aesthetic Quality in Text-to-Image Generation.\narXiv:2402.17245.\nLi, J.; Li, D.; Xiong, C.; and Hoi, S. 2022. BLIP:\nBootstrapping Language-Image Pre-training for Uni-\nfied Vision-Language Understanding and Generation.\narXiv:2201.12086.\nLian, L.; Li, B.; Yala, A.; and Darrell, T. 2023. Llm-\ngrounded diffusion: Enhancing prompt understanding of\ntext-to-image diffusion models with large language models.\narXiv preprint arXiv:2305.13655.\nLin,C.-Y.2004. Rouge:Apackageforautomaticevaluation\nof summaries. InText summarization branches out, 74\u201381.\nMaji, A.; Kumar, R.; Ghosh, A.; Anushka; and Saha, S.\n2025a. SANSKRITI: A Comprehensive Benchmark for\nEvaluatingLanguageModels\u2019KnowledgeofIndianCulture.\narXiv:2506.15355.\nMaji, A.; Kumar, R.; Ghosh, A.; Anushka; Shah, N.; Bo-\nrah, A.; Shah, V.; Mishra, N.; and Saha, S. 2025b. DR-\nISHTIKON: A Multimodal Multilingual Benchmark for\nTesting Language Models\u2019 Understanding on Indian Cul-\nture. In Christodoulopoulos, C.; Chakraborty, T.; Rose, C.;\nand Peng, V., eds.,Proceedings of the 2025 Conference on\nEmpirical Methods in Natural Language Processing, 1289\u2013\n1313. Suzhou, China: Association for Computational Lin-\nguistics. ISBN 979-8-89176-332-6.\nNair, J.; Krishnan, K. A.; and Deetha, R. 2016. An efficient\nEnglish to Hindi machine translation system using hybrid\nmechanism. In2016 international conference on advances\nin computing, communications and informatics (ICACCI),\n2109\u20132113. IEEE.\nNaveed,H.;Khan,A.U.;Qiu,S.;Saqib,M.;Anwar,S.;Us-\nman, M.; Akhtar, N.; Barnes, N.; and Mian, A. 2023. A\ncomprehensive overview of large language models.arXiv\npreprint arXiv:2307.06435.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a method for automatic evaluation of machine trans-\nlation. InProceedingsofthe40thannualmeetingoftheAs-\nsociation for Computational Linguistics, 311\u2013318.\nRafailov, R.; Sharma, A.; Mitchell, E.; Ermon, S.; Man-\nning, C. D.; and Finn, C. 2024. Direct Preference Opti-\nmization:YourLanguageModelisSecretlyaRewardModel.\narXiv:2305.18290.\nRei, R.; Stewart, C.; Farinha, A. C.; and Lavie, A.\n2020. COMET: A Neural Framework for MT Evaluation.\narXiv:2009.09025.\nRuiz,N.;Li,Y.;Jampani,V.;Pritch,Y.;Rubinstein,M.;and\nAberman, K. 2023. Dreambooth: Fine tuning text-to-image\ndiffusion models for subject-driven generation. InProceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition, 22500\u201322510.\nShah, K. 2024. hindi_peoms.\nSkerratt, B. P. 2013. Form and Transformation in Modern\nChinese Poetry and Poetics.Sohl-Dickstein, J.; Weiss, E.; Maheswaranathan, N.; and\nGanguli, S. 2015. Deep unsupervised learning using\nnonequilibrium thermodynamics. InInternational confer-\nence on machine learning, 2256\u20132265. PMLR.\nSong, W. L.; Xu, H.; Wong, D. F.; Zhan, R.; Chao, L. S.;\nandWang,S.2023. TowardsZero-ShotMultilingualPoetry\nTranslation. In Utiyama, M.; and Wang, R., eds.,Proceed-\nings of Machine Translation Summit XIX, Vol. 1: Research\nTrack, 324\u2013335. Macau SAR, China: Asia-Pacific Associa-\ntion for Machine Translation.\nSong, Y.; Sohl-Dickstein, J.; Kingma, D. P.; Kumar, A.; Er-\nmon,S.;andPoole,B.2020. Score-basedgenerativemodel-\ning through stochastic differential equations.arXiv preprint\narXiv:2011.13456.\nTeam, G. 2024a. Gemma.\nTeam, Q. 2024b. Qwen2.5: A Party of Foundation Models.\nWang, S.; Wong, D. F.; Yao, J.; and Chao, L. S. 2024.\nWhat is the Best Way for ChatGPT to Translate Poetry?\narXiv:2406.03450.\nXie,E.;Chen,J.;Chen,J.;Cai,H.;Tang,H.;Lin,Y.;Zhang,\nZ.; Li, M.; Zhu, L.; Lu, Y.; and Han, S. 2024. Sana: Ef-\nficient High-Resolution Image Synthesis with Linear Diffu-\nsion Transformer. arXiv:2410.10629.\nXu, J.; Liu, X.; Wu, Y.; Tong, Y.; Li, Q.; Ding, M.; Tang,\nJ.; and Dong, Y. 2023. ImageReward: Learning and Eval-\nuating Human Preferences for Text-to-Image Generation.\narXiv:2304.05977.\nXu,J.;Liu,X.;Wu,Y.;Tong,Y.;Li,Q.;Ding,M.;Tang,J.;\nand Dong, Y. 2024. Imagereward: Learning and evaluating\nhuman preferences for text-to-image generation.Advances\nin Neural Information Processing Systems, 36.\nZhang,B.;Zhang,P.;Dong,X.;Zang,Y.;andWang,J.2024.\nLong-CLIP: Unlocking the Long-Text Capability of CLIP.\narXiv preprint arXiv:2403.15378.\nZhong, S.; Huang, Z.; Wen, W.; Qin, J.; and Lin, L. 2023.\nSur-adapter: Enhancing text-to-image pre-trained diffusion\nmodels with large language models. InProceedings of the\n31st ACM International Conference on Multimedia, 567\u2013\n578.\n",
    "title": "Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation",
    "authors": [
      "Sofia Jamil",
      "Kotla Sai Charan",
      "Sriparna Saha",
      "Koustava Goswami",
      "Joseph K J"
    ],
    "published": "2025-11-17",
    "arxiv_id": "2511.13689v1",
    "num_pages": 9,
    "num_chars": 41974
  },
  {
    "text": "Protein Secondary Structure Prediction Using 3D Graphs\nand Relation-Aware Message Passing Transformers\nDisha Varshney\u22c6a, Samarth Garg\u22c6b, Sarthak Tyagic, Deeksha Varshneye,\nNayan Deepd, Asif Ekbalf\naBanaras Hindu University, Varanasi, India\nbABV-IIITM, Gwalior, India\ncIndiana University, Bloomington\ndIIIT, Naya Raipur, India\neIndian Institute of Technology Jodhpur, India\nfIndian Institute of Technology Patna, India\nAbstract\nIn this study, we tackle the challenging task of predicting secondary structures\nfrom protein primary sequences, a pivotal initial stride towards predicting ter-\ntiary structures, while yielding crucial insights into protein activity, relation-\nships, and functions. Existing methods often utilize extensive sets of unlabeled\namino acid sequences. However, these approaches neither explicitly capture nor\nharness the accessible protein 3D structural data, which is recognized as a de-\ncisive factor in dictating protein functions. To address this, we utilize protein\nresidue graphs and introduce various forms of sequential or structural connec-\ntions to capture enhanced spatial information. We adeptly combine Graph\nNeural Networks (GNNs) and Language Models (LMs), specifically utilizing a\npre-trained transformer-based protein language model to encode amino acid se-\nquences and employing message-passing mechanisms like GCN and R-GCN to\ncapture geometric characteristics of protein structures. Employing convolution\nwithinaspecificnode\u2019snearbyregion, includingrelations, westackmultiplecon-\nvolutional layers to efficiently learn combined insights from the protein\u2019s spatial\ngraph, revealing intricate interconnections and dependencies in its structural\n\u22c6Equal contribution.\nEmail address:dishavarshney082@gmail.com, samarthgarg0904@gmail.com,\ntyagi.sarthakst2000@gmail.com, deeksha@iitj.ac.in, varshneynayan02@gmail.com,\nasif.ekbal@gmail.com(Asif Ekbal)arXiv:2511.13685v1  [cs.LG]  17 Nov 2025\narrangement. To assess our model\u2019s performance, we employed the training\ndataset provided by NetSurfP-2.0, which outlines secondary structure in 3-and\n8-states. Extensive experiments show that our proposed model, SSRGNet sur-\npasses the baseline on f1-scores.\nKeywords:Proteins, PSSP, DistilProtBERT, SSRGNet, GNNs\n1. Introduction\nProteins serve as essential components within cells and are involved in vari-\nous applications, spanning from therapeutics to materials. They are composed\nof a sequence of amino acids that fold into distinct shapes. With the develop-\nment of affordable sequencing technologies [1, 2], a substantial number of novel\nprotein sequences have been identified in recent times. However, annotating the\nfunctional properties of a newly discovered protein sequence is still a laborious\nand expensive process. Thus, there is a need for reliable and efficient computa-\ntional methods to accurately predict and assign functions to proteins, thereby\nbridging the gap between sequence information and functional knowledge. The\nanalysis of protein structure, particularly the tertiary structure, is highly sig-\nnificant for practical applications related to proteins, such as understanding\ntheir functions and designing drugs [3]. Currently, there are three primary\nmethods employed for predicting the tertiary structure of proteins. These in-\nclude X-ray crystallography and nuclear magnetic resonance (NMR) techniques\n[4], cryo-electron microscopy (cryo-EM) based methods [5], and computer-aided\nab initio prediction [6]. Computer-assisted protein tertiary structure prediction\nhas gained significant attention because it offers convenience and shows superior\nperformance compared to other methods hindered by time-consuming processes\nof X-ray crystallography, sequence length limitations involved with magnetic\nresonance (NMR), or expensive equipment requirements for cryo-EM. In the\ncase of ab initio tertiary structure prediction, the protein\u2019s secondary structure\nplays a crucial role as it provides essential information about the local patterns\nin the protein\u2019s structure. Thus, improving the accuracy of protein secondary\n2\nstructure prediction (PSSP) is vital for achieving more precise predictions of\nthe overall protein structure.\nPSSP (Protein Secondary Structure Prediction) involves assigning a sec-\nondary structure label (coil, alpha helix, beta-sheet) to each amino acid in a\nprotein sequence, representing the local structure. This process is analogous\nto sequence labeling in natural language processing (NLP). Several advanced\ndeep learning models [7, 8, 9] have achieved satisfactory performance in PSSP\nby utilizing high-quality PSSM (Positional-Specific Scoring Matrix) in conjunc-\ntion with one-hot encoded amino acid sequences as evolutionary information.\n[7] employed a deep convolutional network to capture the relationship between\nPSSM features and labels. [8] proposed an enhanced model by incorporating\na conditional random field after the convolutional neural network (CNN) to ef-\nficiently capture sequential dependencies. [10] addressed the problem using a\ntwo-layer LSTM (Long Short-Term Memory), while [9] enhanced the represen-\ntation power of the model by introducing GRU (Gated Recurrent Unit) units\nafter the convolutional layers. [11] introduces a PSSM-Distil framework for pro-\ntein secondary structure prediction, enhancing low-quality PSSM accuracy by\nleveraging teacher networks, feature enhancement, distribution alignment, and\nauxiliary information from a pre-trained protein sequence language model.\nProtein language models pre-trained on extensive protein sequence data\nhavedemonstratedimpressiveperformanceinvariousprotein-relatedtasks, such\nas per-residue secondary structure prediction and per-protein localization and\nmembrane prediction [12, 13]. However, while these models can implicitly cap-\nture inter-residue contact information [14], they cannot explicitly encode pro-\ntein structures to create structure-aware protein representations. To address\nthis challenge, recent research has incorporated protein structures to determine\nsecondary structure [15, 16]. Nevertheless, these models often overlook the vi-\ntal interactions between edges in the protein structure, interactions that play\na pivotal role in modeling protein structures [17]. To bridge this gap and ad-\nvance large-scale structural bioinformatics, the development of highly accurate\ncomputational methods that effectively utilize critical structural information\n3\nis essential. This structural information encompasses details about the spa-\ntial arrangement of amino acids, the chemical bonds connecting them, and the\noverall architecture of the protein\u2019s three-dimensional structure. Understand-\ning this data is paramount for unraveling the processes of protein folding and\nfunctioning, as a protein\u2019s biological activity is intricately linked to its specific\nthree-dimensional structure. Figure 1 visually presents the graph structure cor-\nresponding to an amino acid sequence. In this representation, a sequence of\namino acids is depicted as a graph, with each amino acid residue represented\nas a node and the connections between these nodes as edges. Various types\nof edges, as elaborated in Section 3.3, play crucial roles in representing this\nstructural information. This graph-based approach holds promise for capturing\nessential protein structure information.\nMotivated by this development, we enhance the Protein Language Mod-\nels with a structure-based encoder for protein secondary structure prediction.\nWe propose a novel yet efficient architecture named Secondary Structure Re-\nlational Graph Neural Network (SSRGNet). This approach utilizes protein-\nresidue graphs along with amino acid chains. In the protein-residue graphs,\neach node corresponds to an amino acid\u2019s alpha carbon and includes its 3D\ncoordinates. We enhance these graphs by incorporating diverse edges to ac-\ncount for various sequential and structural relationships between residues. To\nencode sequence information, we employ a state-of-the-art Protein Language\nModel namedDistilProtBert[13]. For structural encoding, we use a simple yet\neffective protein structure encoder which performs relational message passing\non the enhanced protein graphs. This is the first attempt to integrate relational\nmessage passing in GNNs for protein structure encoding to predict secondary\nstructures. We investigate three model architectures that fuse the two encoders\nin a series, parallel, and cross manner. We conduct experiments utilizing the\nNetSurfP-2.0 benchmark for protein secondary structure prediction. The results\non CB513, TS115, and CASP12 test sets verify the superiority of the proposed\nSSRGNet over vanilla Protein Language Models, various protein encoders, and\nexisting structure encoder-enhanced Protein Language Models. These results il-\n4\nPrimary Amino Acid Sequence\nAmino AcidProtein Graph\nR1 (Sequential Relationship) \nR1 (Sequential Relationship) \nR2 (Spatial Pr oximity)\nR3 (Local Envir onment)0000000\n0000000\n0000000\n0000000\n0110101\n00000010000000\n0000000\n0101000\n0010110\n0000000\n00000000000000\n1000100\n0101000\n0010110\n0110101\n00000011001000\n1000100\n0101000\n0010110\n0110101\n0000001 R1\nR2\nR3\nTSE\nC\nT\nAEACG\nHT\nAPrimary\n SecondaryLevels of Pr otein Structur es\nTertiary QuaternaryRGCNFigure 1: Graph Representation of Protein Sequence: The figure illustrates the hierarchical\nlevels of protein structures, ranging from the primary amino acid sequence to secondary, ter-\ntiary, and quaternary structures. The primary sequence is then transformed into a protein\ngraph, where amino acids are represented as nodes and edges capture different relationships:\nR1 (sequential relationship) withE R1=1 shown in blue and withE R1=-1 in yellow, R2 (spa-\ntial proximity) in red, and R3 (local environment) in green. This graph-based representation\nenables structured learning of protein properties using a Relational Graph Convolutional Net-\nwork (RGCN).\nlustrate the great promise of relation-based structure-encoder-enhanced Protein\nLanguage Models for PSSP.\nOur major contributions can be summarized as follows:\n1. WeintroduceanovelandefficientarchitecturenamedtheSecondary Struc-\nture Relational Graph Neural Network (SSRGNet)which adeptly facili-\ntates protein secondary structure prediction by synergistically harnessing\nboth sequence information and structural details through protein-residue\ngraphs and amino acid chains.\n2. We enrich the protein-residue graphs by integrating diverse types of edges\ntoencapsulatevariousrelationshipsbetweenresidues, capturingsequential\npatterns, spatially close interactions, and the impact of the local environ-\nment on the protein\u2019s structure.\n3. Through rigorous experimentation utilizing the NetSurfP-2.0 benchmark,\nwe affirm the superiority of SSRGNet over baseline models on promi-\nnent test sets like CB513, TS115, and CASP12, thereby confirming the\n5\npromising prospects of relation-based structure-encoder-enhanced protein\nlanguage models for Protein Secondary Structure Prediction.\n2. Related Works\nIn this section, we explore past efforts that have employed various statisti-\ncal methods, machine learning algorithms, and deep learning methodologies to\naddress challenges in PSSP.\n2.1. Statistical Methods\nInitially, proteinsecondarystructurepredictionreliedonstatisticalmethods.\nThese approaches were built upon statistical information regarding individual\namino acids and were limited in their application to a small number of proteins\nwith known structures. Despite their relatively basic nature, these methods still\nserve as an initial step in addressing the protein secondary structure problem.\nTwo examples of such methods are the Chou-Fasman algorithm [18] and the\nGarnier-Osguthorpe-Robson algorithm [19]. The Chou-Fasman algorithm [18]\nutilizes amino acid frequencies to determine the occurrence of helices, sheets,\nand coils based on X-ray crystallography data from known protein structures.\nBy establishing probability parameters derived from these frequencies, the algo-\nrithm predicts the presence of helices, sheets, or coils within a given amino acid\nsequence in a protein. The prediction accuracy of the Chou-Fasman algorithm\nfalls within the range of 50% to 60% [20]. The Garnier-Osguthorpe-Robson\n(GOR) algorithm, similar to the Chou-Fasman algorithm, is a theory-based ap-\nproach for protein secondary structure prediction. The GOR method utilizes\nprobability parameters obtained from X-ray crystallography data of established\nprotein structures. Yet, its approach isn\u2019t limited to these specific secondary\nstructure probabilities. The method also takes into account the conditional\nprobability of adjacent structures that share the same formation. Studies indi-\ncate that the GOR algorithm\u2019s prediction accuracy hovers around 57%.\n6\n2.2. Machine learning methods\nOver the years, numerous machine learning approaches, including Support\nVector Machines (SVM), Dynamic Bayesian networks, Random Forests (RF),\nand ensemble methods, have been utilized to predict protein secondary struc-\nture. A frequently employed approach for predicting protein secondary struc-\nture is the Support Vector Machine (SVM). SVM typically utilizes a linear\nhyperplane to classify data. [21] applied SVM to a dataset generated from\nPSSM values and four physicochemical features, achieving a Q3 accuracy of\n79.5%. [22] introduced a novel radical group encoding approach for predict-\ning protein secondary structure (PSSP). Their encoding methodology aims to\nrepresent the 20 common amino acids constituting a protein by utilizing infor-\nmation derived from the stable atomic structures present within these amino\nacids. The researchers conducted experiments on two datasets, CB513 and\n25PDB. The models used for classification are support vector machines (SVM)\nand Bayes classifiers. Comparative analysis against the quadrature encoding\nmethod demonstrated superior performance, yielding an improved accuracy by\n1.2%. Furthermore, researchers have used the Hidden Markov Model (HMM)\nfor predicting protein secondary structures [23]. HMM is a statistical model\nthat predicts future behavior by analyzing existing data and is widely employed\nas a classifier in diverse fields, such as bioinformatics, data mining, pattern\nrecognition, and image processing, among others. In the work by [24], an ex-\ntended hidden semi-markov model was employed for single sequence secondary\nstructure prediction, achieving a Q3 accuracy of 67.9% on the CASP6 dataset.\nOccasionally, classification algorithms may exhibit analogous errors when\ncompared with one another, resulting in committing errors associated with spe-\ncific classes. In order to mitigate these types of errors, ensemble methods use\nmathematical and statistical techniques to amalgamate two or more classifi-\ncation algorithms. An illustration of this approach can be found in the work\nof [25], who combined artificial neural networks (ANN) and support vector ma-\nchines (SVM) using the weighted pooling ensemble which assigns more weight to\nhighly accurate classifiers, yielding a Q3 accuracy of 78.50% for RS126 dataset\n7\nand 76.65% for CB513. [26] employed neural networks on the Position-Specific\nScoring Matrix (PSSM) calculated using the PSI-BLAST algorithm, achieving\na Q3 accuracy range of 76.5% to 78.3%. In a study conducted by [27], k-\nNN, minimum distance, and fuzzy k-NN algorithms were applied to a dataset\nof protein structures, and their performance was compared against multilayer\nperceptron networks. The findings revealed that these methods outperformed\nmultilayer perceptron networks, achieving higher accuracy. [28] proposed an\nensemble technique employing two stochastic supervised machine learning algo-\nrithms, Maximum Entropy Markov Model (MEMM) and Conditional Random\nField (CRF).\n2.3. Deep Learning-based Methods\nThe main objective of protein secondary structure prediction is to classify\nindividual amino acids into specific secondary structural elements, such as the\nalpha helix, beta sheet, and coil. Given that the dataset size aligns with the\nnumberofaminoacids, whichcanbesubstantial, theaccelerationofthelearning\nalgorithm becomes pivotal. The authors in [29] employed a feed-forward neural\nnetwork to predict protein secondary structure. They evaluated this method\nusing a dataset of 64 proteins, where the initial 48 proteins were allocated for\ntraining, and the subsequent 14 were reserved for testing. The achieved accu-\nracy on this dataset was reported as 79%. An innovative method that employs\ntwo-dimensional deep convolutional neural networks to address the task of man-\naging extensive protein datasets was proposed in [30]. This network structure\ncomprises six convolutional layers and five max-pooling layers, facilitating effi-\ncient feature extraction and pattern recognition. They evaluated the model\u2019s\nperformance by conducting experiments on six benchmark datasets, namely\n25PDB and CASP{9\u201313}. The DCRNN model [9] utilizes multi-scale convolu-\ntional neural networks in conjunction with stacked bidirectional gated recurrent\nunits to capture both local and broader contextual information. Furthermore,\n[31] utilized deep convolutional neural networks and achieved an even higher\nQ8 accuracy of 71.4% using the CB513 dataset. The work by [7] presents a su-\n8\npervised generative stochastic network (GSN) and incorporates a convolutional\narchitecture to enable hierarchical representation learning on full-sized data. It\nachieved 66.4% Q8 accuracy on the CB513 dataset. A Bidirectional Recurrent\nNeural Network (BRNN) with Long Short-Term Memory (LSTM) units was\nemployed for protein secondary structure prediction in [10], resulting in a Q8\naccuracy of 67.4% on the CB513 dataset. A combination of recurrent neural\nnetworks and 2D convolutional neural networks proposed in [32], achieved a Q8\naccuracy of 70%. Additionally, [33] utilized a deep learning framework with hy-\nbrid profile-based features and reported Q8 accuracy scores of 75.8% and 73.5%\non the CB513 and CB6133 datasets, respectively.\nSSpro8 [34], employs an ensemble of 100 bidirectional recursive neural net-\nworks to predict an eight-state protein secondary structure. DeepCNF [35],\nmelds the strengths of both conditional neural fields (CNF) and deep convolu-\ntionalneuralnetworkstonotonlyencapsulatemedium-andlong-rangesequence\ninformation but also articulate the interdependency of order/disorder labels\namong neighboring residues field to model. Their method was evaluated using\nthe CASP9 and CASP10 datasets. The study [36] introduced the SSREDNs, a\ndeep recurrent encoder-decoder network that leverages deep layers and recur-\nrent mechanisms to discern the complex nonlinear associations between protein\ninput features and their Secondary Structure (SS). Additionally, it effectively\nmodels the interplay between successive residues in the protein sequence. The\nmodel was trained using the widely recognized CullPDB dataset and evaluated\non both CullPDB and CB513. DeepACLSTM [37] utilizes Asymmetric Convo-\nlutional Networks (ACNNs) in conjunction with bidirectional Long Short-Term\nMemory (LSTM) units for secondary structure prediction. ACNNs delve into\nthe intricate local contexts surrounding amino acids, while the BLSTM neural\nnetworks grasp the extended interrelationships among amino acids. It is trained\nusingtheCB5534andevaluatedon3publiclyavailabletestdatasetsviz.CB513,\nCASP10 and CASP11. In [38], an iterative learning strategy is used to train the\nLSTM-BRNN neural network which is capable of capturing long-range interac-\ntions without using a window. The model is trained on TR4590 and evaluated\n9\non the TS1199 dataset. MUFOLD-SS [39] is an open-source standalone tool for\nPSSP. The approach employs a novel deep inception-inside-inception (Deep3I)\narchitecture to capture interactions between amino acids, encompassing both lo-\ncal and distant relationships. They evaluated the model using CB513, CASP10,\nCASP11, and CASP12 benchmarks. [40] proposed a lightweight convolutional\nnetwork using a label distribution aware margin loss which helps learn minor-\nity class representation for PSSP. They trained the model using CB12510 and\nevaluated using CB513, CASP13, CASP10, CASP12, CASP11 and CASP14 as\ntest sets. Moreover, many other deep network variants have also been proposed\nto perform eight-state prediction [41, 42, 43, 44].\nIn recent years, language models based on protein sequences, termed Pro-\ntein Language Models, have emerged as potent tools for various protein-related\ntasks. Pioneering works such as those reported in [12, 13] showcased the im-\npressive performances of models pre-trained on extensive protein sequence data\nfor tasks like per-residue secondary structure prediction, and per-protein local-\nization and membrane prediction. These models are lauded for their ability\nto implicitly capture inter-residue contact information as highlighted in [14].\nUtilizing masked language modeling losses, existing models [45, 46] are adept\nat encapsulating co-evolutionary data and implicitly seizing inter-residue con-\ntact information. However, a notable limitation lies in their inability to explic-\nitly encode protein structures which precludes the creation of structure-aware\nprotein representations. This gap underscores a pivotal avenue for future re-\nsearch aimed at enhancing the structural encoding capabilities of protein mod-\nels, thereby pushing the boundaries of what can be achieved in protein analysis\nand prediction tasks. MFTrans [47], a deep learning-based multi-feature fusion\nnetwork, employs an MSA Transformer combined with a hybrid CNN-BiGRU\narchitecture to integrate sequence, evolutionary, and hidden state information,\nenhancingproteinsecondarystructurepredictionaccuracythroughamulti-view\nfeature fusion strategy.\nIn comparison to sequence-based approaches, structure-based methods are\ntheoretically more adept at learning a rich protein representation, given that\n10\na protein\u2019s functionality is fundamentally dictated by its structure. This line\nof works seeks to encode spatial information in protein structures by 3D CNNs\n[48] or graph neural networks (GNNs) [49, 50, 51, 52]. For the PSSP task,\n[53] discusses the effect of long-range residue interactions on defining secondary\nstructure in a protein and introduces the Residue Contact Order (RCO) metric\nto study the relationship between residue separation and prediction accuracy,\nutilizing a dataset of 2777 non-homologous proteins. Utilizing their finding, the\nauthors in [16] employed Graph Convolutional Networks (GCN) to amalgamate\nthe information pertaining to amino acids and their interactions. On the other\nhand, Bi-Long Short Term Memory (Bi-LSTM) was utilized owing to its robust\ncapability to apprehend the long-range dependencies among amino acids. They\nleveraged ProtTrans [12] for obtaining sequence embeddings, and utilized the fil-\ntered CB6133 dataset for training purposes. The evaluation of performance was\ncarried out on various datasets including CASP10, CASP11, CASP12, CB513,\nand TS115. [54] creates a graph from the primary amino acid sequence, with\nunique embeddings for each node via orthogonal encoding. Through iterative\ntraversal and GNN methodology, information from neighboring nodes is aggre-\ngated. A Support Vector Machine (SVM) is then applied to identify the 8 states\nof protein secondary structure, achieving a promising 76.89% accuracy on the\nccPDB 2.0 dataset. IGPRED [55] combining a convolutional neural network\nand graph convolutional network, utilizes features like PSI-BLAST PSSM, HH-\nMAKE PSSM, and amino acids\u2019 physicochemical properties, refined through\nBayesian optimization. It shows notable Q3 accuracies on different datasets\nincluding CullPDB, EVAset, CASP10, CASP11, and CASP12, indicating its\neffectiveness in protein secondary structure prediction.\nIn this work, the focus is on predicting secondary structures from protein\nprimary sequences, which serves as a crucial step toward tertiary structure pre-\ndiction, shedding light on protein functions and relationships. While prevailing\nmethods rely on unlabeled amino acid sequences, they often overlook valuable\nprotein 3D structural data, a key determinant of protein functions. To bridge\nthis gap, we leverage the protein residue graphs, incorporating various types of\n11\nrelations between the edges to capture spatial information. These graphs are\nfurther refined by integrating various types of edges to encapsulate different in-\nteractions among residues, sequential trends, spatial proximity interactions, and\nthe influence of the local milieu on the protein\u2019s form. We conducted extensive\nexperiments using the NetSurfP-2.0 benchmark and evaluated our model on\nsignificant test datasets like CB513, TS115, and CASP12, thereby showing the\nencouraging potential of relation-oriented structure-encoder-augmented protein\nlanguage models for Protein Secondary Structure Prediction.\n3. Methodology\n3.1. Problem Statement\nThe protein sequence is composed of 21 standard amino acidsviz. Ala (A),\nArg (R), Asn (N), Asp (D), Cys (C), Gln (Q), Glu (E), Glx (Z), Gly (G),\nHis (H), Lle (I), Leu (L), Lys (K), Met (M), Phe (F), Pro (P), Ser (S), Thr\n(T), Trp (W), Tyr (Y), Val (V), which serve as its fundamental components.\nAdditionally, the infrequent amino acids \"U, Z, O, B\" have been substituted\nwith the symbol \"X.\" We frame PSSP as a sequence-to-sequence task where\neach amino acidx iin a protein sequencePis mapped to a labely i\u2208{alpha-\nhelix (H), beta-strand (E), Coil (C)} for 3-state classification andy i\u2208{residue\nin isolated\u03b2-bridge (B), Extended strand(E), 3-10 helix (G), alpha-helix (H),\u03c0-\nhelix (I), Hydrogen bonded turn (T), Bend (S), Loop or any other residues (C)}\nfor 8-state classification. Figure 2 provides a detailed overview of our proposed\nmethodology.\n3.2. Sequence Encoding\nProtein sequences can provide valuable information about the likelihood of a\nspecific amino acid\u2019s involvement in forming a secondary structure. This is due\nto the distinct electrochemical and geometric characteristics exhibited by each\namino acid. To encode the amino acid sequencex i, we use DistilProtBert [13]\nas our base model. It is a distilled version of the ProtBert-UniRef100 model.\n12\nC E H\nFusion Block\nInput SequenceTokenization \n& Positional\nEncodingTransformer  ModelLast Hidden LayerStack of N-self\nAttention layersAmino acid\nembedding\nR-GCN\nProtein Residue Graph with\nR1, R2, R3 edgesConvert to Graph\nAmino Acid Sequence3-state secondary structur e\nAdd new edgesDistilPr otBert\nATSE\nC E\nT\nAEACG\nHT\nA1000000\n1\n0\n0\n1\n00001000\n1\n0\n1\n0\n0Layer  1 Layer  2\n1001000\n1000100\n0101000\n0010110\n0110101\n0000001Adjacency Matrices\ncorresponding\nto each r elation \nin the graph128 128 128 128\nProjection Layer\n1024 1024 1024128 128 128\n 128\n1024Figure 2: SSRGNet Model Overview: The diagram illustrates the SSRGNet model, which\ncombines a DistilProtBert model, and an R-GCN (Relational Graph Convolution Network)\nmodel. The amino acid sequence is converted into a graph representation featuring three types\nof edges, as shown at the bottom, and passed into the respective encoders. Features obtained\nfor the amino acid and the protein graph are finally fused together to predict the three-state\nand eight-state secondary structure of the protein.\nDistilProtBert is trained on UniRef100, a comprehensive dataset containing\nclustered protein sequences. Apart from using techniques like cross entropy and\ncosine teacher-student losses, DistilProtBert was trained using a masked lan-\nguage modeling (MLM) approach, where random amino acids are masked, the\nmodel learns to predict them based on contextual residues and it exclusively\nprocesses amino acids in uppercase letters. DistilProtBert is primarily a protein\nlanguage model that employs a transformer-based architecture and attention\nmechanism to understand how different pairs of amino acids interact within a\nsequence. The protein sequence is tokenized and positional encoding is incorpo-\nrated. The ensuing vectors are channeled through the DistilProtBert model to\n13\ngenerate context-aware embeddings for each input token, i.e., each amino acid.\nHere, we utilized the last hidden state of the Transformer\u2019s attention stack as\ninput for the downstream protein secondary prediction task. This enables the\nDistilProtBert model to grasp the evolutionary details of proteins and their\ncharacteristics.\nIn order to achieve uniform representation dimensions for all modalities\nwithin the spatial domain, a projection layer is applied after the last hidden\nlayer of the DistilProtBert model. This layer functions as a projection layer\nthat transforms the individual amino-acid representations, derived from the\nDistilProtBert embedding dimension,d p\u22081024, into the graph embedding di-\nmensiond g. As a result, a matrix denoted asHdg\u2208128is formed, containing the\namino-acid representations.\n3.3. Graph Construction\nIn this work, we start by visualizing the 3D structure of proteins as a graph.\nWithin this graph, every amino acid residue is portrayed as an individual node,\nand the relations between them as an edge. Emphasis is placed on including\ndifferent types of relationships among the edges to encapsulate a variety of\nspatial information. In the case of modeling protein interactions using a multi-\nrelational heterogeneous graph, we start by defining the graphG= (V, E)as\nfollows:\n1.V={x i}is the set of all nodes, where:\n\u2022xiis the set of amino acid residues, representing the individual units\nthat form proteins.\n2.Eis the set of edges. All edges(u, v, r)\u2208Ehave a sourceu, a targetv,\nand a relation typer\u2208R. The set of possible relationsR={R1, R2, R3}\ncontains:\n\u2022R1(Sequential Relationship):\n14\n\u2013An edge is established between two amino acid residues if the\nnumber of amino acids separating them in the protein sequence\nis less than a specified threshold value (d s).\n\u2013The type of edge is determined based on the relative position of\ntheresiduesinthesequence, resultinginE R1=2d s\u22121distinctive\ntypes of edges that capture their local relationships.\n\u2013We choosed s= 2as the threshold value. This choice results in\nthree types of edges, each representing a different relationship\nbetween amino acid residues.\n\u2022R2(Spatial Proximity):\n\u2013An edge is added between two amino acid residues if their Eu-\nclidean distance in the three-dimensional structure of the protein\nfalls below a specified threshold value (d ed).\n\u2013We choose(d ed= 10\u00c5)as the threshold value [56]. This means\nthat residues that are physically close to each other in the pro-\ntein\u2019s three-dimensional space and are within 10 \u00c5, will be con-\nnected by an edge.\n\u2022R3(Local Environment):\n\u2013Foreachaminoacidresiduenode, itskclosestneighborsinthree-\ndimensional space are identified based on the Euclidean distance.\n\u2013These nearby nodes are then connected to the target residue\nnode, allowing the graph to incorporate the impact of the local\nneighborhood.\n\u2013The parameterk ncontrols the number of nearest neighbors con-\nsidered, adjusting the extent of local connectivity within the\ngraph. In this case,k n= 10for the experiments.\n3.4. Graph Features\nIn the resulting graph, every amino acid residue is portrayed as an indi-\nvidual node, and the characteristics of these residues are encapsulated in the\n15\nnode\u2019s features. Additionally, the connections between residues are translated\ninto edges, and the attributes of these edges capture the diverse types of spa-\ntial interactions between the amino acids. Both nodes and edges within the\nprotein graph are enriched with features that stem from the protein\u2019s sequence\nand structure. These features are calculated based on the specific attributes\nof the protein, offering a comprehensive representation of its composition and\narrangement.\n1. Node Features: As we explicitly depict the protein\u2019s 3D structure in a\ngraph, each node solely contains the sequence features corresponding to\nthe respective residue. The identity and conservation of a residue are\nquantified by 21 features that capture the relative frequency of each of the\n21 amino acids in alignment with similar proteins. The encoding process\nof the amino acid sequences is similar to the encoding of protein sequences\nas explained in Section 3.2. The resulting representation matrix,Hdg, is\nthe node feature matrix.\n2. Edge Features: Alongside these sequence-based attributes, each node in-\ncorporates features derived from the protein\u2019s structure. These encompass\nthe residue\u2019s local context, its sequential patterns, spatially close inter-\nactions, and the influence of the local environment [56] as explained in\nSection 3.3. The edge features are encoded using the method described in\nSection 3.5.\n3.5. Methods for Relation-based Graph Convolution\nUsing the protein graphs and their features, we aim to model representations\nthat encode their spatial and chemical information. To meet this requirement,\nwe start by constructing our protein graph based on spatial features as detailed\nin Section 3.3. This construction of a multi-relational heterogeneous graph\nallows us to delve into the spatial relationships between amino acid residues\nand grasp the impact of the local environment on protein structure.\nUsing the protein graphs described earlier, we apply a Graph Neural Net-\nwork (GNN) to obtain representations at both the individual amino acid and\n16\ncomplete protein levels. One straightforward example of a GNN is the Graph\nConvolutional Network (GCN) [57], where messages are generated by multi-\nplying node features with a convolutional kernel matrix that is shared across\nall edges. For the given heterogeneous graph, we employ a two-layer R-GCN\n([58]), a graph neural network designed specifically for learning representations\nin multi-relational data. The R-GCN updates the initial node embeddings in\neach layer by considering the neighborhood of each node and taking into ac-\ncount the type of relation it has with its neighbors. In other words, for every\nnodex\u2208V, the R-GCN calculates its embedding,e(k+1)\nx, in the(k+ 1)-th\nconvolutional layer as follows:\ne(k+1)\nx =\u03c3\uf8eb\n\uf8ed1\ncx,rX\nr\u2208RX\nj\u2208N r(x)W(k)\nre(k)\nj+W(k)\n0e(k)\nx\uf8f6\n\uf8f8 (1)\nNr(x)refers to the set of neighbors of nodexconnected by relations of type\nr. The activation function,\u03c3, normalization constant,c x,r, and relation-specific\ntransformations,W randW 0are incorporated by the R-GCN during training.\nFollowing the approach proposed by [58], we setc x,r=|N r(x)|: the number of\nneighbors of nodexconnected by relationr, enabling our R-GCN to generate\ncomprehensive representations of proteins based on their interactions, spatial\nproximity, and local environment. Utilizing convolution in a specified node\u2019s\nneighboring area, inclusive of relations, we layer multiple convolutional stages\nto effectively extract consolidated insights from the protein\u2019s spatial graph, un-\nveiling complex interlinks and dependencies in its structural configuration.\n3.6. Fusion Block\nWe aim to efficiently combine graph neural networks and pretrained lan-\nguage models, particularly employing a pre-trained transformer-based protein\nlanguage model to encode amino acid sequences, and utilizing message-passing\nmechanisms within networks to grasp the geometric attributes of protein struc-\ntures. The sequence features (S seq) from Section 3.3 and graph features (S graph)\nfrom Section 3.4 are concatenated together along the last dimension.\n17\nF=MLP(concat(S seq, Sgraph))(2)\npi,j=eFi\nPN\nj=1eFj(3)\nwhere,S seq=H,S graph =e.\n3.7. Training\nFor the three-state protein secondary structure prediction, the cross-entropy\nloss can be defined as:\nL=\u2212NX\ni=1CX\nj=1yijlog(p ij)(4)\nwhere,\n\u2022Nis the number of samples,\n\u2022Cis the number of classes (e.g., helix, sheet, coil),\n\u2022yijis the true label, and\n\u2022pijis the predicted probability of sampleibelonging to classj.\n4. Experiments\nIn this section, we assess how well the proposed method works for Protein\nSecondary Structure Prediction (PSSP). We start by introducing the datasets\nused and explaining the evaluation metrics used. Then, we detail the imple-\nmentation and report the experimental results under various conditions.\n4.1. Datasets\nWe utilized the NetSurfP-2.01[42] dataset, which characterizes secondary\nstructure in 3-and 8-states, to predict attributes associated with individual to-\nkens, specifically single amino acids, referred to as residues when they form part\n1https://services.healthtech.dtu.dk/services/NetSurfP-2.0/training_data/Train_HHblits.npz\n18\nof proteins. Initially, the structural dataset comprised 12,185 crystal structures\nsourced from the Protein Data Bank (PDB)[59] and filtered by the PISCES\nserverwithspecificcriteria,includinga25%sequencesimilarityclusteringthresh-\nold and a resolution of 2.5 \u00c5 or better. To prevent overfitting, they removed\nsequences that shared more than 25% identity with any sequences in the test\ndatasets, as well as peptide chains containing fewer than 20 residues, resulting\nin 10,837 sequences. Subsequently, they randomly selected 500 sequences (al-\nlocated as the test set) for early stopping and parameter optimization, leaving\n10,337 sequences for training. Therefore, the training and test sets comprised\n10,337 non-redundant proteins and 500 sequences, respectively. To evaluate\nthe performance of the proposed methods, we also use three commonly used\ndatasets, the CB5132(513 protein regions from 434 proteins) [60], TS1153(115\nproteins) [61], and CASP124(21 proteins) [62]. Table 1 and Table 2 shows a de-\ntailed overview of the 3-state and 8-state class distribution secondary structure\nrespectively for the NetSurfP-2.0 training set and the corresponding validation\ndatasets (CASP12, TS115, CB513)[42]. In all the datasets we\u2019ve used, the Loop\nor other residue is denoted by \"C\" instead of \"L\" for eight-state classes. This\nnotation has been consistently applied in our paper.\n3D-coordinates of Amino Acids:We have used the PDB IDs linked\nwith the samples in the dataset to fetch the PDB files from the PDB data\nbank5. We extract the coordinates of each of the residues in the long protein\nsequence and use them for constructing protein graphs as detailed in Section\n3.3. For some PDB IDs in the NetSurfP-2.0 dataset, we could not retrieve the\ncorresponding PDB files. The statistics mentioned in Table 1 and Table 2 are\nreported considering the previous factor.\n2https://services.healthtech.dtu.dk/services/NetSurfP-2.0/training_data/CB513_HHblits.npz\n3https://services.healthtech.dtu.dk/services/NetSurfP-2.0/training_data/TS115_HHblits.npz\n4https://services.healthtech.dtu.dk/services/NetSurfP-2.0/training_data/CASP12_HHblits.npz\n5https://www.rcsb.org/\n19\nTable 1: Class distribution (3-state secondary structure) for the NetSurfP-2.0 training set\nand the corresponding validation datasets (CASP12, TS115, CB513).\nDataset Protein Helix (H) Strand (E) Coil (C)\nCASP12 21 1478 2241 2701\nTS115 115 5380 11641 10480\nCB513 511 18690 29102 35635\nNetSurfP-2.0 10796 585603 1173670 1001075\nTable 2: Class distribution (8-state secondary structure) for the NetSurfP-2.0 training set\nand the corresponding validation datasets (CASP12, TS115, CB513).\nDataset HECSTGBI\nCASP12 1989141614006686332156237\nTS115 1043450855395221028751033295174\nCB513 2555917585177138211971130741105469\nNetSurfP-2.0 887210559154681602209690282378996972640914168\n4.2. Implementation Details\n4.2.1. Language Model\nThe protein sequences were converted to uppercase and tokenized using a\nsingle space, with a vocabulary size of 21. Pre-processing for longer sequences\nexceeding 1024 amino acids was done. All the experiments are implemented\nusing the Pytorch framework [63]. The last hidden layer size for DistilProtBert\nmodel is 1024, and the number of layers is 15. We use the AdamW [64] optimizer\nwith a learning rate fixed to 1e-05.\n4.2.2. Graph Model\nWe opted for PyG6due to its extensive collection of state-of-the-art models\nfor implementing graph neural networks. However, a challenge arose because\n6https://pytorch-geometric.readthedocs.io/en/latest/\n20\nPyG models don\u2019t naturally handle batch inputs, while the hugging face Lan-\nguage Model (LM) outputs data in batches. To overcome this, we utilized\ndiagonal batching, a PyG technique that enables parallelization across multiple\nsamples. In this approach, we stack adjacency matrices diagonally, creating a\nlarge network with isolated subgraphs. Node and target attributes are concate-\nnated in the node dimension. To manage data batching effectively, we developed\na custom PyG in-memory dataset object. R-GCN and GCN models had a hid-\nden size of 128, and the number of layers was set to 2. We again use the AdamW\n[64] optimizer with a learning rate fixed to 3e-04.\nOur model required distinct inputs for language and graph components. LM\nneeded protein sequences, while the graph model required an adjacency matrix\nto represent residue relationships, along with a node feature matrix generated\nby LM. Initially, this matrix was absent, so we dynamically passed it to the\ngraph model during the forward call of the training process.\nFor hyperparameter optimization, we employed a robust Bayesian search\nstrategytoidentifytheoptimallearningrates(lr)forboththeProteinLanguage\nModel and the Graph Neural Networks. Using validation loss as the evaluation\nmetric, we fine-tuned hyperparameters to pinpoint the most suitable learning\nrate (lr) values. Bayesian Optimization was pivotal in creating a probabilistic\nmodel that maps hyperparameter values to their corresponding target values\nfrom the validation set. This systematic exploration of the hyperparameter\nspace led to the selection of learning rate (lr) values that significantly improved\nthe performance of our SSRGNet model, ultimately enhancing the accuracy\nand efficiency of protein secondary structure predictions. We choose the best\nmodel when the loss on the validation set does not decrease further. We use the\nGeForce GTX 2080 Ti as the computing infrastructure. Our codes are available\nat this link7.\n7https://github.com/SamarthGarg09/protein-secondary-structure-prediction\n21\n4.2.3. Time Complexity Analysis\nBERT-base Model Time Complexity.The BERT (Bidirectional Encoder Repre-\nsentations from Transformers) model has a computational complexity for each\nof its 12 layers as follows:\n\u2022Self-Attention Mechanism:O(N2\u00b7d)\n\u2022Feed-Forward Network:O(N\u00b7d2)\nThus, the overall complexity for BERT-base is:\nO(N2\u00b7d+N\u00b7d2)\nRGCN Model Time Complexity.ForaRelationalGraphConvolutionalNetwork\n(RGCN) withKlayers:\n\u2022Message Passing:O(|E| \u00b7d)\n\u2022Update Step:O(|V| \u00b7d2)\nThe total complexity is:\nO(K\u00b7(|E| \u00b7d+|V| \u00b7d2))\nCombined Model Complexity.Combining BERT and RGCN results in:\n\u2022BERT:O(N2\u00b7d+N\u00b7d2)\n\u2022RGCN:O(K\u00b7(|E| \u00b7d+|V| \u00b7d2))\nThe combined complexity is:\nO(N2\u00b7d+N\u00b7d2+K\u00b7(|E| \u00b7d+|V| \u00b7d2))\nThis provides insight into the computational demands of our model, aiding\noptimization for large datasets and complex protein structures.\n4.3. Evaluation metrics\nThe efficacy of the suggested approach and the baseline is assessed through\ntwo criteria: Accuracy, F1-score.\n22\n4.3.1. Accuracy\nThe Q3 accuracy metric determines the percentage of amino acid residues\nwhose predicted secondary structure aligns with the actual structure [65]. Its\ncalculation is articulated in Eq. (8):\nQ3 =NH+NE+NC\nN\u00d7100 (8)\nHere,N Hsignifies the correctly forecasted helix,N Ethe right strand predic-\ntion,N Cthe accurate coil prediction, andNencompasses the total amino acid\nresidues in a given protein.\nSimilarly, the Q8 accuracy metric is computed as delineated in Eq. (9):\nQ8 =NE+NH+NS+NT+NB+NC+NG+NI\nN\u00d7100 (9)\nHerein,N Ccorresponds to the right prediction of Loop or any other residues,\nNBis the correctly anticipated\u03b2-bridge,N Eis the precise extended strand\nprediction,N His the correct prediction of alpha-helix,N Iis\u03c0-helix ,N Tis\nHydrogen bonded turn,N Gis 3-10 helix andN Sis Bend whileNstands for the\ncumulative count of amino acid residues in a protein.\n4.3.2. F1-Score\nThe F1-Score represents the harmonic average of precision and recall.\nPrecision.Precision is the proportion of accurate positive predictions to the\ntotal number of positive predictions made. This metric can be mathematically\nrepresented as shown in Eq. (5):\nPrecision=TP\nTP+FP(5)\nHere, True Positive (TP) signifies the instances where the predicted positive\noutcome is indeed positive. On the other hand, False Positive (FP) represents\nthe cases where a negative outcome is incorrectly classified as positive.\nRecall.The proportion of accurate positive predictions relative to all predicted\ninstances within the specified class is termed as recall, as depicted in Eq. (6):\nRecall=TP\nTP+FN(6)\n23\nwhere FN stands for the false negative value, signifying a positive value that\nhas been erroneously predicted as negative.\nThe metric achieves its maximum value of 1 when both recall and precision\nare perfect. Conversely, if either recall or precision registers a value of zero, the\nF1-Score subsequently becomes zero. The formula to calculate the F1-Score is\nprovided in Eq. (7):\nF1 =2\u00d7precision\u00d7recall\nprecision+recall(7)\n4.4. Baseline models\nIn our experimental evaluation, we conducted a rigorous performance as-\nsessment of the SSRGNet framework by comparing it to a comprehensive set\nof baseline models. Initially, we evaluated our approach against unimodal en-\ncoders, including DCRNN [9], DeepACLSTM [37], DistilProtBert [13]. These\nencoders exclusively focus on representing protein sequences. Furthermore, we\ncompared our proposed model to a multimodal baseline named GCNBLSTM\n[16] that concatenates the graph and sequence representations without fusing\nthe representation of each amino acid and the structural representation.\n1.DCRNN[9]: The Deep Convolutional and Recurrent Neural Network\n(DCRNN)comprisesofafeatureembeddinglayer, multi-scaleCNNlayers,\nthree BiGRU layers, and two fully connected hidden layers. It processes\nprotein amino acid sequences by transforming sparse sequence features\ninto denser vectors and extracting both local and global contextual fea-\ntures through various layers.\n2.DeepACLSTM[37]: DeepACLSTM combines protein sequence and pro-\nfile attributes through asymmetric convolutional neural networks (AC-\nNNs) and bidirectional long short-term memory (BiLSTM) systems. The\nACNNs decipher the complex local nuances of amino acids, while the\nBLSTM systems identify the extensive interconnections among them, en-\nsuring each amino acid residue is categorized considering both its imme-\ndiate environment and wider interdependencies.\n24\n3.DistilProtBert[13]: DistilProtBert is a distilled version of the ProtBert\nmodel, designed for bioinformatics tasks, particularly in protein sequence\nanalysis. It leverages the power of the BERT architecture, trained on a\nmassive corpus of protein sequences, to understand the contextual rela-\ntionships between amino acids in a given protein sequence.\n4.SSGNet: SSGNetcombinestheDistilProtBertmodelandaGCN(Graph\nConvolution Network) model to provide structural insights using the pro-\ntein residue graph (Section 3.3) for the secondary structure prediction\ntask.\n4.5. Ablation Models\nIn this section, we present an ablation study to investigate the impact of\ndifferent fusion techniques, namely Series Fusion, Cross Fusion, and Parallel\nFusion, on the performance of our model (Figure 3). Each fusion technique\nrepresents a unique methodology for processing and combining information.\nWe employed three distinct fusion methodologies explained as follows:\nDistill\nProtBertRGCN\nECTA ...\nAT\nC\nE\na) Series (element-wise addition)Distill\nProtBertRGCN\nECTA ...\nAT\nC\nE\nb) Parallel (concatenated along the\nlast dimension)Distill\nProtBertRGCN\nECTA ...\nAT\nC\nE\nc) Cross (Multi-head Attention)Multi-head Attn\nFigure 3: Comparative Architectural Schemes of Fusion Techniques. The diagram illustrates\nthree distinct methodologies: (a) Series Fusion, where components process information se-\nquentially (element-wise addition); (b) Parallel Fusion, where outputs are concatenated along\nthe last dimension, and (c) Cross Fusion, characterized by intertwined (multi-head attention)\nprocessing layers.\n\u2022Series Fusion: This technique processes information the sequence en-\ncoder (Section 3.2) and graph encoder (Section 3.5) sequentially through\nelement-wise addition.\n25\n\u2022Parallel Fusion: In this the outputs from the sequence encoder and\ngraph encoder are concatenated along the last dimension (Eq. (2)).\n\u2022Cross Fusion: This method of fusion is characterized by intertwined\nprocessing layers employing multi-head attention mechanisms.\n5. Results and Analysis\nWe present the outcomes in Table 3 and Table 4, involving various encoder\nmodels, encompassing unimodal encoders such as DCRNN [9], DeepACLSTM\n[37], DistilProtBert [13], and our multimodal baseline named SSGNet. The uni-\nmodal DCRNN and DeepACLSTM baseline, which relies solely on the amino\nacid sequence of the protein, exhibits the lowest performance across all evalua-\ntion metrics for both three-state and eight-state class predictions. The metric\nof accuracy can be misleading in imbalanced datasets as it might predominantly\nreflect the predictions of the majority class. On the other hand, the F1 score of-\nfers a balanced insight into a model\u2019s performance by considering both precision\nand recall, making it a more reliable metric in scenarios where false positives\nand false negatives have differing implications. Despite this, we conduct a com-\nprehensive analysis employing both metrics. A marked improvement in perfor-\nmance is observed upon utilizing the pre-trained protein language model. Distil-\nProtBert significantly outperforms the DCRNN and DeepACLSTM, improving\nby over 41% on the test set of the NetSurfP-2.0 dataset when evaluated using\nthe F1 score. This substantial enhancement can be ascribed to the pretraining\nregimen of DistilProtBert, which employs masked protein modeling. This ap-\nproach enables the encoder to discern intricate relationships and patterns within\nprotein sequences, thereby contributing to the observed performance boost.\nA notable improvement in performance is witnessed when the unimodal\ngraph encoder GCN is employed within SSGNet, allowing it to surpass the\nperformance benchmarks set by DCRNN and DeepACLSTM. Our proposed\nframeworks, SSRGNet and SSGNet\u2014both with and without relations\u2014exhibit\nsuperior performance over baseline models, including the DistilProtBert model\n26\nTable 3: Comparison of results for predictions in three states in test sets of the NetSurfP-2.0,\nCB513, CASP12, and TS115 datasets. The best performance for F1 is highlighted in bold\nand improvement over the best baseline, and is statistically significant (t-test with p-value at\n0.05 significance level).\nNetSurfP-2.0 CB513 CASP12 TS115\nAccuracy (%) F1 Accuracy (%) F1 Accuracy (%) F1 Accuracy (%) F1\nDCRNN 20.66 0.46 95.47 0.44 15.59 0.44 16.91 0.46\nDeepACLSTM 95.16 0.47 95.19 0.45 93.58 0.44 95.52 0.48\nDistilProtBert 80.60 0.65 79.49 0.61 69.74 0.50 80.93 0.65\nSSRGNet 81.230.67 80.17 0.61 71.930.51 82.01 0.65\nSSGNet 81.10 0.66 80.020.62 71.790.51 81.740.66\nTable 4: Comparison of results for eight-state predictions on the test sets of NetSurfP-2.0,\nCB513, CASP12, and TS115 datasets. The top performance for F1 is denoted in bold and\nimprovement over the best baseline, and is statistically significant (t-test with p-value at 0.05\nsignificance level).\nNetSurfP-2.0 CB513 CASP12 TS115\nAccuracy (%) F1 Accuracy (%) F1 Accuracy (%) F1 Accuracy (%) F1\nDCRNN 15.47 0.50 16.40 0.48 12.85 0.46 14.29 0.48\nDeepACLSTM 15.88 0.47 17.07 0.45 12.87 0.45 14.24 0.46\nDistilProtBert 69.50 0.62 65.560.58 58.780.51 70.21 0.61\nSSRGNet 70.230.63 65.67 0.57 58.61 0.50 70.33 0.60\nSSGNet 69.93 0.62 66.060.58 59.27 0.50 71.140.62\nacross all test sets. This marked difference in performance accentuates the im-\nportance of incorporating structural insights through the SSRGNet encoder for\nmore accurate protein secondary structure prediction. This endeavor under-\nscores the importance of enriching the protein residue graph, as it facilitates a\nmore nuanced understanding and representation of the intricate relationships\nand interactions that govern protein structure. By doing so, it enables the de-\nvelopment of more robust and accurate models for protein secondary structure\n27\nprediction, ultimately contributing to advancing the state of the art in this do-\nmain. Through this enriched representation, our approach leverages a holistic\nunderstanding of the multi-faceted interactions at play, thereby significantly en-\nhancing the predictive capabilities of the SSRGNet framework in discerning the\nprotein\u2019s secondary structure.\nThelimitedimprovementinF1score(1%)comparedtoDistilProtBertalone\ncanbeattributedtothedominanceofsequence-basedfeaturesinDistilProtBert,\nwhich already captures essential secondary structure information, reducing the\nadditional benefit of structural encoding. The R-GCN\u2019s contribution may be\nconstrained by noisy or incomplete PDB data, limited edge definitions in the\nprotein residue graph, or an inability to capture long-range dependencies effec-\ntively. Additionally, the fusion strategy might not optimally integrate sequence\nand structural features, leading to redundancy rather than complementarity.\nThe data imbalance, particularly for underrepresented classes like\u03c0-helices and\nisolated\u03b2-bridges, further limits the structural encoder\u2019s impact. Also, since\nthat our experimental setup utilized a batch size of 1 during training. This\nchoice was primarily dictated by memory and computational constraints given\nthe model complexity and dataset size. Consequently, the reported performance\nmetrics might not fully reflect the potential of our approach. We believe that\ntraining with a larger batch size would likely lead to improved generalization\nand more competitive scores.\nThe confusion matrices presented in Figures {4a, 4b, 4c, 4d} illustrate the\nresults of SSRGNet on the test sets of NetSurfP-2.0, CB513, CASP12, and\nTS115. Each entry in these matrices denotes how frequently a particular true\ncategory is misclassified into another category. One can observe that the\u03c0-helix\n(I), which is the least common class, is predominantly misclassified as an\u03b1-helix\n(H). The isolated\u03b2bridge (B), another rare class, is primarily misidentified as\neither an irregular structure (L) or an extended strand (E). Furthermore, the\n3-10 helix (G) is often mistaken for an irregular structure (L) or an\u03b1-helix (H)\nacross all datasets. Lastly, the Bend (S) class is most commonly misclassified as\nan irregular structure (L). It\u2019s evident that more attention needs to be directed\n28\n(a) NetSurfP-2.0\n (b) CASP12\n(c) CB513\n (d) TS115\nFigure 4: Confusion matrices for SSRGNet on different datasets for eight-state prediction.\ntoward enhancing the classification accuracy of these underrepresented classes\nin subsequent research efforts. These misclassifications likely arise due to the\nscarcity of training examples for\u03c0-helices, isolated\u03b2-bridges, and 3-10 helices in\nthe dataset, making it difficult for the model to learn clear decision boundaries\nfor these classes. Additionally, their structural similarities to more frequent\nclasses like\u03b1-helices and extended strands lead to higher confusion.\nAblation StudyAn ablation study was conducted to assess the perfor-\nmance of our SSRGNet framework across different fusion techniques. The pri-\nmary objective of this experiment was to evaluate the benefits of employing var-\n29\n(a) Accuracy\n (b) F1-scores\nFigure 5: Results for the Ablation Study on different evaluation metrics. Comparison of Loss,\nAccuracy and F1-scores for different fusion methods on various datasets.\nious fusion strategies in enhancing the performance of the SSRGNet framework.\nTo conduct the ablation study, we systematically varied the fusion techniques\nwhile keeping other parameters constant, which allowed us to isolate and un-\nderstand the impact of each fusion method on the overall performance of the\nframework. We evaluated each configuration on the same test set of proteins\n(viz.the test set of NetSurfP-2.0, CB513, CASP12, TS115 dataset) and used\nthe same evaluation metrics as described earlier. The results of the ablation\nstudy, presented in Figure 5a, 5b}, show a trend of performance as we use dif-\nferent fusion methods. First, upon comparing Accuracy values, it is evident\nthat parallel fusion, while simple in concept, is remarkably effective, surpassing\nthe other two fusion strategies for all the test sets. Similarly, for F1-scores, we\nobserve a similar pattern , where parallel fusion improves over serial and cross\nfusion on majority of the test set. Due to this pattern, we propose parallel fusion\nas our final strategy for encoding sequential and structural information of pro-\nteins. Parallel fusion likely performs better because it preserves both sequence\nand structural information without forcing the model to prioritize one over the\nother. Serial fusion may lead to information loss as it combines features in a\nsequential manner, while cross fusion introduces additional complexity without\nsignificant performance gains. The ability of parallel fusion to retain comple-\n30\nmentary information from both modalities likely contributes to its effectiveness.\nTable 5: Comparison of ablation study results for eight-state predictions on the CB513,\nCASP12, and TS115 datasets. Full SSRGNet denotes the full fusion model, while R1 (Se-\nquential), R2 (Spatial Proximity), and R3 (Local Environment) denote models using only the\ncorresponding relation.\nCB513 CASP12 TS115\nModel Variant Accuracy (%) F1 Accuracy (%) F1 Accuracy (%) F1\nSSRGNet 65.67 0.57 58.62 0.50 70.33 0.60\nR1 (Sequential Relationship) 65.61 0.56 58.60 0.49 70.30 0.58\nR2 (Spatial Proximity) 65.68 0.58 58.50 0.51 70.36 0.60\nR3 (Local Environment) 65.70 0.57 58.65 0.50 70.44 0.60\nTable5presentsanablationstudyassessingtheimpactofdifferentrelational\nfactors\u2014Sequential Relationship (R1), Spatial Proximity (R2), and Local En-\nvironment (R3)\u2014on three-state predictions across CB513, CASP12, and TS115\ndatasets. The results demonstrate that each relation contributes comparably\nto the overall performance, with minor variations in accuracy and F1 scores.\nNotably, the full SSRGNet model does not significantly outperform the individ-\nual relations, suggesting that each relation independently captures meaningful\nstructural information. However, the small differences highlight the comple-\nmentary nature of these relational cues, justifying their inclusion in the model.\nThe study underscores the necessity of these three relations in constructing an\neffective graph representation.\n6. Conclusion\nIn conclusion, this study addresses the challenging task of predicting sec-\nondary structures from protein primary sequences, a crucial step towards un-\nderstanding tertiary structures and unraveling insights into protein functions\nand relationships. Existing methods often rely solely on unlabeled amino acid\nsequences, neglecting the valuable 3D structural data that profoundly influences\n31\nprotein functionality. To bridge this gap, we introduce protein residue graphs\nand incorporate various forms of sequential and structural connections to cap-\nture richer spatial information.\nOur approach combines Graph Neural Networks (GNNs) and Language\nModels (LMs). We utilize a pre-trained transformer-based protein language\nmodel to encode amino acid sequences and leverage message-passing mecha-\nnisms like GCN and R-GCN to capture the geometric properties of protein\nstructures. By applying convolution within a node\u2019s local region and consid-\nering relationships, we stack multiple convolutional layers to efficiently extract\ninsights from the protein\u2019s spatial graph, uncovering intricate interconnections\nand dependencies in its structural arrangement.\nFor evaluating our model\u2019s performance, we use the NetSurfP-2.0 train-\ning dataset, which provides secondary structure annotations in both 3- and\n8-states. Our extensive experiments demonstrate that our proposed model,\ncalled SSRGNet, achieves impressive F1-scores of 61.32%, 51.22%, and 65.45%\non publicly available test datasets, specifically CB513, TS115, and CASP12, as\nevaluated by the Q3 metric. This highlights the effectiveness of our approach\nin accurately predicting protein secondary structures and suggests its potential\nutility in various protein-related applications.\nIn the future, we aim to address the several limitations identified in our cur-\nrent work. First, we plan to integrate external structural datasets to pre-train\nthe structure-based encoders, such as R-GCN or transformer-based models, us-\ning contrastive learning. This approach can enable the encoders to learn a more\ndiverse and robust set of structural patterns, improving generalization even in\ncases of sparse or imperfect 3D data. Additionally, pretraining on protein se-\nquencescanhelptheseencodersbettercapturelong-rangedependenciesthatare\notherwise difficult to model through local message passing alone. We also aim\nto refine the current parallel fusion strategy by introducing adaptive gating or\nattention-based mechanisms to dynamically weigh the contributions from both\nsequence and structural modalities.\n32\n7. Ethical Considerations\nWe make use of publicly available datasets. Without violating any copyright\nissues, we followed the policies of the datasets we used.\nReferences\n[1] B. Ma, Novor: real-time peptide de novo sequencing software, Journal of\nthe American Society for Mass Spectrometry 26 (11) (2015) 1885\u20131894.\n[2] B.Ma, R.Johnson, Denovo sequencingandhomologysearching, Molecular\n& cellular proteomics 11 (2).\n[3] M. E. Noble, J. A. Endicott, L. N. Johnson, Protein kinase inhibitors:\ninsights into drug design from structure, Science 303 (5665) (2004) 1800\u2013\n1805.\n[4] K. VV\u00dcTHRICH, Protein structure determination in solution by nuclear\nmagnetic resonance spectroscopy, Science 243 (1989) 4887.\n[5] R.Y.-R.Wang,M.Kudryashev,X.Li,E.H.Egelman,M.Basler,Y.Cheng,\nD. Baker, F. DiMaio, De novo protein structure determination from near-\natomic-resolution cryo-em maps, Nature methods 12 (4) (2015) 335\u2013338.\n[6] D. J. Mandell, T. Kortemme, Computer-aided design of functional protein\ninteractions, Nature chemical biology 5 (11) (2009) 797\u2013807.\n[7] J. Zhou, O. Troyanskaya, Deep supervised and convolutional generative\nstochastic network for protein secondary structure prediction, in: Interna-\ntional conference on machine learning, PMLR, 2014, pp. 745\u2013753.\n[8] S. Wang, J. Peng, J. Ma, J. Xu, Protein secondary structure prediction\nusing deep convolutional neural fields, Scientific reports 6 (1) (2016) 1\u201311.\n[9] Z. Li, Y. Yu, Protein secondary structure prediction using cascaded convo-\nlutional and recurrent neural networks, arXiv preprint arXiv:1604.07176.\n33\n[10] S. K. S\u00f8nderby, O. Winther, Protein secondary structure prediction with\nlong short term memory networks, arXiv preprint arXiv:1412.7828.\n[11] Q.Wang, B.Wang, Z.Xu, J.Wu, P.Zhao, Z.Li, S.Wang, J.Huang, S.Cui,\nPssm-distil: Protein secondary structure prediction (pssp) on low-quality\npssm by knowledge distillation with contrastive learning, Proceedings of\nthe AAAI Conference on Artificial Intelligence 35 (1) (2021) 617\u2013625.doi:\n10.1609/aaai.v35i1.16141.\nURLhttps://ojs.aaai.org/index.php/AAAI/article/view/16141\n[12] A. Elnaggar, M. Heinzinger, C. Dallago, G. Rehawi, Y. Wang, L. Jones,\nT. Gibbs, T. Feher, C. Angerer, M. Steinegger, D. Bhowmik, B. Rost, Prot-\ntrans: Toward understanding the language of life through self-supervised\nlearning, IEEE Transactions on Pattern Analysis and Machine Intelligence\n44 (10) (2022) 7112\u20137127.doi:10.1109/TPAMI.2021.3095381.\n[13] Y. Geffen, Y. Ofran, R. Unger, Distilprotbert: a distilled protein language\nmodelusedtodistinguishbetweenrealproteinsandtheirrandomlyshuffled\ncounterparts, Bioinformatics 38 (Supplement_2) (2022) ii95\u2013ii98.\n[14] A. Rives, J. Meier, T. Sercu, S. Goyal, Z. Lin, J. Liu, D. Guo,\nM. Ott, C. L. Zitnick, J. Ma, R. Fergus, Biological structure and func-\ntion emerge from scaling unsupervised learning to 250 million protein\nsequences, Proceedings of the National Academy of Sciences 118 (15)\n(2021) e2016239118.arXiv:https://www.pnas.org/doi/pdf/10.1073/\npnas.2016239118,doi:10.1073/pnas.2016239118.\nURLhttps://www.pnas.org/doi/abs/10.1073/pnas.2016239118\n[15] T. H. Nahid, F. A. Jui, P. C. Shill, Protein secondary structure prediction\nusing graph neural network, in: 2021 5th International Conference on Elec-\ntrical Information and Communication Technology (EICT), 2021, pp. 1\u20136.\ndoi:10.1109/EICT54103.2021.9733590.\n[16] H. Jin, W. Du, J. Gu, T. Zhang, X. Shi, Combining gcn and bi-lstm\nfor protein secondary structure prediction, in: 2021 IEEE International\n34\nConference on Bioinformatics and Biomedicine (BIBM), 2021, pp. 44\u201349.\ndoi:10.1109/BIBM52615.2021.9669366.\n[17] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger,\nK. Tunyasuvunakool, R. Bates, A. \u017d\u00eddek, A. Potapenko, et al., Highly\naccurate protein structure prediction with alphafold, Nature 596 (7873)\n(2021) 583\u2013589.\n[18] P. Prevelige Jr, G. D. Fasman, Chou-fasman prediction of the secondary\nstructure of proteins: The chou-fasman-prevelige algorithm, in: Prediction\nof protein structure and the principles of protein conformation, Springer,\n1989, pp. 391\u2013416.\n[19] J. Garnier, B. Robson, The gor method for predicting secondary structures\nin proteins, in: Prediction of protein structure and the principles of protein\nconformation, Springer, 1989, pp. 417\u2013465.\n[20] W. Kabsch, C. Sander, How good are predictions of protein secondary\nstructure?, FEBS letters 155 (2) (1983) 179\u2013182.\n[21] M. N. Nguyen, J. C. Rajapakse, Multi-class support vector machines for\nprotein secondary structure prediction, Genome Informatics 14 (2003) 218\u2013\n227.\n[22] Z. Shuai-yan, L. Yi-hui, A novel radical group encoding method for protein\nsecondary structure prediction, in: 2017 IEEE 2nd International Confer-\nence on Big Data Analysis (ICBDA), IEEE, 2017, pp. 939\u2013943.\n[23] K. Asai, S. Hayamizu, K. Handa, Prediction of protein secondary structure\nby the hidden markov model, Bioinformatics 9 (2) (1993) 141\u2013146.\n[24] Z. Aydin, Y. Altunbasak, M. Borodovsky, Protein secondary structure pre-\ndiction for a single-sequence using hidden semi-markov models, BMC bioin-\nformatics 7 (1) (2006) 1\u201315.\n35\n[25] H.Bouziane, B.Messabih, A.Chouarfia, Effectofsimpleensemblemethods\non protein secondary structure prediction, Soft Computing 19 (6) (2015)\n1663\u20131678.\n[26] D. T. Jones, Protein secondary structure prediction based on position-\nspecific scoring matrices, Journal of molecular biology 292 (2) (1999) 195\u2013\n202.\n[27] A. Ghosh, B. Parai, Protein secondary structure prediction using distance\nbased classifiers, International journal of approximate reasoning 47 (1)\n(2008) 37\u201344.\n[28] S. Saha, A. Ekbal, S. Sharma, S. Bandyopadhyay, U. Maulik, Protein sec-\nondary structure prediction using machine learning, in: Intelligent Infor-\nmatics: Proceedings of the International Symposium on Intelligent Infor-\nmatics ISI\u201912 Held at August 4-5 2012, Chennai, India, Springer, 2013, pp.\n57\u201363.\n[29] L. H. Holley, M. Karplus, Protein secondary structure prediction with a\nneural network., Proceedings of the National Academy of Sciences 86 (1)\n(1989) 152\u2013156.\n[30] Y. Liu, Y. Ma, J. Cheng, A novel group template pattern classifiers (gtpcs)\nmethod in protein secondary structure prediction, in: 2017 3rd IEEE In-\nternational Conference on Computer and Communications (ICCC), IEEE,\n2017, pp. 2713\u20132717.\n[31] A. Busia, N. Jaitly, Next-step conditioned deep convolutional neural net-\nworks improve protein secondary structure prediction, arXiv preprint\narXiv:1702.03865.\n[32] Y. Guo, B. Wang, W. Li, B. Yang, Protein secondary structure prediction\nimproved by recurrent neural networks integrated with two-dimensional\nconvolutionalneuralnetworks,Journalofbioinformaticsandcomputational\nbiology 16 (05) (2018) 1850021.\n36\n[33] P. Kumar, S. Bankapur, N. Patil, An enhanced protein secondary structure\nprediction using deep learning framework on hybrid profile based features,\nApplied Soft Computing 86 (2020) 105926.\n[34] C. N. Magnan, P. Baldi, Sspro/accpro 5: almost perfect prediction of pro-\ntein secondary structure and relative solvent accessibility using profiles,\nmachine learning and structural similarity, Bioinformatics 30 (18) (2014)\n2592\u20132597.\n[35] S. Wang, S. Weng, J. Ma, Q. Tang, Deepcnf-d: predicting protein or-\nder/disorder regions by weighted deep convolutional neural fields, Interna-\ntional journal of molecular sciences 16 (8) (2015) 17315\u201317330.\n[36] Y. Wang, H. Mao, Z. Yi, Protein secondary structure prediction by using\ndeep learning method, Knowledge-Based Systems 118 (2017) 115\u2013123.\n[37] Y. Guo, W. Li, B. Wang, H. Liu, D. Zhou, Deepaclstm: deep asymmetric\nconvolutional long short-term memory neural models for protein secondary\nstructure prediction, BMC bioinformatics 20 (1) (2019) 1\u201312.\n[38] R. Heffernan, Y. Yang, K. Paliwal, Y. Zhou, Capturing non-local interac-\ntions by long short-term memory bidirectional recurrent neural networks\nfor improving prediction of protein secondary structure, backbone angles,\ncontact numbers and solvent accessibility, Bioinformatics 33 (18) (2017)\n2842\u20132849.\n[39] C. Fang, Y. Shang, D. Xu, Mufold-ss: new deep inception-inside-inception\nnetworks for protein secondary structure prediction, Proteins: Structure,\nFunction, and Bioinformatics 86 (5) (2018) 592\u2013598.\n[40] W. Yang, Z. Hu, L. Zhou, Y. Jin, Protein secondary structure prediction\nusingalightweightconvolutionalnetworkandlabeldistributionawaremar-\ngin loss, Knowledge-Based Systems 237 (2022) 107771.\n[41] J. Hanson, K. Paliwal, T. Litfin, Y. Yang, Y. Zhou, Improving prediction\nof protein secondary structure, backbone angles, solvent accessibility and\n37\ncontact numbers by using predicted contact maps and an ensemble of re-\ncurrent and residual convolutional neural networks, Bioinformatics 35 (14)\n(2019) 2403\u20132410.\n[42] M. S. Klausen, M. C. Jespersen, H. Nielsen, K. K. Jensen, V. I. Jurtz,\nC. K. Soenderby, M. O. A. Sommer, O. Winther, M. Nielsen, B. Petersen,\net al., Netsurfp-2.0: Improved prediction of protein structural features by\nintegrateddeeplearning, Proteins: Structure, Function, andBioinformatics\n87 (6) (2019) 520\u2013527.\n[43] J. Zhou, H. Wang, Z. Zhao, R. Xu, Q. Lu, Cnnh_pss: protein 8-class sec-\nondary structure prediction by convolutional neural network with highway,\nBMC bioinformatics 19 (4) (2018) 99\u2013109.\n[44] D. P. Ismi, R. Pulungan, et al., Self-attention and asymmetric multi-layer\nperceptron-gated recurrent unit blocks for protein secondary structure pre-\ndiction, Applied Soft Computing 159 (2024) 111604.\n[45] A. Rives, J. Meier, T. Sercu, S. Goyal, Z. Lin, J. Liu, D. Guo, M. Ott, C. L.\nZitnick, J. Ma, et al., Biological structure and function emerge from scaling\nunsupervised learning to 250 million protein sequences, Proceedings of the\nNational Academy of Sciences 118 (15) (2021) e2016239118.\n[46] N. Ferruz, S.Schmidt, B. H\u00f6cker, Protgpt2is a deep unsupervisedlanguage\nmodel for protein design, Nature communications 13 (1) (2022) 4348.\n[47] Y. Chen, G. Chen, C. Y.-C. Chen, Mftrans: A multi-feature transformer\nnetwork for protein secondary structure prediction, International Journal\nof Biological Macromolecules 267 (2024) 131311.\n[48] G. Derevyanko, S. Grudinin, Y. Bengio, G. Lamoureux, Deep convolutional\nnetworks for quality assessment of protein folds, Bioinformatics 34 (23)\n(2018) 4046\u20134053.\n[49] V. Gligorijevi\u0107, P. D. Renfrew, T. Kosciolek, J. K. Leman, D. Berenberg,\nT. Vatanen, C. Chandler, B. C. Taylor, I. M. Fisk, H. Vlamakis, et al.,\n38\nStructure-based protein function prediction using graph convolutional net-\nworks, Nature communications 12 (1) (2021) 3168.\n[50] F. Baldassarre, D. Men\u00e9ndez Hurtado, A. Elofsson, H. Azizpour, Graphqa:\nprotein model quality assessment using graph convolutional networks,\nBioinformatics 37 (3) (2021) 360\u2013366.\n[51] L. Wang, H. Liu, Y. Liu, J. Kurtin, S. Ji, Learning protein representations\nvia complete 3d graph networks, arXiv preprint arXiv:2207.12600.\n[52] B. Jing, S. Eismann, P. Suriana, R. J. Townshend, R. Dror, Learning\nfrom protein structure with geometric vector perceptrons, arXiv preprint\narXiv:2009.01411.\n[53] D. Kihara, The effect of long-range interactions on the secondary structure\nformation of proteins, Protein Science 14 (8) (2005) 1955\u20131963.\n[54] T. H. Nahid, F. A. Jui, P. C. Shill, Protein secondary structure predic-\ntion using graph neural network, in: 2021 5th International Conference\non Electrical Information and Communication Technology (EICT), IEEE,\n2021, pp. 1\u20136.\n[55] Y. G\u00f6rmez, M. Sabzekar, Z. Ayd\u0131n, Igpred: Combination of convolu-\ntional neural and graph convolutional networks for protein secondary struc-\nture prediction, Proteins: Structure, Function, and Bioinformatics 89 (10)\n(2021) 1277\u20131288.\n[56] Z. Zhang, M. Xu, A. Jamasb, V. Chenthamarakshan, A. Lozano, P. Das,\nJ.Tang, Proteinrepresentationlearningbygeometricstructurepretraining,\narXiv preprint arXiv:2203.06125.\n[57] T. N. Kipf, M. Welling, Semi-supervised classification with graph convolu-\ntional networks, arXiv preprint arXiv:1609.02907.\n[58] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov,\nM. Welling, Modeling relational data with graph convolutional networks,\n39\nin: The Semantic Web: 15th International Conference, ESWC 2018, Her-\naklion, Crete, Greece, June 3\u20137, 2018, Proceedings 15, Springer, 2018, pp.\n593\u2013607.\n[59] H. M. Berman, J. Westbrook, Z. Feng, G. Gilliland, T. N. Bhat, H. Weis-\nsig, I. N. Shindyalov, P. E. Bourne, The protein data bank, Nucleic acids\nresearch 28 (1) (2000) 235\u2013242.\n[60] Y. Yang, J. Gao, J. Wang, R. Heffernan, J. Hanson, K. Paliwal, Y. Zhou,\nSixty-five years of the long march in protein secondary structure prediction:\nthe final stretch?, Briefings in bioinformatics 19 (3) (2018) 482\u2013494.\n[61] J. A. Cuff, G. J. Barton, Evaluation and improvement of multiple sequence\nmethods for protein secondary structure prediction, Proteins: Structure,\nFunction, and Bioinformatics 34 (4) (1999) 508\u2013519.\n[62] L. A. Abriata, G. E. Tam\u00f2, B. Monastyrskyy, A. Kryshtafovych,\nM. Dal Peraro, Assessment of hard target modeling in casp12 reveals an\nemerging role of alignment-based contact prediction methods, Proteins:\nStructure, Function, and Bioinformatics 86 (2018) 97\u2013112.\n[63] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., Pytorch: An imperative\nstyle, high-performance deep learning library, Advances in neural informa-\ntion processing systems 32.\n[64] I. Loshchilov, F. Hutter, Decoupled weight decay regularization, arXiv\npreprint arXiv:1711.05101.\n[65] Y. Ma, Z. Jiang, H. Zhang, F. Xie, Y. Zheng, H. Shi, Y. Zhao, J. Shi, Gen-\nerating region proposals for histopathological whole slide image retrieval,\nComputer methods and programs in biomedicine 159 (2018) 1\u201310.\n40\n",
    "title": "Protein Secondary Structure Prediction Using 3D Graphs and Relation-Aware Message Passing Transformers",
    "authors": [
      "Disha Varshney",
      "Samarth Garg",
      "Sarthak Tyagi",
      "Deeksha Varshney",
      "Nayan Deep",
      "Asif Ekbal"
    ],
    "published": "2025-11-17",
    "arxiv_id": "2511.13685v1",
    "num_pages": 40,
    "num_chars": 74729
  },
  {
    "text": "Highlights\nGS-Light: Training-FreeMulti-ViewExtensionofIC-LightforTex-\ntual Position-Aware Scene Relighting\nJiangnan Ye, Jiedong Zhuang, Lianrui Mu, Wenjie Zheng, Jiaqi Hu, Xingze\nZou, Jing Wang, Haoji Hu\n\u2022GS-Light enables fast, position-aware multi-view relighting in 3DGS.\n\u2022Lighting priors extracted via LVLM + geometry & semantics.\n\u2022Cross-View Attention enforce multi-view consistency in relit outputs.\n\u2022Outperforms baselines with better quality in diverse scenes.arXiv:2511.13684v1  [cs.CV]  17 Nov 2025\nGS-Light: Training-Free Multi-View Extension of\nIC-Light for Textual Position-Aware Scene Relighting\nJiangnan Yea, Jiedong Zhuanga, Lianrui Mua, Wenjie Zhenga, Jiaqi Hua,\nXingze Zoua, Jing Wanga, Haoji Hua,\u2217\naCollege of Information Science and Electronic Engineering, Zhejiang University,\nHangzhou, 310027, Zhejiang, China\nAbstract\nWeintroduceGS-Light,anefficient,textualposition-awarepipelinefortext-\nguided relighting of 3D scenes represented via Gaussian Splatting (3DGS).\nGS-Light implements atraining-free extensionof a single-input diffusion\nmodel to handle multi-view inputs. Given a user prompt that may specify\nlighting direction, color, intensity, or reference objects, we employ a large\nvision-language model (LVLM) toparse the prompt into lighting pri-\nors. Using off-the-shelf estimators for geometry and semantics (depth, sur-\nface normals, and semantic segmentation), we fuse these lighting priors with\nview-geometry constraints to compute illumination maps and generate ini-\ntial latent codes for each view. These meticulously derived init latents guide\nthe diffusion model to generate relighting outputs that more accurately re-\nflect user expectations, especially in terms of lighting direction. By feeding\nmulti-view rendered images, along with the init latents, into our multi-view\nrelighting model, we produce high-fidelity, artistically relit images. Finally,\nwe fine-tune the 3DGS scene with the relit appearance to obtain a fully relit\n3D scene. We evaluate GS-Light on both indoor and outdoor scenes, com-\nparing it to state-of-the-art baselines including per-view relighting, video\nrelighting, and scene editing methods. Using quantitative metrics (multi-\nview consistency, imaging quality, aesthetic score, semantic similarity, etc.)\n\u2217Corresponding author.\n\u2217\u2217Email addresses: jiangnan_ye@zju.edu.cn (Jiangnan Ye), zhuangjiedong@zju.edu.cn\n(Jiedong Zhuang), mulianrui@zju.edu.cn (Lianrui Mu), wenjie_zheng@zju.edu.cn\n(Wenjie Zheng), jiaqi_hu@zju.edu.cn (Jiaqi Hu), zeezou@zju.edu.cn (Xingze Zou),\nj_wang@zju.edu.cn (Jing Wang), haoji_hu@zju.edu.cn (Haoji Hu)\nand qualitative assessment (user studies), GS-Light demonstrates consistent\nimprovements over baselines. Code and assets will be made available upon\npublication.\nKeywords:Gaussian Splatting, Diffusion models, Textual relighting\nRelit Video Relit Gasussian Splatting\nSource Ours RelightVid Lumen Ours DGE EditSplat IN2N IGS2GS IGS2GS-IC\n\"bear, forest, sunlight filtering through trees, natural lighting, warm atmosphere,light from top\"\n\"portrait, detailed face, sunshine from window, warm atmosphere,light from the left\"\n\"field, outdoor, starry night, silver moonlight, tranquil tone\"\n\"bicycle, sunset, orange glow, long shadows\"\nFigure 1: Qualitative comparison between ourGS-Lightand other prior work on re-\nlighting videos or gaussian splatting scenes. Our GS-Light produces results with higher\nfidelity and aesthetic, improved multi-view consistency, stronger semantic relevance, and\nenhanced controllability of lighting direction through text prompts.\n2\n1. Introduction\nRelighting a 3D scene \u2014 changing its illumination while preserving its\ngeometry, material attributes, and view coherence \u2014 is a central problem in\ncomputer graphics and vision, with applications spanning augmented/virtual\nreality, film production, and content creation. Modern neural scene represen-\ntation methods such as Neural Radiance Fields (NeRF [1]) and 3D Gaussian\nSplatting (3DGS [2]) have enabled impressive novel view synthesis, and sep-\narately, generative 2D image editing / relighting models (especially diffusion-\nbased ones, e.g. IC-Light [3]) have delivered strong image-level lighting edits.\nHowever, combining these two worlds to achievemulti-view consistent,tex-\ntually controllable, andposition-awarerelighting remains nontrivial. Key\nchallenges include inferring plausible scene lighting from text, ensuring con-\nsistency over views and efficiently dealing with geometry/light interaction\n(shadows, normals, occlusion).\nWith the advent of IC-Light, its powerful 2D image relighting capability\n\u2014 covering illumination harmonization, identity preservation, and lighting\nartistry \u2014 has been rapidly adopted in a wide range of AIGC-related down-\nstream applications. However, since ICLight is a model fine-tuned from Sta-\nble Diffusion [4] v1.5, it inherently introduces 3D inconsistency in multi-view\nediting tasks, posing challenges to lifting its prior knowledge to 3D scene\nediting. In addition, our experiments reveal that current SD-based models\nand multimodal models are generally insensitive to positional information\nin textual prompts (e.g.,left,right), which further hinders the alignment\nbetween relighting results and user intent. Although some visual grounding\nworks (e.g., VPP-LLaVA [5]) strengthen training in this aspect and achieve\npromising results, experiments show that they mainly learn patterns between\nkey location-related texts and the ground truth in the limited training data,\nrather than developing a semantic understanding of abstract spatial informa-\ntion. This is reflected in their poor generalization performance when tested\non out-of-domain data.\nTo address the aforementioned challenges, we proposeGS-Light, a light-\nweight iterative Gaussian Splatting optimization pipeline that bridges text-\nguided2Drelightingdiffusionmodelswith3Dscenerepresentationstoachieve\nposition-aware,multi-view consistent relighting. Our framework is\nbuilt upon two key components. The first is thePosition-Align Mod-\nule (PAM), which aligns positional semantic information between the in-\nputs\u2014comprising scene-rendered images and user editing instructions\u2014and\n3\nthe relighting model, ensuring accurate interpretation of spatial cues. The\nsecond isMV-ICLight, a multi-view relighting model based on IC-Light,\nspecifically designed to enforce cross-view consistency, enabling coherent and\nfaithful relighting across different viewpoints.\nPAMis primarily responsible for the preprocessing of input data. It\nextracts information from both image and text modalities and further es-\ntimates the scene\u2019s geometry and lighting, generating the position-aligned\nillumination map, which is required by the subsequent module. Given a\nuser prompt, we employ an LVLM (e.g., Qwen2.5-vl [6]) to extract lighting\npriors such as direction, color temperature or hue, intensity, and optionally\nreference objects from the given rendered images and input instructions, us-\ning a constrained question\u2013answer template to ensure outputs are limited\nto relighting-relevant parameters. For each view, we further estimate scene\ngeometry and semantics via off-the-shelf predictors, including depth (e.g.,\nVGGT [7]), surface normals (e.g., StableNormal [8]), and semantic masks\n(e.g., LangSAM [9]), which provide cues for shading, occlusion, shadow cast-\ning, and object identification. These priors are then fused with geometry\nconstraints to compute per-pixel illumination maps, using a simple Phong\ndiffuse illumination model [10], which are further initialized as the init la-\ntents of the diffusion model denoising process, enabling fine-grained control\nover lighting in the editing results.\nMV-ICLightis a variant implementation of IC-Light\u2019s cross-view at-\ntention mechanism, enabling a single-image editing model to perform edits\nconsistently across multiple views. Inspired by DGE [11], we introduce an\nimproved epipolar constraint to realize a memory-efficient, training-free ex-\ntension of the relighting model from single-view input to multi-view input.\nConditioned on the illumination maps and aligned latents, we then generate\nper-view relit images that maintain illumination consistency across views.\nFinally, these relit images are integrated into the 3D Gaussian Splatting rep-\nresentation, producing relit scenes that remain faithful to user instructions\nwhile ensuring multi-view coherence.\nGS-Light is designed to achieve high-quality relighting of 3DGS scenes,\nfaithfully adhering to the user\u2019s editing intent. In tests on both indoor and\noutdoor datasets, our method demonstrates clear competitiveness against\nvarious state-of-the-art approaches in terms of reconstruction consistency,\nsemanticeditingsimilarity, andusersubjectiveevaluation. Ourcontributions\nare:\n4\n\u2022We propose GS-Light, the first efficient method that supports textual,\nposition-aware relighting over Gaussian Splatting scenes with multi-\nviewconsistency. Ittakesaround3minutestogenerateper-scenepriors\nonce and around 3 minutes for each scene\u2019s relighting.\n\u2022WedevelopalightingpriorextractionschemeviaquestioninganLVLM,\ncombined with geometry & semantic estimators, to produce view-wise\nillumination and latent initialization.\n\u2022We enforce viewwise consistency using advanced epipolar constraints,\nensuring coherence in relit views and latent space. It worth noting that\nourextensionframeworkiscompatiblewithotherdiffusionmodels(e.g.,\nDiT [12]), which makes sure the great scalability of our method.\n\u2022We validate GS-Light across a variety of scenes (indoor and outdoor),\ndemonstrating superior performance vs baselines in both objective and\nperceptual metrics, while maintaining fast inference.\n2. Related Work\n2.1. 2D Illumination Harmonization / Relighting\nImage relighting methods based on deep neural networks first became\nmainstream. To enable image-based relighting, Light Stage [13] was intro-\nduced to capture the reflectance functions of human faces, while [14] signif-\nicantly reduced the number of required input images. SfSNet [15] leveraged\ndeep neural networks to model 3D faces and decompose material properties\nfor portrait relighting. [16], using a mass transport approach, achieved por-\ntrait illumination transfer. [17] constructed training pairs using One-Light-\nAt-a-Time (OLAT) scans, and [18] further divided the process into diffuse\nrendering and non-diffuse residual stages. Going a step further, Switch-\nLight[19]decomposedsourceimagesintointrinsiccomponents,andpredicted\nthem with separate networks.\nWith the development of diffusion models [20, 4, 21, 12], fine-tuning pre-\ntrained diffusion models for image-to-image tasks such as editing [22, 23, 24],\nstyle transfer [25, 26, 27, 28], geometry estimation [8, 29, 30], and relighting\nhas proven to be an effective approach. Relightful Harmonization [31] used\nthe background image as a condition to generate harmonized foreground illu-\nmination. IC-Light[3], byimposingtheprincipleofconsistentlighttransport\n5\nand leveraging a carefully curated large-scale dataset, fine-tuned Stable Dif-\nfusion [4] v1.5 to achieve state-of-the-art performance in 2D image relighting.\n2.2. 3DGS Editing / Stylization Models\nOne category of methods directly learns features in 3D space and con-\nstrains the loss of the target task through specific regularization terms. [32]\nemploys a texture-guided control mechanism to directly constrain the pa-\nrameters of Gaussians. ARF [33] proposes the NNFM loss, which provides\nbetter 3D consistency compared with the widely used Gram-matrix-based\nloss [34] in style transfer tasks. Building on this, G-Style [35] introduces a\nCLIP similarity term and a total variation term, while StyleGaussian [36]\nembeds VGG features into a radiance field.\nAnother category of methods leverages the strong priors of 2D editing\nmodels to assist in 3D scene editing, mainly addressing inconsistencies in\nmulti-view editing. Instruct-NeRF2NeRF [37] first introduces the Dataset\nUpdate strategy to ensure convergence during scene optimization. Based\non this, ViCA-NeRF [38] adopts multi-view image and depth warping &\nmixup methods to provide 2D models with richer 3D information. Con-\nsistDreamer [39] proposes using 3D-consistent structured noise in 2D dif-\nfusion denoising, which yields better multi-view consistency. InstantStyle-\nGaussian [40] transfers the Instruct-NeRF2NeRF [37] approach to the style\ntransfer task. ProEdit [41] employs a progressive editing strategy to reduce\nthe feasible space of text-aligned editing tasks, thereby mitigating multi-\nview inconsistency. DGE [11] directly modifies the 2D model\u2019s self-attention\ninto cross-view self-attention across keyframes and further applies epipolar-\nconstrained matching to propagate attention results into non-keyframe fea-\nture maps, significantly reducing memory demand during inference. Ed-\nitSplat [42] introduces multi-view fusion guidance, which aligns multi-view\ninformationwithtextpromptsandsourceimagestoensuremulti-viewconsis-\ntency. By collecting a large scale illumination video dataset, RelightVid [43]\nfinetuned IC-Light to adapted to video relighting task, while Lumen [44]\ntraining an end to end DiT-based model.\nOur work builds on the improved epipolar-constrained matching scheme\nof DGE [11] to achieve more consistent editing results under reduced memory\nusage. To further exploit the powerful relighting capability of IC-Light, we\nadapt IC-Light into the DGE framework without modifying or fine-tuning\nits weights, enabling a high-quality, multi-view consistent relighting module\nacross multiple images, which we callMV-ICLight.\n6\nThese methods work well on the multiview images or videos editing or\nrelighting though, however, they can hardly tackle the challenge about the\nunfaithfulresultrelatedtothepositionalconcept(e.g.,light from right)input\nby users.\n2.3. 3DGS with Inverse Rendering / Physically Based Rendering\nGI-GS [45], GUS-IR [46] take a set of pretrained 3D Gaussians with\nnormals and intrisics, then perform a differentiable PBR to model the di-\nrect light and global illumination. GS3 [47] presents spatial and angular\nGaussians with intrinsics and lighting/view directions, using mlps to pre-\ndict shadow and global illumination. RNG [48] add an extra latent vector\nthat describes the reflectance for each Gaussian to model the surface with\nsoft boundaries like fur or fabric. GeoSplatting [49] proposes MGadapter to\ndifferentiably construct a surface-grounded 3DGS from an optimizable mesh\nguidance, enabling a precise light transport calculation.\nThese physically based rendering methods can achieve highly realistic re-\nlighting effects; however, their computational cost \u2014 including both training\nand rendering \u2014 is often very expensive. Moreover, these methods require\nprior knowledge of the light source parameters, such as position, color, and\nintensity. In our relighting task, however, such information must be inferred\nfrom the user\u2019s textual instructions, which is typically beyond the capability\nof this class of methods.\n2.4. Positional Alignment between Images and Text Prompts\nLISA [50] and GLaMM [51] employ the SAM decoder to achieve segmen-\ntation at the pixel level on reference images, whereas LLaVA Grounding [52]\nextends the model with a dedicated grounding head to output bounding\nboxes. VPP-LLaVA [5] enhances MLLMs\u2019 visual grounding by introduc-\ning visual position prompts and a curated 0.6M-sample dataset, achieving\nstate-of-the-art localization performance with strong zero-shot generaliza-\ntion. WhatsUp [53] shows that despite strong performance on VQAv2 [54],\nVL models struggle with basic spatial relations, and they introduce new\nbenchmarks to highlight this limitation.\n3. Method\nIn this section, we present the details ofGS-Light, our proposed pipeline\nfor text-driven relighting of 3D Gaussian Splatting (3DGS) scenes. Our goal\n7\nestimate \nlight positionrender\nReference / Other Views\u2026\nPosition- Aligned Light Intensity\u2026Relit Images\u2026\n\u201cdetailed face, sunshine, \nindoor, warm atmosphere, \nlight from left\u201dText Promptfine-tuning\ncondition\ninitlatentsmulti-view relighting\nPosition -Align\nModuleMV-ICLight\nPretrained GS SceneFigure 2: Circulation pipeline of GS-Light. Starting from a pre-trained Gaussian Splatting\n(GS) scene and a text prompt specifying the relighting instruction, we first render images\nfrom all training views. One training view is selected as the reference view to align the\npositional information in the prompt. Through our proposedPosition-Align Module\n(PAM), we generate position-aligned light intensity maps for all views. These intensity\nmaps are then provided as initialization latents to ourMulti-View ICLight, producing\nmulti-view consistent relit images. Finally, the relit images are used to fine-tune the\nopacity and color parameters of the GS scene, forming a closed-loop tuning circulation.\nRepeating this circulation multiple times ensures that the relit GS converges to a stable\nand consistent result.\nis to design a method that (i) faithfully adheres to user instructions expressed\ninnaturallanguage, especiallythelightdirection, (ii)ensuresmulti-viewcon-\nsistency, and (iii) operates efficiently without requiring per-scene retraining.\nTo achieve this, GS-Light integrates aPosition-Align Module (PAM)for\nprompt\u2013geometry alignment with an extendedMV-ICLightfor training-\nfree multi-view diffusion-based relighting. An overview of the pipeline is\nshown in Fig. 2.\n3.1. Preliminaries\n3.1.1. Diffusion Editors\nDiffusion-based image editing models have achieved state-of-the-art per-\nformance in tasks such as inpainting, illumination harmonization, and re-\nlighting. A diffusion model [20, 4] learns the data distributionp data(x)by\nreversing a gradual noising process. Specifically, the forward process adds\n8\nGaussian noise to a clean imagex 0:\nq(xt|x0) =N(x t;\u221a\u00af\u03b1tx0,(1\u2212\u00af\u03b1t)I),(1)\nwhile the reverse process uses a neural network\u03f5\u03b8(x t, t, c)to predict and re-\nmove noise under conditionc, thereby generating or editing images consistent\nwith the guidance.\nEditing with Diffusion.Diffusion-based editing methods can be broadly di-\nvided into two categories. On the one hand, training-free approaches reuse a\npretrained diffusion model and perform modifications by initializing from a\nnoised version of an existing imagex 0, then denoising under a new condition\ncedit:\nx\u2032\n0=Denoise(x t, cedit), x t\u223cq(x t|x0),(2)\nwhich enables preserving the structure ofx 0while applying edits guided\nbyc edit. On the other hand, training-based approaches rely on finetuning\nor retraining with triplet data(x src, cedit, xtgt)to directly learn the condi-\ntional distributionp(x tgt|xsrc, cedit)for editing. In this work, we adopt the\nIC-Lightmodel, adhered to the latter paradigm, which is specifically de-\nsigned for illumination control and artistic lighting editing, and finetuned on\na large-scale and carefully curated dataset on the base of Stable Diffusion\nv1.5 model. IC-Light introduces dedicated conditioning channels for illumi-\nnation harmonization, enabling edits that respect global lighting style while\npreserving content fidelity. However, being trained purely in the 2D image\ndomain, IC-Light lacks mechanisms to enforce cross-view consistency, which\nlimits its direct applicability to 3D scene editing.\n3.1.2. 3D Gaussian Splatting\nA 3D Gaussian distribution defines a probability density in 3D:\nG(x) =e\u22121\n2(x\u2212\u00b5)\u22a4\u03a3\u22121(x\u2212\u00b5)(3)\nwhere\u00b5\u2208R3,\u03a3\u2208R3\u00d73define the mean and covariance matrix.\n3DGaussianSplatting(3DGS)[2]representsascenewithasetofanisotropic\nGaussian primitivesG={(\u00b5i,\u03a3i,ci, \u03b1i)}G\ni=1where\u00b5i,\u03a3idetermine each\nGaussian primitive\u2019s distribution, with associated attributes colorc i\u2208[0,1]3\nand opacity\u03b1 i\u2208[0,1]. During rendering, Gaussians are projected onto the\nimage plane and rasterized in tiles for parallel efficiency. For a pixelp, which\n9\nis receiving contributions from a sorted set of GaussiansG ialong the view\ndirection, its colorCis obtained via differentiable\u03b1-blending:\nC(p) =GX\ni=1\u03b1iTici (4)\nwhereT i=Q\nj<i(1\u2212\u03b1 j)is the accumulated transmittance from front to\nback,Gis the total number of Gaussians.\nThe entire process is fully differentiable. GS initializes primitives through\nthe Structure from Motion [55] (SfM) process and applies densification and\npruning during training based on gradient magnitudes in NDC coordinates\nto control the number of Gaussians, making training and finetuning both\nefficient and simple. GS-Light finetunes GS using relit images by freezing the\nposition and shape of each Gaussian and disabling the densification strategy,\nwhich allows rapid optimization of scene appearance while avoiding memory\noverhead issues.\n3.2. Position-Align Module (PAM)\nInput\n \"detailed face,\nsunshine, indoor,\nwarm atmosphere\"\n\"..., warm atmosphere,\nlight from left\"\n\"..., warm atmosphere,\nlight from right\"\nFigure 3: IC-Light relighting results on different light direction instructions, which show\naweakorwrong responsetowards the position-related information.\nWhatsUp [53] points out that existing multimodal large models have a\nrelatively weak semantic understanding of positional relationships in the tex-\ntual modality. Experiments show that the same phenomenon also occurs in\ntext-to-image models, as illustrated in Fig. 3. The Position-Align Module is\nresponsible for bridging the semantic gap between text prompts and geomet-\nric cues, producing position-aware illumination maps that are later used to\ninitialize the diffusion process, which is shown in Fig. 4. PAM consists of 2\nstages:\n10\nVGGT & \nStableNormal\nLang -SAM\u201cdetailed face, \nsunshine, indoor, \nwarm atmosphere, \nlight from left \u201d\nreference view text promptVision -Language Input\nSYSTEM\nTemplate:{\n\u201cPosition\u201d: #POS# ,\n\u201cObject\u201d: #OBJ#\n}\nUSER\nTell me what light source \nposition is relative to which objects in the edited image.multi-model tokens\nPreset VQA Templateinject\nASISTANT\n{\n\u201cPosition\u201d: \u201cleft\u201d,\n\u201cObject\u201d: \u201cface\u201d\n}\nQwen2.5 -VL Answerdepths & normals\nRendered Views\nsegmentationx\nyzleft offset\ncamera coordinateinitposlight intensity\nref view obj textinput flow\nlight offset flowgeometry flowsegmentation flow\noutput flowFigure 4: Details ofPAM. Given the rendered views and a text prompt, Qwen2.5-VL\nis employed with a preset VQA template to parse the user\u2019s intended lighting direction\nand reference object. Pretrained models VGGT, StableNormal, and Lang-SAM are then\napplied to estimate the initial light position and scene geometry. By combining these\nestimates with the parsed light-position offset, PAM produces light-intensity maps that\nare spatially aligned with the input positional intent across all views.\n3.2.1. Parsing Lighting Priors via LVLM\nGivenauserinstruction,weemployalargevision-languagemodel(LVLM)\nsuch as Qwen2.5-vl [6] to extract structured lighting priors. The priors in-\nclude lighting direction (e.g.,light from the left) and reference objects (e.g.,\ndetailed face). To ensure robustness and avoid irrelevant outputs, we design\na constrained question\u2013answer template that restricts the LVLM\u2019s output\nspace to lighting-relevant descriptors only. This provides reliable, semanti-\ncally grounded cues for subsequent relighting.\nGiven a 3DGS sceneG, a set of training input viewsV={v n}Nv\nn=1, where\nNvis the number of training views, and the user-specified relighting instruc-\ntionc edit, we begin by rendering the GS scene into each view to obtain multi-\nview imagesI={I n}Nv\nn=1that contain scene information. We assume that\nthe illumination conditions and lighting direction described by the user are\nreferenced from a specific viewv ref. Without loss of generality, we consider\n11\nvref\u2208 V.\nTo extract the user\u2019s intended lighting direction and the corresponding\nreference object, we employ a predefined Q&A templateTthat prompts the\nLVLM to respond in a fixed format:Light is on the {DIRECTION} of the\n{OBJECT}. With the fixed format, we can easily parse the direction and\nobject prompts with a simple regular expression matching.\npdir, pobj=QwenVL(I ref, ceidit,T)(5)\nHere, QwenVL refers to Qwen2.5-VL [6], a powerful open-source LVLM.\npdir\u2208 {left,right,top,bottom}is direction prompt andp objis the reference\nobject prompt. By enumerating the possible cases ofp dir, we can determine\nthe light source offset direction that matches the user\u2019s intent.\nNext, we also need to perform text\u2013image alignment based on the ref-\nerence object, in order to determine the initial position of the light source.\nUsing a pretrained segmentation foundation model, we can obtain the mask\nof the reference objectM obj:\nMobj=LangSAM(I ref, pdir)(6)\nHere, LangSAM is a text-guided segmentation model that integrates\nGroundingDINO [56] and SAM2.1 [9], and outputs high-quality object masks\nbased on the given prompt. With this mask, we can compute the pixel co-\nordinates of the reference object in the imagepobj:\npobj=1\n|M obj|X\np\u2208M objp(7)\nwhich will be used for subsequent estimation of the light source position.\n3.2.2. Geometric and Semantic Understanding & Latent Initialization\nGeometry Estimation.To complement text-based priors, we estimate per-\nview scene geometry and semantics using off-the-shelf models. Scale-aligned\ndepth mapsD={D n}Nv\nn=1are obtained via VGGT [7], which is a general\nframework designed for multi-view geometry perception with an end-to-end\ntransformer-based architecture. Then surface normalsN={N n}Nv\nn=1are\nestimated via StableNormal [8], which predicts more smooth and proper\nnormals on the base of a powerful generative diffusion prior model, rather\nthan directly inferring from depth mapsD.\n12\n\u2206init\u2206\ud835\udc5d\ud835\udc5ddir\ninitposestimated pos\nobject segmentation ref camera coordinate\ud835\udc67\ud835\udc67\n\ud835\udc65\ud835\udc65\ud835\udc66\ud835\udc66\nref view Gaussians\nLVLM Answer\nFigure 5: The process of estimating light source position.\nSemantic Understanding.As mentioned earlier, we can use LangSAM to\nalign the reference imageI refwith the user\u2019s instructionsc eidit, thereby esti-\nmating the reference object\u2019s pixel coordinatespobj. Considering that light-\ning usually comes from above, we assume here that the initial light source\nposition is located diagonally above on the side of the reference object fac-\ning the camera (the exact initial position is not critical; what matters is the\nchange in the light source position before and after relighting). Following the\nconvention of 3DGS, we adopt the OpenCV camera coordinate system (with\nthe x, y, z axes of the camera pointing to the right, downward, and into the\nscreen, respectively):\npl=\u0014pobj\ndref\u0015\n+\u2206 init (8)\nwhereplis the light position in camera coordinate,d refis the depth at pixel\ncoordinatepobj.\u2206 initis the relative offset to initialize the light position.\nBased on the direction promptp dirprovided by the LVLM, we can add a\ncorresponding offset\u2206p dirto the initial light source position so that it aligns\nwith the user\u2019s description of the light source direction:\np\u2032\nl=pl+\u2206p dir (9)\nTo better understand the light position estimation process, we illustrate\nit as Fig. 5.\nLatent Initialization.With geometry and light source ready, we can the com-\npute the distribution of light intensity across the views by a modified Phong-\n13\nlike diffuse illuminating model [10]:\nId= max(\u2212\u27e8l in,n\u27e9,0)\u03b3(10)\nwhereI dis the diffuse part reflected by the surface, which indicates the\nlight intensity.l indenotes incident direction andndenotes surface normal,\n\u27e8\u00b7,\u00b7\u27e9is inner product operation,\u03b3is a hyperparameter to balance the bright-\nness distribution.\nThe illumination intensity maps{I d}Nv\nn=1serve as initialization signals in\nthe diffusion process. Specifically, we encode the illumination maps into\nlatent space and inject them into IC-Light\u2019s denoising steps as init latentsl d\nld= VAE (I d)(11)\nwhereVAEis the image encoder of SD-1.5. This provides explicit geometry-\naware lighting conditioning, allowing the model to align its generation trajec-\ntory with the desired lighting direction and intensity, significantly reducing\nprompt\u2013output mismatch.\n3.3. MV-ICLight\nAlthough IC-Light is renowned for its powerful image relighting capabil-\nity, like other 2D image editing models, it also struggles to ensure consistency\nin the outputs when given multi-view inputs. (Insert Image). To extend IC-\nLight from single-view to multi-view editing, we introduceMV-ICLight,\nwhich enforces cross-view coherence during relighting.\n3.3.1. Cross-View Attention\nTo enable multi-view editing, it is necessary for the model to aggregate\nglobal information from multi-view inputs during inference, thereby estab-\nlishing a mechanism for inter-view information exchange and supplementa-\ntion. A natural idea is to replace the self-attention in the Diffusion U-Net\nwith cross-view attention, as shown in the upper part of Fig. 6. Specifically,\nfor a given set of query and key tokens{Q}F\nf=1,{K}F\nf=1from thef-th frame,\neach ofQ fcan perceive all tokens from other frames, instead of only the\nsame frame which is the case of traditional self-attention. The Cross-View\nAttention Score can be formulated as:\nCVAttn (Q, K, f) = Softmax\u0012Qf\u00b7[K 1,\u00b7\u00b7\u00b7, K F]\u221a\nd\u0013\n(12)\nwheredis the dimension of the query and key embeddings.\n14\n\u2026\u2026 \u2026 \u2026\u2026tokenizeKey frames\nRest frames\ud835\udc53\ud835\udc53views\ud835\udc53\ud835\udc53\u00d7\ud835\udc5d\ud835\udc5d\u00d7\ud835\udc36\ud835\udc36\n\ud835\udc5d\ud835\udc5dtokens\nreshape\u2026 \u2026 \u2026\u20261\u00d7\ud835\udc53\ud835\udc53\ud835\udc5d\ud835\udc5d\u00d7\ud835\udc36\ud835\udc36\nself attention\n& reshape\u2026\n\u20262 closest\nkey framesmax similarity along\nepipolar lineinterpolateInput features\nOutput features\nSelf-attn batch\nFrame of interest\nPixel of interestEpipolar line\nCorrespondence\nFigure 6: Schematic diagram of self attention in MV-ICLight. Upper part: the implemen-\ntation mechanism of Cross-View Attention. Lower part: epipolar constrain for non-key\nframes from DGE [11].\n3.3.2. Multi-View Relighting with Advanced Epipolar Constraints\nDirectly extending self-attention across multiple frames will significantly\nincrease computational and memory overhead, since the cost of self-attention\ngrowsquadraticallywiththenumberofinputtokens. InheritedfromDGE[11],\nwe subsample a few key frames from all training views to perform a full-size\ninference and implement an epipolar-matching mechanism to fill up the fea-\nturemapsoftherestframes, asshowninthelowerpartofFig.6. Wekeepthe\nnotations from Sec. 3.2.1 and DGE. Given a set of key framesK \u2282 V, where\nVis the training set, a feature map\u03a8 v\u2032corresponding to imageI v\u2032, where\nv\u2032/\u2208 K, and features\u03a8 k\u2217of the nearest keyframeI k\u2217, the correspondence\nmapM v\u2032is given by:\nMv\u2032[pu] = argmin\npv,pTv\u02c6Fpu=0D(\u03a8 v\u2032[pu],\u03a8 k\u2217[pv])(13)\nwhereDis the cosine distance,puandpvis the pixel-wise index,k\u2217is\nthe key view closest to viewv\u2032, and unlike DGE, \u02c6F=F\n\u2225F\u2225+\u03f5is the normalized\nfundamental matrix corresponding to the two viewsv\u2032andk\u2217, where\u03f5is a\nsmall constant introduced to avoid division-by-zero overflow.\nSince the fundamental matrixFis defined up to an arbitrary nonzero\n15\nscale factor, we normalized it to have unit norm to ensure numerical sta-\nbility during inference. Experiments demonstrate that, as shown in Tab. 6,\nby using our normalized fundamental matrix \u02c6F, the failure of epipolar con-\nstraints in the shallow attention blocks of the UNet is largely avoided. As\na result, inference stability is greatly improved, and the generated results\nexhibit significantly better multi-view consistency.\nDuring inference, as shown in Eq. 11, the spatially aligned illumination\ndiffuse map is encoded into latentsl d, which are then noised up to timestep\nT\u2032(< T max), as the starting point for denoising, whereT maxis the total de-\nnoising steps number. The latents representation of the original imagesl I, is\nalso used as a condition to keep the geometry and details of results consistent\nwith original images, and concatenated withl dalong the channel dimension:\nlH\n8\u00d7H\n8\u00d72C\ni = Concat(lH\n8\u00d7H\n8\u00d7C\nd , lH\n8\u00d7H\n8\u00d7C\nI )(14)\nwherel iistheinitnoisedlatentstoperformpartialDDIM[21]pipeline,H, W\nis resolution of source images andCis the channel dimension of latents.\n3.3.3. GS Tuning and Iterative Dataset Updating Strategy\nThroughMV-ICLight,therelitimagesacrossmultipleviewsindeedachieve\nbetter consistency compared to independently relighting each view, though\nslight inconsistencies still remain. To address this, we adopt theIterative\nDataset Updatestrategy from IN2N [37]: after everyK intsteps, the GS\nscene is rendered from all viewpoints and relit using MV-ICLight, and the\nresulting images are directly updated as the training data for the correspond-\ning views. This process is repeatedK reaptimes. The GS scene eventually\nconverges to a multi-view consistent relighting result, whereK intandK reap\nare hyperparameters controlling the update interval and number of iterations\nduring fine-tuning.\nTo apply the multi-view relighting edits to the GS-represented scene, we\ndirectly use the edited results as the training set to fine-tune the Gaussians:\nG\u2032= argmin\nci,\u03b1i\u2208Gi,\u2200Gi\u2208GX\nv\u2208VL (Render(G, v), Iv\nrelit)(15)\nwhereG\u2032is the final relit GS scene,Lis the loss function of 3DGS train-\ning, typically composed of an L1 loss and an SSIM loss,Render(G, v)is the\nGaussian rendering at viewpointv, andIv\nrelitis the relit image generated from\nMV-ICLight.\n16\n4. Experiments\nIn this section, we first present the implementation details of our method,\nfollowed by both qualitative and quantitative comparisons with existing ap-\nproaches. We then conduct an ablation study to further analyze the effec-\ntiveness of our method.\nDatasets.The relighting tasks are performed on several datasets of indoor\nand outdoor scenes, including IN2N [37] dataset, MipNerf360 [57], Scan-\nnet++ [58], etc., where 3D Gaussian Splatting models (or dense multi-view\nimagery) are available. We use the GPT-5 [59] model to generate var-\nious scene-related relighting instructions, and each instruction is iterated\nover possible lighting directionsp dir\u2208 {left,right,top,bottom}to construct\nthe benchmark. Specifically, we select all scenes from IN2N and MipN-\neRF360, along with the first 10 scenes from ScanNet++, resulting in a total\nof 25 scenes. For each scene, we assign three randomly generated relighting\nprompts and random lighting directions, leading to 75 relighting tasks in\ntotal, which constitute our benchmark. We also conducted a user study to\ncollect participants\u2019 preferences regarding the relighting results produced by\ndifferent models.\nImplementation Details.Under our 2D-to-MV framework, the capability of\nthe 2D image editor largely determines the quality of the final 3D editing\nresults. For fairness, we adopt several prior works based on IP2P [23] for 3D\nscene editing and IC-Light [3] for scene/video relighting as our baselines. To\ncontrolthememoryconsumptionduringinference, weuse50\u201370inputimages\nof 512\u00d7512 resolution for all datasets. We adopt classifier-free guidance with\na text guidance scale of 7.5, applied throughout the generation process. For\nthe Dataset Update Strategy, we set the update intervalK intto 500 and the\nnumber of updatesK reapto 2, consistent with DGE [11]. During Gaussian\nSplatting fine-tuning, we disable gradient computation for all parameters\nexcept color and opacity, and also deactivate the densification strategy.\nEvaluation.We conduct both qualitative and quantitative evaluations of our\nmethod. For quantitative analysis, following DGE, we evaluate CLIP score\nand CLIP directional score (noted asCLIP-TandCLIP-D) between ren-\ndered images and target text prompt to measure the alignment of 3D relit\nscenes andinstruction. In addition, metrics such asPSNR, SSIM, and LPIPS\nare commonly used to evaluate how well a Gaussian Splatting (GS) scene fits\n17\nTable 1: Quantitative comparison on IN2N [37] dataset. Best and second results are\nhighlighted inboldand with underline .\nMethod CLIP-T\u2191CLIP-D\u2191VBench\nSubject Background Aesthetic Image\nConsistency\u2191Consistency\u2191Quality\u2191Quality\u2191\nRelighting on Videos\nRelightVid 0.2383 0.0611 0.7806 0.8624 0.5991 61.31\nLumen* 0.1754 -0.0336 0.7215 0.8336 0.4720 53.88\nOurs 0.2580 0.1170 0.8320 0.8884 0.6317 62.22\nRelighting on Gaussian Splatting\nDGE 0.2369 0.0918 0.8614 0.9017 0.5619 60.65\nEditSplat 0.1983 -0.0021 0.8730 0.9094 0.5203 53.94\nIN2N 0.2222 0.04100.87570.9098 0.5425 51.08\nIGS2GS\u20200.2055 0.0111 0.8594 0.9268 0.4249 30.27\nIGS2GS-IC\u20210.1800 -0.0102 0.86420.93300.3938 26.76\nOurs 0.2580 0.11710.8748 0.9270 0.604357.80\n*the generating resolution setting of benchmark is 512\u00d7512, however the Lumen\u2019s recom-\nmended resolution is 480\u00d7832, which may cause a performance decline.\n\u2020denotes Instruct-GS2GS, an improved version adapted from IN2N for GS training, using\nInstructPix2Pix [23] as the 2D image editor.\n\u2021denotes replacing the 2D image editor InstructPix2Pix with IC-Light [3].\nthe corresponding real-world scene. These metrics can also partially reflect\nthe multi-view consistency of rendered images. However, in our experiments,\nwe observed that when fine-tuning a pretrained GS scene, these metrics tend\nto favor models with smaller editing degrees, since less-edited supervision\nimages make it easier for the GS to converge toward high multi-view consis-\ntency\u2014closer to that of the original training images, which is unfair to mod-\nels that produce more complex and visually refined results. Therefore, we\nomit these metrics in method comparison when evaluating the final relight-\ning performance, and instead adopt VBench [60], an end-to-end video quality\nassessment framework. We select the following indicators\u2014Subject Con-\nsistency,Background Consistency,Aesthetic Quality, andImaging\nQuality\u2014tocomprehensivelyevaluatetherelightingresultsofdifferentmod-\nels. It is worth noting that since VBench is video-based, we need to provide\na continuous video as input. For GS-based relighting methods, we generate a\nsmooth camera trajectory by interpolating between the training viewpoints\nfor each scene in the benchmark, and then render videos along these trajec-\ntories. For methods that perform relighting directly on videos, we take the\n18\nTable 2: Quantitative comparison on MipNerf360 [57] and Scannet++ [58] dataset. Best\nand second results are highlighted inboldand with underline .\nMethodMipNerf360 Scannet++\nCLIP-T\u2191CLIP-D\u2191S.C.\u2191B.C.\u2191A.Q.\u2191I.Q.\u2191CLIP-T\u2191CLIP-D\u2191S.C.\u2191B.C.\u2191A.Q.\u2191I.Q.\u2191\nRelighting on Videos\nRelightVid 0.2346 0.0578 0.7260 0.8558 0.5593 61.160.23770.0267 0.6885 0.8494 0.5688 67.01\nLumen 0.1865 -0.0198 0.6108 0.8007 0.4180 45.62 0.2011 -0.0624 0.5687 0.7900 0.4275 43.17\nOurs 0.2403 0.0883 0.7386 0.8684 0.5834 66.930.2309 0.06580.6636 0.8478 0.5447 57.08\nRelighting on Gaussian Splatting\nDGE 0.2304 0.0810 0.8343 0.8944 0.5888 69.71 0.24520.0551 0.8550 0.9236 0.5213 39.28\nEditSplat 0.2201 0.0363 0.8349 0.8898 0.5571 62.44 0.2291 0.0163 0.8767 0.92610.5544 43.36\nIN2N 0.2264 0.0433 0.8674 0.9118 0.5210 49.21 0.2142 -0.01350.88100.9225 0.5309 46.73\nIGS2GS 0.2160 0.0422 0.8633 0.9184 0.4769 39.86 0.2243 0.0195 0.8628 0.9238 0.5079 38.46\nIGS2GS-IC 0.2064 0.02510.8754 0.92780.4473 28.86 0.1977 -0.0287 0.8715 0.9233 0.4628 24.82\nOurs 0.2402 0.08810.8421 0.90500.604768.81 0.2308 0.06580.8629 0.92230.5792 50.62\nrelit frames corresponding to the training viewpoints, sort them by viewing\ndirection, concatenate them into a single video, and then evaluate it using\nVBench.\n4.1. Comparisons with Prior Work\nOurbaselinesincludestate-of-the-art3DeditingmethodssuchasIN2N[37],\nDGE [11], and EditSplat [42], as well as video relighting approaches includ-\ning RelightVid [43] and Lumen [44]. In addition, we develop an IC-Light [3]-\nbased variant of IGS2GS, which adapts IN2N to Gaussian Splatting scenes,\nreplacing the original 2D image editor InstructPix2Pix [23].\nWe present qualitative comparisons between our method and baselines\non the IN2N, Mip-NeRF360, and ScanNet++ datasets to visually assess the\neffectiveness of our framework. In Fig. 1, our approach produces multi-view\nconsistent edits that faithfully reflect the user-provided lighting condition,\nwhile maintaining high-quality geometry and texture details across views.\nAs shown in Tab. 1 and Tab. 2, our method consistently outperforms prior\napproaches across all datasets and metrics. Specifically, our model achieves\nthe highest scores while keeping the inference process in few minutes, indicat-\ning superior visual fidelity, cross-view consistency, semantic alignment and\ntime consuming.\nTab. 3 presents the preferences of 29 users for GS relighting methods\nshown in Fig. 1, based on dimensions such as subjective preference, image-\ntext matching, and lighting/artistic effects. The data indicate that our\nmethod is favored by users.\nIn summary, compared with prior methods, our method generates more\n19\nTable 3: User preference study for various gaussian splatting relighting methods. We\ncollected 29 users\u2019 rankings of relighting results from different models along three di-\nmensions\u2014subjective preference, artistic aesthetics, and lighting-control consistency\u2014and\ncomputed the average rank.\nMethodAvg. Rank (#)\u2193\nSubjective Preference Artistic Aesthetics Lighting-Control Consistency\nDGE 2.15 2.23 2.31\nEditSplat 3.23 3.28 3.48\nIN2N 4.15 4.15 4.09\nIGS2GS 5.45 5.28 5.05\nIGS2GS-IC 4.36 4.26 4.01\nOurs 1.57 1.69 1.87\nTable 4: Component-wise ablation on IN2N dataset. Each component is removed progres-\nsively to analyze its contribution.\nPAM CV-Attn IC-Light CLIP-T\u2191CLIP-D\u2191S.C.\u2191B.C.\u2191A.Q.\u2191I.Q.\u2191\n\u2713 \u2713 \u27130.2580 0.1170 0.8748 0.9270 0.6043 57.80\n\u00d7\u2713 \u27130.2467 0.1115 0.8734 0.9243 0.6130 58.22\n\u00d7 \u00d7\u27130.2667 0.1188 0.8631 0.9160 0.5957 56.36\n\u00d7 \u00d7 \u00d70.2369 0.0918 0.8614 0.9016 0.5619 60.65\nrealistic lighting and color adjustments, preserves fine-grained scene struc-\ntures, and exhibits fewer artifacts in occluded or texture-rich regions.\n4.2. Ablation Study\nNext, we conductanablationstudytoevaluate theeffectivenessofseveral\nkey components in our editing pipeline: IC-Light Relighting Model, Multi-\nView Consistent Inference and Position-Align Module. Tab. 4 shows the\ncomponent-wise quantitative ablation study and then qualitative result of\neach component will be present.\nComponent-wise Ablation.We sequentially remove key components\nfrom our full pipeline, position-alignment module (w/o PAM), cross-view\nattention module (w/o CV-Attn), and IC-Light as the 2D image editor (w/o\nIC-Light). Table 4 reports the corresponding metrics scores on the IN2N\ndataset. Removing PAM and CV-Attn leads to a noticeable drop in semantic\nalignment and multi-view consistency, which shows the effectiveness of our\nproprosed modules. However, we observed an abnormal increase in the CLIP\nscore when only IC-Light is used. We speculate that this may be because\nthe CLIP score emphasizes the overall alignment between the image and the\n20\nTable 5: Multi-view consistency ablation on Cross-view Attention. We evaluate PSNR,\nSSIM, LPIPS for renderings of relit GS scene on IN2N dataset.\nMethod PSNR\u2191SSIM\u2191LPIPS\u2193\nw/o CV-Attn 13.12 0.4602 0.4253\nw/ CV-Attn 18.88 0.6267 0.2549\ntext, while lacking training objectives related to lighting consistency. The\nImaging Quality metric exhibits relatively \u201crandom\u201d behavior on this task,\nwhichmaybebecauseitfocusesmoreonimagesharpnessratherthanlighting\neffects\u2014an aspect that our proposed module is not designed to address.\nRelighting without Multi-View Consistency.To evaluate the ef-\nfectiveness of our multi-view consistent relighting, we replace it with inde-\npendent per-view relighting for the same set of views and use those results\nto fine-tune Gaussians. As shown in Tab. 5, incorporating multi-view con-\nsistency significantly improves reconstruction metrics such as PSNR, SSIM\nand LPIPS, demonstrating the advantage of maintaining cross-view coher-\nence. Furthermore, Fig. 7 illustrates intermediate 2D relighting results with\nand without multi-view consistency. It can be clearly observed that con-\nsistent relighting leads to visually coherent appearances across views, while\nindependent relighting causes noticeable discrepancies among them.\nEffectiveness of Position-Align Module.We present the relighting\nresults of different methods when the input instructions include lighting posi-\ntion information, as shown in Fig. 8. Experimental results demonstrate that,\nwith the help of the PAM module, our method can more accurately capture\nthe lighting direction described in the editing instruction, thereby producing\nrelighting results that are more faithful to the user\u2019s intent.\nFundamental Matrix Normalization.In practice, we found that the\nepipolar constraint frequently fails during UNet inference, which is typically\ncaused by numerical overflow. To further investigate the cause, we take the\nface scene from IN2N as an example. Fig. 9 (a) shows the proportion of pixels\nacross different UNet layers (i.e., different resolutions) and the ratio of those\naffected by overflow. We observe that nearly half of the pixels experience\nnumerical overflow, severely disrupting the process of finding the maximum\nvalue along the epipolar line. As shown in Fig. 9 (b), regions with numerical\ninstability cause the search range of the epipolar constraint to shrink, leading\nto suboptimal matches, and in more severe cases, the matching process may\n21\ninputs/instruction relit imagesGS renders\nMV-ICLight\n\"detailed face, sunshine, indoor,\nwarm atmosphere\"\nIC-Light\n MV-ICLight\n\"detailed bear statue, marble,\ntwilight, golden autumn forest,\nsunset glory\"\nIC-Light\n MV-ICLight\n\"detailed clear face, cool tunes,\nstage lighting, blue spotlight\"\nIC-Light\n MV-ICLight\n\"office desk, indoor, night scene,\ncool fluorescent light\"\nIC-Light\n MV-ICLight\n\"bonsai tree, indoor, warm desk\nlamp illumination, cozy\natmosphere\"\nIC-Light\nFigure 7: Ablation study of multi-view consistency. For each scene, first row shows the\nrelit images and renders from fine-tuned GS scene with ourMV-ICLight, while second\nrow shows the ones with vanilla IC-Light. OurMV-ICLightshows a great multi-view\nconsistency between sampled perspectives which contributes to divergency of GS finetun-\ning. On the other hand, vanilla IC-Light without any multi-view consistency constrain\ngenerates diverse relit images, which results a degrade and blurry GS rendering.\n22\ninstruction inputs/init latents with PAM w/o PAM\n\"detailed face, sunshine,\nindoor, warm\natmosphere,\"\n\"..., light fromleft\"\n\"..., light fromright\"\n\"detailed bear statue,\nmarble, twilight, golden\nautumn forest,\"\n\"..., sunset glory fromleft\nside\"\n\"..., sunset glory from\nrightside\"\n\"young man, detailed face,\nnatural lighting, outdoor,\nwarm,\"\n\"..., light from thetop\"\n\"..., light from the\nbottom\"\nFigure 8: Visualization of PAM component. For each scene, first row shows the input\nimages and common instruction, as well as the depths and normals estimated from pre-\ntrained models. Next, each line represents supplementary instructions for different light\ndirections, sequentially displaying the corresponding initial latents and the relighting re-\nsults of whether the PAM component is used.\n23\nPixel of interest\nEpipolar line\nOverflow area\n@322\n(a) (b) Inner : Pixel \nNumber ProportionOuter : Overflow RateFigure 9: (a) Statistics of overflow occurrences across different resolutions (UNet layers);\n(b) Visualization of how overflow disrupts the epipolar constraints (at322resolution)\ndegenerateintoglobalmatching. Fortunately,wediscoveredthattheoverflow\nissue originates from the lack of normalization in the fundamental matrix\ncomputed by PyTorch, resulting in excessively large values (up to the order\nof106) in the matrixF. Tab. 6 reports the average number of UNet layers\nexperiencing overflow per inference before and after normalization, as well as\nthe relighting results in terms of PSNR, SSIM and LPIPS. The experiments\ndemonstrate that our improved normalized fundamental matrix significantly\nenhances the numerical stability of the inference process and improves the\nmulti-view consistency of the editing results.\nTable 6: Ablation on epipolar constraint normalization of fundamental matrix. We eval-\nuate on face scene from IN2N dataset.\nMethod Avg. Overflow Rate\u2193PSNR\u2191SSIM\u2191LPIPS\u2193\nw/o normalization 49.82% 20.81 0.7312 0.1621\nw/ normalization 0.00% 21.55 0.7328 0.1599\nThese ablation studies collectively demonstrate the importance of our\nproposed modules and design choices in achieving semantically faithful and\ngeometrically coherent 3D scene edits.\n5. Limitations and Future Work\nThere are several limitations to GS-Light. First, our reliance on off-the-\nshelf geometry / normal estimators means that if depth or normal maps are\n24\ninaccurate (e.g. due to occlusions, reflective or transparent surfaces), the\nlighting fusion and shadow estimation may fail or artifacts may arise. Sec-\nond, the projection back into the 3DGS representation may leave uncovered\nregions (views or surfaces not well seen in images), leading to inconsistency\nor blurring. Third, strongly specular or anisotropic materials are difficult to\nhandle in inference only pipelines without BRDF fitting.\nFutureworkcouldincludeintegratinglightweightmaterialpriorsorBRDF\nestimation; extending LVLM prior extraction to more complex lighting (mul-\ntiple lights, colored ambient, etc.); better handling of occlusion and shadows\nvia differentiable visibility; possibly allowing optional per-scene fine-tuning\nwhen higher fidelity is required.\n6. Conclusion\nWe presented GS-Light, a training-efficient, text-guided, position-aware\nmethod for scene relighting in Gaussian Splatting representations. By com-\nbining prompt-derived lighting priors and view-consistency constraints, our\npipeline generates multi-view coherent relit images and relit 3D scenes, espe-\ncially in lighting directions, outperforming several baselines while operating\npurely at inference time. We believe GS-Light provides a useful step toward\nmore accessible, controllable, and consistent relighting for 3D content.\nReferences\n[1] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoor-\nthi, R. Ng, Nerf: Representing scenes as neural radiance fields for view\nsynthesis, Communications of the ACM 65 (1) (2021) 99\u2013106.\n[2] B. Kerbl, G. Kopanas, T. Leimk\u00fchler, G. Drettakis, 3d gaussian splat-\nting for real-time radiance field rendering., ACM Trans. Graph. 42 (4)\n(2023) 139\u20131.\n[3] L. Zhang, A. Rao, M. Agrawala, Scaling in-the-wild training for\ndiffusion-basedilluminationharmonizationandeditingbyimposingcon-\nsistent light transport, in: The Thirteenth International Conference on\nLearning Representations, 2025.\n[4] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, B. Ommer, High-\nresolution image synthesis with latent diffusion models, in: Proceedings\n25\nof the IEEE/CVF conference on computer vision and pattern recogni-\ntion, 2022, pp. 10684\u201310695.\n[5] W. Tang, Y. Sun, Q. Gu, Z. Li, Visual position prompt for mllm based\nvisual grounding, arXiv preprint arXiv:2503.15426 (2025).\n[6] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang,\nS. Wang, J. Tang, et al., Qwen2. 5-vl technical report, arXiv preprint\narXiv:2502.13923 (2025).\n[7] J. Wang, M. Chen, N. Karaev, A. Vedaldi, C. Rupprecht, D. Novotny,\nVggt: Visual geometry grounded transformer, in: Proceedings of the\nComputer Vision and Pattern Recognition Conference, 2025, pp. 5294\u2013\n5306.\n[8] C. Ye, L. Qiu, X. Gu, Q. Zuo, Y. Wu, Z. Dong, L. Bo, Y. Xiu, X. Han,\nStablenormal: Reducing diffusion variance for stable and sharp normal,\nACM Transactions on Graphics (TOG) 43 (6) (2024) 1\u201318.\n[9] N.Ravi, V.Gabeur, Y.-T.Hu, R.Hu, C.Ryali, T.Ma, H.Khedr, R.R\u00e4-\ndle, C. Rolland, L. Gustafson, E. Mintun, J. Pan, K. V. Alwala, N. Car-\nion, C.-Y. Wu, R. Girshick, P. Doll\u00e1r, C. Feichtenhofer, Sam 2: Segment\nanything in images and videos, arXiv preprint arXiv:2408.00714 (2024).\nURLhttps://arxiv.org/abs/2408.00714\n[10] B. T. Phong, Illumination for computer generated pictures, in: Seminal\ngraphics: pioneering efforts that shaped the field, 1998, pp. 95\u2013101.\n[11] M.Chen, I.Laina, A.Vedaldi, Dge: Directgaussian3deditingbyconsis-\ntent multi-view editing, in: European Conference on Computer Vision,\nSpringer, 2024, pp. 74\u201392.\n[12] W. Peebles, S. Xie, Scalable diffusion models with transformers, in: Pro-\nceedings of the IEEE/CVF international conference on computer vision,\n2023, pp. 4195\u20134205.\n[13] P. Debevec, T. Hawkins, C. Tchou, H.-P. Duiker, W. Sarokin, M. Sagar,\nAcquiring the reflectance field of a human face, in: Proceedings of\nthe 27th annual conference on Computer graphics and interactive tech-\nniques, 2000, pp. 145\u2013156.\n26\n[14] Z. Xu, K. Sunkavalli, S. Hadap, R. Ramamoorthi, Deep image-based\nrelighting from optimal sparse samples, ACM Transactions on Graphics\n(ToG) 37 (4) (2018) 1\u201313.\n[15] S. Sengupta, A. Kanazawa, C. D. Castillo, D. W. Jacobs, Sfsnet: Learn-\ning shape, reflectance and illuminance of facesin the wild\u2019, in: Proceed-\nings of the IEEE conference on computer vision and pattern recognition,\n2018, pp. 6296\u20136305.\n[16] Z. Shu, S. Hadap, E. Shechtman, K. Sunkavalli, S. Paris, D. Samaras,\nPortrait lighting transfer using a mass transport approach, ACM Trans-\nactions on Graphics (TOG) 36 (4) (2017) 1.\n[17] T. Sun, J. T. Barron, Y.-T. Tsai, Z. Xu, X. Yu, G. Fyffe, C. Rhemann,\nJ. Busch, P. E. Debevec, R. Ramamoorthi, Single image portrait relight-\ning., ACM Trans. Graph. 38 (4) (2019) 79\u20131.\n[18] T. Nestmeyer, J.-F. Lalonde, I. Matthews, A. Lehrmann, Learning\nphysics-guided face relighting under directional light, in: Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, 2020, pp. 5124\u20135133.\n[19] H. Kim, M. Jang, W. Yoon, J. Lee, D. Na, S. Woo, Switchlight: Co-\ndesign of physics-driven architecture and pre-training framework for hu-\nman portrait relighting, in: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2024, pp. 25096\u201325106.\n[20] P. Dhariwal, A. Nichol, Diffusion models beat gans on image synthesis,\nAdvances in neural informationprocessingsystems 34 (2021)8780\u20138794.\n[21] J. Song, C. Meng, S. Ermon, Denoising diffusion implicit models, arXiv\npreprint arXiv:2010.02502 (2020).\n[22] G. Kim, T. Kwon, J. C. Ye, Diffusionclip: Text-guided diffusion mod-\nels for robust image manipulation, in: Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, 2022, pp. 2426\u2013\n2435.\n[23] T. Brooks, A. Holynski, A. A. Efros, Instructpix2pix: Learning to follow\nimageeditinginstructions, in: ProceedingsoftheIEEE/CVFconference\non computer vision and pattern recognition, 2023, pp. 18392\u201318402.\n27\n[24] R. Mokady, A. Hertz, K. Aberman, Y. Pritch, D. Cohen-Or, Null-text\ninversion for editing real images using guided diffusion models, in: Pro-\nceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, 2023, pp. 6038\u20136047.\n[25] A. Hertz, A. Voynov, S. Fruchter, D. Cohen-Or, Style aligned image gen-\neration via shared attention, in: Proceedings of the IEEE/CVF Confer-\nenceonComputerVisionandPatternRecognition, 2024, pp.4775\u20134785.\n[26] T. Qi, S. Fang, Y. Wu, H. Xie, J. Liu, L. Chen, Q. He, Y. Zhang,\nDeadiff: An efficient stylization diffusion model with disentangled rep-\nresentations, in: Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 2024, pp. 8693\u20138702.\n[27] H. Wang, M. Spinelli, Q. Wang, X. Bai, Z. Qin, A. Chen, Instantstyle:\nFree lunch towards style-preserving in text-to-image generation, arXiv\npreprint arXiv:2404.02733 (2024).\n[28] H. Wang, P. Xing, R. Huang, H. Ai, Q. Wang, X. Bai, Instantstyle-plus:\nStyletransferwithcontent-preservingintext-to-imagegeneration, arXiv\npreprint arXiv:2407.00788 (2024).\n[29] B. Ke, A. Obukhov, S. Huang, N. Metzger, R. C. Daudt, K. Schindler,\nRepurposing diffusion-based image generators for monocular depth es-\ntimation, in: Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 2024, pp. 9492\u20139502.\n[30] J. Li, H. Tan, K. Zhang, Z. Xu, F. Luan, Y. Xu, Y. Hong,\nK. Sunkavalli, G. Shakhnarovich, S. Bi, Instant3d: Fast text-to-3d with\nsparse-view generation and large reconstruction model, arXiv preprint\narXiv:2311.06214 (2023).\n[31] M. Ren, W. Xiong, J. S. Yoon, Z. Shu, J. Zhang, H. Jung, G. Gerig,\nH. Zhang, Relightful harmonization: Lighting-aware portrait back-\nground replacement, in: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2024, pp. 6452\u20136462.\n[32] Y. Mei, J. Xu, V. M. Patel, Reference-based controllable scene styliza-\ntion with gaussian splatting, arXiv preprint arXiv:2407.07220 (2024).\n28\n[33] K. Zhang, N. Kolkin, S. Bi, F. Luan, Z. Xu, E. Shechtman, N. Snavely,\nArf: Artistic radiance fields, in: European Conference on Computer\nVision, Springer, 2022, pp. 717\u2013733.\n[34] L. A. Gatys, A. S. Ecker, M. Bethge, Image style transfer using con-\nvolutional neural networks, in: Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2016, pp. 2414\u20132423.\n[35] \u00c1. S. Kov\u00e1cs, P. Hermosilla, R. G. Raidou, G-style: Stylized gaussian\nsplatting, in: Computer Graphics Forum, Vol. 43, Wiley Online Library,\n2024, p. e15259.\n[36] K. Liu, F. Zhan, M. Xu, C. Theobalt, L. Shao, S. Lu, Stylegaussian:\nInstant 3d style transfer with gaussian splatting, in: SIGGRAPH Asia\n2024 Technical Communications, 2024, pp. 1\u20134.\n[37] A. Haque, M. Tancik, A. A. Efros, A. Holynski, A. Kanazawa, Instruct-\nnerf2nerf: Editing 3d scenes with instructions, in: Proceedings of\nthe IEEE/CVF international conference on computer vision, 2023, pp.\n19740\u201319750.\n[38] J. Dong, Y.-X. Wang, Vica-nerf: View-consistency-aware 3d editing of\nneural radiance fields, Advances in Neural Information Processing Sys-\ntems 36 (2023) 61466\u201361477.\n[39] J.-K. Chen, S. R. Bulo, N. M\u00fcller, L. Porzi, P. Kontschieder, Y.-X.\nWang, Consistdreamer: 3d-consistent 2d diffusion for high-fidelity scene\nediting, in: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2024, pp. 21071\u201321080.\n[40] X.-Y. Yu, J.-X. Yu, L.-B. Zhou, Y. Wei, L.-L. Ou, Instantstylegaussian:\nEfficient art style transfer with 3d gaussian splatting, arXiv preprint\narXiv:2408.04249 (2024).\n[41] J.-K. Chen, Y.-X. Wang, Proedit: Simple progression is all you need for\nhigh-quality 3d scene editing, Advances in Neural Information Process-\ning Systems 37 (2024) 4934\u20134955.\n[42] D. I. Lee, H. Park, J. Seo, E. Park, H. Park, H. D. Baek, S. Shin, S. Kim,\nS. Kim, Editsplat: Multi-view fusion and attention-guided optimization\n29\nfor view-consistent 3d scene editing with 3d gaussian splatting, in: Pro-\nceedings of the Computer Vision and Pattern Recognition Conference,\n2025, pp. 11135\u201311145.\n[43] Y. Fang, Z. Sun, S. Zhang, T. Wu, Y. Xu, P. Zhang, J. Wang, G. Wet-\nzstein, D.Lin, Relightvid: Temporal-consistentdiffusionmodelforvideo\nrelighting, arXiv preprint arXiv:2501.16330 (2025).\n[44] J. Zeng, Y. Liu, Y. Feng, C. Miao, Z. Gao, J. Qu, J. Zhang,\nB. Wang, K. Yuan, Lumen: Consistent video relighting and harmonious\nbackground replacement with video generative models, arXiv preprint\narXiv:2508.12945 (2025).\n[45] H. Chen, Z. Lin, J. Zhang, Gi-gs: Global illumination decomposition on\ngaussiansplattingforinverserendering,arXivpreprintarXiv:2410.02619\n(2024).\n[46] Z. Liang, H. Li, K. Jia, K. Guo, Q. Zhang, Gus-ir: Gaussian\nsplatting with unified shading for inverse rendering, arXiv preprint\narXiv:2411.07478 (2024).\n[47] Z. Bi, Y. Zeng, C. Zeng, F. Pei, X. Feng, K. Zhou, H. Wu, Gs3: Ef-\nficient relighting with triple gaussian splatting, in: SIGGRAPH Asia\n2024 Conference Papers, 2024, pp. 1\u201312.\n[48] J. Fan, F. Luan, J. Yang, M. Hasan, B. Wang, Rng: Relightable neural\ngaussians, in: Proceedings of the Computer Vision and Pattern Recog-\nnition Conference, 2025, pp. 26525\u201326534.\n[49] K. Ye, C. Gao, G. Li, W. Chen, B. Chen, Geosplating: Towards ge-\nometry guided gaussian splatling for physically-based inverse rendering\n(2024).\n[50] X. Lai, Z. Tian, Y. Chen, Y. Li, Y. Yuan, S. Liu, J. Jia, Lisa: Rea-\nsoning segmentation via large language model, in: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2024, pp. 9579\u20139589.\n[51] H. Rasheed, M. Maaz, S. Shaji, A. Shaker, S. Khan, H. Cholakkal, R. M.\nAnwer, E. Xing, M.-H. Yang, F. S. Khan, Glamm: Pixel grounding large\n30\nmultimodal model, in: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2024, pp. 13009\u201313018.\n[52] H. Zhang, H. Li, F. Li, T. Ren, X. Zou, S. Liu, S. Huang, J. Gao,\nLeizhang, C. Li, et al., Llava-grounding: Grounded visual chat with\nlargemultimodalmodels, in: EuropeanConferenceonComputerVision,\nSpringer, 2024, pp. 19\u201335.\n[53] A. Kamath, J. Hessel, K.-W. Chang, What\u2019s\" up\" with vision-language\nmodels? investigating their struggle with spatial reasoning, arXiv\npreprint arXiv:2310.19785 (2023).\n[54] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, D. Parikh, Making\nthe v in vqa matter: Elevating the role of image understanding in visual\nquestionanswering, in: ProceedingsoftheIEEEconferenceoncomputer\nvision and pattern recognition, 2017, pp. 6904\u20136913.\n[55] J. L. Schonberger, J.-M. Frahm, Structure-from-motion revisited, in:\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2016, pp. 4104\u20134113.\n[56] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, Q. Jiang, C. Li,\nJ. Yang, H. Su, et al., Grounding dino: Marrying dino with grounded\npre-training for open-set object detection, in: European conference on\ncomputer vision, Springer, 2024, pp. 38\u201355.\n[57] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, P. Hedman,\nMip-nerf 360: Unbounded anti-aliased neural radiance fields, in: Pro-\nceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, 2022, pp. 5470\u20135479.\n[58] C. Yeshwanth, Y.-C. Liu, M. Nie\u00dfner, A. Dai, Scannet++: A high-\nfidelity dataset of 3d indoor scenes, in: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2023, pp. 12\u201322.\n[59] OpenAI, Gpt-5 system card,https://cdn.openai.com/\ngpt-5-system-card.pdf(2025).\n[60] Z. Huang, Y. He, J. Yu, F. Zhang, C. Si, Y. Jiang, Y. Zhang, T. Wu,\nQ. Jin, N. Chanpaisit, et al., Vbench: Comprehensive benchmark suite\n31\nfor video generative models, in: Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, 2024, pp. 21807\u2013\n21818.\n32\n",
    "title": "Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting",
    "authors": [
      "Jiangnan Ye",
      "Jiedong Zhuang",
      "Lianrui Mu",
      "Wenjie Zheng",
      "Jiaqi Hu",
      "Xingze Zou",
      "Jing Wang",
      "Haoji Hu"
    ],
    "published": "2025-11-17",
    "arxiv_id": "2511.13684v1",
    "num_pages": 33,
    "num_chars": 61696
  },
  {
    "text": "1\nCross-Learning from Scarce Data via Multi-Task\nConstrained Optimization\nLeopoldo Agorio, Juan Cervi \u02dcno, Miguel Calvo-Fullana, Alejandro Ribeiro, and Juan Andr \u00b4es Bazerque.\nAbstract\u2014A learning task, understood as the problem of\nfitting a parametric model from supervised data, fundamentally\nrequires the dataset to be large enough to be representative of the\nunderlying distribution of the source. When data is limited, the\nlearned models fail generalize to cases not seen during training.\nThis paper introduces a multi-taskcross-learningframework\nto overcome data scarcity by jointly estimatingdeterministic\nparameters across multiple, related tasks. We formulate this joint\nestimation as a constrained optimization problem, where the con-\nstraints dictate the resulting similarity between the parameters of\nthe different models, allowing the estimated parameters to differ\nacross tasks while still combining information from multiple\ndata sources. This framework enables knowledge transfer from\ntasks with abundant data to those with scarce data, leading\nto more accurate and reliable parameter estimates, providing\na solution for scenarios where parameter inference from limited\ndata is critical. We provide theoretical guarantees in a controlled\nframework with Gaussian data, and show the efficiency of our\ncross-learning method in applications with real data including\nimage classification and propagation of infectious diseases.\nIndex Terms\u2014Supervised learning, multi-task, optimization.\nI. INTRODUCTION\nThe machine learning problem, in general, involves extract-\ning information from a dataset, which is typically achieved\nby fitting the parameters of a model [1], whether it be a\nneural network or a more specific parametric function that\nincorporates additional knowledge about the data source. Once\nfitted, this parametric model can be used for classification, pre-\ndiction, or estimation, serving various purposes. For example,\none might want to classify input signals, as in the case of a\nrobot or autonomous vehicle that captures an image and needs\nto detect whether there is an obstacle to avoid [2]. One might\nalso want to predict, for instance, the solar energy generation\ncapacity of a power system, in order to set prices in a day-\nahead market [3]. In some cases, estimating the parameters\nthemselves constitutes the learning objective. For example, an\ninfection propagation model, such as for influenza or COVID,\ncan be used to predict the number of infected individuals, but\nthe parameters themselves contain information about how the\npopulation behaves and which policies are most effective [4].\nThis work was supported in part by Spain\u2019s Agencia Estatal de Investigaci \u00b4on\nunder grant RYC2021-033549-I.\nL. Agorio and J. A. Bazerque are with the Department of Electrical En-\ngineering, School of Engineering, Universidad de la Rep \u00b4ublica, Montevideo,\n11300, Uruguay (e-mail: lagorio@fing.edu.uy,jbazerque@fing.edu.uy)\nJ. Cervi \u02dcno is with the Massachusetts Institute of Technology, Cambridge,\nMA 02139, USA (e-mail: jcervino@mit.edu).\nM. Calvo-Fullana is with the Department of Engineering, Universitat\nPompeu Fabra, Barcelona, 08018, Spain (e-mail: miguel.calvo@upf.edu).\nA. Ribeiro is with the Department of Electrical and Systems Engineer-\ning, University of Pennsylvania, Philadelphia, PA 19104, USA (e-mail:\naribeiro@seas.upenn.edu).Appropriate parameters are typically found by minimizing\na loss or fit function, which measures how well the model\nexplains the data for a given parameter set. Beyond this,\na fundamental design decision is whether to assume the\nparameters are deterministic or random, a choice that opens\ntwo distinct methodological paths [5]. In the Bayesian model,\nparameters are treated as random variables governed by a prior\ndistribution, which may in turn be modeled by a second layer\nof hyperparameters. In the contrasting deterministic paradigm,\nthe parameters are assumed to be fixed, unknown quantities\nthat define the data\u2019s underlying distribution. Both the prob-\nabilistic and deterministic approaches are valid. The choice\nbetween them often depends on one\u2019s confidence in a potential\nprior distribution. For instance, if parameters are known to lie\nwithin a certain interval, imposing a uniform prior distribution\nover that interval can improve estimation, especially in a low-\ndata regime. However, if this prior information is incorrect, it\nwill introduce bias, degrading the estimation quality. In this\npaper, we adopt the deterministic parameter paradigm. We do\nnot assume a prior distribution but instead seek to incorporate\nadditional information to improve performance, particularly\nwhen working in alow-data regime, as it the norm in diverse\nfields such as medical data [6], speech recognition [7], and\nanomaly detection [8].\nOur approach sits at the intersection of multi-task learning\n[9], multi-agent systems, and meta-learning [10], [11]. Multi-\ntask learning has the potential of augmenting the data by\naddressing different tasks together, incorporating data from\nmultiple sources or domains and combining it into a joint\nlearning problem. If these sources are related, then they cross-\nfertilize to improve performance. Such data augmentation is\nparticularly useful when data from all sources are scarce,\nor in cases in which there is a particular incipient source\nwhich has not produced enough data yet. This bring us\nto the concept of meta-learning, which seeks to determine\nwhether models trained on past data are useful for explaining\nfuture data\u2014especially when that future data comes from a\npreviously unseen source. Through meta-learning, we can use\npast experience to learn quickly in a new situation, combining\ninformation from similar past cases with the limited data\navailable in the new context. The intuition from multi-task\nand meta-learning leads us to proposecross-learningas a\nmethodology for combining the training from multiple sources.\nThe central challenge lies inwhatto share between sources\nandhowto share it [12], [13]. Various approaches have been\nproposed. In the deep learning context, some studies explore\nsharing common data representations, which often translates to\nsharing the early layers of a neural network [14], [15]. OtherarXiv:2511.13680v1  [cs.LG]  17 Nov 2025\n2\nworks have developed deep architectures with specific blocks\nshared across tasks [16], [17], often empirically determining\nwhich shared blocks yield the most significant impact. In\nthese approaches, part of the architecture is common, while\nanother part remains private to each task. Measuring task-\nrelatedness is also an active research area in the field of multi-\ntask learning. Some work focuses on grouping tasks to learn\nthem jointly within each group [18]. Other methods evaluate\ntask differences by measuring discrepancies between their\nlearned models [19], or use a convex surrogate of the learning\nobjective to model task relationships [20]. In contrast to\nBayesian formulations that model task-relatedness via shared\npriors [21], [22], our approach does not assume any prior\nstructure on the learned functions.\nIn this paper, we formalize and extend cross-learning for\nsupervised learning, a methodology based on a constrained\noptimization framework we originally applied in the reinforce-\nment learning paradigm [23], [24]. The core idea is to manage\nthe bias-variance trade-off in multi-task settings explicitly. Our\npreliminary work [25] drew intuition from the two extremes\nof multi-task learning: (i) training separate, task-specific es-\ntimators (using only local data, which can suffer from high\nvariance) and (ii) training a single, consensus model on all\ndata (which can introduce high bias if tasks are dissimilar).\nCross-learning finds a balance between these two extremes by\nallowing task-specific models while imposing constraints that\nkeep them close to each other, thereby controlling the bias-\nvariance trade-off. This work builds on that foundation with\nseveral key contributions. First, we provide a theoretical proof\nthat the cross-learning formulation successfully exploits this\ntrade-off, outperforming both the naive separate and consensus\napproaches. Second, we generalize the method to support\narbitrary functional constraints, lifting intuition from [26].\nThis is a critical extension, particularly for neural networks,\nwhere models with close parameters can still produce highly\ndistant output distributions. By imposing constraints directly\non the model outputs rather than the parameter space, we can\nmore effectively regularize the model\u2019s behavior. Third, we\ndevelop new algorithms to solve these optimization problems\nin the dual domain. Finally, we validate our theoretical findings\nand algorithmic performance through experiments on two\ndistinct problems. The first involves fitting an infection model\nusing data from the COVID-19 pandemic, treating data from\ndifferent countries as separate but related tasks. The second\nis an image classification task to distinguish objects using\ndistinct representations of the same object. The experimental\nresults confirm that cross-learning consistently outperforms the\nalternative approaches of training completely separate models\nor merging all data into a single consolidated model.\nII. MULTI-TASK LEARNING\nWe consider the problem of jointly optimizing a set of para-\nmetric functions, each solving a regression or classification\ntask over a separate dataset. These datasets are collected from\ndifferent butrelatedsources; therefore, we seek a method to fit\nthedeterministicparameters of the associated functions jointly.\nFormally, consider a finite set of taskst\u2208[1, . . . , T], and letthe parametric functionf:X \u00d7\u0398\u2192 Ybe the map between\nthe input spaceX \u2282RP, and output spaceY \u2282RQparame-\nterized by\u03b8\u2208RS. Our goal is to find the parameterizations\nthat minimize the expected loss\u2113:RQ\u00d7RQ\u2192R+over the\nprobability distributionp t(x, y),\n{\u03b8\u22c6\nt}T\nt=1= arg min\n{\u03b8t}T\nt=11\nTTX\nt=1Ept(x,y)[\u2113(y, f(x, \u03b8 t))].(P ML)\nWe do not assume, however, to have access to the distribu-\ntionsp t(x, y). Instead, we align with the multi-task learning\nliterature, assuming that each tasktis equipped with a dataset\nDt, containingN tsamples(x i, yi)\u2208(X,Y), i= 1, . . . , N t.\nThe datasetsD tare assumed to be drawn according to the\nunknown joint probabilitiesp t(x, y). The empirical version of\nthe multi-task learning problem (P ML) can thus be written as,\n{\u02c6\u03b8t}T\nt=1= arg min\n{\u03b8t}T\nt=11\nTTX\nt=11\nNtNtX\ni=1\u2113(yi, f(x i, \u03b8t)).(P S)\nThe sole difference between the empirical multi-task learning\nproblem (P S) and its statistical counterpart (P ML) is the fact\nthat the expectations overp t(x, y)have been replaced by an\nempirical sum overD t. Under a deterministic model for the\nparameters\u03b8 t, this empirical problem (P S) isseparableacross\ntasks. However, given that the number of samplesN tis finite,\nsolving for the parameters \u02c6\u03b8tseparately can be detrimental. If\nthere is correlation across tasks, solving them separately can\nlead to a loss of information, as data from one task could be\nbeneficial for learning others. Such separate estimation defeats\nthe primary purpose of multi-task learning, which is to learn\ntasks jointly. To leverage the joint information from different\nsources, a simple approach is to merge all datasets and learn\na common solution for all tasks, or equivalently, imposing a\nconsensus constraint\u03b8 t=\u03b8for allt, forcing all tasks to be\nmodeled by the same parameter\u03b8 c, solution to\n\u02c6\u03b8c= arg min\n\u03b81\nTTX\nt=11\nNtNtX\ni=1\u2113(yi, f(x i, \u03b8)).(P C)\nThisconsensusapproach (P C) does provide a joint solution\nto our multi-task learning problem, merging data from all\nsources. However, forcing a strict consensus might be too\nrestrictive in practice, as a solution that is good for all tasks\nmight not be the best for any individual task. Hence, we\nexplore a more balanced approach that trades off between\nlearning individual parameters \u02c6\u03b8t, as in theseparableproblem\n(PS), and a common parameter \u02c6\u03b8cas in theconsensusmulti-\ntask learning problem (P C).\nA. Multi-Task Learning Bias-Variance Trade-Off\nTo motivate our proposed approach, let us present a reduc-\ntive yet informative example to illustrate how bias and variance\naffect the solutions to (P S) and (P C).\nExample 1.Consider the task of recovering a parameter\u03b8\u22c6\nt\u2208\nRdfrom samples corrupted by additive, zero-mean, i.i.d. noise\nytn=\u03b8\u22c6\nt+\u03b7tn, n= 1, . . . , N t (1)\n3\nwith\u03b7 tn\u223c N(0, \u03c3I). To estimate\u03b8\u22c6\ntfrom the samplesy tn,\nwe minimize the mean square error as follows,\n\u02c6\u03b8t= arg min\n\u03b81\nNtNtX\nn=1||ytn\u2212\u03b8||2=1\nNtNtX\nn=1ytn.( \u00afPS)\nThe estimator \u02c6\u03b8tin (\u00afPS), is a particular case of the separable\nestimator (P S) with a constant regressorf(x, \u03b8) =\u03b8and\nsquare loss\u2113(y, f(x, \u03b8)) =\u2225y\u2212\u03b8\u22252. If the true parameters\n{\u03b8\u22c6\nt}are close to one another, it might be beneficial to consider\na single estimator based on all available samples,\n\u02c6\u03b8c= arg min\n\u03b8TX\nt=11\nNtNX\nn=1||ytn\u2212\u03b8||2,( \u00afPC)\nwhich is the simplified version of (P C) for the model in\n(1). Notice that in this reduced case, the consensus estimator\nadmits a closed-form solution given by the average of all sam-\nples, which can be expressed as the average \u02c6\u03b8c=1\nTPT\nt=1\u02c6\u03b8t.\nof the separate estimates \u02c6\u03b8tin (\u00afPS), Since the noise is zero-\nmean, the estimators in ( \u00afPS) are unbiased, and by virtue of the\ni.i.d. model, their variance reduces with the number of samples\naccording to\nvar(\u02c6\u03b8t) =E\u0014\r\r\r\u02c6\u03b8t\u2212E[\u02c6\u03b8t]\r\r\r2\u0015\n=d\nNt\u03c32.(2)\nOn the other hand, we show in Appendix A, that the consensus\nestimator has variance var( \u02c6\u03b8c) =1\nT2PT\nt=1d\nNt\u03c32, which\nsimplifies if all datasets have the same sizeN t=N,\nvar(\u02c6\u03b8c) =1\nTd\nN\u03c32.(3)\nIntuitively, pooling data from all tasks increases the effective\ndataset size fromN t=Nin (2) toTNin (3), thereby\nreducing the uncertainty in the average estimate. However, this\nreduction in variance comes at the cost of introducing bias:\nEh\n\u02c6\u03b8c\u2212\u03b8\u22c6\nti\n=1\nTTX\n\u03c4=1E[\u02c6\u03b8\u03c4]\u2212\u03b8\u22c6\nt=1\nTTX\n\u03c4=1(\u03b8\u22c6\n\u03c4\u2212\u03b8\u22c6\nt)\u0338= 0.\nIn the light of this basic example, we propose the following\ncross-learningestimator that balances bias and variance by\nlying in between the fully separable solution (P S) and the\nconsensus counterpart (P C). This approach allows the esti-\nmated parameters to differ across tasks while still combining\ninformation from multiple data sources,\n{\u03b8\u2020\nt}, \u03b8\u2020\ng= arg min\n{\u03b8t},\u03b8g1\nTTX\nt=11\nNtNtX\ni=1\u2113(yi, f(x i, \u03b8t))(P CL)\nsubject to:\u2225\u03b8 t\u2212\u03b8g\u2225 \u2264\u03f5, t= 1, . . . , T.\nThecross-learningproblem (P CL) introduces a task-specific\nparameter\u03b8\u2020\ntfor each task and a global parameter\u03b8\u2020\ngshared\namong all tasks. This formulation connects the separable\nmulti-task learning problem (P S) and the consensus approach\n(PC) by imposing a constraint on the closeness of the task-\nspecific parameters to the global parameter. This constraint\nrelaxes the strict equality of the consensus problem, allowing\nthe solutions for different tasks to vary.The centrality parameter\u03f5controls the degree of similarity\nbetween the task-specific solutions. A larger\u03f5allows solutions\nto be more task-specific, while a smaller\u03f5encourages them\nto be closer to the global solution. Note that, by enforcing\n\u03f5= 0, all solutions are forced to be equal and (P CL) becomes\nequivalent to enforce consensus in (P C). Conversely for a\nsufficiently large\u03f5, the constraint becomes inactive, and (P CL)\nis equivalent to the separable problem (P S).\nThe advantages of solving problem (P CL) are twofold. First,\nunlike the separable problem it combines information across\ntasks through the constraint, thereby exploiting data from\nrelated tasks. Second, unlike the consensus approach, it allows\nfor task-specific solutions, leading to better performance on\nindividual tasks. As we will show, cross-learning controls the\nbias-variance trade-off by enforcing the task-specific solutions\nto be close. We will formally prove that for the simplified\nmodel in Example 1, there exists a value of\u03f5for which our\ncross-learning estimator achieves a guaranteed improvement\nin mean squared error compared to both the separable and\nconsensus estimators. And we will demonstrate that these\nimprovements generalize to more complex scenarios in ex-\nperiments with real data.\nIII. PERFORMANCEANALYSIS\nThroughout this section, we will consider the simplified\nmodel given by (1), with the associated regressorf(x, \u03b8) =\u03b8\nand the quadratic loss\u2113(y, f(x, \u03b8)) =\u2225y\u2212\u03b8\u22252.Under these\nassumptions, \u02c6\u03b8t= (1/N t)PNt\nn=1yntis a sufficient statistic\nand the cross-learning estimator in (P CL) simplifies to\n{\u03b8\u2020\nt}, \u03b8\u2020\ng= arg min\n\u03b8t,\u03b8gTX\nt=1||\u02c6\u03b8t\u2212\u03b8t||2(\u00afPCL)\nsubject to||\u03b8 t\u2212\u03b8g|| \u2264\u03f5, t= 1, . . . , T.\nWe will compare this estimator ( \u00afPCL) to its fully separable ( \u00afPS)\nand strict consensus ( \u00afPC) alternatives using the mean squared\nerror as a performance metric, for which we define\nES=1\nTTX\nt=1\r\r\r\u02c6\u03b8t\u2212\u03b8\u22c6\nt\r\r\r2\n,(4)\nEC=1\nTTX\nt=1\r\r\r\u02c6\u03b8c\u2212\u03b8\u22c6\nt\r\r\r2\n,(5)\nECL(\u03f5) =1\nTTX\nt=1\r\r\r\u03b8\u2020\nt(\u03f5)\u2212\u03b8\u22c6\nt\r\r\r2\n,(6)\nfor the errors resulting from the separable ( \u00afPS), consensus ( \u00afPC),\nand cross-learning ( \u00afPCL) estimators, respectively. The error is\nmeasured with respect to the true data-generating parameters\n\u03b8\u22c6\nt. In what follows, we show that by appropriately tuning the\ncentrality parameter\u03f5, the error of our cross-learning estimator\n(\u00afPCL) can always be made smaller than the error produced by\nboth the separate ( \u00afPS) and consensus estimators ( \u00afPC).\nTo begin with, we compare the cross-learning estimator to\nthe consensus one. We note thatE CL(\u03f5) =E Cwhen\u03f5= 0\nsince in this case the constraint in ( \u00afPCL) imposes consensus.\nIf we define the difference in error as\u2206E(\u03f5) :=E CL(\u03f5)\u2212\n4\nEC, which is zero at\u03f5= 0, it is sufficient to show that the\nderivative ofE[\u2206E(\u03f5)]is strictly negative at\u03f5= 0. This would\nguarantee the existence of an\u03f5 >0such thatE[\u2206E(\u03f5)]<\n0, meaning that the mean squared error of the cross-learning\nestimator is strictly lower than the consensus alternative, i.e.\nE[ECL(\u03f5)]<E[E C]. The following proposition proves that\nthe derivative ofE[\u2206E(\u03f5)]is indeed strictly negative.\nProposition 1.The cross-learning estimator( \u00afPCL), for an\n\u03f5approaching zero, achieves a strictly lower mean squared\nerror than the consensus estimator( \u00afPC). That is\nlim\n\u03f5\u21920+1\n\u03f5(E[E CL(\u03f5)\u2212 E C])<0.(7)\nProof.Leveraging Lemma 4 in Appendix B we have\nlim\n\u03f5\u21920+1\n\u03f5(E[E CL(\u03f5)\u2212 E C]) =\u22122\nTTX\nt=1(\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc)\u22a4E[\u02c6ut](8)\nwhere we defined\u02c6u t= (\u02c6\u03b8t\u2212\u02c6\u03b8c)/\u2225\u02c6\u03b8t\u2212\u02c6\u03b8c\u2225to simplify notation.\nWe will prove thatE[\u02c6u t] =a t(\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc)witha t>0, strictly.\nAfter doing so, we will be able to substitute it in (8) and obtain\nthe desired sign for\nlim\n\u03f5\u21920+1\n\u03f5(E[E CL(\u03f5)\u2212 E C]) =\u22122X\ntat\u2225\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc\u22252<0(9)\nIn order to proveE[\u02c6u t] =a t(\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc), let us analyze the\ndistribution of the difference \u02c6\u03b8t\u2212\u02c6\u03b8c. With \u02c6\u03b8t=\u03b8\u22c6\nt+ntand\n\u02c6\u03b8c=\u03b8\u22c6\nc+ (1/T)PT\nt=1ntwe can write\n\u02c6\u03b8t\u2212\u02c6\u03b8c=\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc+\u0012\n1\u22121\nT\u0013\nnt\u22121\nTX\nk\u0338=tnk (10)\nIfntare independent zero-mean Gaussian noise vectors with\ncovariance\u03c32Ifor allt= 1, . . . , Twe conclude that \u02c6\u03b8t\u2212\u02c6\u03b8c=\n\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc+vt, with zero-mean Gaussian noisev t\u223c N(0, \u03c32\nvI)\nof variance\u03c32\nv:= (T\u22121)\u03c32/T. Under this noise distribution,\nwe can proceed to compute\nE[\u02c6ut] =1\n(2\u03c0\u03c32v)n/2Z\nRn\u02c6ute\u2212||vt||2/(2\u03c32\nv)dvt.(11)\nwhere\u02c6u tis a function of the integration variablev tgiven by\n\u02c6ut= (\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc+vt)/\u2225\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc+vt\u2225. To evaluate this integral,\nwe introduce the polar change of variables illustrated in Fig. 1.\nSpecifically, we express the vector \u02c6\u03b8t\u2212\u02c6\u03b8c=\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc+vt=rtut\nin polar form with the variabler t\u2208R, r t\u22650representing\nits norm, andu trepresenting the phasor in the unit sphere\nS. Notice that by definition,\u02c6u tis unit norm and points in\nthe direction of \u02c6\u03b8t\u2212\u02c6\u03b8c. Thus, we can identifyu t= \u02c6utand\nexpress the change of variables asv t=rt\u02c6ut\u2212(\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc)with\nrt\u2208(0,\u221e),and\u02c6u t\u2208 S:={u\u2208Rn:||u||= 1}, and rewrite\nE[\u02c6ut] =Z\nSZ\u221e\nr=0urn\u22121\n(2\u03c0\u03c32v)n/2e\u2212\u2225ru\u2212(\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc)\u22252\n2\u03c32v drdu=Z\nSup(u)du\n(12)\n1S\u02c6\u03b8t\u2212\u02c6\u03b8c\n\u02c6utrt\u03c3v vt\n\u03b8\u22c6\nt\u2212\u03b8\u2217\ncFig. 1: Intuitive interpretation of proof and the symmetry of\n\u02c6ut= (\u02c6\u03b8t\u2212\u02c6\u03b8c)/\u2225\u02c6\u03b8t\u2212\u02c6\u03b8c\u2225on step (14) of the proof of Proposi-\ntion 1. The vector \u02c6\u03b8t\u2212\u02c6\u03b8cadmits two representations, one as\n\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc+vtand a second one asr tut, withr t=\u2225\u02c6\u03b8t\u2212\u02c6\u03b8c\u2225,\nwhich leads to the change of variablesv t=rtut\u2212\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc.\nFurthermore, vectors\u02c6u tcome in pairs. That is, for each blue\npoint representing the vector\u02c6u t, there is a purple mirror image\nacross\u03b8\u22c6\nt\u2212\u03b8\u22c6\ncsuch that its inner product with\u03b8\u22c6\nt\u2212\u03b8\u22c6\nchas\nsame magnitude but opposite sign.\nwhere we incorporated the Jacobian|J|=rn\u22121\nt, and defined\np(u)as\np(u) =Z\u221e\nr=0rn\u22121\u03c3v\n(2\u03c0\u03c32v)n/2e\u2212\u2225ru\u2212(\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc)\u22252\n2\u03c32v dr(13)\n=Z\u221e\nr=0rn\u22121e\u2212q(r)\n(2\u03c0\u03c32v)n/2er\n\u03c3v(\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc)\u22a4udr(14)\nand substitutedq(r) := (r2\u03c32\nv+\u2225\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc\u22252)/(2\u03c32\nv). We finish\nthe proof with a symmetry argument. Notice thatp(u)in (14)\ndepends onuthrough the inner product between(\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc)and\nu. Therefore two vectorsuwith opposite angles to(\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc)\n(as those depicted in blue and purple in Fig. 1) receive opposite\nweightsp(u)in the integral (12). That means that aggregate\ncontribution of these two mirrored points to (12) is a vector\nin the same direction as\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc. Since the sphereShas axial\nsymmetry around\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc, allu\u2208 Shas a mirrored image\naround\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc. Thus, the integralR\nSup(u)duin (12) is a\nvector pointing in the direction of\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc. It follows thatE[\u02c6u t]\nmust be colinear with(\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc), i.e.,E[\u02c6u t] =a t(\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc)with\nat\u2208R. Furthermore, according to (14) vectorsuwith acute\nangles to(\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc)have heavier weights in the integral, hence\nat>0, strictly, as we wanted to prove.\u25a0\nProposition 1 is relevant to our analysis in the sense that\nit guarantees the existence of a value\u03f5 >0for which the\ncross-learning estimator yields a lower mean-squared error\nthan the consensus one. Indeed, with consensus and cross-\nlearning estimators being equivalent for\u03f5= 0, (7) implies\nthat by increasing\u03f5slightly, we obtain better performance.\nNow that we have established that the cross-learning esti-\nmator can outperform the consensus one, it remains to show\nthat it also improves with respect to the separable estimator.\nFor this purpose, we establish two bounds forE CL(\u03f5)in terms\nofES. The first one, in Proposition 2, states that if we choose\n\u03f5large enough, thenE CL(\u03f5)\u2264 E S.\nProposition 2.Let{y tn}Nt\nn=1 be the dataset associated with\ntaskt= 1, . . . , T, and \u02c6\u03b8t=1\nNtPNt\ni=1ytnits sample average.\n5\n\u03f5\n\u03b8\u2020\ng\n\u03b8\u22c6\nt\u03b8\u22c6\n1\u03b8\u22c6\n2\n\u03b8\u2020\nt\n\u02c6\u03b81\u02c6\u03b8t\u02c6\u03b82\nFig. 2: Geometric argument for the proof of Proposition 2.\nThe generators\u03b8\u22c6\ntare inside the ballB(\u03b8\u2020\ng, \u03f5). The separate\nestimates \u02c6\u03b8tare projected intoB(\u03b8\u2020\ng, \u03f5)resulting in the cross-\nlearning estimator\u03b8\u2020\nt. Points \u02c6\u03b8tin the red region will not be\nchanged, so that \u02c6\u03b8t=\u03b8\u2020\nt, and points in the blue region will be\nprojected to the surface where the error to\u03b8\u22c6\ntwill be lower.\nIf\u03f5is chosen large enough to satisfy\n\u03f5\u2265max\ntmax\n\u03c4\u2225\u03b8\u22c6\nt\u2212\u02c6\u03b8\u03c4\u2225,(15)\nthen the deterministic error of the cross-learning estimator is\nsmaller than the error of the separable estimator, i.e.,\nECL(\u03f5)\u2212 E S\u22640.(16)\nProof.We will start by showing that (15) implies that the\ngenerators\u03b8\u22c6\ntbelong to the ball of center\u03b8\u2020\ngand radius\u03f5,\nwhich we denote byB(\u03b8\u2020\ng, \u03f5), i.e.,\n||\u03b8\u22c6\nt\u2212\u03b8\u2020\ng|| \u2264\u03f5,for allt.(17)\nFor that purpose, we consider Lemma 1 in Appendix B, which\nestablishes that\u03b8\u2020\ngcan be written as a convex combination\n\u03b8\u2020\ng=PT\nt=1\u03b3t\u02c6\u03b8tof the separable estimators, with coefficients\n\u03b3t\u22650such thatPT\nt=1\u03b3t= 1. It follows\u03b8\u22c6\nt\u2212\u03b8\u2020\ng=PT\n\u03c4=1\u03b3\u03c4(\u03b8\u22c6\nt\u2212\u02c6\u03b8\u03c4), and thus\n||\u03b8\u22c6\nt\u2212\u03b8\u2020\ng|| \u2264TX\n\u03c4=1\u03b3\u03c4||\u03b8\u22c6\nt\u2212\u02c6\u03b8\u03c4|| \u2264max\n\u03c4=1,...,T||\u03b8\u22c6\nt\u2212\u02c6\u03b8\u03c4|| \u2264\u03f5,(18)\nso that (17) holds for allt= 1, . . . , T.\nNext, notice that by virtue of Lemma 2 in Appendix B, the\ncross-learning estimator\u03b8\u2020\ntis the projection\u03b8\u2020\nt=PB(\u02c6\u03b8t)of\u02c6\u03b8t\nto the ballB:=B(\u03b8\u2020\ng, \u03f5)of radius\u03f5and center\u03b8\u2020\ng(see Figure\n2). We have shown in (18) that all\u03b8\u22c6\ntbelong toB. Thus, we\nhaveP B(\u03b8\u22c6\nt) =\u03b8\u22c6\nt, and since the projection is non-expansive\nwe can bound\n||\u03b8\u22c6\nt\u2212\u03b8\u2020\nt||=||P B(\u03b8\u22c6\nt)\u2212P B(\u02c6\u03b8t)|| \u2264 ||\u03b8\u22c6\nt\u2212\u02c6\u03b8t||,(19)\nfor allt= 1, . . . , T. The desired result (16) follows from\naveraging both sides of (19) acrosst.\u25a0\nExample 2To showcase Propositions 1 and 2, we present a\ncontrolled example in which data come from a multivariate\nGaussian distribution inR2. For each task, we consider the\ncenters at[\u00b5,0],[\u2212\u00b5,0],[0, \u00b5], and[0,\u2212\u00b5]. In this case, the\nmean square error is\u00b52+\u03c32/2for the consensus estimator and\n2\u03c32for the separable case. Figure 3 shows the errorE CL(\u03f5)\n0 2 4 6 8 100.811.21.4\n\u03f5MSE\u03c3= 1\nsampled\nclosed-form\n0 2 4 6 8 102.533.54\n\u03f5MSE\u03c3= 2\nsampled\nclosed-formFig. 3: Mean square error of the cross-learning estimate as a\nfunction of\u03f5. The value of\u03f5interpolates between consensus\n(\u03f5= 0), and separate estimation (\u03f5=\u221e). We present the\ncase with small variance (\u03c3= 1) wherein the separable\nestimator outperforms the consensus one, and the case with\nlarge variance (\u03c3= 2) in which consensus is better than\nseparate estimation. In either case, the plots exhibit a value\nof\u03f5\u2208(0,\u221e)for which cross-learning outperforms both the\nconsensus and separable estimators.\nfound in simulations for\u03f5\u2208(0,10). There is a value of\u03f5\nin Figure 3 such that the cross-learning estimator outperforms\nboth the consensus and separable estimators. This corresponds\nwith Proposition 1, which proves that there exists a value\u03f5\nsuch that the cross-learning estimator outperforms its consen-\nsus counterpart. It also lines up with Proposition 2, which\nshows that if\u03f5\u2265\u03f5 0= max tmax \u03c4\u2225\u03b8\u22c6\nt\u2212\u02c6\u03b8\u03c4\u2225>0, the mean\nsquare error of the cross-learning estimator is bounded from\nabove by the mean square error of the separate counterpart.\nThe comparison in Figure 3, however, looks better than our\ntheoretical result in Proposition 2, which only established a\nlower-than-or-equal type of bound. Thus, Proposition 2 did\nnot guarantee the performance of the cross-learning estimator\nto be strictly better than the separate counterpart. To show\nthat indeed the cross-learning estimator outperforms the sep-\narate one, we need yet another proposition. In this direction,\nconsider the set\nC(\u03f5)={( \u02c6\u03b81, . . . , \u02c6\u03b8T):\u2203t\u2208{1,. . . ,T}s.t.\u2225 \u02c6\u03b8t\u2212\u03b8\u2020\ng\u2225\u22653\u03f5}.(20)\nwith\u03b8\u2020\ngbeing the centroid in ( \u00afPCL). Proposition 3, below,\nestablishes that the cross-learning presents a strictly lower\nerror than the separable estimator when( \u02c6\u03b81, . . . , \u02c6\u03b8T)is chosen\nfrom the setC(\u03f5). Furthermore, the probability ofC(\u03f5)is\nstrictly positive, which together with the uniform bound in\nProposition 2 is sufficient to prove that the mean squared error\naveraged across all datasets inside and outsideC(\u03f5)is strictly\nlower when using cross-learning.\nProposition 3.There exists a value\u03f5 0>0such that for all\n\u03f5 > \u03f5 0, if\u03f5and( \u02c6\u03b81, . . . , \u02c6\u03b8T)\u2208 C(\u03f5)are substituted in( \u00afPCL),\n6\nthe errorE CL(\u03f5)of the resulting estimator\u03b8\u2020\ntis bounded by\nECL(\u03f5)\u2264 E S\u22121\nT\u03f52.(21)\nProof.For each dataset( \u02c6\u03b81, . . . , \u02c6\u03b8T)and\u03f5 >0compute\u03b8\u2020\ng\nvia cross-learning, and define\n\u03f50= inf{\u03f5 >0 :\u2225\u03b8\u22c6\nt\u2212\u03b8\u2020\ng\u2225 \u2264\u03f5for allt= 1, . . . , T}.(22)\nWe have shown in (17) that for any\u03f5\u2265max tmax \u03c4\u2225\u03b8\u22c6\nt\u2212\u02c6\u03b8\u03c4\u2225\nas in (15), the condition\u2225\u03b8\u22c6\nt\u2212\u03b8\u2020\ng\u2225 \u2264\u03f5in (22) is satisfied. Thus,\nthe set in (22) is nonempty and the infimum in (22) is well\ndefined. Furthermore,\u03f5 0is strictly positive, since it depends\non the pairwise distances between the ground truth parameters\n\u03b8\u22c6\ntwhich are assumed to be different one each other. Next, we\nconsider the setC(\u03f5), defined in (20) and depicted in Figure\n2, which collects (sufficient statistics of) datasets( \u02c6\u03b81, . . . , \u02c6\u03b8T)\nsuch that at least one of its points \u02c6\u03b8tbelongs to a (blue) zone\noutside of the ballB(\u03b8\u2020\ng, \u03f5). Projecting \u02c6\u03b8tintoB(\u03b8\u2020\ng, \u03f5)results\nin a\u03b8\u2020\ntthat is closer to\u03b8\u22c6\nt. Following this intuition, we will\nshow that (21) is satisfied for all( \u02c6\u03b81, . . . , \u02c6\u03b8T)\u2208 C(\u03f5). In this\ndirection, we use the triangle inequality to write\n3\u03f5\u2264 \u2225 \u02c6\u03b8t\u2212\u03b8\u2020\ng\u2225=\u2225 \u02c6\u03b8t\u2212\u03b8\u22c6\nt+\u03b8\u22c6\nt\u2212\u03b8\u2020\ng\u2225(23)\n\u2264 \u2225\u02c6\u03b8t\u2212\u03b8\u22c6\nt\u2225+\u2225\u03b8\u22c6\nt\u2212\u03b8\u2020\ng\u2225 \u2264 \u2225 \u02c6\u03b8t\u2212\u03b8\u22c6\nt\u2225+\u03f5,(24)\nwhere we used (22) and (20), valid for one \u02c6\u03b8t. It follows that\n\u2225\u02c6\u03b8t\u2212\u03b8\u22c6\nt\u2225 \u22652\u03f5.(25)\nOn the other hand, since\u03b8\u2020\ntis the projection of \u02c6\u03b8tonto the\nballB(\u03b8\u2020\ng, \u03f5), then \u02c6\u03b8t\u2212\u03b8\u2020\ntmust be co-linear with \u02c6\u03b8t\u2212\u03b8\u2020\ngsuch\nthat\u02c6\u03b8t\u2212\u03b8\u2020\nt=\u00b5t(\u02c6\u03b8t\u2212\u03b8\u2020\ng). Hence, we can write \u02c6\u03b8t\u2212\u03b8\u2020\ng=\n\u02c6\u03b8t\u2212\u03b8\u2020\nt+\u03b8\u2020\nt\u2212\u03b8\u2020\ng=\u00b5 t(\u02c6\u03b8t\u2212\u03b8\u2020\ng) +\u03b8\u2020\nt\u2212\u03b8\u2020\ngor equivalently\n(1\u2212\u00b5 t)(\u02c6\u03b8t\u2212\u03b8\u2020\ng) =\u03b8\u2020\nt\u2212\u03b8\u2020\ng. Hence, we can bound\n\u2225\u03b8\u2020\nt\u2212\u03b8\u22c6\nt\u2225=\u2225\u03b8\u2020\nt\u2212\u03b8\u2020\ng+\u03b8\u2020\ng\u2212\u03b8\u22c6\nt\u2225\n=\u2225(1\u2212\u00b5 t)(\u02c6\u03b8t\u2212\u03b8\u2020\ng) +\u03b8\u2020\ng\u2212\u03b8\u22c6\nt\u2225\n\u2264 \u2225\u02c6\u03b8t\u2212\u03b8\u22c6\nt\u2225+\u00b5 r(\u2225\u03b8\u2020\ng\u2212\u03b8\u22c6\nt\u2225 \u2212 \u2225 \u02c6\u03b8t\u2212\u03b8\u22c6\nt\u2225)\n\u2264 \u2225\u02c6\u03b8t\u2212\u03b8\u22c6\nt\u2225+\u00b5 r(\u03f5\u22122\u03f5)\nwhere we used (22) and (25). Moreover, since according to\n(20) \u02c6\u03b8t/\u2208 B(\u03b8\u2020\ng, \u03f5), the projecting constraint must be active,\nyielding\u03f5=\u2225\u03b8\u2020\nt\u2212\u03b8\u2020\ng\u2225= (1\u2212\u00b5 t)\u2225\u02c6\u03b8t\u2212\u03b8\u2020\ng\u2225 \u22653\u03f5(1\u2212\u00b5 t)\naccording to (20), which implies\u00b5 t\u22652/3. It follows,\n\u2225\u03b8\u2020\nt\u2212\u03b8\u22c6\nt\u2225 \u2264 \u2225 \u02c6\u03b8t\u2212\u03b8\u22c6\nt\u2225 \u2212\u00b5 r\u03f5\u2264 \u2225 \u02c6\u03b8t\u2212\u03b8\u22c6\nt\u2225 \u22122\n3\u03f5.(26)\nTo conclude the proof, we square both sides of (26) and\nsubstitute (25) to obtain\n\u2225\u03b8\u2020\nt\u2212\u03b8\u22c6\nt\u22252\u2264\u0012\n\u2225\u02c6\u03b8t\u2212\u03b8\u22c6\nt\u2225 \u22122\u03f5\n3\u00132\n(27)\n\u2264 \u2225\u02c6\u03b8t\u2212\u03b8\u22c6\nt\u22252\u221222\u03f5\n3\u2225\u02c6\u03b8t\u2212\u03b8\u22c6\nt\u2225+\u00122\u03f5\n3\u00132\n(28)\n\u2264 \u2225\u02c6\u03b8t\u2212\u03b8\u22c6\nt\u22252\u221222\u03f5\n3(2\u03f5) +\u00122\u03f5\n3\u00132\n(29)\n\u2264 \u2225\u02c6\u03b8t\u2212\u03b8\u22c6\nt\u22252\u2212\u03f52.(30)The inequality (30) is true for at least onet\u2208 {1, . . . , T}\nsuch that\u2225 \u02c6\u03b8t\u2212\u03b8\u2020\ng\u2225 \u22653as defined inC(\u03f5). For all other\nt\u2208 {1, . . . , T}we still have (19). Recall that (19) relies on\n\u2225\u03b8\u22c6\nt\u2212\u03b8\u2020\ng\u2225 \u2264\u03f5in (17), which holds for all\u03f5 > \u03f5 0. Thus, we\ncan average acrosstand the slack\u03f52in (30) will reduce by a\nfactorTwhich results in (21).\u25a0\nIf we can show thatC(\u03f5)has positive probability, then the\ncross-learning estimator will have a strictly lower mean-square\nerror than the separable one. We will follow that path in\nestablishing that the cross-learning estimator outperforms the\nseparate and consensus ones in our main result.\nTheorem 1.Consider a set of deterministic parameters\u03b8\u22c6\nt,\nt= 1, . . . , Tand their associated datasets collecting data cor-\nrupted by zero-mean Gaussian noise\u03bd tn\u223c N(0, \u03c3)according\nto the additive modely tn=\u03b8\u22c6\nt+\u03bdtn, t= 1, . . . , T, n=\n1, . . . , N t. Define the square errorsE S,EC, andE CL(\u03f5), as\nin(4),(5), and(6)by quantifying the error between a single\nground truth parameter\u03b8\u22c6\ntand the solutions of the separable\n(\u00afPS), consensus( \u00afPC), and cross-learning( \u00afPCL)estimators,\nrespectively. Under these definitions, we have\nE\u0014\ninf\n\u03f5>0ECL(\u03f5)\u2212 E C\u0015\n<0,(31)\nE\u0014\ninf\n\u03f5>0ECL(\u03f5)\u2212 E S\u0015\n<0.(32)\nProof.We start by comparing the errors yield by consensus\nand by the separate estimator, dividing the proof in two com-\nplementary casesE[E C]\u2264E[E S]orE[E C]>E[E S]. Starting\nwith the caseE[E C]<E[E S], we established in Proposition\n1 that there exists an\u03f5 >0such that (31) holds. Since we\nassumedE[E C]<E[E S], then (32) must also hold. In the case\nE[ES]<E[E C], we will first argue that for any dataset, we\ncan find a value of\u03f5such thatE CL(\u03f5)\u2212 ESis lower than or\nequal to zero. But, this is a direct consequence of Proposition\n2, since the right-hand side of (15) only depends on the dataset.\nTherefore, (32) holds, and since we are considering the case\nE[ES]<E[E C], then (31) must hold. To ensure that the\ninequality in (32) is strict we argue that the setC(\u03f5)in (20)\nis nonempty because we can separate the points \u02c6\u03b8taway in\nFig. 2 keeping\u03b8\u2020\ngunchanged, so that\u2225 \u02c6\u03b8t\u2212\u03b8\u2020\ng\u2225surpasses3\u03f5\nin (20). Furthermore, if we allow\u03b8\u2020\ngto move infinitesimally,\nwe can move the points \u02c6\u03b8tin a setC(\u03f5)with positive volume,\nso that the probability ofC(\u03f5)is positive under our Gaussian\ndata model. This positive probability together with (21) yields\n(16), concluding the proof.\u25a0\nThe strict inequalities in Theorem 1 mean that by selecting\nthe value of\u03f5properly (possibly depending on the dataset\nsamples{y tn}) the cross-learning estimator outperforms its\nseparable and consensus counterparts. Through the supporting\nPropositions 1 and 2, which are described by Figures 1 and\n2 respectively, we recognize that the cross-learning constraint\ncan simultaneously reduce the bias affecting the consensus\nestimator, and mitigate the higher variance of a separate\nresult, attaining an optimal balance in between. Although\nthese theoretical findings were formally established within a\ncontrolled framework with Gaussian data, this intuition about\n7\nhow the cross-learning method achieves this optimal balance\nin the variance-bias tradeoff generalizes to more complex sce-\nnarios. Specifically, this balance carries out to the experiments\nof Section VI-A which are performed on real data. Before\npresenting these experiments, we propose a variant of cross-\nlearning that is preferable when the model outputs, as opposed\nto their parameters, are assumed to be close to one another.\nIV. CROSS-LEARNING WITHCOUPLEDOUTPUTS\nOur cross-learning formulation (P CL) so far has put empha-\nsis on the model parameters. While there are many problems\nwhere obtaining the parameters of a model is the primary goal,\nin most cases, they are just a vehicle to obtain a regression\nor classification output. Particularly when neural networks\nare involved, the optimal values of the filter taps provide\nlittle to no intuition into the phenomenon being learned.\nMoreover, close distances between these parameters may not\nreflect similar outputs to the same inputs. In these cases,\nit may be reasonable to leverage similarities in the model\noutputs themselves, as opposed to their parameters, for which\nwe introduce an alternative cross-learning formulation with\nfunctional constraints\n{\u03b8\u2020\nt}, \u03b8\u2020\ng= arg min\n{\u03b8t},\u03b8g1\nTTX\nt=11\nNtNtX\ni=1\u2113(yi, f(x i, \u03b8t))](P CLF)\nsubject to1\nNtNtX\ni=1|f(x i, \u03b8t)\u2212f(x i, \u03b8g)| \u2264\u03f5,\nwith\u2113:RQ\u00d7RQ\u2192R +, \u2113(y 1, y2) = 0iify 1=y 2,\nbeing a non-negative loss function that measures the quality\nof learning. As before, we fit the parameters\u03b8 tof the models\nf(x, \u03b8 t)jointly across tasks, but now the model outputs are\ncoupled via the functional constraints.\nWhile we will not provide a formal analysis of this case,\nwe claim that this cross-learning formulation (P CLF) with\nfunctional constraints comes to also exploit the bias-variance\ntrade-off inherent to the problem. We support this claim with\nthe experiments of Section VI-B, and by relating (P CLF) to the\nparametric case (P CL). Specifically, we notice that (P CLF) is a\nrelaxation of (P CL), as stated by the following proposition.\nProposition 4.If the modelf(x, \u03b8)isL-Lipschitz in the\nparametrization, i.e.,|f(x, \u03b8 t)\u2212f(x, \u03b8 g)| \u2264L|\u03b8 t\u2212\u03b8g|, then\nthe feasible setsC CL(\u03f5)andC CLF(\u03f5)for the cross-learning\nestimators(P CL)and(P CLF)with coupled parameters and\nconstraints, respectively, satisfyC CL(\u03f5)\u2286 C CLF(L\u03f5).\nProof.Let(\u03b8 t, \u03b8g)\u2208 C CL(\u03f5)such that\nCCL(\u03f5) ={(\u03b8 t, \u03b8g) :\u2225\u03b8 t\u2212\u03b8g\u2225 \u2264\u03f5}.(33)\nhence,\u2225\u03b8 t\u2212\u03b8g\u2225 \u2264\u03f5holds. Furthermore, by application of\nthe Lipschitz property of the model functionf, we have that\n\u2225f(x i, \u03b8t)\u2212f(x i, \u03b8g)\u2225 \u2264 \u2225\u03b8 t\u2212\u03b8g\u2225 \u2264L\u03f5(34)\nAveraging over the samplesi= 1, . . . , N twe obtain the\nbound1\nNtPNt\ni=1\u2225f(x i, \u03b8t)\u2212f(x i, \u03b8g)\u2225 \u22641\nNtPNt\ni=1L\u03f5=L\u03f5.\nThis implies(\u03b8 t, \u03b8g)\u2208 C CLF(L\u03f5)since according to (P CLF),\nCCLF(\u03f5)={(\u03b8 t, \u03b8g):1\nNtPNt\ni=1|f(x i, \u03b8t)\u2212f(x i, \u03b8g)|\u2264\u03f5}.\u25a0The inclusionC CL(\u03f5)\u2286 C CLF(L\u03f5)means that (P CLF)\nimposes aweakercoupling between task-specific parameters\nand the shared representation compared to the parametric prob-\nlem. Consequently, while both formulations promote cross-\ntask information sharing, the output constraint does so in\na more flexible manner. In other words, (P CLF) allows for\ngreater variability among task-specific parameters as long as\nthe corresponding function outputs remain sufficiently close.\nIn this case, the solution of (P CLF) with\u03f5= 0does not\nimply strict model consensus, since the parameters could\ndiffer across tasks and the outputs could also differ for inputs\nnot seen during training. AsN tgrows and the training set\nbecomes more representative, the models reach consensus,\nwhich introduces bias as described in previous sections. If\nNtreduces, we still expect a residual bias under this weaker\nbroad-sense consensus enforced by (P CLF) with\u03f5= 0. On\nthe other hand, when\u03f5\u2192 \u221ethe conclusion is more direct.\nThat is, we recover the fully separable case and its higher\nvariance as we did when coupling the parameters. As in the\nparametric case, the cross-learning estimator, in this new form\nwith coupled outputs, sets itself in between these consensus\nand separable counterparts.\nV. DUALALGORITHMS\nUpon presenting the cross-learning estimators (P CL) and\n(PCLF) as formulations to better balance the variance-bias\ntrade-offs, in this section we propose two methods to solve\ntheir corresponding optimization problems.\nA. Dual Algorithm for Parametric Constraints\nOur first algorithm is built using the Alternating Direction\nMethod of Multipliers (ADMM) [27]. This formulation is\nsuitable to solve the optimization problem (P CL) that defines\nthe cross-learning estimator with parametric constraints. In the\ncase in which an algorithm is available to solve the separable\nproblem (P S), this ADMM formulation allows us to solve the\ncross-learning problem (P CL) using the separable solution as a\nbuilding block. We want to obtain the optimal cross-learning\nparameters\u03b8\u2020\ntfor each of the taskst= 1, . . . , T. To reduce\nnotation, let us define the\u03b8= (\u03b8 1, . . . , \u03b8 T, \u03b8g)containing all\noptimization variables and write the loss in terms of\u03b8and the\ndata(x t, yt)for tasktas\n\u2113(\u03b8) =X\nt\u2113(yt, f(x t, \u03b8t)) =X\nt||yt\u2212f(x t, \u03b8t)||2(35)\nUnder this notation, the cross-learning problem to solve is\nmin\n\u03b8\u2113(\u03b8),subject to\u03b8\u2208 C(36)\nwithC=C CL(\u03f5)being the feasible set defined in (33). By\nintroducing the barrier function\nC(\u03b8) =(\n0\u03b8\u2208 C\n\u221e\u03b8 /\u2208 C(37)\ntogether with an auxiliary variablez, we can write (36) as\nmin\n\u03b8,z\u2113(\u03b8) +C(z),subject to\u03b8=z(38)\n8\nAlgorithm 1Cross-Learning with Parametric Constraints\n1:Initialize:\u03b80, z0= 0,andu0= 0\n2:fork= 0,1,2, . . . , k max do\n3:Compute\u03bb k=\u03bb0\u0393kmodK\n4:foreach taskt= 1, . . . , Tdo\n5:\u03b8k+1\nt= arg min \u03b8\u2113\u03bbk(yt, f(x t, \u03b8), z t, ut)\n6:end for\n7:\u03b8k+1\ng=zk\ng\u2212uk\ng\n8:zk+1=PC(\u03b8k+1+uk)\n9:uk+1=uk+\u03b8k+1\u2212zk+1\n10:end for\nWritten as (38), we can solve cross-learning via the ADMM\n\u03b8k+1= Prox \u03bbk\u2113\u0000\nzk\u2212uk\u0001\n(39)\nzk+1=PC\u0000\n\u03b8k+1+uk\u0001\n(40)\nuk+1=uk+vk+1\u2212zk+1(41)\nwith the proximal operator of the loss in (39) defined by\n\u03b8k+1= arg min\n\u03b8X\nt\u2113(yt, f(x t, \u03b8t))+1\n2\u03bbk\u2225\u03b8\u2212zk+uk\u22252(42)\nwhere\u03bb k=\u03bb0\u0393kis a weighting parameter that progressively\naccentuates the effect of the loss over the quadratic penalty.\nTo take full advantage of the ADMM, we expand the penalty\n\u2225\u03b8\u2212zk+uk\u22252=PT\nt=1\u2225\u03b8t\u2212zk\nt+uk\nt\u22252+\u2225\u03b8 g\u2212zk\ng+uk\ng\u22252with\nzk\ntanduk\ntdenoting the elements ofzkanduk, respectively,\ncorresponding to taskt, andzk\nganduk\ngbeing the ones\ncorresponding to the centroid. Such an expansion facilitates\na distributed solution of (42) in which the parameters\u03b8 tfor\ntasktare obtained by solving\n\u03b8k+1\nt= arg min\n\u03b8\u2113reg(yt, f(x t, \u03b8t), zt, ut)(43)\nwith the regularized loss defined by\u2113 \u03bbk(yt, f(x t, \u03b8t), zt, ut) =\n\u2113(yt, f(x t, \u03b8t)) +1\n2\u03bbk\u2225\u03b8t\u2212zk\nt+uk\nt\u22252.\nThe centroid, in turns, is only involved in the penalty in (42).\nHence, it admits the closed-form\u03b8 g=zg\u2212ug. This describes\nthe entire procedure, which is summarized in Algorithm 1.\nB. Dual Algorithm for Coupled Outputs\nNext, we construct a primal-dual algorithm to solve (P CLF).\nWith\u03bb t>0denoting the dual variable associated with domain\nt, and\u03bb= [\u03bb 1, . . . , \u03bb T]T, the Lagrangian associated with\n(PCLF) takes the form\nL(\u03b8t, \u03b8g, \u03bb) =1\nTTX\nt=11\nNtNtX\ni=1\u2113(yi, f(x i, \u03b8t))\n+\u03bbt\u00121\nNtNtX\ni=1|f(x i, \u03b8t)\u2212f(x i, \u03b8g)| \u2212\u03f5\u0013\n.(44)\nWe define the dual function for problem (P CLF) asd(\u03bb) :=\nmin\u03b8t,\u03b8gL(\u03b8t, \u03b8g, \u03bb), and the corresponding convex dual prob-\nlem as the maximization ofd(\u03bb)in the positive orthant, that\nis,D\u22c6\nCLF:= max {\u03bbt\u22650}d(\u03bb)[28].\nProblemD\u22c6\nCLF can be solved [29], [30] by alternating\ngradient steps on the Lagrangian and the dual function. UponAlgorithm 2Cross-Learning Functional Algorithm\n1:Initialize models{\u03b80\nt}, \u03b80\ng, and dual variables\u03bb= 0\n2:forepochse= 1,2, . . .do\n3:forbatchiin epochedo\n4:Update params.\u03b8k+1\nt=\u03b8k\nt\u2212\u03b7P\u02c6\u2207\u03b8tL(\u03b8t, \u03b8g, \u03bb)\u2200t\n5:Updategparams.\u03b8k+1\ng=\u03b8k\ng\u2212\u03b7P\u02c6\u2207\u03b8gL(\u03b8t, \u03b8g, \u03bb)\n6:end for\n7:Update dual variable for allt\u2208[1, . . . , T]\n8:\u03bbk+1\nt=\u0014\n\u03bbk\nt+\u03b7D\u0012\n1\nNtPNt\ni=1|f(x i, \u03b8t)\u2212f(x i, \u03b8g)|\u2212\u03f5\u0013\u0015\n+\n9:end for\nselecting step-sizes\u03b7 P>0and\u03b7 D>0, and initializing the\nparameters{\u03b80\nt}, \u03b80\ng, and the dual variables\u03bb0\nt, we update the\nparameters by taking a gradient descent step onL(\u03b8 t, \u03b8g, \u03bb),\n\u03b8k+1\nt=\u03b8k\nt\u2212\u03b7P\u2207\u03b8tL(\u03b8t, \u03b8g, \u03bb)for allt\u2208[1, . . . , T],(45)\n\u03b8k+1\ng=\u03b8t\ng\u2212\u03b7P\u2207\u03b8gL(\u03b8t, \u03b8g, \u03bb),(46)\nand then the multipliers with a gradient ascent step overd(\u03bb)\n\u03bbk+1\nt=\u0014\n\u03bbk\nt+\u03b7D\u00121\nNtNtX\ni=1|f(x i, \u03b8t)\u2212f(x i, \u03b8g)| \u2212\u03f5\u0013\u0015\n+\nfort= 1, . . . T, projecting the result into the nonnegative\northant via[\u00b7] += max{0,\u00b7}. The overall is detailed in\nAlgorithm 2. We refer the reader to [31] for a proof of\nconvergence in a stochastic setup.\nVI. NUMERICALEXAMPLES\nIn this section, we test our cross-learning estimators on\nreal-world data in two scenarios: a time-series prediction task\nusing COVID-19 epidemiological data [32] and an image\nclassification task using the Office-Home dataset [33].\nA. COVID-19 SIR Model Fitting\nIn this section, we formulate an SIR fitting problem for the\nCOVID-19 pandemic data obtained from the OWID database\n[32]. Although complex models are often required to capture\nthe full dynamics of an epidemic, we use a simple, inter-\npretable, bi-parametric SIR model. This choice will allow for\na clear interpretation of our cross-learning method. The SIR\nmodel is described by a system of differential equations\ndS\nd\u03c4=\u2212\u03b2\nNSI,dI\nd\u03c4=\u03b2\nNSI\u2212\u03b3I,dR\nd\u03c4=\u03b3I,(47)\nwhere variablesS,I, andRstand for the number of suscep-\ntible, infected, and removed individuals, respectively, which\nevolve with time\u03c4, and the constantN=S+I+Rstands\nfor the initial total population.\nWe consider the multiple tasks of predicting the infection\ntime series(S t, It, Rt)forTdifferent countries. The loss\u2113\nis defined as the mean squared error between the predicted\nand observed infection data for each country. By following\nAlgorithm 1, which optimizes the regularized losses\u2113 \u03bbk, we\nobtain specific parameters\u03b8\u2020\nt= (\u03b2 t, \u03b3t)for each country,\ntogether with a centroid\u03b8\u2020\ng. The central question we investigate\n9\n500 550 600 650 700 750 80001234\u00d7106\nTime (days)Infected PopulationARG BRA CHL COL DEU\nESP FRA GBR ITA MEX\nPER PRY URY USA\nFig. 4: Infected population over time forT= 14different\ncountries in the OWID dataset. Countries exhibit different\ndynamics for the evolution of their infected populations.\nis: Would it have been possible to predict the peak and timing\nof Argentina\u2019s (ARG) COVID-19 wave using only early data\nfrom their own incipient records, supplemented with data from\nprevious waves in other countries? To answer this, we use\nthe epidemic waves fromT\u22121 = 13countries, shown\nin Figure 4. Our goal is to predict the latter part of the\nARG wave using only its initial data points. We evaluate\nperformance using two metrics: the lag error, which measures\nthe difference in days between the predicted and actual peak\nof the infection wave, and the peak error, which measures\nthe difference in the number of infected people at the peak.\nFigure 5 illustrates the prediction results when using only the\nfirst 10 days of ARG wave data and all previously available\ndata from Fig. 4. The separate estimator, which uses only\nARG data, fails to capture the peak. With a fitted recovery\nrate\u03b3= 0, it incorrectly predicts unbounded exponential\ngrowth. On the other hand, enforcing consensus, yields a single\ncommon pair of(\u03b2, \u03b3)parameters for allT= 14countries,\naveraging out the unique dynamics of the ARG wave, resulting\nin a prediction that severely underestimates the severity of\nthe outbreak. In contrast, the cross-learning approach (with\n\u03f5= 0.1) effectively leverages the data from all 15 countries.\nIt produces a model that accurately predicts both the timing\n(lag) and the magnitude (peak) of the ARG infection wave.\nUnder cross-learning, the distance of the final parameters\nin parameter space depends on the decision of the value\u03f5.\nParticularly\u03f5=\u221eforces no closeness between parameters,\nbeing equivalent to a separate approach, while\u03f5= 0forces\nall parameters to be equal, resulting in strict consensus. Figure\n6 compares the separate SIR estimators to the cross-learning\nones for\u03f5= 0.1. A separate estimator for ARG yields\u03b3= 0,\nwhich is expected since the parameter\u03b3, specifically, is tied\nto the downward slope, and there are not enough points in\nthe ARG dataset to fit it properly. By contrast, the parameters\n\u03b8\u2020\nt= (\u03b2 t, \u03b3t)are closer to the centroid\u03b8\u2020\ng. Thus, the cross-\nlearning estimator forces both ARG parameters to be nonzero.\nClose inspection of the SIR model (47) reveals that the peak\nof infections occurs when\u03b2 tSt/Nt=\u03b3t, since that implies\ndIt/d\u03c4= 0. Furthermore,\u03b2 tand\u03b3 tmust be similar to each\nother if the peak occurs at an early stage of the epidemic,\nin whichS t/Nt\u22431. As Figure 6 shows, this similarity is\ncaptured by the cross-learning estimator and is a key enabler\n0 5 10 15 20 25 3001234\u00d7106\nTime (days)Infected PopulationSeparate\nCross-learning\nConsensusFig. 5: SIR model predictions for ARG using the separate\n(\u03b2= 0.1481, \u03b3= 0.0), cross-learning(\u03b2= 0.6608, \u03b3=\n0.4388), and consensus(\u03b2= 0.3497, \u03b3= 0.2802)estimators.\nFilled and hollow black dots represent the training and test\ndatasets, respectively. Cross-learning achieves a more accurate\nprediction of the peak of infections with an error of0.07%\nin the number of cases and finding the exact day when the\npeak occurs, as compared to errors of2474%and76.38%and\ntime lags of108and8days for the separate and consensus\nestimators, respectively.\n0 1 201\nUSAESPFRA\nBRAMEXPRYITA\nARGCOL\nCHLPER\nDEUGBR\n\u03b2\u03b3\n0.6 0.7 0.80.50.6\nGlobal\nUSAESPFRA\nBRAMEX\nPRYITA\nARGCOL\nCHLPER\nDEUGBR\n\u03b2\u03b3\nFig. 6: Learned SIR parameters(\u03b2, \u03b3)per country by separate\nestimators (left) and cross-learning (right).\nfor predicting the peak in Figure 5, where the separate estima-\ntor fails. Notice also that, while the centralized estimator also\nforces\u03b2 c\u2243\u03b3c, it reduces the influence of ARG data on the\nestimation. This causes distortion on the parameter\u03b2, which\nmodels the initial upright slope. As seen in Figure 5, such a\ndistortion results in a worse prediction of the peak compared\nthe one obtained by our cross-learning approach. Remarkably,\ncross-learning does not require specific knowledge of these\nmodel insights but captures them by just setting a rather\nsimple similarity constraint for the parameters across tasks.\nFurthermore, we performed an ablation study whose results\nare shown in Figure 7. Choosing\u03f5= 0.1is optimal, in the\nsense that it yields both the minimum peak and lag errors.\nStill, there is a wide range of\u03f5values yielding errors that\nare orders of magnitude lower than those corresponding to the\nseparate or consensus estimators, indicating that the approach\nis robust and does not require extensive hyperparameter tuning\nto provide significant benefits.\nFinally, we analyze the prediction error as a function of the\nnumber of available data points for ARG. In Figure 8, we\nshow the error using separate, consensus, and cross-learning\napproaches. The independent approach requires a substantial\namount of data (nearly 20 days) to correctly identify the\n10\n0 0.2 0.4 0.6 0.8 110\u2212310\u22121101\n\u03f5Peak ErrorSeparate\nCross-learning\nConsensus\n0 0.2 0.4 0.6 0.8 1100101102\n\u03f5Lag Error\nFig. 7: Study on the effect of the centrality parameter\u03f5on the\npeak error (top) and lag error (bottom) in logarithmic scale.\nThe cross-learning estimator coincides with the consensus one\nat\u03f5= 0, reduces to a minimum error (with no lag for\u03f5\u2208\n{0.15,0.20}.) and then reaches its assymptotic value, the error\ncorresponding to the separate estimator at\u03f5= 1.0.\n0 5 10 15 20 25 300102030\nNErrorSeparate\nCross-learning\nConsensus\n0 5 10 15 20 25 3000.20.40.60.81\nFig. 8: Prediction error as a function of the number of available\nARG data points (N). The bottom panel provides a zoomed-in\nview of the low-error region.\ntrend and make an accurate prediction. In contrast, our cross-\nlearning method achieves a low error with as few as10days\nof data. The performance of the consensus approach remains\npoor regardless of the amount of ARG data, as this specific\ninformation is diluted within the global dataset.\nB. Office-Home Dataset Classification\nIn this subsection, we benchmark our cross-learning method\nwith coupled outputs on an image classification problem with\nreal data coming from the dataset that we introduced in Figure\n9. We consider the problem of classifying images belonging\ntoP= 65different categories (which in Figure 9 are Alarm,\nBike, Glasses, Pen, and Speaker), andT= 4different\nArt\n Clipart\n Product\n World\nAlarm\n Bike\n Glasses\n Pen\n Speaker\nFig. 9: Example images from5of the65categories from the\n4domains composing the Office-Home dataset [33]. The4\ndomains are Art, Clipart, Product and Real World. In total,\nthe dataset contains15,500images of different sizes.\ndomains. The Office-Home dataset [33] consists of15,500\nRGB images in total coming from the domains (i.e. tasks)\n(i) Art: an artistic representation of the object, (ii) Clipart:\na clip art reproduction, (iii) Product: an image of a product\nfor sale, and (iv) Real World: pictures of the object captured\nwith a camera. Intuitively, by looking at the images, we can\nconclude that the domains are related. The minimum number\nof images per domain and category is15and the image size\nvaries from the smallest image size of18\u00d718to the largest\nbeing6500\u00d74900pixels. We pre-processed the images by\nnormalizing them and fitting their size to224\u00d7224pixels.\nWe classify these images with neural networksf(x, \u03b8 t),\nwith their architecture based on AlexNet [34], reducing the\nsize of the last fully connected layer to256neurons. We do\nnot pre-train these networks, but optimize their weights from\ndata via cross-learning using the cross-entropy loss [1], after\nsplitting the dataset in4/5of the images for training and1/5\nfor testing. Since neural networks are involved, we opt for the\nversion of cross-learning with coupled outputs. Specifically,\nwe use Algorithm 2 to trainT+1 = 5neural networks for the\nT= 4domains plus the centroid, with step-sizes\u03b7 P= 0.003\nand\u03b7 D= 10, respectively, for different values of\u03f5. We also\nused the same split dataset to test the consensus classifier,\nwhich is equivalent to merging the images from all domains\nand training a single neural network on the whole dataset,\nand theT= 4classifiers which train their neural networks\nseparately from images of their own specific domains. For\ncomparison purposes, we also run Algorithm 1 on this dataset,\nwhich implements cross-learning with coupled parameters.\nWe show our results in Figure 10, where we use the\nclassification accuracy of the trained networks on the test set as\nfigure of merit. To begin with, we empirically corroborate that\nthe dataset share mutual information across domains, since the\nconsensus classifier achieves an accuracy of35.59%, outper-\nforming the separate training whose accuracy is31.94%. This\nmeans that utilizing samples from other domains improves the\nperformance of the learned classifier. This is the setting in\nwhich cross-learning is most helpful as it will help reduce\nbias. As it can be seen in Figure 10, a salient fact about\n11\n0.03 0.120.2 0.5 0.8\n303438424650\u03f5(output constraints)\nAccuracy (%)\n10\u2212510\u2212410\u2212310\u2212210\u22121303438424650\n\u03f5(parameter constraints)Accuracy (%)\nFig. 10: Overall accuracy in percentage of correctly classified\nimages measured on unseen data (test set). The consensus\nestimator (green) coincides with the parametric one (orange)\nwhen\u03f5= 0and the tends to the separate one (red) as\u03f5\u2192 \u221e.\nThe output-constrained cross-learning estimator (blue) obtains\nthe highest accuracy of 44.5%. at\u03f5= 0.2.\nthe experiments is that the cross-learning estimator in both\nthe parameter, as well as the output constraint, consistently\noutperforms both consensus and separate estimators, reaching\na maximum accuracy of44.5%at\u03f5= 0.2when coupling\nthe outputs. Indeed, in this scenario, the case with output\nconstraints shows an improvement over parameter constraints.\nHowever, in both cases the estimator has a positive slope\ncoming from\u03f5= 0, and a negative one when\u03f5is big\nenough, attaining a maximum inbetween, which reproduces\nthe theoretical findings of Section III.\nAs a byproduct, Algorithm 2 generates dual variables\u03bb tthat\ncontain valuable information of the problem at hand. Figure 11\nshows the dual variables\u03bbas a function of the epoch. A larger\ndual variable indicates that the constraint is harder to satisfy,\nand a dual-variable equal to0indicates that the constraint is\ninactive. As seen in 11, all\u03bb tare non-negative, which means\nthat we are effectively in a regime where there the classifiers\nare utilizing data from other tasks. If we look at the relative\nvalues of the dual variables, we see that the domain Art has\nthe smallest value, whereas the domain Real World has the\nlargest one. Given that Art has the least amount of samples, it\nis the domain that mostly benefits from images coming from\nother tasks. On the other hand, the largest dual variable is\nassociated with the real-world dataset, have images with more\ndetails, textures, and shapes, and are therefore more difficult\nto classify (cf. Figure 9).\nVII. CONCLUSIONS\nWe proposed a constrained optimization method to achieve\nmulti-task learning under a deterministic paradigm for the\nmodel parameters. Leveraging a controlled Gaussian model,\nwe were able to prove that by forcing the parameter estimators\nto be close to one another, we obtain a strictly lower mean-\nsquared error than if we use separate or consensus counter-\nparts. Then we tested our cross-learning method on a practical\nproblem with real data, where we fit parameters modeling the\npropagation of an infectious disease across different popula-\ntions using a tailored algorithm based on proximal operators.\nThe results obtained in this scenario replicated the theoretical\n0 50 100 150 200 25010152025\nEpochMagnitude of Dual VariableArt Clipart\nProduct Real WorldFig. 11: Dual variable associated with each constraint for the\ncase of\u03f5= 0.12. Intuitively, a larger dual variable indicates\nthat the constraint is harder to satisfy.\nones, with cross-learning predicting the peak of infections\nwith an error of0.07%, compared to errors of76.38%when\nassuming that all populations follow the same propagation\nmodel, and2474%when estimating separate parameters. In the\ncontext of classification with neural networks, we proposed a\nvariant of our cross-learning method that constrains the model\noutputs, rather than their parameters, to be at a close distance.\nSuch a variant was also tested on real data, comprising images\nfrom different categories and domains. In this new test, the\ncross-learning estimator achieved a higher accuracy of44.5%\nwhen compared to33.97%and24.44%for the consensus and\nseparate counterparts, respectively.\nAPPENDIX\nA. Bias-Variance\nLet us compute the bias and variance of the consensus\nestimator \u02c6\u03b8c= (1/T)PT\nt=1\u02c6\u03b8tas an estimator for the ground\ntruth parameter\u03b8\u22c6\nt, under the model \u02c6\u03b8t=\u03b8\u22c6\nt+\u03b7twith\nzero-mean noise\u03b7 tof variance\u03c32. The bias depends on the\npairwise distances between the ground truth estimators, i.e.,\nEh\n\u02c6\u03b8c\u2212\u03b8\u22c6\nti\n=E\"\n1\nT\u22a4X\n\u03c4=1\u0010\n\u02c6\u03b8\u03c4\u2212\u03b8\u22c6\nt\u0011#\n=1\nT\u22a4X\n\u03c4=1\u0010\n\u02c6\u03b8\u03c4\u2212\u03b8\u22c6\nt\u0011\nso it introduces bias unless all\u03b8\u22c6\ntcoincide. On the other hand\nvar(\u02c6\u03b8c) =E\u0014\r\r\r\u02c6\u03b8c\u2212Eh\n\u02c6\u03b8ci\r\r\r2\u0015\n=E\uf8ee\n\uf8f0\r\r\r\r\r1\nTTX\nt=1\u0010\n\u02c6\u03b8t\u2212\u03b8\u22c6\nt\u0011\r\r\r\r\r2\uf8f9\n\uf8fb\n=1\nTTX\nt=1E\u0014\r\r\r\u0010\n\u02c6\u03b8t\u2212\u03b8\u22c6\nt\u0011\r\r\r2\u0015\n=1\nT2TX\nt=1d\nNt\u03c32.\nwhich is smaller, the larger the number of tasksTis.\nB. Auxiliary Proofs of Proposition 1\nWe first establish that the cross-learning centroid is a convex\ncombination of the separate estimates.\nLemma 1.Consider the centroid\u03b8\u2020\ngsolution to( \u00afPCL), and the\nseparate estimates \u02c6\u03b8t= (1/N t)PNt\nn=1ynt. There exist a set\nof coefficients\u03b3 t\u2208[0,1]satisfyingPT\nt=1\u03b3t= 1,such that\n\u03b8\u2020\ng=TX\nt=1\u03b3t\u02c6\u03b8t.(48)\n12\nProof.The Lagrangian of problem ( \u00afPCL) takes the form\nL(\u03b8t, \u03b8g, \u00b5t) =TX\nt=1\r\r\r\u02c6\u03b8t\u2212\u03b8t\r\r\r2\n+TX\nt=1\u00b5t\u0000\n\u2225\u03b8t\u2212\u03b8g\u22252\u2212\u03f52\u0001\nSetting its gradient with respect to\u03b8 tto zero, we obtain\n\u03b8\u2020\nt=\u02c6\u03b8t\n1 +\u00b5 t+\u03b8\u2020\ng\u00b5t\n1 +\u00b5 t=\u03f5t\u02c6\u03b8t+ (1\u2212\u03f5 t)\u03b8\u2020\ng (49)\nafter defining\u03f5 t= 1/(1 +\u00b5 t)so that(1\u2212\u03f5 t) =\u00b5 t/(1 +\u00b5 t).\nOn the other hand, setting the gradient of the Lagrangian with\nrespect to\u03b8 gto zero results in\nTX\nt=1\u0010\n\u03b8\u2020\ng\u2212\u03b8\u2020\nt\u0011\n\u00b5t= 0.(50)\nHence, we can substitute (49) in (50) to obtain\nTX\nt=1\u00b5t\u03b8\u2020\ng=TX\nt=1\u00b5t\u03b8\u2020\nt=TX\nt=1\u00b5t\u0010\n(\u03f5t)\u02c6\u03b8t+ (1\u2212\u03f5 t)\u03b8\u2020\ng\u0011\n(51)\n=X\n\u00b5t\u03f5t\u02c6\u03b8t+X\n\u00b5t(1\u2212\u03f5 t)\u03b8\u2020\ng,(52)\nwhich we can solve for\u03b8\u2020\ngas in\n\u03b8\u2020\ng=TX\nt=1\u00b5t\u03f5t\u02c6\u03b8tPT\ni=1\u00b5i\u03f5t=TX\nt=1(1\u2212\u03f5 t)\u02c6\u03b8tPT\ni=1(1\u2212\u03f5 t).(53)\nwhere we used\u00b5 t\u03f5t=\u00b5t/(1+\u00b5 t) = (1\u2212\u03f5 t), so that (53) takes\nthe form (48) if we identify\u03b3 t\u225c(1\u2212\u03f5 t)/PT\ni=1(1\u2212\u03f5 i).\u25a0\nLemma 2.Consider the centroid\u03b8\u2020\ngsolution to( \u00afPCL), and the\nseparate estimates \u02c6\u03b8t= (1/N t)PNt\nn=1ytn. The cross-learning\nestimate\u03b8\u2020\ntin(\u00afPCL)is the projection of \u02c6\u03b8tto the ballB(\u03b8\u2020\ng, \u03f5)\nof center\u03b8\u2020\ngand radius\u03f5.\nProof.By construction\u03b8\u2020\nt\u2208 B(\u03b8\u2020\ng, \u03f5). Furthermore, according\nto (49) in the proof of Lemma 1, the cross-learning estimate\u03b8\u2020\nt\nlies in a convex combination of\u03b8\u2020\ngand\u02c6\u03b8t. For those constraints\nthat are inactive, complementary slackness requires\u00b5 t= 0.\nSubstituting\u00b5 t= 0in (49) we obtain\u03b8\u2020\nt=\u02c6\u03b8t, which means\nthat\u02c6\u03b8talso lies inside the ball, and hence they are projections\nof each other. If the constraint is active, then\u2225\u03b8\u2020\nt\u2212\u03b8\u2020\ng\u2225=\u03f5,\nwhich means that\u03b8\u2020\ntlies on the intersection of the border of\nB(\u03b8\u2020\ng, \u03f5)with the segment between \u02c6\u03b8tand\u03b8\u2020\ng, and that is the\nprojection of \u02c6\u03b8tto the ball again.\u25a0\nIn the next Lemma we show that the consensus estimator\n\u02c6\u03b8cis close to the centroid\u03b8\u2020\ng, which will be needed down the\nroad to assess the performance of cross-learning.\nLemma 3.The error between the consensus estimator \u02c6\u03b8cand\nthe cross-learning centroid\u03b8\u2020\ngis given by\n\u03b8\u2020\ng\u2212\u02c6\u03b8c=1\nTTX\nt=1\u03f5t\u0010\n\u02c6\u03b8c\u2212\u02c6\u03b8t\u0011\n+O(\u03f52)(54)\nwith\u03f5 t=\u03f5/\u2225 \u02c6\u03b8t\u2212\u02c6\u03b8c\u2225+O(\u03f52) =O(\u03f5)in the limit as\u03f5\u21920.\nProof.We know from Lemma 1 that\u03b8\u2020\ng=PT\nt=1\u03b3t\u02c6\u03b8twith\n\u03b3t=(1\u2212\u03f5 t)PT\nk=1(1\u2212\u03f5 k)=1\nT(1\u2212\u03f5 t)\n(1\u2212\u03b5)(55)where\u03b5:= (1/T)PT\nk=1\u03f5k. A closer look at (55) reveals that\n(1\u2212\u03b5)(1 +\u03b5) = 1\u2212\u03b52\u21d21\n(1\u2212\u03b5)= 1 +\u03b5+O(\u03b52).(56)\nCombining (56) with (55) we obtain\n\u03b3t=1\nT(1\u2212\u03f5 t)(1 +\u03b5+O(\u03f52)) =1\nT\u0000\n1 +\u03b5\u2212\u03f5 t+O(\u03f52)\u0001\n,\nwhich can be substituted in\u03b8\u2020\ng=PT\nt=1\u03b3t\u02c6\u03b8t, to obtain\n\u03b8\u2020\ng\u2212\u02c6\u03b8c=TX\nt=1\u0012\n\u03b3t\u22121\nT\u0013\n\u02c6\u03b8t=1\nTTX\nt=1\u0000\n\u03b5\u2212\u03f5 t+O(\u03f52)\u0001\u02c6\u03b8t\n=\u03b5\u02c6\u03b8c\u22121\nTTX\nt=1\u03f5t\u02c6\u03b8t+O(\u03f52) =1\nTTX\nt=1\u03f5t(\u02c6\u03b8c\u2212\u02c6\u03b8t)+O(\u03f52).\nTo finish the proof, we need to show that\u03f5 tis of order\u03f5\nas\u03f5\u21920. In this regime, we can assume without loss of\ngenerality that the constraints are active, and write\n\u03f5=\u2225\u03b8\u2020\nt\u2212\u03b8\u2020\ng\u2225=\u2225\u03f5 t\u02c6\u03b8t+ (1\u2212\u03f5 t)\u03b8\u2020\ng\u2212\u03b8\u2020\ng\u2225=\u03f5 t\u2225\u02c6\u03b8t\u2212\u03b8\u2020\ng\u2225.(57)\nwhich implies that\u03f5 t=O(\u03f5). Still, we need one more step to\nprove\u03f5 t=\u03f5/\u2225 \u02c6\u03b8t\u2212\u02c6\u03b8c\u2225+O(\u03f52). So we rewrite (57) as\n\u03f5t=\u03f5\u2225\u02c6\u03b8t\u2212\u03b8\u2020\ng\u2225\u22121=\u03f5\u0010\n\u2225\u02c6\u03b8t\u2212\u02c6\u03b8c+\u02c6\u03b8c\u2212\u03b8\u2020\ng\u22252\u0011\u22121/2\n=\u03f5\u0010\n\u2225\u02c6\u03b8t\u2212\u02c6\u03b8c\u22252+\u2225\u02c6\u03b8c\u2212\u03b8\u2020\ng\u22252+ 2(\u02c6\u03b8t\u2212\u02c6\u03b8c)\u22a4(\u02c6\u03b8c\u2212\u03b8\u2020\ng)\u0011\u22121/2\n=\u03f5\u2225\u02c6\u03b8t\u2212\u02c6\u03b8c\u2225\u22121(1 + 2R t)\u22121/2(58)\nwithR t=\u0010\n\u2225\u02c6\u03b8c\u2212\u03b8\u2020\ng\u22252+ (\u02c6\u03b8c\u2212\u03b8\u2020\ng)\u22a4(\u02c6\u03b8t\u2212\u02c6\u03b8c)\u0011\n\u2225\u02c6\u03b8t\u2212\u02c6\u03b8c\u2225\u22122.\nSince we already proved that \u02c6\u03b8c\u2212\u03b8\u2020\ng=O(\u03f5), we can write\n\u03f5t=\u03f5(1 + 2O(\u03f5))\u22121/2\n\u2225\u02c6\u03b8t\u2212\u02c6\u03b8c\u2225=\u03f5(1\u2212 O(\u03f5))\n\u2225\u02c6\u03b8t\u2212\u02c6\u03b8c\u2225=\u03f5\n\u2225\u02c6\u03b8t\u2212\u02c6\u03b8c\u2225+O(\u03f52),\nwhich is the result that we wanted to prove.\u25a0\nThe next lemma provides an expression for the difference\nbetween the cross-learning sample errorE CLand its consensus\ncounterpartE Cin the regime\u03f5\u21920. Remarkably, this\nexpression does not depends on the solution\u03b8\u2020\ntbut on simpler\nvariables that we can model in closed form.\nLemma 4.Consider the sample errorsE CandE CL(\u03f5),\nresulting from the consensus and cross-learning estimators,\nas defined in(5)and(6), respectively. As\u03f5\u21920,E CL(\u03f5)\nconverges toE Caccording to\nECL(\u03f5)\u2212E C=\u22122\u03f51\nTTX\nt=1(\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc)\u22a4\u02c6\u03b8t\u2212\u02c6\u03b8c\n\u2225\u02c6\u03b8t\u2212\u02c6\u03b8c\u2225+O(\u03f52),(59)\nwhere\u03b8\u22c6\nc=1\nTPT\nt=1\u03b8\u22c6\ntaverages the ground truth parameters.\nProof.Let start by comparing the square errors of the con-\nsensus and the cross-learning estimators\n\u2225\u03b8\u2020\nt\u2212\u03b8\u22c6\nt\u22252=\u2225\u03b8\u2020\nt\u2212\u02c6\u03b8c+\u02c6\u03b8c\u2212\u03b8\u22c6\nt\u22252(60)\n=\u2225\u02c6\u03b8c\u2212\u03b8\u22c6\nt\u22252+\u2225\u03b8\u2020\nt\u2212\u02c6\u03b8c\u22252\u22122(\u03b8\u22c6\nt\u2212\u02c6\u03b8c)\u22a4(\u03b8\u2020\nt\u2212\u02c6\u03b8c)\n=\u2225\u02c6\u03b8c\u2212\u03b8\u22c6\nt\u22252\u22122I t+O(\u03f52)(61)\n13\nwhere we definedI t= (\u03b8\u2020\nt\u2212\u02c6\u03b8c)\u22a4(\u03b8\u22c6\nt\u2212\u02c6\u03b8c), and substituted\n\u2225\u03b8\u2020\nt\u2212\u02c6\u03b8c\u22252=O(\u03f52)since the cross-learning constraint imposes\n\u2225\u03b8\u2020\nt\u2212\u03b8\u2020\ng\u2225 \u2264\u03f5and Lemma 3 implies\u2225\u03b8\u2020\ng\u2212\u02c6\u03b8c\u2225=O(\u03f5).\nTo expressI tin terms of \u02c6\u03b8t, we add and subtract\u03b8\u2020\ng\nIt= (\u03b8\u2020\nt\u2212\u03b8\u2020\ng+\u03b8\u2020\ng\u2212\u02c6\u03b8c)\u22a4(\u03b8\u22c6\nt\u2212\u02c6\u03b8c)(62)\n= (\u03f5 t(\u02c6\u03b8t\u2212\u03b8\u2020\ng) +\u03b8\u2020\ng\u2212\u02c6\u03b8c)\u22a4(\u03b8\u22c6\nt\u2212\u02c6\u03b8c)(63)\n=\u03f5t(\u02c6\u03b8t\u2212\u02c6\u03b8c)\u22a4(\u03b8\u22c6\nt\u2212\u02c6\u03b8c) +J t+O(\u03f52)(64)\nwhere we definedJ t= (\u03b8\u2020\ng\u2212\u02c6\u03b8c)\u22a4(\u03b8\u22c6\nt\u2212\u02c6\u03b8c)and substituted\n\u03f5t(\u02c6\u03b8t\u2212\u03b8\u2020\ng) =\u03f5 t(\u02c6\u03b8t\u2212\u02c6\u03b8c) +O(\u03f52)using Lemma 3 again.\nNext, we averageJ tacross tasks to obtain\n1\nTTX\nt=1Jt=1\nTTX\nt=1(\u03b8\u2020\ng\u2212\u02c6\u03b8c)\u22a4(\u03b8\u22c6\nt\u2212\u02c6\u03b8c)=(\u03b8\u2020\ng\u2212\u02c6\u03b8c)\u22a4(\u03b8\u22c6\nc\u2212\u02c6\u03b8c)\n=1\nTTX\nt=1\u03f5t(\u02c6\u03b8c\u2212\u02c6\u03b8t)\u22a4(\u03b8\u22c6\nc\u2212\u02c6\u03b8c) +O(\u03f52)(65)\nafter substituting (54) for(\u03b8\u2020\ng\u2212\u02c6\u03b8c). Then we conclude the\nproof by averagingI t, which results in\n1\nTTX\nt=1It=1\nTTX\nt=1\u03f5t(\u02c6\u03b8t\u2212\u02c6\u03b8c)\u22a4(\u03b8\u22c6\nt\u2212\u02c6\u03b8c)(66)\n+1\nTTX\nt=1\u03f5t(\u02c6\u03b8c\u2212\u02c6\u03b8t)\u22a4(\u03b8\u22c6\nc\u2212\u02c6\u03b8c) +O(\u03f52)(67)\n=1\nTTX\nt=1(\u03b8\u22c6\nt\u2212\u03b8\u22c6\nc)\u22a4(\u02c6\u03b8t\u2212\u02c6\u03b8c)\n\u2225\u02c6\u03b8t\u2212\u02c6\u03b8c\u2225+O(\u03f52)(68)\nwith\u03f5 t=\u03f5/\u2225 \u02c6\u03b8t\u2212\u02c6\u03b8c\u2225+O(\u03f52)as in Lemma 3.\u25a0\nREFERENCES\n[1] T. Hastie, R. Tibshirani, J. H. Friedman, and J. H. Friedman,The Ele-\nments of Statistical Learning: Data Mining, Inference, and Prediction.\nNew York: Springer, 2009, vol. 2.\n[2] M. Teichmann, M. Weber, M. Zoellner, R. Cipolla, and R. Urtasun,\n\u201cMultinet: Real-time joint semantic reasoning for autonomous driving,\u201d\ninIEEE Intelligent Vehicles Symposium (IV), 2018, pp. 1013\u20131020.\n[3] H. Lan, C. Zhang, Y . Y . Hong, Y . He, and S. Wen, \u201cDay-ahead spa-\ntiotemporal solar irradiation forecasting using frequency-based hybrid\nprincipal component analysis and neural network,\u201dApplied Energy, vol.\n247, pp. 389\u2013402, 2019.\n[4] M. Dorr, \u201cFitting the sir epidemiological model to influenza data,\u201d\nUniversity of Maine, Tech. Rep., 2019.\n[5] S. M. Kay,Fundamentals of Statistical Signal Processing: Estimation\nTheory. Prentice-Hall, Inc., 1993.\n[6] Y . Zhou, H. Chen, Y . Li, Q. Liu, X. Xu, S. Wang, P. Yap, and D. Shen,\n\u201cMulti-task learning for segmentation and classification of tumors in 3d\nautomated breast ultrasound images,\u201dMedical Image Analysis, vol. 70,\np. 101918, 2021.\n[7] X. Cai, J. Yuan, R. Zheng, L. Huang, and K. Church, \u201cSpeech emotion\nrecognition with multi-task learning,\u201d inInterspeech, vol. 2021, 2021,\npp. 4508\u20134512.\n[8] M. Georgescu, A. Barbalau, R. T. Ionescu, F. S. Khan, M. Popescu, and\nM. Shah, \u201cAnomaly detection in video via self-supervised and multi-task\nlearning,\u201d inProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2021, pp. 12 742\u201312 752.\n[9] R. Caruana, \u201cMultitask learning,\u201dMachine Learning, vol. 28, no. 1, pp.\n41\u201375, 1997.\n[10] C. Finn, P. Abbeel, and S. Levine, \u201cModel-agnostic meta-learning for\nfast adaptation of deep networks,\u201d inProc. of the 34th Int. Conf. on\nMachine Learning, vol. 70, 2017, pp. 1126\u20131135.[11] K. Chua, Q. Lei, and J. D. Lee, \u201cHow fine-tuning allows for effective\nmeta-learning,\u201d inAdvances in Neural Information Processing Systems,\nvol. 34, 2021, pp. 8871\u20138884.\n[12] Y . Zhang and Q. Yang, \u201cA survey on multi-task learning,\u201darXiv preprint\narXiv:1707.08114, 2017.\n[13] P. Vafaeikia, K. Namdar, and F. Khalvati, \u201cA brief review of\ndeep multi-task learning and auxiliary task learning,\u201darXiv preprint\narXiv:2007.01126, 2020.\n[14] A. Maurer, M. Pontil, and B. Romera-Paredes, \u201cThe benefit of multitask\nrepresentation learning,\u201dJournal of Machine Learning Research, vol. 17,\nno. 81, pp. 1\u201332, 2016.\n[15] Y . Yang and T. Hospedales, \u201cDeep multi-task representation learning: A\ntensor factorisation approach,\u201darXiv preprint arXiv:1605.06391, 2016.\n[16] I. Misra, A. Shrivastava, A. Gupta, and M. Hebert, \u201cCross-stitch net-\nworks for multi-task learning,\u201d inProceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2016, pp. 3994\u20134003.\n[17] M. Wallingford, H. Li, A. Achille, A. Ravichandran, C. Fowlkes,\nR. Bhotika, and S. Soatto, \u201cTask adaptive parameter sharing for\nmulti-task learning,\u201d inProceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2022, pp. 7561\u20137570.\n[18] T. Standley, A. Zamir, D. Chen, L. Guibas, J. Malik, and S. Savarese,\n\u201cWhich tasks should be learned together in multi-task learning?\u201d inInt.\nConference on Machine Learning. PMLR, 2020, pp. 9120\u20139132.\n[19] J. Ma, Z. Zhao, X. Yi, J. Chen, L. Hong, and E. H. Chi, \u201cModeling task\nrelationships in multi-task learning with multi-gate mixture-of-experts,\u201d\ninProceedings of the 24th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, 2018, pp. 1930\u20131939.\n[20] Y . Zhang and D. Yeung, \u201cA convex formulation for learning task\nrelationships in multi-task learning,\u201darXiv preprint arXiv:1203.3536,\n2012.\n[21] E. V . Bonilla, K. Chai, and C. Williams, \u201cMulti-task gaussian process\nprediction,\u201d inAdvances in Neural Information Processing Systems,\nvol. 20, 2007.\n[22] J. Hensman, N. D. Lawrence, and M. Rattray, \u201cHierarchical bayesian\nmodelling of gene expression time series across irregularly sampled\nreplicates and clusters,\u201dBMC Bioinformatics, vol. 14, no. 1, pp. 1\u201312,\n2013.\n[23] J. Cervi \u02dcno, J. A. Bazerque, M. Calvo-Fullana, and A. Ribeiro, \u201cMeta-\nlearning through coupled optimization in reproducing kernel hilbert\nspaces,\u201d inAmerican Control Conference (ACC), 2019, pp. 4840\u20134846.\n[24] \u2014\u2014, \u201cMulti-task reinforcement learning in reproducing kernel hilbert\nspaces via cross-learning,\u201dIEEE Transactions on Signal Processing,\nvol. 69, pp. 5947\u20135962, 2021.\n[25] \u2014\u2014, \u201cMulti-task supervised learning via cross-learning,\u201d in29th Euro-\npean Signal Processing Conf. (EUSIPCO), 2021, pp. 1381\u20131385.\n[26] \u2014\u2014, \u201cMulti-task bias-variance trade-off through functional constraints,\u201d\ninICASSP 2023-2023 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2023, pp. 1\u20135.\n[27] S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Ecksteinet al., \u201cDistributed\noptimization and statistical learning via the alternating direction method\nof multipliers,\u201dFoundations and Trends\u00ae in Machine learning, vol. 3,\nno. 1, pp. 1\u2013122, 2011.\n[28] S. Boyd and L. Vandenberghe,Convex Optimization. Cambridge Uni-\nversity Press, 2009.\n[29] L. F. O. Chamon, S. Paternain, M. Calvo-Fullana, and A. Ribeiro,\n\u201cThe empirical duality gap of constrained statistical learning,\u201d inIEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2020, pp. 8374\u20138378.\n[30] L. F. O. Chamon and A. Ribeiro, \u201cProbably approximately correct\nconstrained learning,\u201d inAdvances in Neural Information Processing\nSystems, vol. 33, 2020.\n[31] L. F. O. Chamon, S. Paternain, M. Calvo-Fullana, and A. Ribeiro,\n\u201cConstrained learning with non-convex losses,\u201dIEEE Transactions on\nInformation Theory, 2022.\n[32] E. Mathieu, H. Ritchie, L. Rod \u00b4es-Guirao, C. Appel, D. Gavrilov,\nC. Giattino, J. Hasell, B. Macdonald, S. Dattani, D. Beltekian, E. Ortiz-\nOspina, and M. Roser, \u201cCovid-19 pandemic,\u201dOur World in Data, 2020,\nhttps://ourworldindata.org/coronavirus.\n[33] H. Venkateswara, J. Eusebio, S. Chakraborty, and S. Panchanathan,\n\u201cDeep hashing network for unsupervised domain adaptation,\u201d inPro-\nceedings of the IEEE conference on computer vision and pattern\nrecognition, 2017, pp. 5018\u20135027.\n[34] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImagenet classifica-\ntion with deep convolutional neural networks,\u201d inAdvances in Neural\nInformation Processing Systems, F. Pereira, C. Burges, L. Bottou, and\nK. Weinberger, Eds., vol. 25. Curran Associates, Inc., 2012.\n",
    "title": "Cross-Learning from Scarce Data via Multi-Task Constrained Optimization",
    "authors": [
      "Leopoldo Agorio",
      "Juan Cervi\u00f1o",
      "Miguel Calvo-Fullana",
      "Alejandro Ribeiro",
      "Juan Andr\u00e9s Bazerque"
    ],
    "published": "2025-11-17",
    "arxiv_id": "2511.13680v1",
    "num_pages": 13,
    "num_chars": 66127
  },
  {
    "text": "QUILL: An Algorithm\u2013Architecture Co-Design for\nCache-Local Deformable Attention\nHyunwoo Oh, Hanning Chen, Sanggeon Yun, Yang Ni, Wenjun Huang, Tamoghno Das, Suyeon Jang,\nand Mohsen Imani\nUniversity of California, Irvine\nEmail:{hyunwooo, m.imani}@uci.edu\nAbstract\u2014Deformable transformers deliver state-of-the-art de-\ntection but map poorly to hardware due to irregular memory\naccess and low arithmetic intensity. We introduce QUILL, a\nschedule-aware accelerator that turns deformable attention into\ncache-friendly, single-pass work. At its core, Distance-based Out-\nof-Order Querying (DOOQ) orders queries by spatial proxim-\nity; the look-ahead drives a region prefetch into an alternate\nbuffer\u2014forming a schedule-aware prefetch loop that overlaps\nmemory and compute. A fused MSDeformAttn engine executes\ninterpolation, Softmax, aggregation, and the final projection\n(W\u2032\u2032\nm) in one pass without spilling intermediates, while small\ntensors are kept on-chip and surrounding dense layers run on\nintegrated GEMMs. Implemented as RTL and evaluated end-to-\nend, QUILL achieves up to 7.29\u00d7higher throughput and 47.3\u00d7\nbetter energy efficiency than an RTX 4090, and exceeds prior\naccelerators by 3.26\u20139.82\u00d7in throughput and 2.01\u20136.07\u00d7in\nenergy efficiency. With mixed-precision quantization, accuracy\ntracks FP32 within\u22640.9 AP across Deformable and Sparse\nDETR variants. By converting sparsity into locality\u2014and locality\ninto utilization\u2014QUILL delivers consistent, end-to-end speedups.\nIndex Terms\u2014Deformable Attention, Vision Transformer,\nHardware/Software Co-design, DETR, ML Accelerator.\nI. INTRODUCTION\nDetection transformers (DETR) [1] have attracted\nwidespread attention for their end-to-end object detection\ncapabilities, often outperforming CNN-based detectors in\nsimplicity and adaptability [2]. As a result, DETR-based\nmodels have seen rapid adoption in tasks such as 2D\nrecognition and multimodal AI systems [1]\u2013[6]. However,\noriginal DETR variants still suffer from high computational\ncost and slow convergence, mainly due to dense attention\nmechanisms that obscure gradient flow from input features [7].\nDeformable DETR [7] addresses these issues with multi-\nscale deformable attention (MSDeformAttn), sampling only a\nhandful of spatial locations per query. Inspired by deformable\nconvolutions [8], this design significantly reduces FLOPs and\naccelerates training. Consequently, as shown in Fig. 1, MS-\nDeformAttn underpins state-of-the-art detector modules [9]\u2013\n[15] and has been adopted in emerging tasks such as video\nrestoration [16] and large vision\u2013language models [17].\nDespite MSDeformAttn\u2019s algorithmic efficiency, practical\ndeployments on GPUs expose notable latency and throughput\ndrawbacks [18]\u2013[20], limiting adoption in real systems [21].\nOur profiling on an RTX 4090 highlights four observations:\n1. Fewer FLOPs, higher latency (Fig. 2).Although the\nbackbone accounts for most FLOPs, thedeformable trans-\nCo-DETR\t(ViT-L)\nDeformable\tDETR\t(ResNet-50)\nYOLOX-X\nCOCO\tmAP\nParameter\tCount\t(M)\n0\nFocal-Stable-DINO\nGroup\tDETR\tv2\nInternImage-XL\nPlain-DETR\nRelation-DETR\n100\n200\n300\n400\n500\n600\n700\n35\n40\n50\nDeformable\tDETR\n(ResNeXT-101)\n60\n70\nFaster\tR-CNN+FPN\nCo-DETR\t(Swin-L)\nDETR\nDETR\nMask\tR-CNN\t(ResNet-101+CBN)\nDeformable\tDETR \nDETR\nOthers\nAchieved\thigher\taccuracy\nwith\tfewer\tparametersFig. 1:Landscape of SOTA object detection models.\nBackbone (ResNet-50)\n\u03c1\n: \nQuery keeping ratio\n43%\n57%\n79%\n21%\n36%\n64%\n80%\n20%\n17%\n83% \n74%\n26%\nA\nB\nC\nA\nB\nC\nFLOPs shares\nLatency shares\nDeformable Transformer\nrequires \nlower FLOPs\n but\nhigher latency!\nC\n: Sparse DETR, \n\u03c1=0.1\nB\n: Sparse DETR, \n\u03c1=0.5\nA\n: Deform. DETR, \n\u03c1=1.0\nDef. Transformer\nFig. 2:FLOPs vs. latency on RTX 4090comparing the backbone to the\ndeformable transformer block.\nformer block dominates run time. MSDeformAttn\u2019s mathe-\nmatically sparse patternfragments memory access, triggering\nfrequent stalls.\n2. Pruning reduces FLOPs but not latency proportion-\nally (Fig. 3(a)).Query pruning [22] lowers FLOPs roughly\nwith the keeping ratio\u03c1[13], but latency gains remain modest\n(e.g., even at\u03c1=0.1) due to scattered sampling overheads.\n3. Decoder cost remains substantial even with 98.5%\nfewer queries (Fig. 3(a), top).The decoder uses a tiny set\nof queries (e.g., 300; a98.5% reductionvs. the encoder\u2019s\n20,097), yet it stillaccounts for\u223c33% of runtime, indicat-\ning anorder-of-magnitude higher per-query cost. Modern\nDeformable DETR variants exacerbate this by increasingN d\n(e.g.,N d=900[10], [15]).\n4. Three fundamental bottlenecks (Fig. 3(b)).A roofline\nview identifies cache misses ( 1), low arithmetic intensity ( 2),\nfrequent bank conflicts ( 3) [19] as dominant causes oflow\nPE utilizationon current platforms.\nThese results reveal a mismatch between MSDeformAttn\u2019s\nsparse behavior and modern GPU architectures, motivating an\nalgorithm\u2013architecture co-designthat preserves model out-\nputs while reorganizing execution for locality and utilization.\nMost transformer accelerators target standard attention\nwhere accesses are denser and more regular [23]\u2013[27]. In\ncontrast,MSDeformAttn is inherently irregular, sampling from\nscattered references across multi-level features. Recent MS-arXiv:2511.13679v1  [cs.AR]  17 Nov 2025\nArithmetic Intensity (FLOPs/Byte)\n\u03c1=0.1\n\u03c1=0.5\n\u03c1=1.0\n\ue049roughput (TOPS)\nN: \n-49.3\n%\nLat.: \n+5.4%\nDecoder\n\ue048eries\nEncoder\n\ue048ery\nPruning\nDecoder\nDecoder\n\ue048ery\nSelection\nEncoder\nEncoder\n\ue048eries\nN\ne\n=20097\nN\ne\n=N\u22c5\n\u03c1=10049\nN\ne\n=2010\nN\nd\n=300\nN\nd\n=300\nN\nd\n=300\n(b) Roo\ufb02ine analysis on Def. Trans. and proposed solutions\nDistance-based\nOut-of-Order\n\ue048erying\nDedicated\nMSDeformAttn\nEngine\n64%\n33%\n55%\n10\n%\n22%\n8\n%\n38%\n14\n%\n31%\n10\n%\n\u2190 Memory bound\nCompute bound \u2192\nBatch=1\nBatch=4\n\u03c1=1.0\n\u03c1=0.1\n81.9\nFLOPs/B\nCache misses,\nBank con\ufb02ict\n13.0\n7.6\n\u2778\n\u2776\n12.7\n7.4\nLow arith.\nintensity\n\u2777\nOnly 7-13%\nvs. ideal perf.\n81.9\n7.5\n0.5\n12.8\n1.6\n13.1\n7.6\n1.7\n0.6\nPeak: 82.6 TOPS\n\u2776\nN\ne|d\n: \n\ue048ery count\n-98.5%\n62.7 ms\nAP: 48.0\n66.3 ms\nAP: \n49.3\nN: \n-77.7%\nLat.: \n-27.6%\n48.0 ms\nAP: \n48.2\nLat.: \nLatency\n(a) GPU latency breakdowns on Deformable Transformer\nN=20097\n\u2777\n\u2778Fig. 3:GPU performance breakdowns of Deformable DETR and query-pruned models on RTX 4090. (a) Latency across internal blocks. (b) Roofline\nview of bottlenecks and mitigation levers.\nDeformAttn accelerators [19], [20] leave critical gaps:(i)\nDEFA[19] processes queries in a fixed sequential order and\nrelies on a sliding-window cache, which isill-suited to the\ndecoder and pruned encoderwhere sparsity dominates,\nleading tosevere cache misses; moreover, it runs bilinear\ninterpolation, Softmax, and linear layers asseparate micro-\nkernel passes, forcing full memory load\u2013store between passes.\n(ii) UEDA[20]struggles to scale to standard Deformable\nDETRsettings (e.g.,N e=20,097,D=256) due toFPGA\nresource constraintsand limited algorithmic leverage. To\nour knowledge, no prior art (1)addresses sparse query\nirregularity at runtimeand (2)unifies encoder, decoder,\nand FFNin one execution path.\nWe presentQUILL, analgorithm\u2013architecture co-design\nthat turns deformable attention into cache-friendly work while\npreserving model semantics. Unlike fixed traversal/sliding-\nwindow schemes [19] or FPGA-restricted pipelines [20],\nQUILL introduces a runtime, proximity-aware reordering that\nrestructures execution for locality. Our contributions are:\n\u2022DOOQ runtime (query-locality scheduling with aligned\nprefetch).Distance-based Out-of-Order Querying (DOOQ)\nreorders queries by spatial proximity to raise cache hit rate.\nIts lookup window exposes predictable addresses, enabling\ndouble-bufferedprefetch aligned to the schedule, overlap-\nping fetch with compute and cutting cache-miss stalls ( 1).\n\u2022Fused MSDeformAttn execution path.Asingle-passpath\nthat fuses sampling, weighting (Softmax), and aggregation\nto lift arithmetic intensity ( 2) and avoid intermediate stores;\nconflict-aware buffers curb bank conflicts ( 3), addressing\ninefficiencies in prior designs [19].\n\u2022RTL prototype & full-stack evaluation.A synthesizable\nRTL of the full accelerator (DOOQ scheduler, feature cache,\nfused core, gather\u2013scatter, GEMM I/F), functionally verified\nagainst a reference MSDeformAttn, with cycle-accurate sim-\nulation, post-synthesis PPA, and end-to-end throughput/en-\nergy vs. RTX 4090 and DEFA (incl. ablations onw d,r).\nAcross Deformable DETR variants, QUILL achieves up to\n7.29\u00d7higher throughputand47.3\u00d7better energy effi-\nciencythan an RTX 4090 GPU. Compared to DEFA [19],\nQUILL improves throughput by3.26\u20139.82\u00d7, attributable to\nDOOQ\u2019s locality gains together with the fused execution path.\nLevel 0\nLevel 1\nLevel 2 \np\u0302\nW\n\u0394\np\u0302\n\u00d7Q\n\u0394\np\next_p\u0302\nBilinear Interpolation\nSampling\nW\nA\n\u00d7Q\nSoftmax\n!= None\nMulti-scale\nFeature Map\n\u03a3=1\n0.1\n0.1\n0.1\n0.1\n0.2\n0.1\n0.1\n0.1\n0.1\nAttention Weighting\nAggregation\nSampled\nfeatures\nReconstructed\nfractional\nfeatures\nconcat\nresult\nA\np\u0302\n:Reference point\n\u0394\np\n: Sampling offsets\nA\n: Attention weights\nRandom scattered\nsampling reduces\ncache locality.\nDRAM\nBank conflicts\nW\np\u0302\n\u00d7Q\nW\nm\n\u00d7\nx\n\u00d7\nSolved by prior art\n(DEFA[18], UEDA[19])\nPartially solved \nonly for \ndense decoder queries\nMemory-heavy\ncompute-light\nIncludes other\nAttention Head's\nresult (m \n\u2208\n\u2208\n M)\nUpdated\nQuery\nW\nm\n'\u00d7\n\u2778\n\u2777\n\u2776Fig. 4:MSDeformAttn dataflow linked to Eq.(1), with hardware bottlenecks\n1\u20133marked at the memory touchpoints: 1cache misses from scattered\nsamples; 2low arithmetic intensity (few ops per byte); 3bank conflicts\naround fractional 2\u00d72 fetches.\nII. PRELIMINARIES& HARDWARECHALLENGES\nWe give ahardware-centric view of MSDeformAttn\nand explain why itssparse, data-dependentaccess pattern\ndominates latency on existing platforms. We then show how\ncommon sparsification (e.g.,query pruning) further degrades\nlocality, motivating our algorithm\u2013architecture co-design.\nA. MSDeformAttn: A Hardware View\nMSDeformAttn [7] computes, for queryq, reference point\npand spatial embedding feature mapx,\nMSDeformAttn(q, p, x) =\nMX\nm=1Wm\u00b7\"LX\nl=1KX\nk=1Aqmlk\u00b7W\u2032\nm\u00b7x(\u02c6p q+ \u2206p qmlk)#\n(1)\nwherem, l, kindex head, feature level, and sampling point.\nEach query gathers a fewfractionallocations per head across\nmulti-level features (via 2\u00d72 bilinear interpolation), applies\nattention weightsA, and aggregates the result. FLOPs are\nmodest, but theirregular, multi-level gathersfragment memory\naccess (Fig. 4), yielding 1cache misses, 2low arithmetic\nintensity, and 3frequent bank conflicts [19].\n(a) Query pruning concept and impact on data locality\n4\n5\n6\nFrequent\nCache \nHit\n!\nE\n0\nD\nFrequent\nCache \nMiss\n!\nScoring\nNetwork\nDense Q \n4\n5\n6\nSparse Q \nE\n0\nD\nOrdered Query\nShuffled Query\nQuery\nquery\nsource\np\u0302[:N\n\u22c5\n\u22c5\n\u03c1]\nFFN\nGather\nScatter\nquery\nsource\nFFN\n(b) Encoder layer\n(c) Decoder layer\nMS\nDef.\nAtt.\nMS\nDef.\nAtt.\nNone\n(\nN\ne\n=N\n\u22c5\n\u22c5\n\u03c1\n, \nD)\n(\nN\nd\n=300\n, D)\n(N, D)\n(N\nd \n, D)\n(N, D)\ntop-\nk\n(\u03c1% \nor N\nd\n)\nSelf \nAttention\nCross\nAttention\n0\n1\n2\n3 \n4\n5\n6\n7 \n8\n9\nA\nB \nC\nD\nE\nF\n0\n4\n8\nC\n1\n5\n9\n2\n6\nA\nE\n3\n7\nB\nF\nD\nD\nE\n0\n75\n54\n42\n6\n34\n8\n84\ntop-\nk\n scores\nSampling points - \np\u0302+\n\u0394\np\nReference points - \np\u0302\nObjects to track \nRequired tokens - \nR\n1\u00d7D\n \nfor each!Fig. 5:Sparsity amplifies irregular access.(a) Top-kpruning cuts FLOPs\nbut scatters queries, reducing cache hit rate. (b) Sparse encoder (top-\u03c1%). (c)\nDecoder using top-N dtokens. Both induce less cache-friendly patterns.\nB. Query Pruning Hurts Data Locality\nAs shown in Fig. 5, Sparse DETR [13] prunes encoder\nqueries to the top-\u03c1%to reduce FLOPs, and decoder variants\nselect the top-N dtokens from encoder outputs. These steps\nscatter and shufflethe surviving queries in space, further\ndegrading spatial/temporal locality. As a result, latency on\nGPUs does not scale with the FLOP reduction [22], [28], [29].\nC. Fundamental Bottlenecks and Implications\nAcross encoder and decoder (and under pruning), de-\nformable transformers facethreehardware bottlenecks:\n1Cache misses:scattered, cross-level gathers defeat locality.\n2Low arithmetic intensity:few arithmetic ops per fetched\nbyte leave compute underutilized.\n3Bank conflicts:fractional 2\u00d72 patch fetches elevate colli-\nsion risk [19].\nThese factors limit throughput and efficiency on extent plat-\nforms, even FLOPs are reduced. Closing this gap requires a\nruntimethatrestores localityand makes next accesses pre-\ndictable enough todrive double-buffered (ping\u2013pong) prefetch,\npaired with a fused execution path that avoids redundant load\u2013\nstore traffic\u2014principles we adopt in our co-design.\nIII. ALGORITHM\u2013ARCHITECTURECO-DESIGN\nQUILL targets the three bottlenecks from Sec. II by turning\ndeformable attention into cache-friendly work. At a high level,\nDOOQ chooses a spatially local execution order for queries;\nthose predicted next addresses drive a region-based prefetch\ninto an alternate cache buffer; the fused core consumes the\ncurrent buffer and completes interpolation, weighting (Soft-\nmax), aggregation, and projection without spilling intermedi-\nates; and small, reusable tensors stay on-chip while a light\ngather\u2013scatter makes sparse I/O contiguous. The surrounding\ndense layers run on standard systolic GEMMs. Unlike fixed\nExternal Memory\nOutput\nSRAM\nPre-MSDeformAttn\nGEMM Engine (QKV, Scoring)\nSo\ue039max\nIndex\nGen.\nWeight\nGen.\nBilinear\nInterpolation\nLinear\nProj.\nL\n1 \nFeature\nBu\ufb00ers\nL\n2\nL\n3\nL\n4\nMSDeformAttn Core Engine\np\u0302\n(ref. point)\nSRAM\nPost-MSDeformAttn\nGEMM Engine (FFN)\nGather-Scatter Unit\n(N)\nSparse\nIndex Bu\ufb00er\nAddr.\nGen.\nSnoop\nReplace\nSRAM to ExtMem Addr.\nDRAM Bank\nHost Interface\nHost Interface\nW\nM\n''\nSRAM\nQUILL\nAccelerator\nDRAM Bank\nDRAM Bank\nb\nc\nd\ne\ne\nReplace\nEngine to SRAM\nAddr.\nFeature\nCaching Unit\n\u0394\np\nBuf.\nA\nBuf.\na\ncFig. 6:Overview of QUILL.The design has two pillars: ( a\u20dd) a feature\ncaching unit driven by DOOQ and aligned double-buffered prefetch to\nmitigate cache misses ( 1); and ( b\u20dd) a fused MSDeformAttn core that\nexecutes interpolation, Softmax, aggregation, and projection in one pass to\nlift arithmetic intensity ( 2) and curb conflict hotspots ( 3). Support blocks\ninclude ( c\u20dd) on-chip reuse SRAMs for small, frequently reused tensors, ( d\u20dd)\na light gather\u2013scatter path for sparse I/O, and ( e\u20dd) standard GEMM engines\nfor surrounding dense layers.Core of QUILL: DOOQ with aligned prefetch,\nand a fused single-pass core (vs. fixed/sliding traversal).\ntraversal or sliding-window caches [19] and FPGA-restricted\npipelines [20], the schedule adapts online and exposes pre-\ndictable next addresses that a prefetcher can exploit. We refer\nto this DOOQ\u2192look-ahead\u2192ping\u2013pong path as a schedule-\naware prefetch (SAP) loop.\nOur co-design follows two principles. First, use aruntime,\ncontent-agnostic schedule(DOOQ) that restores locality with-\nout changing model semantics and exposes predictable next\naddresses for prefetch. Second, execute MSDeformAttn in\nasingle passso locality gains translate directly into higher\narithmetic intensity and fewer stalls.\nA. ( a\u20dd) Feature Caching Unit: DOOQ with aligned prefetch\nThe feature cache (Fig. 6, a\u20dd) forms a closed loop: select the\nnext queries, prefetch their regions, and compute on the current\nbuffer. DOOQ supplies the order; a small selector realizes it;\nand double buffering turns predictable addresses into overlap\nbetween memory and compute. The loop is illustrated in\nFig. 7, and summarized in Alg. 1.\nWithin a lookup window of sizew d, DOOQ prefers the\nnext queries whose reference points\u02c6pare spatially closest to\nthe current one, so adjacent queries reuse many of the same\nfeature lines across levels. IfM(q)is the set of cache lines\nneeded by queryq, miss-driven stall for execution order\u03c3is\nTstall(\u03c3) =n\u22121X\ni=1tfetch\u00b7\f\fM(q \u03c3(i+1) )\\M(q \u03c3(i))\f\f,(2)\nand DOOQ acts directly on this set difference. Algorithm 1\n(line 2) performs the greedy nearest-neighbor selection in\u2113 1;\nFig. 7 depicts the corresponding distance+sorting unit. Unlike\nfixed traversal or sliding-window caches [19], the order adapts\nonline to the current distribution of\u02c6pwhile keeping hardware\nsimple (we use\u2113 1to avoid squaring and square roots with\nnegligible impact on reuse).\nCache 0\nLookup \nWindow\n7\n7\n4\ncur.\npoint\n5\n4\nnext point\np\u0302\n11\nCache 1\nPrefetch Manager\nDRAM\nPrefetch\nFeed\nFeed\nPrefetch\nDeformAtten\nEngine\nOutput SRAM \naddr=q_idx\np\u0302\n9\np\u0302\n7\np\u0302\n5\np\u0302\n3\np\u0302\n1\np\u0302\n5\np\u0302\na\np\u0302\n0\np\u0302\n1\np\u0302\nb\np\u0302\nc\np\u0302\na\np\u0302\nd\np\u0302\nb\na, c \u2208 {2n}, b, d \u2208 {2n+1}, n \u2208 {0,1,\u2026}\np\u0302\n3\np\u0302\n7\nCurrent \nref. point\n\u2460 \n \n Current\nLookup Points\n4\ndist=\n5\ndist=\n4\n3\n3\n2\n3\n4\ndist=\n7\ndist=\n7\n2\n2\n\u2461\n Get Distance &\nSort by Distance\nhit=\n8%\nhit=\n8%\nhit=\n24%\nNext\ncache\nwdw.\nhit=\n36%\n(a) Distance-based Out-of-Order \ue048erying (DOOQ)\nNext\nref. \npoint\n\u2462\n Cache Hit Rate\nComparison\n(c) Ping-pong DOOQ\nIntegration\np\u0302\n9\np\u0302\n5\np\u0302\n1\nmin. dist.!\nCur. cache window\n\u2460\n\u2462\nhit\n=\n36%\n\u2461\n(b) Closest Point\nCalculation Logic\n[x, y, query_idx]\ndist=||p\u0302\ni\n-p\u0302\nj\n||\n1\nCyclic Bitonic\nSorter\nw\nd\n=4\np\u0302\ni\npopFig. 7:DOOQ and aligned prefetch.DOOQ chooses spatially closest queries\nwithin a window; a lightweight distance+sorting (cyclic bitonic) unit selects\nthe next set; and a ping\u2013pong prefetch overlaps memory with compute using\naddresses from the DOOQ window. Together these form the schedule-aware\nprefetch (SAP) loop: schedule\u2192look-ahead\u2192overlapped prefetch.\nCandidates pass through a cyclic bitonic sorter (Fig. 7,\n\u201cdistance+sorting\u201d) that reuses compare-and-swap elements\nover multiple cycles; the per-query slack (aboutD/p dcycles)\namortizes its cost for typicalw d. Area scales linearly withw d\n(time-multiplexedO(w d)comparators overO(log2wd)steps),\nand timing fits within theD/p dslack. This guarantees a one-\nstep look-ahead: when the core beginsq \u03c3(i), the cache already\nknows the reference points forq \u03c3(i+1) .\nThat look-ahead drives supply. The prefetch in Alg. 1\n(line 3) issues a regional fetch around each chosen\u02c6pinto\nthe alternate buffer while the core consumes the current\none (the ping\u2013pong path in Fig. 7). At the handoff, only\nincremental lines are fetched if the next query stays within\nthe prefetched region. Under ping\u2013pong, the residual stall per\nstep ismax{0, t fetch(q\u03c3(i+1)|wd)\u2212t comp(q\u03c3(i))}, and DOOQ\nshrinks the first term by maximizing the overlap between\nM(q \u03c3(i))andM(q \u03c3(i+1) ). With average hit ratehand regional\nreuse\u03c1, the covered miss time per step is approximately\nmin{t comp,(1\u2212h\u03c1)t fetch}.\nFlow control is elastic: the emit/update in Alg. 1 (line 4)\nadvances the loop; FIFOs handle backpressure, a small per-\nlevel victim path recovers rare off-region offsets, and query\nIDs preserve semantic order.B. ( b\u20dd) Fused MSDeformAttn Core\nThe core turns locality into throughput by avoiding inter-\nmediate spills; we execute MSDeformAttn in a single pass.\nFolding the projections intoW\u2032\u2032\nm=Wm\u00b7W\u2032\nmyields\nMSDeformAttn(q, p, x) =\nMX\nm=1W\u2032\u2032\nm\u00b7\"LX\nl=1KX\nk=1Aqmlk\u00b7x(\u02c6p q+ \u2206p qmlk)#\n(3)Algorithm 1DOOQ (schedule-aware prefetch loop)\nRequire:lookup windowWof reference points{\u02c6p}, current\nqueryq i, region radiusr\n1:whileW \u0338=\u2205do\n2:q\u22c6\u2190arg min q\u2208W\u2225\u02c6pq\u2212\u02c6pqi\u22251 selection\n3:PrefetchRegion(\u02c6p q\u22c6, r)to alternate buffer\n4:Emit(q\u22c6);W \u2190 W \\ {q\u22c6};q i\u2190q\u22c6update\n5:end while\n+\n\u00d7\nw\ni \n\u00d7\nw\ni\n+\n+\n\u00d7\nw\ni\n+\n\u00d7\nw\ni\n\u00d7\n\u00d7\n+\n\u00d7\n\u00d7\n+\n+\nL=1 Buffer\nL=4 Buffer\np\u0302\nq\n \n(Reference Points)\n\u0394\np\nqmlk\n(Sampling Points)\nFeature\nIndices\nL\n\u22c5\n\u22c5\nK of fmap\n[0:4][0:p\nd\n]\nBiLerp\nWeights\nw\nij\n\u00d7\n\u00d7\n+\n\u00d7\n\u00d7\n+\n+\nA\nqmlk\nN\n(N, M, L, K\n\u22c5\n\u22c5\n2)\n(N, M, L, K)\nA\nqmlk\n'\n(Attention Scores)\n[0:p\nd\n]\n(D/p\nd\n, D, p\nd\n)\nW\nm\n'' SRAM\n(Linear Projector)\n D\n(1, p\nd\n) \u00d7 (p\nd\n, D)\nper cycle\nW\nm\n''\nIndex\nGen.\nWeight\nGen.\nOutput SRAM\nSoftmax\nUnit\nBiLerp\nLinear Projection\n(N, D)\n+\n+\n\u00d7\n\u00d7 \n+\n\u00d7\n\u00d7 \n+\n\u00d7\n\u00d7 \n+\n\u00d7\n\u00d7\n+\n+\n+\n+\n\u00d7\n\u00d7 \n+\n\u00d7\n\u00d7 \n+\n\u00d7\n\u00d7 \n+\n\u00d7\n\u00d7\n+\n+\n+\nAggregation\n(1, D) \u00d7 (D, D)\nD\nm\n / p\nd\nIteration\nFlow\nOne\nQuery\n(\nN\n, D)\nM\n(1, \nD\n)\n \nEvery\nQuery\nOne\nHead\n \nper cycle\n(1, \nD\nm\n)\n(1, \np\nd\n)\nFig. 8:Fused MSDeformAttn core.Interpolation, Softmax, aggregation, and\nprojection (W\u2032\u2032\nm) are executed in one pass; level buffers are organized to serve\n2\u00d72 fetches with minimal collisions.\nso interpolation, Softmax, aggregation, and projection proceed\nwithout intermediate writes that depress arithmetic intensity.\nIndex and weight generators produce 2\u00d72 coordinates and\nbilinear weightsw ij; the samew ijbroadcasts across the\nD-wide vector, which keeps the multiplier count low. The\n2\u00d72 gather plus interpolation uses four multipliers and three\nadders per element, with the small generators shared across\nsamples over multiple cycles. Softmax runs in-core to avoid\ntraffic, using shared exp/add hardware (Pad \u00b4e-based exponen-\ntial) across cycles. Aggregation feeds a projector backed by\non-chip storage forW\u2032\u2032\nm, which is loaded once per block and\nreused across queries. Level buffers are laid out so the com-\nmon 2\u00d72 access pattern maps to distinct banks; the DOOQ\nschedule further reduces simultaneous contention across levels,\ntrimming conflict hotspots without requiring large SRAMs.\nWe reorder acrossqueriesonly; within each query theL\u00d7K\nsampling order is unchanged, and Softmax is computed per\nquery, preserving semantics.\nC. ( c\u20dd\u2013e\u20dd) Support blocks\nSmall, frequently reused tensors stay on-chip to cut redun-\ndant transfers and help arithmetic intensity: reference points\u02c6p\n(two coordinates per query) and the projectorW\u2032\u2032\nm(sized byM\nandD). IfDorMgrows,W\u2032\u2032\nmstreams in stripes alongDwith\na tiny double buffer. For sparse encoders (top-\u03c1%) and token-\nselection decoders (top-N d), a light gather\u2013scatter unit remaps\nindices so external memory accesses remain burst-friendly;\nresults carry query IDs so downstream order is preserved. The\nsurrounding dense layers (pre-attention/projection and FFN)\nrun on standard systolic GEMM engines.\nIn summary, DOOQ raises locality so fewer new lines are\nfetched; aligned prefetch hides most remaining latency; and\nthe fused pass converts the locality into sustained utilization\nand end-to-end speedups. Section IV quantifies howw dand\nraffect hit rate, stall time, and throughput across encoder/de-\ncoder and pruned settings, and attributes gains to improved hit\nrateh, regional reuse\u03c1, and fused-pass intensity.\nIV. EVALUATION\nWe evaluate QUILL end-to-end and quantify how its\nschedule-aware prefetch and fused single-pass execution\ntranslate into system gains. We implement a synthesiz-\nable RTL generator in Chisel [30], verify with Verila-\ntor 5.009, and synthesize with Synopsys Design Compiler\nusing a topographical flow [31] in TSMC 28 nm HVT at\n1 GHz. External memory is modeled as HBM2 (256 GB/s,\n1.21 pJ/bit [32]). Unless noted, we use COCO 2017 [33]\nand standard Deformable DETR settings\u0000\nNe=\u230820097\u00b7\n\u03c1\u2309, N d=300, D=256, M=8, L=4, K=4\u0001\n, consistent with\ncommon SOTA variants [10]\u2013[15]. The region-prefetch radius\nrisfixedper level to the empirical maximum sampling offset\nfrom the model; we ablate only the DOOQ windoww d.\nA. Ablation: DOOQ Lookup Window (w d)\nFig. 9 sweepsw dagainst a same-capacity direct-mapped\nbaseline (no DOOQ logic) and reports encoder/decoder/total\nhit rate, memory energy efficiency, and area.\nEncoder locality grows quickly withw d(Fig. 9(a)) and\nexceeds80%byw d=1024\u2014a13.9\u00d7improvement over base-\nline\u2014because spatially adjacent queries reuse feature lines\nacross levels once the window exposes them to the cache. De-\ncoder locality (Fig. 9(b)) saturates at55\u201363%nearw d\u2248256:\nwithN d=300candidates, the window already spans most near\nneighbors, and the residual accesses are inherently scattered\nby the token-selection policy. Total hit rate (Fig. 9(c) tracks\nthe encoder trend as the encoder dominates the query count.\nEnergy improves monotonically withw d(Fig. 9(d)) as fewer\nnew lines are fetched per step in Equation (2), while the time-\nmultiplexed sorter keeps area growth predictable (Fig. 9(e)):\nO(w d)comparators reused overO(log2wd)steps fit in the\nper-query slack (D/p dcycles) without timing risk. Balancing\nhit, energy, and area, we choosew d=512for the rest of\nthe study: up to10.0\u00d7hit-rate gain vs. baseline,1.2\u20133.1\u00d7\nmemory-energy improvement, and<0.42 mm2overhead\u2014an\nattractive energy return per mm2.\nbase481632641282565121024\nbase481632641282565121024\nbase4816326412825651210240Hit rate (%)(a) Encoder20406080100Sparse DETR, \u03c1=0.1Sparse DETR, \u03c1=0.5Deformable DETR\n(b) Decoder(c) Totalwdwdwd\n0.2 0.3 0.7 1.4 3.0 6.6 30.1 64.1 181.8 481632641282565121024(e) Area OverheadOverhead (%)50100150200Dense queries: Saturated from the start\nSaturate over wd=256decreasingbenefits\n2.8\u00d7wd4.7\u00d7\n0Log-scaledimprovementMax 55~63%\n481632641282565121024(d) Energy Efficiency12340Efficiency (\u00d7)wdUp to 3.5\u00d7improvementFig. 9:DOOQ sweep overw d.Encoder/decoder/total hit rate, memory energy\nefficiency, and area vs. a same-capacity direct-mapped cache.\n59 41 152 EncoderDecoderABC(a) End-to-end latency(b) Latency per query(d) Query throughput1020304050246800.3M0.6M0.9M1.2M1.5M120160(c) End-to-endthroughput40ABCABCABC3.67\u00d73.31\u00d7000Ideal: 4.48\u00d7A: Deformable DETRB: Sparse DETR \u03c1=0.5C: Sparse DETR \u03c1=0.1Latency (\u00b5s)FPSQueries / s4.06\u00d7Ideal:4.48\u00d7Latency (ms)80\nFig. 10:End-to-end performance.(a) Overall latency. (b) Per-query latency.\n(c) Overall throughput. (d) Query throughput. Sparsity scales near-ideally\n(\u03c1=0.5\u21920.1).\nB. End-to-End Latency and Throughput\nFig. 10 shows how locality gains turn into system-level\nspeedups. Moving from\u03c1=0.5to\u03c1=0.1reduces end-to-end\nlatency by4.06\u00d7(Fig. 10(a)), close to the ideal4.48\u00d7dictated\nby query counts (10,349\u21922,310), and clearly exceeding the\nGPU\u2019s scaling baseline (cf. Fig. 3). The small gap at\u03c1=0.5\nversus dense is consistent with the mid-sparsity hit-rate dip:\nencoder requests still reuse well, while decoder misses begin\nto dominate.\nPer-query behavior (Fig. 10(b)) matches this: dense encoder\nqueries are fastest; sparsity tightens decoder locality and\nlowers median per-query latency, yet some off-region accesses\nremain that the local reordering in Equation (2) cannot fully\nhide\u2014explaining the residual gap to ideal scaling.\nThroughput scales accordingly. Overall throughput\n(Fig. 10(c)) reaches3.67\u00d7at\u03c1=0.1vs.\u03c1=0.5, near the ideal\n4.48\u00d7. Query throughput (Fig. 10(d)) is highest for the dense\nencoder\u2014up to3.31\u00d7over sparse decoder queries\u2014thanks to\nstronger reuse and more uniform access patterns. In practice,\naggressive pruning (\u03c1\u21920.1) delivers not only fewer FLOPs\nbut also sustained wall-clock speedups.\nTABLE I: Hardware evaluation at 1 GHz, 0.81 V (TSMC 28 nm HVT).\nMarkers ( a\u20dd\u2013e\u20dd) follow Fig. 6. \u201cOthers\u201d includes gather\u2013scatter ( d\u20dd) and\nclk/host interface.\nBlock Area Area Power Power\n(mm2) (%) (mW) (%)\nDOOQ scheduler/sorter ( a\u20dd) 0.416 3.83 83.4 2.92\nFeature cache L1\u2013L4 ( a\u20dd) 1.306 12.04 241.6 8.44\nFused MSDeformAttn core ( b\u20dd) 0.146 1.35 182.7 6.39\nGEMM engines (32\u00d732+64\u00d764) ( e\u20dd) 1.919 17.69 1406.0 49.15\nOn-chip SRAMs (other) ( c\u20dd) 6.297 58.04 788.6 27.56\nOthers (incl. d\u20dd) 0.766 7.06 158.6 5.54\nTotal 10.85 100 2860.9 100\n44.47 46.30 45.30 46.96 65.87 43.57 45.82 45.04 46.92 65.43 A: Deformable DETRB: Sparse DETR \u03c1=0.5C: Sparse DETR \u03c1=0.1D: De. DETR (2-stage)ABCDBaseline(FP32)QUILLDetection APE: Co-DETR (ViT-L)E70405060\nFig. 11:Detection accuracy (AP): FP32 vs. QUILL.A: Deformable DETR;\nB: Sparse DETR (\u03c1=0.5); C: Sparse DETR (\u03c1=0.1); D: 2-stage; E: Co-DETR\n(ViT-L). QUILL is within\u22640.9AP of FP32.\nC. Area and Power Breakdown\nWe next quantify the silicon budget of QUILL. Table I maps\nblocks to Fig. 6; gather\u2013scatter ( d\u20dd) is folded intoOthers.\nArea is memory-centric: on-chip SRAMs ( c\u20dd) plus the fea-\nture cache ( a\u20dd) occupy7.60 mm2(\u224870%). Power is compute-\ncentric: the two GEMMs ( e\u20dd) draw49%, reflecting FFN/pro-\njections. The fused MSDeformAttn core ( b\u20dd) is compact\n(1.35% area, 6.39% power), while DOOQ control (scheduler/-\nsorter) is small\u20143.83%area and2.92%power\u2014so most of\nthe \u201cDOOQ budget\u201d is the cache capacity that enables the\nhit-rate gains in Fig. 9. For smaller footprints, first right-size\nthe output SRAM (tiling/streaming), then tunew d;ris fixed\nby the model\u2019s offset envelope, and the selector scales near-\nlinearly, fitting in theD/p dslack. Withw d=512, the cache\npath adds<0.42mm2yet yields up to10\u00d7higher hit rate and\n1.2\u20133.1\u00d7memory-energy improvement.\nD. Model Accuracy (Baseline vs. QUILL)\nWe evaluate AP under FP32 and under QUILL\u2014our\nfused single-pass fixed-point path withmixed precision\n(W8A8\u2192A18 for linear/projection and bilinear, W16A8\u2192A28\nfor attention-weighted aggregation/Softmax). Model semantics\nare unchanged except DOOQ scheduling. Figure 11 covers\nDeformable DETR, Sparse DETR at\u03c1=0.5/0.1, a two-stage\nvariant, and large Co-DETR (ViT-L). Across all cases, QUILL\nis within0.04\u20130.90AP of FP32, preserving accuracy while\nenabling the locality-driven speedups above.\nE. Cross-Platform Analyses\nWe benchmark QUILL against a high-end GPU and recent\nMSDeformAttn accelerators.\nvs. RTX 4090.Fig. 12 compares latency, throughput vs.\nbatch size (B), andB=1energy. QUILL delivers up to5.14\u00d7\nlower latency,7.29\u00d7higher throughput, and47.3\u00d7better\nB=1energy efficiency, with the largest margins at\u03c1=0.1.\n18.0 14.7 47.3 1.20 1.61 5.72 3.02 2.75 7.29 1.41 1.75 5.14 Speedup (\u00d7)(a) Latency(b) Throughput28010203040Efficiency (\u00d7)(c) Energy efficiencyABCABC0A: Deformable DETRB: Sparse DETR \u03c1=0.5C: Sparse DETR \u03c1=0.1\nABC4B=1 vs B=1B=4 vs B=3RTX 4090CORSA2.94\u00d72.66\u00d73.23\u00d7650GPU Over B=4is excludeddue to extreme overhead with minimal gains.B=8uses23.4/24GB (98%) VRAM,290W powerfora 5.6% throughput gain.Fig. 12:GPU comparison (RTX 4090).(a) Latency. (b) Throughput vs.\nbatch size. (c)B=1energy efficiency.\nTABLE II:Comparison with other accelerators, including not only SOTA\nMSDeformAttn (MSDA) accelerators.\nMetric UnitThis work\nQUILLDAC\u201924\nDEFA[19]ASPDAC\u201925\nUEDA[20]\nWorkload - MSDA, FFNMSDA\n(Encoder only)MSDA\n(Shrunken\u2217)\nPrecision -W8A8\u2192A18,\nW16A8\u2192A28INT12 Not specified\nTechnology - 28 nm 40 nm FPGA\nArea mm\u00b2 10.85 2.63 -\nPower mW 2860.9 99.8 3070.0\nFrequency MHz 1000 400 200\nPeak Throughput TOPS 12.06 0.42 0.68\nPower Efficiency TOPS / W 4.23 4.19 0.22\nArea Efficiency TOPS / mm\u00b2 1.11 0.16 -\nNormalized\u2217\nPower EfficiencyTOPS / W 4.23 6.84 -\nNormalized\u2217\nArea EfficiencyTOPS / mm\u00b2 1.11 0.28 -\nNormalized\u2217\nThroughputFPS 1,270.5a-3832.7b390.3 108.0\nUtilization\u2217\nEfficiencyFPS / TOPS 31.76a-95.82b9.76 2.70\nNormalized\u2217\nEnergy EfficiencyFPS / W 134.3a-405.1b66.7 -\naDeformable DETR.bSparse DETR\u03c1=0.1.\u2217Normalized to 28 nm process using the industry scaling model in [34].\nBatch scaling is favorable: we exceed the GPU atB=3(GPU\nneedsB=4), avoiding large-BVRAM/power overheads.\nvs. Dedicated accelerators.Table II places QUILL along-\nside DEFA [19] and UEDA [20]. Unlike DEFA (encoder-\nonly, micro-kernelized, INT12) and UEDA (FPGA, reduced\nMSDA), QUILL acceleratesMSDeformAttn end-to-end with\nFFNin a single fused pass while preserving FP32 se-\nmantics. This scope plus DOOQ-driven locality yields the\nhighest peak throughput(12.06 TOPS) andarea effi-\nciency(1.11 TOPS/mm2) with competitivepower efficiency\n(4.23 TOPS/W). On normalized end-to-end metrics, QUILL\nsustains31.76\u201395.82 FPS/TOPSand134.3\u2013405.1 FPS/W\n(Deformable DETR\u2192Sparse DETR,\u03c1=0.1). Versus DEFA,\nthroughput utilization and energy efficiency are3.26\u20139.82\u00d7\nand2.01\u20136.07\u00d7higher; UEDA trails due to FPGA frequen-\ncy/resource limits and reduced configs.\nOverall, converting deformable attention into cache-friendly,\nsingle-pass work lets QUILL translate sparsity into end-to-end\ngains on real models while staying within\u22640.9AP of FP32.\nV. CONCLUSION\nWe presented QUILL, an algorithm\u2013architecture co-design\nthat turns MSDeformAttn into cache-friendly, single-pass\nwork. By coupling DOOQ\u2019s scheduling with a fused MSDe-\nformAttn core and integrated GEMMs, QUILLdelivers near-\nlinear sparse scaling and end-to-end gains: up to 7.29\u00d7higher\nthroughput and 47.3\u00d7betterB=1energy efficiency than an\nRTX 4090, and up to 9.82\u00d7/6.07\u00d7higher throughput/energy\nefficiency than prior accelerators.\nThree directions remain: (i) adaptive scheduling/prefetch\n(learned, multi-step) that tunesw dand cache policy online; (ii)\napplying the schedule-aware prefetch loop to other sparse ops\n(cross-attention, video, point clouds) and to larger backbones\nvia streamedW\u2032\u2032\nmtiling; and (iii) a compiler/runtime that auto-\ntunes micro-architectural knobs (parallelismp d, GEMM tiling,\ncache partitioning) with tighter quantization-aware training\n(QAT) for mixed precision. Together, these extend QUILL\u2019s\ncore idea\u2014turn sparsity into locality, then utilization\u2014to the\nnext wave of detection and multimodal transformers.\nREFERENCES\n[1] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, \u201cEnd-to-end object detection with transformers,\u201d in16th\nEur. Conf. Comput. Vis. (ECCV), Glasgow, United Kingdom, Aug. 2020,\np. 213\u2013229.\n[2] Y . Zhaoet al., \u201cDETRs Beat YOLOs on Real-time Object Detection,\u201d in\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Seattle, WA,\nUSA, Jun. 2024, pp. 16 965\u201316 974.\n[3] X. Gu, H. Fan, Y . Huang, T. Luo, and L. Zhang, \u201cContext-Guided Spatio-\nTemporal Video Grounding,\u201d inIEEE/CVF Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Seattle, WA, USA, Jun. 2024, pp. 18 330\u201318 339.\n[4] A. Kamath, M. Singh, Y . LeCun, G. Synnaeve, I. Misra, and N. Carion,\n\u201cMDETR - Modulated Detection for End-to-End Multi-Modal Under-\nstanding,\u201d inIEEE/CVF Int. Conf. Comput. Vis. (ICCV), Montreal, QC,\nCanada, Oct. 2021, pp. 1760\u20131770.\n[5] J. Xuet al., \u201cPixel Aligned Language Models,\u201d inIEEE/CVF Conf.\nComput. Vis. Pattern Recognit. (CVPR), Seattle, WA, USA, Jun. 2024,\npp. 13 030\u201313 039.\n[6] W. Huanget al., \u201cTell Me What to Track: Infusing Robust\nLanguage Guidance for Enhanced Referring Multi-Object Tracking,\u201d\narXiv preprint arXiv:2412.12561, 2024. [Online]. Available: https:\n//arxiv.org/abs/2412.12561\n[7] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, \u201cDeformable\nDETR: Deformable Transformers for End-to-End Object Detection,\u201d in\nInt. Conf. Learn. Represent. (ICLR), Virtual, May 2021, pp. 1\u201316.\n[8] X. Zhu, H. Hu, S. Lin, and J. Dai, \u201cDeformable ConvNets V2: More\nDeformable, Better Results,\u201d inIEEE/CVF Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Long Beach, CA, USA, Jun. 2019, pp. 9300\u20139308.\n[9] Z. Zou, K. Chen, Z. Shi, Y . Guo, and J. Ye, \u201cObject Detection in 20\nYears: A Survey,\u201dProc. IEEE, vol. 111, no. 3, pp. 257\u2013276, 2023.\n[10] Z. Zong, G. Song, and Y . Liu, \u201cDETRs with Collaborative Hybrid\nAssignments Training,\u201d inIEEE/CVF Int. Conf. Comput. Vis. (ICCV),\nParis, France, Oct. 2023, pp. 6748\u20136758.\n[11] H. Zhanget al., \u201cDINO: DETR with Improved DeNoising Anchor\nBoxes for End-to-End Object Detection,\u201d inInt. Conf. Learn. Represent.\n(ICLR), Kigali, Rwanda, May 2023, pp. 1\u201319.\n[12] F. Li, H. Zhang, S. Liu, J. Guo, L. M. Ni, and L. Zhang, \u201cDN-\nDETR: Accelerate DETR Training by Introducing Query Denoising,\u201d in\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), New Orleans,\nLA, USA, Jun. 2022, pp. 13 619\u201313 627.\n[13] B. Roh, J. Shin, W. Shin, and S. Kim, \u201cSparse DETR: Efficient End-\nto-End Object Detection with Learnable Sparsity,\u201d inInt. Conf. Learn.\nRepresent. (ICLR), Virtual, Apr. 2022, pp. 1\u201323.\n[14] S. Liuet al., \u201cDetection transformer with stable matching,\u201d inIEEE/CVF\nConf. Comput. Vis. Pattern Recognit. (CVPR), Van Couver, BC, Canada,\nJun. 2023, pp. 6491\u20136500.\n[15] C.-B. Zhang, Y . Zhong, and K. Han, \u201cMr. DETR: Instructive Multi-\nRoute Training for Detection Transformers,\u201d inIEEE/CVF Conf. Com-\nput. Vis. Pattern Recognit. (CVPR), Nashville, TN, USA, Jun. 2025, p.\nto appear.\n[16] J. Lianget al., \u201cRecurrent Video Restoration Transformer with Guided\nDeformable Attention,\u201d inAdv. Neural Inf. Process. Syst. 35 (NeurIPS),\nNew Orleans, LA, USA, 2022, pp. 378\u2013393.\n[17] W. Wanget al., \u201cVisionLLM: Large Language Model is also an Open-\nEnded Decoder for Vision-Centric Tasks,\u201d inAdv. Neural Inf. Process.\nSyst. 36 (NeurIPS), New Orleans, LA, USA, 2023, pp. 61 501\u201361 513.[18] P. Donget al., \u201cSpeedDETR: Speed-aware Transformers for End-to-end\nObject Detection,\u201d in40th Int. Conf. Mach. Learn. (ICML), Honolulu,\nHawaii, USA, 2023.\n[19] Y . Xuet al., \u201cDEFA: Efficient Deformable Attention Acceleration via\nPruning-Assisted Grid-Sampling and Multi-Scale Parallel Processing,\u201d\nin61st Annu. Des. Autom. Conf. (DAC), San Francisco, CA, USA, Jun.\n2024.\n[20] K. Sun, M. Wang, J. Zhou, and Z. Wang, \u201cUEDA: A Universal And\nEfficient Deformable Attention Accelerator For Various Vision Tasks,\u201d\nin30th Asia S. Pac. Des. Autom. Conf. (ASP-DAC), Tokyo Japan, Jan.\n2025, pp. 163\u2013169.\n[21] T. Liang and G. Zeng, \u201cFSH-DETR: An Efficient End-to-End Fire\nSmoke and Human Detection Based on a Deformable DEtection TRans-\nformer (DETR),\u201dSensors, vol. 24, no. 13, 2024.\n[22] R. Child, S. Gray, A. Radford, and I. Sutskever, \u201cGenerating Long\nSequences with Sparse Transformers,\u201darXiv preprint arXiv:1904.10509,\n2019. [Online]. Available: http://arxiv.org/abs/1904.10509\n[23] J. Zhuanget al., \u201cSSR: Spatial Sequential Hybrid Architecture\nfor Latency Throughput Tradeoff in Transformer Acceleration,\u201d in\nACM/SIGDA Int. Symp. Field Program. Gate Arrays (FPGA), Monterey,\nCA, USA, 2024, p. 55\u201366.\n[24] S. Tuli and N. K. Jha, \u201cAccelTran: A Sparsity-Aware Accelerator for\nDynamic Inference With Transformers,\u201dIEEE Trans. Comput.-Aided\nDesign Integr. Circuits Syst., vol. 42, no. 11, pp. 4038\u20134051, 2023.\n[25] Q. Guo, J. Wan, S. Xu, M. Li, and Y . Wang, \u201cHG-PIPE: Vision\nTransformer Acceleration with Hybrid-Grained Pipeline,\u201d inIEEE/ACM\nInt. Conf. Comput. Aided Des. (ICCAD), Newark, NJ, USA, Oct. 2024,\np. to appear.\n[26] C. Chen, L. Li, and M. M. Sabry Aly, \u201cViTA: A Highly Efficient\nDataflow and Architecture for Vision Transformers,\u201d inDes. Autom.\nTest Eur. (DATE), Valencia, Spain, 2024, pp. 1\u20136.\n[27] W. Li, Y . Luo, and S. Yu, \u201cRAWAtten: Reconfigurable Accelerator for\nWindow Attention in Hierarchical Vision Transformers,\u201d inDes. Autom.\nTest Eur. (DATE), Antwerp, Belgium, 2023, pp. 1\u20136.\n[28] E. Kwon, H. Song, J. Park, and S. Kang, \u201cMobile Accelerator Exploiting\nSparsity of Multi-Heads, Lines, and Blocks in Transformers in Computer\nVision,\u201d inDes. Autom. Test Eur. (DATE), Antwerp, Belgium, 2023, pp.\n1\u20136.\n[29] Y . Zhai, B. Li, B. Yan, and J. Wang, \u201cSTAR: An Efficient Softmax\nEngine for Attention Model with RRAM Crossbar,\u201d inDes. Autom. Test\nEur. (DATE), Antwerp, Belgium, 2023, pp. 1\u20132.\n[30] J. Bachrachet al., \u201cChisel: constructing hardware in a Scala embedded\nlanguage,\u201d in49th Annu. Des. Autom. Conf. (DAC), San Francisco, CA,\nUSA, Jun. 2012, pp. 1216\u20131225.\n[31] \u201cDC Ultra: Concurrent Timing, Area, Power, and\nTest Optimization,\u201d Synopsys, Tech. Rep., 2018. [On-\nline]. Available: https://www.synopsys.com/content/dam/synopsys/\nimplementation%26signoff/datasheets/dc-ultra-ds.pdf\n[32] S. Ghodrati, H. Sharma, C. Young, N. S. Kim, and H. Esmaeilzadeh,\n\u201cBit-Parallel Vector Composability for Neural Acceleration,\u201d in57th\nAnnu. Des. Autom. Conf. (DAC), Virtual, Jul. 2020, pp. 1\u20136.\n[33] T.-Y . Linet al., \u201cMicrosoft COCO: Common Objects in Context,\u201d in\n13th Eur. Conf. Comput. Vis. (ECCV), Zurich, Switzerland, 2014, pp.\n740\u2013755.\n[34] W. Huang, K. Rajamani, M. R. Stan, and K. Skadron, \u201cScaling with\nDesign Constraints: Predicting the Future of Big Chips,\u201dIEEE Micro,\nvol. 31, no. 4, pp. 16\u201329, Jul. 2011.\n",
    "title": "QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention",
    "authors": [
      "Hyunwoo Oh",
      "Hanning Chen",
      "Sanggeon Yun",
      "Yang Ni",
      "Wenjun Huang",
      "Tamoghno Das",
      "Suyeon Jang",
      "Mohsen Imani"
    ],
    "published": "2025-11-17",
    "arxiv_id": "2511.13679v1",
    "num_pages": 7,
    "num_chars": 38612
  },
  {
    "text": "T-SAR: A Full-Stack Co-design for CPU-Only T ernary\nLLM Inference via In-Place S IMD A LU R eorganization\nHyunwoo Oh, KyungIn Nam, Rajat Bhattacharjya, Hanning Chen, Tamoghno Das, Sanggeon Yun,\nSuyeon Jang, Andrew Ding, Nikil Dutt, and Mohsen Imani\nDepartment of Computer Science, University of California, Irvine\nEmail: {hyunwooo, m.imani }@uci.edu\nAbstract \u2014Recent advances in LLMs have outpaced the compu-\ntational and memory capacities of edge platforms that primarily\nemploy CPUs, thereby challenging efficient and scalable deploy-\nment. While ternary quantization enables significant resource\nsavings, existing CPU solutions rely heavily on memory-based\nlookup tables (LUTs) which limit scalability, and FPGA or GPU\naccelerators remain impractical for edge use. This paper presents\nT-SAR, the first framework to achieve scalable ternary LLM\ninference on CPUs by repurposing the SIMD register file for\ndynamic, in-register LUT generation with minimal hardware\nmodifications. T-SAR eliminates memory bottlenecks and maxi-\nmizes data-level parallelism, delivering 5.6\u201324.5 \u00d7and 1.1\u201386.2 \u00d7\nimprovements in GEMM latency and GEMV throughput, respec-\ntively, with only 3.2% power and 1.4% area overheads in SIMD\nunits. T-SAR achieves up to 2.5\u20134.9 \u00d7the energy efficiency of an\nNVIDIA Jetson AGX Orin, establishing a practical approach for\nefficient LLM inference on edge platforms.\nIndex Terms \u2014Large Language Models, SIMD, Instruction Set\nArchitecture, Quantization, Ternary LLM.\nI. I NTRODUCTION\nLarge Language Models (LLMs) have become ubiquitous\ntoday, with numerous applications, including those in coding\nassistants, document analysis, and interactive conversational\ninterfaces across consumer and enterprise systems [1]. How-\never, LLMs require substantial resources for inference, with\nbillions of parameters driving extensive matrix operations and\ncreating significant latency during autoregressive generation,\nwhere each token requires a full model forward pass [1].\nTraditionally, LLMs have been deployed on cloud servers\nwith high-power GPUs, NPUs, or specialized accelerators to\nhandle their extreme compute and memory demands (e.g.,\n>140 GB memory for Llama2-70B [2]). Increasingly, how-\never, there is a need to run LLMs directly on the edge for\nscenarios such as coding copilots with proprietary source\ncode, document analysis of confidential business data, and\npersonalized assistants handling sensitive information [3]. In\nthese settings, reliance on cloud computing is often infeasible\ndue to intellectual property concerns, data privacy regulations\nlike GDPR and HIPAA [4], limited network connectivity, or\nprohibitive cloud costs for continuous inference workloads.\nThus, to enable standalone LLM deployment on edge de-\nvices at lower cost, several techniques have been proposed,\nincluding pruning [5], [6], quantization [7]\u2013[10], knowledge\ndistillation [11], [12], and weight binarization [13]\u2013[15].\nWithin quantization, ternary quantization has emerged as\na particularly promising approach [16]\u2013[18]. By constraining\n404550556065ABCDE0.010.1110ABCDEFloatLLMTernaryLLMModel Size (GB)ModelModelAccuracy3.9BA2.4B1.5B1.1B830MBCDE1/8Model SizeSmallAccuracyLossHighly scalableregarding memory usage, inference throughput, and accuracyCommon Sense /Reasoning Average(a) Scalability Analysis of Ternary LLMs\n(c) Memory Access RequestsBreakdowns By Memory SizeTernaryLUTsALURegfileCacheCPUWeightsActivationsDRAMOutput\u2460\u2460\u2460\u2461\u2461\u2461\u2461\u2460Ternary LUT Generation\u2461LUTGEMM / GEMV OperationTernaryLUTsOthersWeights> 75%< 20%Measured in Ternary LLM Modelswith 125M -100B ParametersMajor Bottleneck(b) Overview of SOTA Kernel\u2019sGEMM / GEMV DataflowBinaryLLMNumber of ParametersFig. 1: Motivation for scalable ternary LLM acceleration. (a) Ternary\nLLMs provide 8 \u00d7size reduction with minimal accuracy loss, making them\nsuitable for edge deployment. (b)GEMM/GEMV dataflow: SOTA LUT-\nbased kernels store TLUTs in DRAM, causing frequent memory access\nrequests. (c)Memory access breakdown: TLUTs dominate system memory\nrequests\u2014over 75%\u2014across models from 125M to 100B parameters, creating\na major bottleneck for CPU inference.\nweights to {-1, 0, 1 }, it achieves 8 \u00d7memory compression\n(Fig. 1(a)) while maintaining 93\u201399% of full-precision ac-\ncuracy, presenting even better model-size scaling than binary\nLLMs [18]. This enables dramatic cost reduction and practical\ndeployment on resource-constrained edge devices.\nHowever, traditional (edge-) CPUs face a fundamental ar-\nchitectural misalignment when ternary LLMs are deployed,\nresulting in degraded performance. State-of-the-art (SOTA)\nmethods such as T-MAC [19] and BitNet.cpp [20] replace\nmultiply-accumulate (MAC) operations with dynamic lookup\ntables (LUTs) stored in memory (caches and DRAM), shift-\ning workloads from compute-bound to memory-bound ones,\nachieving 2.4-6.2\u00d7 the throughput compared to the FP16-based\nkernels [20]. As shown in Fig.1(c), ternary LUT (TLUT) ac-\ncesses account for over 75% of system memory requests, cre-\nating bandwidth pressure that underutilizes Single-Instruction\nMultiple-Data (SIMD) execution units ubiquitous in commod-\nity processors [21]. This excessive memory traffic cancels out\nmuch of the computational benefit of ternary quantization.\nWhile dedicated AI accelerators and FPGAs [22]\u2013[24]\nachieve high ternary inference efficiency, their cost and in-\ntegration complexity preclude widespread edge deployment.\nLikewise, server-class features such as Intel AMX [25] orarXiv:2511.13676v1  [cs.AR]  17 Nov 2025\ncustom ISA extensions with large compute arrays [26]\u2013[28]\nremain unavailable on edge CPUs due to area and power\nconstraints. In contrast, the SIMD units that current approaches\nunderutilize already provide high-bandwidth register files with\ndatapaths naturally aligned to ternary operations. Yet no prior\nwork has exploited these SIMD registers for in-register LUT\ncomputation, leaving an opportunity to overcome memory\nbottlenecks and fully harness existing parallel hardware.\nTherefore, in this paper we introduce T-SAR, a full-stack\nco-design framework for scalable, high-throughput ternary\nLLM inference on edge CPUs, achieved by leveraging ex-\nisting SIMD hardware with only minimal modifications. The\ncore innovation of T-SAR is repurposing the SIMD vec-\ntor register file for dynamic, in-register LUT genera-\ntion\u2014eliminating costly memory traffic and maximizing\ndata-level parallelism without new compute arrays or com-\nplex datapath extensions. While we focus on the x86 A VX2\nISA, the idea extends naturally to other SIMD ISAs (e.g.,\nARM NEON, RISC-V Vector [29]), requiring only parameter\nretuning. T-SAR\u2019s design spans four tightly integrated layers:\n\u2022Algorithmic Layer: A ternary-to-binary decomposition\nand data packing scheme for efficient LUT computing.\n\u2022ISA Layer: Minimal extensions that add register-\nto-register LUT-based General Matrix Multiplication\n(GEMM)/General Matrix-Vector Multiplication (GEMV),\nsupporting dynamic computation within SIMD units.\n\u2022Microarchitecture Layer: Power and area overhead\nanalyses based on lightweight wiring/multiplexing adjust-\nments for SIMD units, validated by ASIC synthesis.\n\u2022Software Layer: An adaptive kernel dataflow that max-\nimizes throughput across diverse models and platforms.\nFrom high-performance to low-power edge CPUs, T-SAR\nachieves 5.6\u201324.5 \u00d7GEMM latency reduction and 1.1\u201386.2 \u00d7\nGEMV throughput improvement over SOTA CPU base-\nlines [19], [20] on ternary LLMs from 125M to 100B param-\neters, with only 3.2% power and 1.4% area overhead in SIMD\nunits. Notably, T-SAR also delivers 2.5\u20134.9 \u00d7higher energy ef-\nficiency than the Jetson AGX Orin GPU on Llama-8B [17] and\nFalcon3-10B [30]. These results demonstrate that systematic\nco-design can unlock the latent potential of ubiquitous SIMD\nhardware for practical edge LLM deployment, narrowing the\ngap with specialized accelerators while leveraging the world\u2019s\nmost widely deployed compute platform: CPUs.\nII. M OTIVATION\nWe now characterize the bottleneck in SOTA ternary kernels\nthat motivates the co-design framework T-SAR.\nWhile ternary LLMs promise efficient inference by quantiz-\ning weights to {\u22121,0,1}and replacing costly floating-point\nmultiplications with low-cost integer operations, the SOTA\napproach for executing them introduces a critical bottleneck.\nAs shown in Fig. 2(a,b), these models are implemented in\n\u2018BitLinear\u2018 layers, where the core matrix multiplication is\nhandled by an LUT-based GEMM/GEMV method [19], [20].\nThis technique partitions each input vector (with Ktotal\ninputs) into atomic blocks of size c. For each block, all 3c\n1\u00d76912\u00d72560 Kernel\nQuantization\nLUTGEMM\nDequantization\nint8\nfp16/\nint16\nfp32\nfp32\n(a) Ternary LLM Architecture\nQ\nAtten\ntion\nReLU\n2\nK\nV\n+\nInput\nEmbed.\nO\nBit\nLinear \nBit\nLinear \nBit\nLinear\nBit\nLinear\nN\no\nr\nm\nN\no\nr\nm\nBit\nLinear\nBit\nLinear\n+\nTernary Self-Attention\nFFN\nOutput\nEmbed.\n(b) BitLinear Layer\n(c) Memory Footprint Breakdowns\nRAM Occupation\nTLUTs\n(\n87.6%\n)\n526.5\nMB\n4.24\nGB\nKV Cache\n(0.9%)\nTLUTs\n(0.01%)\nWeights\n(87.7%)\nKV Cache\n(7.5%)\nOthers\n(4.8%)\nMemory Accesses\nWeights\n(10.9%)\nOthers\n(0.6%)\nTernary LUTs (TLUTs) \ndominate\n \nmemory accesses\nalthough they occupy only \nsmall amount of RAM\nBitnet-b1.58-2B-4T\nInference (Decode)\nTernary Transformer Block\n(d) BitLinear Layer Execution Time Breakdown\nTLUT\nMemRead\nTLUT\nMemWrite\nTLUT\nPrecompute\nOthers\nTLUT\nAccumulate\nTotal: 605.1 \n\u03bc\ns\nTLUT Memory R/W operations \ndominate \nexecution time (91.6%)\nCPU: AMD Ryzen 9950X\n82.4%\n2.1%\n2.8%\n3.5%\n9.2%Fig. 2: Ternary LLMs: Architecture and bottleneck analysis. (a) Ternary\ntransformer with BitLinear layers. (b)BitLinear layer workflow including\nquantization and LUTGEMM. (c)BitNet-b1.58-2B-4T memory footprints:\nTLUTs, though tiny in RAM, dominate memory accesses. (d)BitLinear\nGEMV time breakdown: Memory R/W dominates execution.\npossible dot product results are pre-computed and stored in\nan LUT, transforming the computation from O(K)arithmetic\noperations per output to O(K/c)table lookups. The total\nLUT storage per layer becomes O((K/c)\u00b73c), that trades off\narithmetic complexity for memory accesses, resulting in the\nfastest CPU implementations today.\nThough theoretically efficient, this trade-off introduces a\ncritical memory access bottleneck . As Fig. 3(a) illustrates, the\nSOTA dataflow relies on pre-computing all LUT values and\nstoring them in system memory. During inference, weights are\nused to index these LUTs, requiring frequent, random memory\naccesses that fail to efficiently utilize modern cache hierar-\nchies and powerful SIMD hardware. Our analysis reveals the\nseverity of this issue. Although ternary LUTs (TLUTs) occupy\nless than 0.01% of total RAM in a representative model, they\nare accessed so frequently that they account for a staggering\n87.6% of all memory transactions (Fig. 2(c)). This mem-\nory saturation means the workload becomes memory-bound\nrather than compute-bound , with 91.6% of the execution\ntime spent on memory read/write (R/W) operations (Fig. 2(d)).\nTo overcome these limitations, T-SAR aims to eliminate\nexternal LUT loads and fully exploit the SIMD datapath ,\nshown in Fig. 3(b). T-SAR generates compressed LUTs on-\nthe-fly inside SIMD registers via custom ISA extensions, elim-\ninating memory TLUT traffic and supporting fused GEMV-\naccumulation operations to increase throughput. However,\nachieving in-register LUT computation is non-trivial: the re-\nquired LUT size ( 3c) does not match the fixed 2nbitwidth of\nSIMD registers, and the limited register file size constrains the\n(a) Prior LUT-based\nGEMM / GEMV\nActivation\n(INT8)\nLUT\n0\n[0:3\nc\n] \nLUT\n1\n[0:3\nc\n] \nLUT\n2\n[0:3\nc\n] \n...\nPrecompute\nLUTs\nDRAM\nWeight\n{-1,0,1}\na\n0\n[0:c]\nLUT\n0:c\n[0:3\nc\n]\nw\n0\n[0:c]\n0\n[0:c]\n0\n1\n2\n+\nOutput\n(b) T-SAR: Streamlined Dataflow\n via ISA extension\nActivation\n(INT8)\na\n0\n[0:c]\nOn-the-fly gen.\nCompressed LUTs\nLUT\n0\n[0:2\nc+1\n]\nSIMD Reg.\nWeight\n{(-1,1), (0,1)}\nLUT\n0:k \n[0:2\nc+1\n]\n+\nw\n0\n[0:c]\nCustom\nISA\nLUT GEMV\nOutput\nAccumulate\nfusion\nLUT\nGEMV\nAccum.Fig. 3: Prior LUT-based CPU solution vs. T-SAR. (a) Prior: precomputed\nLUTs loaded from DRAM. (b) T-SAR: on-the-fly compressed LUTs generated\nin SIMD registers.\nnumber of LUTs that are held simultaneously. This necessi-\ntates the careful co-design detailed in the following sections.\nIII. T-SAR C O-DESIGN STACK\nTo resolve the memory bottleneck mentioned in Sec-\ntion II, T-SAR introduces a full-stack co-design that transforms\nLUT-based operations from memory-bound to compute-bound\ntasks. The core innovation is that we eliminate memory traffic\nby generating LUTs on-the-fly, directly within the CPU\u2019s\nhigh-speed SIMD register file. This section details the tightly\nintegrated algorithmic, ISA, and microarchitectural layers that\nenable this transformation.\nA. Algorithm: Ternary-to-Binary Decomposition\nThe primary challenge of in-register LUT generation is the\narchitectural mismatch between the base-3 nature of ternary\nweights and the base-2 structure of SIMD hardware. A naive\nLUT for a block of cweights would require 3centries, which\ndoes not align with SIMD register bitwidths.\nT-SAR overcomes this with a novel LUT compression via\nweight transformation , as shown in Fig. 4. We decompose a\nternary weight block w\u2208 {\u2212 1,0,1}cand its corresponding\ninput activations a\u2208Rcinto two separate binary forms:\n\u2022Dense weights :wD\u2208 {\u2212 1,+1}c, where wD,i=wiif\nwi\u0338= 0, else +1.\n\u2022Sparse weights :wS\u2208 {0,1}c, where wS,i= 1ifwi= 0,\nelse0.\nThis allows the original dot product to be re-expressed as a\nsubtraction of two binary dot products, removing the influence\nof the zero-weighted elements:\ny=cX\ni=1wiai=cX\ni=1wD,iai\u2212cX\ni=1wS,iai\nWith this transformation, instead of one ternary LUT with 3c\nentries, we only need two binary LUTs (for wDandwS), each\nof size 2c. The total storage per block becomes 2c+1, which\nperfectly matches the power-of-two width of SIMD registers\nand avoids significant data-path augmentation.\n+1\n\u22121 \n\u22121\n+1\n+1\n\u22121\n0 \n0\n0\nTernary\nWeights\nw\nij\n\u2208\n\u2208\n{-1, 1}\nw\nij\n=0\n+1 \n+1\n+1\n+1\n+1\n+1\nReplace\n 0 to +1\n\u00d7\nActiv-\nations\n0 \n0\n0\n\u2212\n\u00d7\na\n1\na\n2\na\n3\n\u2248\na\n1\na\n2\na\n3\n\u00d7\na\n1\na\n3\na\n2\na\n1 \na\n2\na\n3 \nTernary \nWeights\nT-SAR Algorithm\n+1\n\u22121 \n\u22121\n+1\n+1\n\u22121\nBase Algorithm\nActivations\nSparse Weights\nDense Weights\na\n1\na\n2\na\n3\n0+0+0\nLUTs\n0+0+a\n3\n\u2212a\n1\n\u2212a\n2\n\u2212a\n3\n0, 0, 0 \n0, 0, 1\n2, 2, 2\n3\nc\n=27\nelements\na\n1\na\n2\na\n3\na\n1\n+a\n2\n+a\n3\nD LUTs \n\u2212a\n1\n\u2212a\n2\n\u2212a\n3\n0, 0, 0 \n0, 0, 1\n1, 1, 1\n2\u00d72\nc\n=16\nelem.\n\u2212a\n1\n\u2212a\n2\n+a\n3\na\n1\n+a\n2\n+a\n3\nS LUTs \n+0+0+0 \n+0+0+a\n3\n2\nn\n bit\n2\nn\n bit\n3\nc \nLUT items\n2\nn\n bit\n2\nc+1 \nLUT items\nActivations\nActivations\nT-SAR LUT Compression\nBase LUT Generation\nData-path Mismatch!\nData-path Match!\nSIMD\nRegistersFig. 4: Proposed LUT GEMV Algorithm for LUT compression, matching\nthe LUT size to the data-path.\n0: \u22121\n1: +1\n0: 0\n1: 1\n2\nb\n2\nb\nDense\nWeights\nSparse\nWeights\n\u2212\n+a\n1\n+a\n2\n+a\n2\n+a\n1\n+0\nTernary Weights\nCase:\n    \nc = 2\n,\n \ns = 2\n    \nk = 4\n = \nc\n \n\u00d7\n \ns\n    w = {+1, 0}, a={a\n1\n, a\n2\n}\nOutput = \n\u03a3\nw\ni\n\u22c5\n\u22c5\na\ni\n = +1\n\u22c5\n\u22c5\na\n1\n+0\n\u22c5\n\u22c5\na\n2\n+1\n+1\n0\n1\n00:   0\n01: +1\n11: \u22121\n1=01\n0=00\nTernary\nCoding\nSparse\nCoding\nDense\nCoding\nWeight Encoding Conversion\nLUT Generation\nCompile-Time\nRun-Time\nLUT GEMV\n2\n2\n\u00d72\na\n1\na\n2\na\n3\nAct.\n00: \u2212a\n1\n\u2212a\n2\n01: \u2212a\n1\n+a\n2\n10: +a\n1\n\u2212a\n2\n11: +a\n1\n+a\n2\n00: +0+0\n01: +0+a\n2\n10: +a\n1\n+0\n11: +a\n1\n+a\n2\nTLUT_\nc\n\u00d7\ns\nTGEMV_\nk\n\u00d7m\ns\n of \n2\nc+1\n \nLUTs\nSIMD Reg.\nSIMD Reg.\n\u2212\n\u2212\n+\n(1,\nk\n)\u00d7(\nk\n,m)\nISA\nm\nFig. 5: T-SAR\u2019s LUT-based kernel framework overview.\nB. ISA Extensions for In-Register Execution\nAs shown in Fig. 5, the decomposition mentioned above\nenables a two-phase kernel framework. At compile time,\nternary weights are encoded into dense and sparse binary\nforms. At run time, a minimal set of ISA extensions execute\nthe LUT-based GEMV directly within SIMD registers. These\ninstructions are parameterized by:\n\u2022c: the block size.\n\u2022s: the number of input blocks processed per instruction.\n\u2022k: the number of input channels processed per instruction\n(k=c\u00d7s).\n\u2022m: the number of output channels.\nThe workflow is: (1) the TLUT_c\u00d7s instruction generates\ntwo binary LUTs from activations and places them in SIMD\nregisters; (2) the TGEMV_k\u00d7m uses these register-resident\nLUTs with pre-encoded weights to perform the (1, k)\u00d7(k, m)\nGEMV; and (3) the final ternary result is reconstructed by\n1\n\u2212\n\u2212\n\u2212\n\u2212\n+\n+\n+\na\n2\na\n1\n0\n\u2212\n\u2212\n+\n\u2212\n0\n16-bit\u00d72\nc+1\n=128-bit\nTLUT_\n2\n\u00d7\n4\n, YMM8:9, XMM\n1st Cycle\n2nd Cycle\nYMM8\nYMM9\n2\u00d7\nk\n=16-bit\nint16\u00d716\nT-SAR Cfg.\nc = 2\ns = 4\nk = 8 = \nc\n \n\u00d7\n \ns\nm = 16\n\u2212a\n1\n\u2212a\n2\n\u2212a\n1\n+a\n2\n+a\n1\n\u2212a\n2\n+a\n1\n+a\n2\n+0\n+0\n+0\n+a\n2\n+a\n1\n+0\n+a\n1\n+a\n2\n4\nint8\nint16\nint8\n128\u00d72 = 256-bit\nTGEMV_\n8\n\u00d716, YMM, [YMM8:9], YMM\n128\u00d7\ns \n= 256\u00d72 = 512-bit\n4\n+\nYMM\nMicro-op\nCounter\n4\u00d764-bit = 256-bit YMM\nBase ISA: x86 AVX2\nSIMD_register_bitwidth \n= 256-bit (YMM)\nSIMD_ALU_bitwidth \n=\n \n8-bit, \n16-bit\n, 32-bit, 64-bit\nSIMD_maximum_count \n= 256-bit \u00f7 16-bit = 16\nDotp_maximum_count \n= 256-bit \u00f7 (16-bit \u00d7 2) = 8\nYMM8\nYMM9\nWeights \nAct.\nLUTs\nSIMD\nALUs\nDot\nProd.\nAdder\nTrees\nSIMD_util_ratio\n= 4 \u00d7 4 / 16\nDot\nProd.\nAccum.\nReq\ncycles\n= SIMD_m_c \u00f7 4\n= 4\n(a) ISA Constraints and T-SAR Configuration\n(b) Ternary LUT Instruction\n(c) Ternary GEMV Instruction\n(d) Instruction Encoding\nVEX3 Encoding\nVEX\n3\nOp\ncode\nMod\nR/M\n8'hC4\nVEX3\nPrefix\nVEX[1]\n5'h04\n1\nVEX[2]\n3'h0\nsrc1\nModR/M\nVEX[1]\nVEX[2]\n 8'h00: TLUT_2\u00d74\n 8'h01: TLUT_4\u00d74\n 8'h10: TGEMV_8\u00d716\n 8'h11: TGEMV_16\u00d716\n7\n0\n7\n0\n7\n0\ndst\n2'h3\ndst\n[2:0]\nsrc2\n[2:0]\nsrc1\n[3:0]\nsrc2Fig. 6: T-SAR\u2019s ISA extension applied to x86 A VX2 SIMD ISA , demonstrating how the T-SAR instruction primitives are realized with only minimal hardware\nchanges, utilizing existing SIMD ALUs and adder trees. (a)T-SAR configuration and base ISA constraints. (b)TLUT_c\u00d7s example. (c)TGEMV_k\u00d7m example.\n(d)Instruction encoding details for A VX2, with designed examples of TLUT_c\u00d7s andTGEMV_k\u00d7m instructions.\nsubtraction. This register-resident design aligns computation\nwith the SIMD datapath, eliminating the memory bottleneck.\nC. Microarchitectural Implementation\nFig. 6 demonstrates a practical ISA extension realization on\nthe x86 A VX2 ISA. For the example configuration shown in\nFig. 6(a), we set c=2,s=4,k=8, and m=16 .\nThe TLUT_2\u00d74 instruction (Fig. 6(b)) generates four\nregister-resident LUTs, each with 2c+1= 8 16-bit entries,\noccupying two 256-bit YMM registers [31] ( 4\u00d78\u00d716 = 512\nbits). To minimize hardware changes, the operation is split\ninto two \u00b5-ops, each writing 256 bits per cycle.\nTheTGEMV_8\u00d716 instruction (Fig. 6(c)) performs a (1,8)\u00d7\n(8,16)GEMV , producing 16 outputs. It involves s\u00d7m= 64\nsubtractions and m= 16 s-to-1 adder tree (ADT) operations,\ndistributed over four \u00b5-ops. These instructions reuse the ex-\nisting 256-bit YMM datapath, including all 16 16-bit SIMD\nALUs and 4-to-1 ADTs originally for dot product instructions.\nOnly minor wiring and multiplexer additions are required,\nkeeping area and power overhead minimal.\nThe instruction encoding, detailed in Fig. 6(d), uses standard\nVEX3 fields for both TLUT and TGEMV primitives. For\ninstructions that span multiple registers (e.g., TLUT_2\u00d74\nwriting to YMM8:9 or TGEMV_8\u00d716 reading from YMM8:9),\nthe destination or source register is interpreted as a register\npair: if dst is 0x1000, the operation uses YMM8 and YMM9.\nD. Software Kernel and Dataflow Optimization\nThe performance of GEMM/GEMV operations is highly\ndependent on data reuse patterns, which vary across different\nlayers of an LLM. To maximize throughput, T-SAR employs\nan adaptive kernel scheduling strategy. We implement two\nmicrokernel dataflows\u2014 activation-persistent (AP) and output-\npersistent (OP)\u2014to flexibly match these patterns (Fig. 7).\nThe AP dataflow (Fig. 7(a)) retains input activations in\nregisters across the inner loop, minimizing LUT recomputation\nand increasing input and weight cache hits. This is effective\nfor layers with high activation and weight reuse (i.e., high N\nN\nK\nM\n(1) M\n(2)\nN\n(a) Activation-Persistent Dataflow\nK \u2013 N \u2013 M Iteration\nActivation\n(N, K)\nWeight\n(K, M)\n(3)\nK\nK\nK\nK\nN\nN\n(3)\nK\nN\nK\nM\n(b) Output-Persistent Dataflow\nN \u2013 M \u2013 K Iteration\nActivation\n(N, K)\nWeight\n(K, M)\n(3)\nN\nK\nK\nK\nN\nN\n(1)\nK\n(1)\nK\nN\nM\nN\nN\nN\nM\nN\nN\nOutput\n(N, M)\n(2) M\nfor (k = 0; k < K; k += 8) {\n for (n = 0; n < N; n += 1) {\n  \nlut = TLUT_2x4(act[n*K+k]);\n  for (m = 0; m < M; m += 16) {\n   \noutput[n*m] += TGEMV_8x16(lut, w[M*k+8*m]);\n  }}}\n\u00d7\n=\nfor (n = 0; n < N; n += 1) {\n for (m = 0; m < M; m += 16) {\n  for (k = 0; k < K; k += 8) {\n   \nlut = TLUT_2x4(act[n*K+k]);\n   output[n*m] += TGEMV_8x16(lut, w[K*m+16*k]);\n  }}}\nOutput\n(N, M)\n\u00d7\n=Fig. 7: The T-SAR\u2019s kernel dataflow selections. (a) Activation-persistent\ndataflow minimizing the TLUT_c\u00d7s invocations and increases input/weight\ncache hits. (b)Output-persistent dataflow reducing total memory footprints.\nandK). In contrast, the OP dataflow (Fig. 7(b)) keeps out-\nput accumulators local until computation completes, reducing\nmemory write-back traffic. This is beneficial in layers with a\nhigh number of output channels (high M).\nAt compile-time, T-SAR\u2019s inference framework empirically\nselects the fastest kernel for each layer, ensuring maximum\nperformance across the entire model.\nIV. E XPERIMENTS AND EVALUATION\nWe now evaluate T-SAR across diverse models and plat-\nforms to validate three key claims: (1) T-SAR delivers sig-\nnificant end-to-end speedups for both prefill (GEMM-heavy)\nand decode (GEMV-heavy) phases in autoregressive LLMs;\n(2) these improvements arise from fundamentally reducing the\nmemory bottleneck of SOTA LUT-based methods; and (3) they\nare achieved with minimal hardware overhead, making T-SAR\nhighly efficient even compared to edge GPUs.\nA. Experimental Setup\nISA and Simulator: We extend gem5-AVX 20.1.0.0\n[32], [33] (DerivO3CPU) to model the T-SAR ISA. New\nTABLE I: gem5 simulator configurations for evaluation platforms.\nSystem Type CPU Model Simulation Mode Cores Freq. L1 I/D Cache L2 Cache L3 Cache DRAM\nWorkstation AMD Ryzen 9950X\nDerivO3CPU16 5.7 GHz 32 KB / 48 KB 1 MB/core 64 MB shared DDR5-6400 MHz\nLaptop AMD Ryzen 7840U 8 5.1 GHz 32 KB / 32 KB 1 MB/core 16 MB shared DDR5-4400 MHz\nMobile Intel Processor N250 4 3.8 GHz 64 KB / 32 KB 2 MB shared 6 MB shared DDR5-4400 MHz\nPrefill Latency(N=128)(\u2193 is better)DecodeThroughput(\u2191 is better)T-SARBitnet.cppT-MACSpeedupSecondsTokens/sParameters\n05101520250.1110100100010000100000 125M 700M 2B 7B 30B 100B05101520250.0010.010.11101001000125M700M2B7B30B100BLaptop(8 threads)\nParametersParameters\nSpeedup (\u00d7)\n05101520250.1110100100010000100000 125M 700M 2B 7B 30B 100B05101520250.0010.010.11101001000125M700M2B7B30B100BSpeedup (\u00d7)Mobile(4 threads)\nParametersWorkstation(16 threads)\nParameters\n05101520250.1110100100010000100000 125M 700M 2B 7B 30B 100B05101520250.0010.010.11101001000125M700M2B7B30B100B\nParameters86.2\u00d7Speedup = T-SAR \u00f7best(TL-2, T-MAC)\nFig. 8: End-to-end performance across platforms.\nTLUT_c\u00d7s andTGEMV_k\u00d7m operations are added to the\nA VX2 pipeline with cycle-accurate \u00b5-op sequencing and\nregister-pair reads/writes. Each instruction was verified by\nexecuting hand-written assembly with byte-pattern encodings.\nKernels: We design three kernel variants for\neach LUT-GEMV pair ( TLUT_2 \u00d74+TGEMV_8 \u00d716,\nTLUT_4 \u00d74+TGEMV_16 \u00d716), resulting in six kernels: (1)\nAP-min: activation-persistent with minimal register usage;\n(2) AP-max: activation-persistent with maximal register use\nto reduce iterations; (3) OP: output-persistent for minimal\nwrite-back traffic. All are implemented in C++ with inline\nassembly and compiled with GCC 9.4.0.\nBaselines: We compare against two SOTA LUT-based\nbaselines: Bitnet.cpp TL-2 [20] and T-MAC [19]. To ensure\nfairness, our kernels include both input quantization and output\ndequantization stages (shown in Fig. 2(b)).\nModels and Protocol: We evaluate BitNet models from\n125M to 100B parameters [34]. Prefill runs with N= 128\ntokens (batch=1) to build the KV cache; decode measures\nsteady-state throughput using the KV cache. Thread counts\nare fixed at {16, 8, 4 }for{Workstation, Laptop, Mobile }.\nMetrics: We report prefill latency, decode throughput, ker-\nnel execution time, and kernel memory access requests.\nPlatforms: Three representative x86 CPU classes are mod-\neled (Table I): Workstation (Ryzen 9950X, 16 cores), Laptop\n(Ryzen 7840U, 8 cores), Mobile (Intel N250, 4 cores).\nB. End-to-End Results\nPrefill (GEMM-heavy): As shown in Fig. 8 (top), T-SAR\ndelivers geo-mean prefill speedups of 8.8\u00d7 (Workstation), 8.4\u00d7\n(Laptop), and 12.4\u00d7 (Mobile) across all models (125M\u2013100B).\nSince GEMM is compute-bound, register-resident LUT gen-\neration and fused accumulation let efficiency gains translate\nalmost directly into throughput until cache/memory contention\nemerges. This explains why prefill benefits exceed decode\ngains. For example, Mobile\u2019s 7B prefill drops from >20 s to\nunder 1.7 s\u2014enabling interactive LLM use on devices where\nGPUs cannot be deployed.\n0.06 0.08 0.00 0.39 0.41 0.16 7.45 7.89 1.43 0.66 0.66 0.19 4.53 4.66 1.74 101.59 109.18 17.36 11.3\u00d78.7\u00d710.6\u00d711.5\u00d711.4\u00d710.9\u00d713.6\u00d713.8\u00d712.1\u00d702468101214\n0.010.101.0010.00100.00\n768\u00d730723072\u00d7768768\u00d77682560\u00d769126912\u00d725602560\u00d725608912\u00d74556845568\u00d789128912\u00d78192MB\nReduction (\u00d7)(a) GEMM (N=128, Prefill) Memory Requests\n(b) GEMV (N=1, Decode) Memory Requests6.7 8.3 2.2 49.0 50.2 18.7 949.3 1,010.8 182.3 62.6 67.0 17.5 446.3 445.6 168.1 9,309.0 10,446.5 1,704.3 9.3\u00d78.1\u00d78.1\u00d79.1\u00d78.9\u00d79.0\u00d79.8\u00d710.3\u00d79.3\u00d702468101101001,00010,000\n768\u00d730723072\u00d7768768\u00d77682560\u00d769126912\u00d725602560\u00d725608912\u00d74556845568\u00d789128912\u00d78192Bitnet-b1.58-125MBitnet-b1.58-2B-4TBitnet-b1.58-100BMB\nReduction (\u00d7)\nT-SARBitnet.cppSpeedupMB\nReduction (\u00d7)Fig. 9: Memory request volume (MB) of the kernels executed in the\nrepresentative BitNet model inference (125M, 2B-4T, 100B) . (a) GEMM\n(N=128) prefill. (b) GEMV (N=1) decode.\nDecode (GEMV-heavy): Fig. 8 (bottom) shows that base-\nline GEMV is dominated by repeated TLUT fetches. T-\nSAR removes this traffic entirely, exposing SIMD compute\nthroughput. Relative gains peak on Workstation (6.4\u00d7), due to\nlarger caches delaying memory-system bandwidth saturation;\nLaptop and Mobile reach 4.1\u20134.2\u00d7. Mobile\u2019s smaller gain re-\nflects earlier bandwidth saturation despite the request-volume\nreduction.\nLink to bottlenecks: The prefill/decode gap mirrors\nGEMM\u2019s compute-bound and GEMV\u2019s bandwidth-bound na-\nture\u2014setting up the trends seen in memory-system analysis.\nC. Memory-System Impact\nThe central claim of T-SAR is that it removes the memory\nbottleneck by generating LUTs directly in registers. As shown\nin Fig. 9, this reduces memory request volume (MB) by\n8.7\u201313.8\u00d7 compared to TL-2 [20], with GEMV showing larger\nrelative cuts because the baseline is TLUT-dominated and\nTLUT fetches are eliminated. For GEMM, TL-2\u2019s denser\nweight packing (1.67 bits/weight) limits relative reductions,\nbut the resulting stall decreases still improve ALU utilization.\nRequest reduction grows with model size ( K\u00d7M) as more\nLUT calls are avoided, but latency gains diverge: (1) GEMV\ncase- Large cuts yield smaller returns as lower Last-level\ncache (LLC) hit rate reduces effective bandwidth and forces\nearly saturation\u2014most evident in Mobile 1 \u00d78192\u00d745568:\n89%\u219262%\u2014capping latency drops. (2) GEMM case- Even\nmodest cuts yield large drops since compute-bound phases\nconvert freed cycles and higher locality into utilization; LLC\nhit rate stays high (89% \u219291%) until contention. These effects\npredict the thread-scaling behavior shown in Fig. 10.\nD. Kernel Microbenchmarks and Thread Scaling\nFrom Fig. 10, we make the following observations:\nGEMM case: For large shapes (128\u00d72560\u00d76912,\n128\u00d76912\u00d72560), T-SAR sustains scaling up to 8\u201316 threads\n(Workstation) and 4\u20138 (Laptop) before L3/DRAM contention\ndominates. Scaling is not perfectly linear, but flattens later\n(a) GEMM (Prefill) Kernel Execution Times(b) GEMV (Decode) Kernel Execution Times220200110100550500\n110100220200550500\n0.55500.5550110100Workstation (9950X)\nThreadsmsmsms1248161248124ThreadsThreadsLaptop (7840U)Mobile (N250)128\u00d72560\u00d76912128\u00d76912\u00d72560Kernel Cfg.N\u00d7K\u00d7M128\u00d72560\u00d72560T-SARBitnet.cpp\n14.4\u00d715.4\u00d78.7\u00d7\n9.4\u00d711.4\u00d75.7\u00d7\n8.7\u00d79.6\u00d75.5\u00d7\n0.0070.070.70.0070.070.70.0070.070.70.11100.010.110.11100.020.220.010.11\n220200Workstation (9950X)\nThreadsmsmsms1248161248124ThreadsThreadsLaptop (7840U)Mobile (N250)1\u00d72560\u00d769121\u00d76912\u00d72560Kernel Cfg.N\u00d7K\u00d7M1\u00d72560\u00d7256013.3\u00d75.3\u00d73.3\u00d7\n7.7\u00d77.2\u00d74.4\u00d73.4\u00d76.2\u00d76.9\u00d73.3\u00d79.8\u00d77.1\u00d76.3\u00d710.3\u00d711.5\u00d75.3\u00d79.5\u00d77.9\u00d75.3\u00d710.6\u00d75.8\u00d75.7\u00d710.2\u00d75.8\u00d79.0\u00d710.5\u00d75.3\u00d7T-SARBitnet.cppFig. 10: Multi-thread scaling for BitNet-b1.58-2B-4T. T-SAR vs. TL-2 for GEMM (left) and GEMV (right). Solid lines denote absolute latency (log\nscale). Arrows show relative speedup.\nthan GEMV due to the compute-bound nature and higher\ndata locality, allowing freed cycles from reduced stalls to\nbe fully exploited. This yields up to 13\u00d7speedup at 4\nthreads\u2014matching the large prefill gains in Fig. 8.\nGEMV case: Despite the largest proportional memory\nsavings, GEMV saturates effective bandwidth quickly\u2014often\nby 2\u20134 threads on Mobile and 4\u20138 on Workstation/Lap-\ntop\u2014leading to early plateaus and smaller decode-time im-\nprovements. Here, extra cores cannot offset bandwidth limits.\nHence, we see that compute-bound kernels (GEMM)\nsustain scaling and achieve larger absolute gains across\nplatforms, while memory-bound kernels (GEMV) plateau\nquickly\u2014highlighting T-SAR\u2019s impact and limits for real-\nworld deployment for edge platforms.1\nE. Hardware Overheads\nTo size the cost of T-SAR, we synthesized a 256-bit SIMD\nunit (vector add/mul/dot-product, write-back interface) at\n1 GHz in TSMC 28 nm using Cadence Genus 21.10, both with-\noutandwith the T-SAR logic (shown in Table II). Multi-mode\nmulti-corner (MMMC) synthesis covered ssg\u2194ffg,VDD\u2208\n[0.81,1.05]V ,T\u2208[0,125]\u25e6C; Area and power are reported\nattt0p9v25c . The T-SAR instructions reuse existing ALU\nlanes and register file\u2014no new arithmetic units or scratchpads.\nAdditions are: (i) a 256-bit vector write-back MUX to inject\nTLUT words into the register file, (ii) small operand-bus wires\nand input MUXes for TLUT/TGEMV (no extra read ports),\nand (iii) a tiny control/scoreboard block to sequence TLUT\nwrites and fused accumulation.\nResult: As shown in Table II, the area increases by +1.4%\n(73,560 \u219274,590 \u00b5m2) and active power consumption under\nkernel-like switching rises by +3.2% (5,904\u21926,090 mW),\ndominated by toggling on the new MUX paths. Results\ncorrespond to a single 256-bit SIMD slice (no SRAM arrays)\n1Note: We report prefill with N=128 due to simulation cost. TL-2\u2019s weight\npacking (1.67-bit) is denser than our 1+1-bit split, resulting in approximately\n20% more static memory occupation, but end-to-end is dominated by TLUT\ntraffic rather than weight RAM size, hence T-SAR\u2019s advantage. We demon-\nstrate our framework to x86 due to the broad applicability, but retargeting to\nNEON or RISC-V Vector (RVV) [29] only requires c, s, k, m tuning due to\nthe different SIMD lane width but extant dot product extensions. For instance,\nexisting ARM NEON\u2019s 128-bit datapath with SDOT/UDOT instruction\nsupport (since ARMv8.2-A [35]) realizes the TLUT_2\u00d74 +TGEMV_8\u00d78 .TABLE II: Synthesis of a 256-bit SIMD slice (TSMC 28 nm, 1 GHz) with\nand without T-SAR ISA.\nBlockArea ( \u00b5m2) Power (mW)\nBase T-SAR \u2206 Base T-SAR \u2206\nSIMD ALUs + write-back interface 73,560 73,560 0.0% 5,904 5,904 0.0%\nT-SAR \u2192write-back MUX 0 588 +0.8% 0 41 +0.7%\nOperand-bus wires and input MUX 0 147 +0.2% 0 24 +0.4%\nOthers (control/scoreboard, decode) 0 295 +0.4% 0 121 +2.0%\nTotal 73,560 74,590 +1.4% 5,904 6,090 +3.2%\nand exclude caches and register files; absolute area will scale\nwith the number of slices and integration.\nF . Cross-Platform Comparison\nTo further evaluate our CPU-based solution, we compare\nit directly against an edge GPU in the NVIDIA Jetson AGX\nOrin [36] SoC, using identical model checkpoints and runtime\nsettings (batch=1; steady-state decode). For CPUs, T-SAR\npower is estimated from CPU package power under TL-2 de-\ncode via the measured dynamic overhead in Table. II: PT-SAR =\n1.032\u00b7PTL-2. Energy per token is E=PT-SAR/(tokens/s )from\nthe measured T-SAR throughput from gem5 simulator.\nTABLE III: Cross-platform decode throughput and energy/token (batch=1).\nPower boundary: CPU package; GPU module.\nLlama-b1.58-8B Falcon3-b1.58-10B\nPlatform tokens / s J / token tokens / s J / token\nWorkstation CPU (9950X, 4nm, T-SAR) 128.96 0.616 103.93 0.795\nLaptop CPU (7840U, 4nm, T-SAR) 61.00 0.405 49.65 0.540\nMobile CPU (N250, 10nm, T-SAR) 5.18 0.733 4.30 0.953\nJetson AGX Orin GPU (8nm, llama.cpp) 16.78 1.839 13.25 2.620\nTakeaways. With matched per-model checkpoints and set-\ntings, T-SAR on CPUs outperforms Jetson on Workstation\nandLaptop across both families: Llama-b1.58-8B shows 7.7\u00d7\n/ 3.0\u00d7 (tokens / s / lower J / token) on workstation and 3.6\u00d7 /\n4.5\u00d7 on laptop; Falcon3-b1.58-10B shows 7.8\u00d7 / 3.3\u00d7 and\n3.7\u00d7 / 4.9\u00d7, respectively. On Mobile , throughput is lower\nthan Jetson (0.31\u20130.32\u00d7), yet energy/token remains 2.5\u20132.75\u00d7\nlower, consistent with our decode memory-bandwidth analysis.\nV. C ONCLUSION\nWe presented T-SAR, a full-stack co-design framework\nfor CPU-only ternary LLM inference. The key idea is to\nmove LUT generation from memory into SIMD registers,\nturning TLUT fetches into in-register compute and fusing\naccumulation so BitLinear layers shift from bandwidth-bound\nto datapath-bound execution. This yields portable speedups\nacross CPUs with no new ALUs and only minor mux/control\nlogic, supported by AP/OP kernels that adapt per layer to each\nmodel and platform. Beyond the reported gains, the approach\ngeneralizes to RVV and NEON [29], invites advanced LUT\ncompression and compiler scheduling, and integrates naturally\nwith sparsity or further quantization. In short, a small ISA\nextension realigns the bottleneck and enables interactive LLM\ninference on CPUs\u2014even mobile ones\u2014while keeping hard-\nware changes minimal.\nREFERENCES\n[1] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu,\nRichard Socher, Xavier Amatriain, and Jianfeng Gao. Large language\nmodels: A survey. arXiv preprint arXiv:2402.06196 , 2024.\n[2] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Alma-\nhairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal\nBhargava, Shruti Bhosale, et al. Llama 2: Open Foundation and Fine-\nTuned Chat Models. arXiv preprint arXiv:2307.09288 , 2023.\n[3] Yue Zheng, Yuhao Chen, Bin Qian, Xiufang Shi, Yuanchao Shu, and\nJiming Chen. A review on edge large language models: Design,\nexecution, and applications. ACM Comput. Surv. , 57(8), March 2025.\n[4] Kiarash Ahi. Risks & Benefits of LLMs & GenAI for Platform\nIntegrity, Healthcare Diagnostics, Cybersecurity, Privacy & AI Safety:\nA Comprehensive Survey, Roadmap & Implementation Blueprint. arXiv\npreprint arXiv:2506.12088 , 2025.\n[5] Xinyin Ma, Gongfan Fang, and Xinchao Wang. LLM-Pruner: On the\nStructural Pruning of Large Language Models. In Adv. Neural Inf.\nProcess. Syst. 36 (NeurIPS) , pages 21702\u201321720, New Orleans, LA,\nUSA, 2023.\n[6] Qichen Fu, Minsik Cho, Thomas Merth, Sachin Mehta, Mohammad\nRastegari, and Mahyar Najibi. LazyLLM: Dynamic Token Prun-\ning for Efficient Long Context LLM Inference. arXiv preprint\narXiv:2407.14057 , 2024.\n[7] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-\nChen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song\nHan. AWQ: Activation-aware weight quantization for on-device LLM\ncompression and acceleration. In Annu. Conf. Mach. Learn. Syst. 6\n(MLSys) , volume 6, pages 87\u2013100, Santa Clara, CA, USA, 2024.\n[8] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.\nGPT3.int8(): 8-Bit Matrix Multiplication for Transformers at Scale. In\nAdv. Neural Inf. Process. Syst. 35 (NeurIPS) , pages 30318\u201330332, New\nOrleans, LA, USA, 2022.\n[9] Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu,\nZhiyuan Liu, Weidong Liu, and Wanxiang Che. Onebit: Towards ex-\ntremely low-bit large language models. In Adv. Neural Inf. Process. Syst.\n37 (NeurIPS) , pages 66357\u201366382, Vancouver, BC, Canada, December\n2024.\n[10] Pedro Palacios, Rafael Medina, Giovanni Ansaloni, and David Atienza.\nHEEPstor: an Open-Hardware Co-design Framework for Quantized\nMachine Learning at the Edge. In 22nd ACM Int. Conf. Comput. Front.:\nWorkshops Spec. Sess. (CF) , page 22\u201325, Cagliari, Italy, 2025.\n[11] Achintya Kundu, Fabian Lim, Aaron Chew, Laura Wynter, Penny Chong,\nand Rhui Dih Lee. Efficiently Distilling LLMs for Edge Applications.\narXiv preprint arXiv:2404.01353 , 2024.\n[12] Yiming Ma, Chaoyao Shen, Linfeng Jiang, Tao Xu, and Meng Zhang.\nTKD: An Efficient Deep Learning Compiler with Cross-Device Knowl-\nedge Distillation. In Des. Autom. Test Eur. (DATE) , pages 1\u20137, Lyon,\nFrance, 2025.\n[13] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin, Xin Jiang, Qun\nLiu, Michael Lyu, and Irwin King. BinaryBERT: Pushing the Limit\nof BERT Quantization. In Jt. Conf. 59th Annu. Meet. Assoc. Comput.\nLinguist. 11th Int. Jt. Conf. Nat. Lang. Process. (ACL-IJCNLP) , pages\n4334\u20134348, Virtual, August 2021.\n[14] Haotong Qin, Yifu Ding, Mingyuan Zhang, Qinghua YAN, Aishan Liu,\nQingqing Dang, Ziwei Liu, and Xianglong Liu. BiBERT: Accurate Fully\nBinarized BERT. In Int. Conf. Learn. Represent. (ICLR) , Virtual, April\n2022.[15] Zhihang Yuan, Yuzhang Shang, and Zhen Dong. PB-LLM: Partially\nbinarized large language models. In Int. Conf. Learn. Represent. (ICLR) ,\nVienna, Austria, May 2024.\n[16] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang,\nLingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. BitNet:\nScaling 1-bit Transformers for Large Language Models. arXiv preprint\narXiv:2310.11453 , 2023.\n[17] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang,\nShaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, and Furu Wei.\nThe Era of 1-Bit LLMs: All Large Language Models Are in 1.58 Bits.\narXiv preprint arXiv:2402.17764 , 2024.\n[18] Ayush Kaushal, Tejas Vaidhya, Arnab Kumar Mondal, Tejas Pandey,\nAaryan Bhagat, and Irina Rish. Surprising Effectiveness of Pretraining\nTernary Language Model at Scale. In Int. Conf. Learn. Represent.\n(ICLR) , pages 1\u201348, 2025.\n[19] Jianyu Wei, Shijie Cao, Ting Cao, Lingxiao Ma, Lei Wang, Yanyong\nZhang, and Mao Yang. T-MAC: CPU Renaissance via Table Lookup for\nLow-Bit LLM Deployment on Edge. In 21th European Conference on\nComputer Systems (EuroSys) , pages 278\u2013292, Rotterdam, Netherlands,\nMarch 2025.\n[20] Jinheng Wang, Hansong Zhou, Ting Song, Shijie Cao, Yan Xia, Ting\nCao, Jianyu Wei, Shuming Ma, Hongyu Wang, and Furu Wei. Bit-\nnet.cpp: Efficient Edge Inference for Ternary LLMs. arXiv preprint\narXiv.2502.11880 , 2025.\n[21] Diana Vut \u00b8 \u02d8a-Popescu, Ionut \u00b8 C \u02d8at\u02d8alin Antofi, C \u02d8at\u02d8alin Bogdan Ciobanu, and\nCsaba Zolt \u00b4an Kert \u00b4esz. Simd extensions-a historical perspective. In\n2024 IEEE 30th International Symposium for Design and Technology\nin Electronic Packaging (SIITME) , pages 108\u2013115. IEEE, 2024.\n[22] Chenyang Yin, Zhenyu Bai, Pranav Venkatram, Shivam Aggarwal,\nZhaoying Li, and Tulika Mitra. TerEffic: Highly Efficient Ternary LLM\nInference on FPGA. arXiv preprint arXiv:2502.16473 , 2025.\n[23] Ye Qiao, Zhiheng Chen, Yifan Zhang, Yian Wang, and Sitao Huang.\nTeLLMe: An Energy-Efficient Ternary LLM Accelerator for Prefilling\nand Decoding on Edge FPGAs. arXiv preprint arXiv:2504.16266 , 2025.\n[24] Ruiqi Chen, Jiayu Liu, Shidi Tang, Yang Liu, Yanxiang Zhu, Ming Ling,\nand Bruno Da Silva. ATE-GCN: An FPGA-Based Graph Convolutional\nNetwork Accelerator with Asymmetrical Ternary Quantization. In Des.\nAutom. Test Eur. (DATE) , pages 1\u20136, Lyon, France, 2025.\n[25] Seonjin Na, Geonhwa Jeong, Byung Hoon Ahn, Aaron Jezghani, Jeffrey\nYoung, Christopher J. Hughes, Tushar Krishna, and Hyesoon Kim.\nFlexInfer: Flexible LLM Inference with CPU Computations. In Annu.\nConf. Mach. Learn. Syst. 7 (MLSys) , 2025.\n[26] Giorgos Armeniakos, Alexis Maras, Sotirios Xydis, and Dimitrios\nSoudris. Mixed-Precision Neural Networks on RISC-V Cores: ISA\nExtensions for Multi-Pumped Soft SIMD Operations. In IEEE/ACM\nInt. Conf. Comput. Aided Des. (ICCAD) , pages 1\u20139, Newark, NJ, USA,\nOctober 2024.\n[27] Angelo Garofalo, Giuseppe Tagliavini, Francesco Conti, Davide Rossi,\nand Luca Benini. XpulpNN: Accelerating Quantized Neural Networks\non RISC-V Processors Through ISA Extensions. In Des. Autom. Test\nEur. (DATE) , pages 186\u2013191, Virtual, March 2020.\n[28] Navaneeth Kunhi Purayil, Matteo Perotti, Tim Fischer, and Luca Benini.\nAraXL: A Physically Scalable, Ultra-Wide RISC-V Vector Processor\nDesign for Fast and Efficient Computation on Long Vectors. In Des.\nAutom. Test Eur. (DATE) , pages 1\u20137, Lyon, France, 2025.\n[29] Ju-Hung Li, Jhih-Kuan Lin, Yung-Cheng Su, Chi-Wei Chu, Lai-Tak\nKuok, Hung-Ming Lai, Chao-Lin Lee, and Jenq-Kuen Lee. SIMD Ev-\nerywhere Optimization from ARM NEON to RISC-V Vector Extensions.\narXiv preprint arXiv:2309.16509 , 2023.\n[30] Falcon-LLM Team. The Falcon 3 Family of Open Models, December\n2024. Available: https://huggingface.co/blog/falcon3.\n[31] Chris Lomont. Introduction to intel advanced vector extensions. Intel\nwhite paper , 23(23):1\u201321, 2011.\n[32] Nathan Binkert, Bradford Beckmann, Gabriel Black, Steven K. Rein-\nhardt, Ali Saidi, Arkaprava Basu, Joel Hestness, Derek R. Hower, Tushar\nKrishna, Somayeh Sardashti, Rathijit Sen, Korey Sewell, Muhammad\nShoaib, Nilay Vaish, Mark D. Hill, and David A. Wood. The gem5\nSimulator. ACM SIGARCH Comput. Archit. News , 39(2):1\u20137, August\n2011.\n[33] Seungmin Lee, Youngsok Kim, Dukyun Nam, and Jong Kim. Gem5-\nA VX: Extension of the Gem5 Simulator to Support A VX Instruction\nSets. IEEE Access , 12:20767\u201320778, 2024.\n[34] Jinheng Wang, Hansong Zhou, Ting Song, Shaoguang Mao, Shuming\nMa, Hongyu Wang, Yan Xia, and Furu Wei. 1-bit AI Infra: Part 1.1,\nFast and Lossless BitNet b1.58 Inference on CPUs. arXiv preprint\narXiv.2410.16144 , 2024.\n[35] Arm Ltd. A64 SIMD Vector Instructions. https://developer.arm.com/\ndocumentation/100069/0609/A64-SIMD-Vector-Instructions. Accessed:\nMarch 26, 2025.\n[36] Mark Barnell, Courtney Raymond, Steven Smiley, Darrek Isereau, and\nDaniel Brown. Ultra Low-Power Deep Learning Applications at the\nEdge with Jetson Orin AGX Hardware. In IEEE High Perform. Extreme\nComput. Conf. (HPEC) , pages 1\u20134, 2022.\n",
    "title": "T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization",
    "authors": [
      "Hyunwoo Oh",
      "KyungIn Nam",
      "Rajat Bhattacharjya",
      "Hanning Chen",
      "Tamoghno Das",
      "Sanggeon Yun",
      "Suyeon Jang",
      "Andrew Ding",
      "Nikil Dutt",
      "Mohsen Imani"
    ],
    "published": "2025-11-17",
    "arxiv_id": "2511.13676v1",
    "num_pages": 8,
    "num_chars": 41844
  },
  {
    "text": "Scientific Data Compression and Super-Resolution Sampling\nMinh Vu Andrey Lokhov\nTheoretical Division\nLos Alamos National Laboratory\nLos Alamos, NM 87545Theoretical Division\nLos Alamos National Laboratory\nLos Alamos, NM 87545\nAbstract\nModern scientific simulations, observations,\nand large-scale experiments generate data at\nvolumes that often exceed the limits of stor-\nage, processing, and analysis. This chal-\nlenge drives the development of data reduc-\ntion methods that efficiently manage mas-\nsive datasets while preserving essential phys-\nical features and quantities of interest. In\nmany scientific workflows, it is also crucial\nto enable data recovery from compressed\nrepresentations\u2014a task known as super-\nresolution\u2014with guarantees on the preserva-\ntion of key physical characteristics. A no-\ntable example is checkpointing and restart-\ning, which is essential for long-running sim-\nulations to recover from failures, resume af-\nter interruptions, or examine intermediate re-\nsults. In this work, we introduce a novel\nframework for scientific data compression\nand super-resolution, grounded in recent ad-\nvances in learning exponential families. Our\nmethod preserves and quantifies uncertainty\nin physical quantities of interest and supports\nflexible trade-offs between compression ratio\nand reconstruction fidelity.\n1 Introduction\nThe accelerating pace of scientific discovery has led\nto a deluge of data from simulations, experiments,\nand observational platforms. In domains such as\nclimate modeling, high-energy and nuclear physics,\nastrophysics, fluid dynamics, and materials science,\nmodern workflows routinely produce petabytes of data\n(Peterka et al., 2019). However, these datasets of-\nten far exceed available resources for storage, trans-\nmission, and downstream analysis. One of the no-\ntable differences between scientific and conventional\nimage data lies in the known physical context: scien-tific images must preserve physically meaningful fea-\ntures (such as correlations or energy), while the qual-\nity of conventional images is prioritized through visual\nquality and human perception. The challenge of data\nreduction\u2014efficiently compressing scientific data while\npreserving essential physical features\u2014has emerged as\na central problem across the computational science\ncommunity. Development of rigorous data-reduction\ntechniques that learn key physical characteristics and\npossess data-reconstruction ability with quantified un-\ncertainties was highlighted as one of the grand research\npriorities by the U.S. Department of Energy\u2019s Office of\nScience workshops (Klasky et al., 2021).\nThis ability of recovering high-quality data data from\ncompressed representations while is critical for solving\ndown-stream inference and computational tasks that\nrequire processing of full data samples. One canoni-\ncal use case of relevance to many scientific fields such\nas radiation transport, climate modeling, and plasma\nphysics (Bowers et al., 2004; Hanssen, 2001; Chen\net al., 2021; Matsekh et al., 2020) is checkpointing\nand restarting in high-performance computing simu-\nlations. This mechanism enables recovery from fail-\nures by periodically saving a comprehensive snapshot\nof the simulation states (Plank et al., 1994), but stor-\nage of massive datasets produced at extreme scales\npresents significant input-output challenges (Sancho\net al., 2004; Ferreira et al., 2014). Another example\nis Bayesian inference in inverse problems or denois-\ning tasks (Gondara, 2016; Durieux et al., 2020; Hul-\nbert et al., 2019), where full-scale samples recovered\nfrom reduced data are needed to support statistically\nvalid inference. In both cases, recovering data that\npreserves domain-relevant physical quantities of inter-\nest (QoIs) is crucial. Lossless compression techniques\nnaturally solve the problem by preserving full infor-\nmation, but typically yield modest compression ratios\nthat are insufficient for cutting-edge applications (Son\net al., 2014). In contrast, lossy compression meth-\nods (Ferreira et al., 2014; Tiwari et al., 2014; Garg\net al., 2018; Calhoun et al., 2019) have shown promise\nin achieving orders of magnitude higher compressionarXiv:2511.13675v1  [cs.LG]  17 Nov 2025\nScientific Data Compression and Super-Resolution Sampling\nratios for a number of scientific applications (including\nclimate, chemistry, combustion) (Baker et al., 2016;\nRoe and Brooks, 2022; Ding et al., 2025), but they typ-\nically offer no guarantees that QoIs\u2014such as conserved\nquantities or physical constraints\u2014are preserved in re-\nconstruction. In the field of computer vision, the con-\ncept of reconstruction of high-resolution images from\ncompressed low-resolution ones is known assuper-\nresolution(Freeman et al., 2002). However, optimal\nalgorithms for compression and recovery of scientific\ndata obeying physical constraints have received far\nless attention and are currently lacking. While in\nthe field of computer vision various perceptual qual-\nity metrics exist in order to evaluate the effectiveness\nof image super-resolution, in scientific applications we\nare specifically concerned with physical laws and con-\nstraints which our samples obey and our models must\nlearn (Karniadakis et al., 2021).\nThis challenge motivates a key question: How can we\nperform data reduction in a way that preserves the\nability to reconstruct data from compressed represen-\ntations while statistically conserving physical quanti-\nties of interest? In order to answer this question, we\nput forward the concept of aphysics-informed super-\nresolution sampling. Our approach build on recent ad-\nvances in learning of exponential families, where the\nsufficient statistics are chosen to align with the QoIs\nthat we wish to preserve. Recent rigorous analysis\nof algorithms such as Interaction Screening for dis-\ncrete data and Score Matching for continuous data\nshow that these models can be efficiently learned from\nscientific data with quantified uncertainty. Concep-\ntually, this approach offers an optimal compression\nscheme, where data is fully reduced to learned parame-\nters of maximum-entropy distributions. Samples from\nthis model are guaranteed to preserve chosen phys-\nical properties encoded as sufficient-statistics of the\nmodel. The down-side of this representation is that\nit is decoupled from a sampling procedure: non-trivial\nenergy-based models in high dimensions are difficult\nto sample from, especially using Markov Chain Monte\nCarlo (MCMC) methods that may exhibit exponen-\ntially large mixing times.\nTo facilitate efficient reconstruction, instead of dis-\ncarding data samples after the learning procedure,\nwe separately store a compressed version of the orig-\ninal data using a lossy compression method. When\nneeded, this compressed data is decoded to initialize\nsampling procedures by warm-starting MCMC chains\nbased on learned model, enabling correction of the\ndata distribution and conservation of QoIs. This\nworkflow greatly reduces the computational overhead\nwhile maintaining the statistical fidelity of the recon-\nstructed data. This simple yet effective strategy lever-\noriginal raw data \n+ features to preserve \ud835\udc44(\u0526\ud835\udc65)super -resolved samples\nwith statistically preserved \ud835\udc44(\u0526\ud835\udc65)compressed stored data + learned model \ud835\udc5d\u0526\ud835\udc65\u221dexp\ud835\udf03,\ud835\udc44(\u0526\ud835\udc65)\nlearning\nexp familylossy\ncompression\ncorrection \nwith a few \nMCMC stepslossy \ndecompression\n(a) Data compression step (b) Super -resolution sampling stepFigure 1:A schematic representation of our proposed ap-\nproach. (a) The compression step involves learning a com-\npact representation of the data distribution by learning a\nmodel in the exponential family with conserved quantities\nof interestQ(\u20d7 x), where the desired QoIs are used as suffi-\ncient statistics. We separately store a compressed version\nof the original data using lossy compression. (b) When\nneeded, this compressed data is decoded to initialize sam-\npling procedures from the learned model, enabling efficient\nlocal correction of the data distribution and leading to a\ncorrect statistics of QoIsQ(\u20d7 x).\nages the fact that training samples already lie near\nthe learned distribution and can dramatically reduce\nsampling costs, while maintaining statistical fidelity of\nthe recovered data. Our approach is summarized in\nFigure 1. Below, we study this framework for both\ndiscrete and continuous data in controlled settings,\nand apply it to representative scientific machine learn-\ning use cases: samples from an analog quantum com-\nputer and aluminum stress-test data from high-fidelity\nmolecular dynamics codes. We empirically demon-\nstrate that only a few correction steps with MCMC\nare sufficient to recover statistically accurate recon-\nstructions that preserve physical QoIs. This approach\nbridges the gap between principled statistical model-\ning and practical requirements in scientific computing,\noffering a scalable, QoI-aware method for data reduc-\ntion and reconstruction in data-intensive science.\n2 Methods\nIn this section, we describe all the elements of our\napproach in detail: concept of exponential families\nand sufficient statistics; learning methods; methods\nfor compression and decompression; and, finally, our\nrationale for the proposed super-resolution sampling.\n2.1 Exponential Families and Sufficient\nStatistics\nWith the goal of learning data distributions enabling\nsubsequent super-resolution preserving physical con-\nstraintsQ(\u20d7 x), we focus on theexponential familydis-\ntributions (Wainwright et al., 2008), which can repre-\nMinh Vu, Andrey Lokhov\nsent any positive probability measures:\nP(\u20d7 x) =1\nZexp(E(\u20d7 x)),(1)\nwhere\u20d7 xis a collection of random variables,E(\u20d7 x) is\nusually known as theenergy functionof the model,\nandZis the normalization factor called theparti-\ntion function. Given asufficient statisticof interest\nQ(\u20d7 x), the most general exponential family distribution\ninformative ofQ(\u20d7 x) is given by the energy function\nparametrized by a vector of parameters \u20d7\u03b8:\nE(\u20d7 x) = \u20d7\u03b8\u00b7Q(\u20d7 x).(2)\nFor example, if the sufficient statistic of interestQ(\u20d7 x)\nrepresents the first and second moments of continuous-\nvalued data (single and two-point correlation func-\ntions\u27e8x i\u27e9and\u27e8x ixj\u27e9), then the exponential family is\ngiven by a multivariate Gaussian distribution, where\n\u20d7\u03b8= (\u20d7 \u00b5,\u03a3) is composed of the means\u20d7 \u00b5and the covari-\nance \u03a3. For the binary data\u03c3 i\u2208 {0,1}, the maxi-\nmum entropy general model of the first and the sec-\nond moment of the data (magnetizations\u27e8\u03c3 i\u27e9and pair-\ncorrelation functions\u27e8\u03c3 i\u03c3j\u27e9) is given by the celebrated\nIsing model of statistical physics, with the energy func-\ntionE Ising(\u20d7 \u03c3) =P\nihi\u03c3i+P\ni<jJij\u03c3i\u03c3j, where the\nexponential family\u2019s natural parameters \u20d7\u03b8= ( \u20d7h,\u20d7J)\nare given by the local fields and the matrix of pair-\nwise couplings. To maintain consistency, we will use\nxfor continuous variables and\u03c3for discrete variables\nthroughout the paper.\nThe multivariate probability distributions defined by\nthe sufficient statisticQ(\u20d7 x) are often referred to as\nMarkov random fields (Moussouris, 1974), Gibbs dis-\ntributions (Mezard and Montanari, 2009), or undi-\nrectedgraphical models, where the graph structure\nencodes the structure of conditional dependencies of\nE(\u20d7 x) (Wainwright et al., 2008). The task of recon-\nstructing the parameters \u20d7\u03b8of the probability distribu-\ntion from data samples{\u20d7 x(k)}k=1,...,M is commonly re-\nferred to aslearningof graphical models. This type of\nreconstruction of the parameters \u20d7\u03b8from data presents\nan advantage compared to the black-box models, be-\ncause it gives an explicit form of the data distribu-\ntionP(\u20d7 x) that can be used in subsequent inference\ntasks. Moreover, the learned vector of parameters \u20d7\u03b8\nprovides a compact representation of the data: forN-\ndimensional data\u20d7 x\u2208RN, if we are only interested in\nmoments of order at most two, such as the correlation\nfunctionx ixj, onlyO(N2) parameters are required for\na full characterization of the distribution. The down-\nside of this representation is that it is decoupled from\na sampling procedure: non-trivial functionsE(\u20d7 x) are\nknown to be difficult to sample.\nIn this paper, we propose to learn energy functionscontaining the QoIs as sufficient statistics, which will\nresult in a maximum entropy distribution which con-\nserve of these quantities of interest. In what follows,\nwe discuss our approach to learning exponential family\ndistributions from data, and our solution for circum-\nventing the sampling issue.\n2.2 Learning of Exponential Families\nReconstruction of parameters \u20d7\u03b8in the energy function\n(2) is not a trivial task in both discrete and continu-\nous settings. For instance, popular methods such as\nmaximum likelihood are intractable due to the diffi-\nculty of evaluation of the partition functionZwhich\nin general has exponential complexity in the dimen-\nsion of the random variables (Cooper, 1990). Prov-\nably polynomial-time algorithms for consistent expo-\nnential family recovery have only been established re-\ncently, see e.g. (Bresler, 2015; Vuffray et al., 2016,\n2020; Koehler et al., 2022; Pabbaraju et al., 2023).\nDiscrete graphical models.In discrete variable\nsettings, we will use the Generalized Regularized In-\nteraction Screening Estimator (GRISE) introduced in\n(Vuffray et al., 2020) which is applicable to learning\nof all graphical models with finite-alphabet variables\nand multi-body interactions, and nearly matching op-\ntimal sample-complexity in cases where information-\ntheoretic bounds are known. In this paper, for simplic-\nity, we will focus on a special subclass of binary vari-\nable with pairwise interactions, known as Ising models\n(Vuffray et al., 2016). In its simplest form, GRISE for\nIsing models is based on minimizing a convex local loss\nfunction \u2013 the Interaction Screening Objective (ISO):\n(\u02c6Ji,\u02c6hi) = argmin(Ji,hi)Si(Ji, hi), where\nSi=1\nMMX\nm=1exp\u0010\n\u2212X\nj\u0338=iJij\u03c3(m)\ni\u03c3(m)\nj\u2212hi\u03c3(m)\ni\u0011\n.(3)\nSymmetrized estimated couplings that are sufficiently\nsmall to zero can be thresholded, thus recovering the\ngraph structure in the cases where the graph is sparse.\nContinuous exponential families.In the case of\nGibbs distributions with continuous variables, we use\nScore Matching (Hyv\u00a8 arinen and Dayan, 2005) as the\nalgorithm of choice. The idea behind Score Match-\ning is to bypass the intractability of the normalization\nconstantZ \u03b8by exploiting the gradient of the distribu-\ntion with respect to data\u20d7 x. The gradient of the log-\nlikelihood is called the score function,\u2207logp \u03b8(x) =:\n\u03c8\u03b8(x). The parameters \u20d7\u03b8of the exponential family dis-\ntributions can be estimated by defining the following\nscore matching objective: Given a data distribution\npd(x) and an approximating distributionp \u03b8(x) with\nScientific Data Compression and Super-Resolution Sampling\nparameters, the score matching objective is defined as\nJ(\u03b8) =1\n2Z\npd(x)\u2225\u03c8 \u03b8(x)\u2212\u03c8 d(x)\u22252dx.(4)\nThe consistency properties of Score Matching have\nbeen rigorously established in recent works (Koehler\net al., 2022; Pabbaraju et al., 2023) for exponential\nfamilies with polynomial energy functions.\n2.3 Sampling from the learned Graphical\nModel\nOnce the energy function of the model is learned, in\nprinciple, one can use Markov Chain Monte Carlo\n(MCMC) methods to produce independent samples\nfrom the resulting density.\nGlauber dynamics for discrete Gibbs distribu-\ntions.Glauber dynamics (Glauber, 1963) is a well-\nknown method for sampling from the Ising model and\nclosely related to the Gibbs sampling procedure. The\nprocess begins with an arbitrary initial configuration\n\u03c3t=0\u2208 {\u22121,+1}p, which is often chosen at random.\nAt each time step, a single node\u03c3 iis selected uni-\nformly at random for updating according to the con-\nditional probability of the variable. For example, for\nIsing model with zero local fields, the update takes on\nvalue +1 with probability\nP(\u03c3t+1\ni= +1|\u03c3t\nj\u0338=i) =exp\u0000\n2P\nj\u0338=iJij\u03c3t\nj\u0001\n1 + exp\u0000\n2P\nj\u0338=iJij\u03c3t\nj\u0001(5)\nand is\u22121 otherwise. Notably, each spin update de-\npends only on neighboring spins. It can be shown that\nthe Gibbs distribution (1) is stationary with respect\nto the Glauber dynamics. If the dynamics quickly ap-\nproaches equilibrium \u2013 that is, when the mixing time\nof the model is small \u2013 they can be used to simulate\ni.i.d. samples from (1). While some Markov chains\nrapidly mix to their equilibrum distributions, it is well-\nknown that in many other cases, the existence of \u201cbot-\ntlenecks\u201d between transition states may lead to slow\nmixing, oftentimes exponential in the dimension.\nLangevin dynamics for continuous exponential\nfamily models.In the case of continuous models,\none can use Langevin dynamics for producing samples\nfrom the learned data distributionp(x). In the dis-\ncretized time version, the Markov chain is initialized\nfrom an arbitrary point, and then is iterated as follows:\nxt+1=xt+\u03f5\u2207 xlogp(x) +\u221a\n2\u03f5zt,(6)\nwherex tare the samples drawn from the procedure\nat each iteration,t= 0,1, ..., T,\u2207 xlogp(x) is the gra-\ndient of the log density which gives us a step in thedirection of the gradient,z t\u223c N(0, I) denotes the\ninjected noise, and\u03f5is a scaling factor that lets us\ncontrol the magnitude of the step in the gradient di-\nrection. Similarly to Glauber dynamics for discrete\nmodels, Langevin dynamics is guaranteed to mix to\nthe equilibrium distribution (1), albeit at an exponen-\ntial time for many families of models with non-trivial\nenergy functions (Wainwright et al., 2008).\n2.3.1 Super-Resolution Sampling with\nDecompressed Data Initialization\nUsually learning and sampling problems are considered\nin separation. In typical learning settings, data is often\ndiscarded after the model is learned (see, e.g., score-\nbased generative model (Song and Ermon, 2019; Song\net al., 2020)), whereas the resulting model can have\na slow mixing time. We argue that while the Markov\nchains may suffer from convergence issues when initial-\nized from a random initial condition, the situation is\ndrastically different if we are able to initialize several\nMCMC dynamics starting with independent samples\nrepresentative of the distributionE(\u20d7 x). The idea of\n\u2212log\ud835\udc5d(\u0526\ud835\udc65) \n\u0526\ud835\udc65super -resolved samples\ninitialization from reduced data for MCMC correction learned energy\nlandscape\nof original data\nFigure 2:Intuition behind our super-resolution sampling\nproposal: a stored reduced sample enables initialization\nof a MCMC method in the vicinity of the original sample,\nwhich then samples in a local part of the phase space based\non the learned energy functionE(\u20d7 x). In practical situa-\ntions, this approach may not suffer from the slow mixing of\nMCMC sampling starting from random initial conditions,\nand highlight the value of the stored reduced data.\ndata-based initialization has been used in the empir-\nical machine learning literature for a long time as a\nmeans of overcoming expensive training, for example\nas a part of the mechanics of \u201ccontrastive divergence\u201d\ntraining for energy-based methods and other approxi-\nmations to Maximum Likelihood Estimation (Hinton,\n2002; Xie et al., 2016; Nijkamp et al., 2019, 2020). In\ntheoretical settings, data-based initialization was re-\ncently shown to be effective in improving sampling of\na large class of models. A direct analysis along these\nlines was done in (Koehler and Vuong, 2023) in the\ncase of a mixture of strongly log-concave distributions\nsupported on multiple well-separated clusters. The re-\ncent work (Koehler et al., 2024) provided a more gen-\nMinh Vu, Andrey Lokhov\neral analysis for exponential families, showing provable\nmixing benefits of initialization with original samples.\nIn our setting of data compression and recovery, the\noriginal data samples are not available. However,\ndrawing inspiration from the data-based initializa-\ntions, our proposal consists in finding maximum data\ncompression rate under which decompressed samples\nare still sufficiently localized in different regions of the\nspace of configurations, and can be useful to warm-\nstart Markov chains. The rationale for this principle\nis illustrated in Fig. 2. Repeating this procedure us-\ning several stored reduced noise samples thus enables a\nphysics-informed super-resolution, i.e., sampling from\nP(\u20d7 x) which automatically enforces the quantities of in-\nterestQ(\u20d7 x). An exact lossy compression and recovery\nscheme used is not important, but we discuss a popular\nchoice that we adopt in this work next.\n2.4 Lossy Data Compression and Recovery\nGiven a set of independent original samples, we con-\nsider the lossy compression scheme based on discrete\ncosine transform (DCT) (Ahmed et al., 2006), which is\nbehind one of the most popular digital data compres-\nsion and processing algorithms (Rao and Yip, 2014)\n(such as JPEG and HEIF). DCT converts each sample\nxnfrom the spatial domain into the frequency domain,\nrepresented by coefficients of basis functions\nXk=N\u22121X\nn=0xncosh\u03c0\nN(n+1\n2)ki\n, k= 0, ..., N\u22121.\n(7)\nThe inverse of DCT, which is simply DCT multiplied\nby 2/N, converts the data back to spatial domain,\nwhich we use to for lossy recovery of samples and\nfor subsequent initialization of Markov chains in the\nsuper-resolution sampling scheme. Multidimensional\nvariants of the various DCT types follow straightfor-\nwardly from the one-dimensional definitions: they are\nsimply a separable product of DCTs along each di-\nmension. The inverse of a multi-dimensional DCT is\nagain a separable product of the inverses of the corre-\nsponding one-dimensional DCTs. The premise of DCT\nmethod is that in many cases, one can reconstruct\nthe data very accurately from only a few coefficients.\nThe DCT coefficients, which contain up to the desired\nthreshold of energy in the image,\nmax\nJJ\ns.t.\u0010JX\nk=1X2\nk\u0011\u000e\n\u2225X\u22252\u2264E presv, E presv\u2208[0,1]\n(8)\nare stored as the compressed data, and the remaining\ncoefficients are disregarded. By adjusting theE presv,we can adjust the level of compression, defined as\nC= 1\u2212E presv . The higherE presv results in lower\nlevel of compressionC, and as a result less computa-\ntional effort needed to correct the distribution, leading\nto fast reconstruction of high quality data. A direct ap-\nplication of the above equations would requireO(N2)\noperations; however, DCT can also be computed using\nFast Fourier Transforms (FFT) withO(NlogN) op-\nerations. In our numerical experiments below, we use\nthe DCT implementation through FFTW.jl package\nin Julia, connected to the FFTW library (Frigo and\nJohnson, 2005) (a C subroutine library for computing\nthe Discrete Fourier Transform in high dimensions).\n3 Results\nIn this section, we present a series of numerical exper-\niments to evaluate our proposed approach. We test on\nsynthetic Ising models (small enough for exact sam-\npling), real data from an analog quantum computer,\nmultivariate normal distributions, synthetic data from\nlattice field theory (generated via Langevin dynamics),\nand scientific data from high-fidelity materials simula-\ntions. All experiments were run on an M2 MacBook\nPro (32 GB RAM) with a one-hour time limit per\nrun. Our experiments follows the same setups: Given\na dataset, we (i) learn an exponential-family model\n(with GRISE or score matching) consistent with the\nrecorded QoIs; (ii) compress samples via DCT with\nvarying compression levelsC\u2208 {0.1,0.3, . . . ,0.9}; and\n(iii) decompress the samples and apply MCMC correc-\ntion (using Glauber or Langevin dynamics guided by\nthe learned model) until the reconstructed QoIs match\nthe stored QoIs within a desired tolerance. Each ex-\nperiment is repeated for 5 times with randomly gen-\nerated models and datasets, and the averaged results\nare reported. As a benchmark, we include comparisons\nwith (a) a standard autoencoder trained to minimize\nreconstruction error and (b) a regularized autoencoder\nthat adds a QoI penalty to the loss. More detailed in-\nformation of our implementation, codes, and data are\nprovided in the Supplementary Materials.\n3.1 Experimental Results on Discrete Data\nIsing model:We first test our workflow on a discrete\nIsing model defined on a two-dimensional lattice, with\np= 16 nodes, randomly chosen coupling and zero lo-\ncal fields. We generateM= 106i.i.d samples from\nthe model by computing exact probabilities of all 216\npossible configurations. The goal is to compress this\ndataset while preserving key statistical QoIs\u2014in this\ncase selected to be the first and second moments of the\ndiscrete variables, i.e.,m 1(\u03c3) =1\nMPM\nm=1\u03c3(m)and\nm2(\u03c3) =1\nM\u22121PM\nm=1(\u03c3(m)\u2212m 1(\u03c3))(\u03c3(m)\u2212m 1(\u03c3))\u22a4.\nScientific Data Compression and Super-Resolution Sampling\nFigure 3:Maximum element-wise errors of the first and second moments of reconstructed samples computed across\ndifferent compression levels in synthetic (left) and real (right) datasets withdiscrete data. Results are averaged over\n5 randomly generated models and experiments. The maximum element-wise error means and standard deviations are\nshown for 4 scenarios: (i) vanilla DCT decompressed samples, (ii) after application of our super-resolution correction\n(with 10, 12, and 10 MCMC steps for Ising-QoIs, Ising-TV, and D-Wave, respectively), (iii) reconstruction from standard\nautoencoders, and (iv) reconstruction from regularized autoencoders.\nThe error tolerance of reconstruction is set to 0.05,\nmeasured as the maximum element-wise error in the\nfirst and second momentse 1=\u2225m 1(\u03c3)\u2212m 1(\u02dc\u03c3)\u2225\u221e\nande 2=\u2225m 2(\u03c3)\u2212m 2(\u02dc\u03c3)\u2225 max, where\u02dc\u03c3are the recon-\nstructed samples. Figure 3 (left) illustrates the QoI er-\nrors before and after correction, showing that across all\ntested compression levels, applying 10 correction steps\nsubstantially reduces the errors\u2014underscoring the ef-\nfectiveness of our hybrid approach. Our analysis of the\nnumber of correction steps required to reach the pre-\nscribed error threshold indicates that more aggressive\ncompression generally demands additional correction\nsteps; however, the overall impact of compression on\ncorrection performance is minimal. Even under high\ncompression, 10 correction steps are typically sufficient\nto recover the QoIs within the specified tolerance. For\ncomparison, Markov chains with random initialization\nrequire roughly 250 steps to produce samples match-\ning the target QoIs within the same tolerance. Ad-\nditionally, Figure 3 (left) shows that both standard\nand regularized autoencoders exhibit rapidly increas-\ning errors as compression intensifies, while our pro-\nposed super-resolution method consistently maintains\nnear-zero error across all compression levels. In ad-\ndition to QoIs errors, we assess our framework using\nTotal Variation (TV) distance\u2014a more comprehensive\nmeasure directly comparing probability distributions.\nFigure 3 (middle) shows the TV distance between re-\nconstructed data before and after correction and the\nground-truth distribution, for varying levels of com-\npression. In this example, 12 correction steps are suf-\nficient to reduce TV distance below 0.05 across all\ncompression levels, while randomly initialized Markov\nchains require over 250 steps to achieve a comparable\nlevel of accuracy. The results demonstrate that ourapproach generalizes well to more stringent distribu-\ntional metrics beyond QoIs errors. In what follows, we\nfocus on preservation of QoIs as an easily computable\nsuccess metric.\nD-Wave analog quantum computer:Building on\nthe previous controlled settings, we now evaluate our\nframework on real data from a D-Wave 2000Q quan-\ntum annealer, an analog quantum computer that per-\nforms quantum annealing by evolving an initial state\ntoward a ground state of an encoded Ising Hamiltonian\n(Bunyk et al., 2014). Due to thermal noise, tempera-\nture rescaling, and intrinsic device biases, the effective\nmodel from which the samples are produced is signif-\nicantly distorted and does not exactly correspond to\nthe input Ising model (Vuffray et al., 2022; T\u00a8 uys\u00a8 uz\net al., 2025). Thus, the actual couplings and fields\non the chip remain unknown. Using 4\u00d7106samples\nfrom 16 qubits arranged in the Chimera graph topol-\nogy, we test our approach in a setup analogous to the\nsynthetic Ising case, compressing the data while seek-\ning to preserve first- and second-order correlations as\nQoIs. Unlike the synthetic scenario, these QoIs no\nlonger fully characterize the underlying distribution.\nFigure 3 (right) presents the QoI errors from decom-\npressed samples and after correction, again showing\nsignificantly reduction in the QoI error, across multi-\nple compression levels, when only 10 correction steps\nis applied. Remarkably, the QoIs are conserved to\na great accuracy, despite the true model containing\nhigher-order moments as sufficient statistics.\n3.2 Experiments with Continuous Data\nMultivariate normal distribution:To test our\napproach in the continuous setting, we use a 16-\nMinh Vu, Andrey Lokhov\nFigure 4:Maximum element-wise QoIs errors of reconstructed samples computed across different compression levels in\nsynthetic (left) and real (right) datasets withcontinuous data. Results are averaged over 5 randomly generated models\nand experiments. We use the same method as in Figure 3. In our super-resolution correction part, we use 7, 20, and 50\ncorrection steps for the Multivariate Normal, \u03a64-theory, and Aluminum design experiments, respectively.\ndimensional Normal distributionN(\u00b5, ,\u03a3), where the\nmean vector\u00b5and covariance matrix\u03a3=CC\u22a4are\nrandomly generated with entries uniformly sampled\nfrom [\u22120.5,0.5] and ensuring that cond(\u03a3)\u226450 for\nnumerical stability. We generateM= 105i.i.d sam-\nples fromN(\u00b5,\u03a3) and aim to compress the dataset\nwhile preserving key statistical quantities of interest:\nthe first (m 1(x)) and second (m 2(x)) moments of the\ndata. In this experiment, we found that only 7 cor-\nrection steps are sufficient to restore the QoIs within\nthe error tolerance of 0.05. Figure 4 (left) shows\nsignificantly reduction in the QoIs error when using\nour super-resolution method compared to samples ob-\ntained by lossy decompression and autoencoders. For\na random initialization of the Markov chains, we found\nthat it would take approximately 50 steps to generate\nsamples matched the target QoIs with the same error.\nScalar lattice field theory:We next test our\nprocedure on the scalar \u03a64-theory, a fundamental\nmodel in quantum field theory that is widely used\nto study phase transitions and scalar dynamics in\nparticle and condensed matter physics. We con-\nsider the two-dimensional lattice version of the the-\nory on a 16\u00d716 grid, re-casted in the dimension-\nless formP(x) =1\nZexp\u0000\n\u2212\u03b1P\nix4\ni\u2212\u03b2P\nix2\ni\u2212\n\u03b3P\n(i,j)\u2208E xixj\u0001\n, with parameters\u03b8= (\u03b1, \u03b2, \u03b3) sam-\npled uniformly from [0.1,0.12]\u222a[0.2,0.22]\u222a[0.3,0.32].\nUsing Langevin dynamics withn= 50,000 steps\nand step size \u2206T= 0.001, we generateM= 105\ni.i.d. samples and verify parameter accuracy through\nscore matching, ensuring| \u02c6\u03b8\u2212\u03b8| max\u22640.01. Our\ngoal is to compress this dataset while preserving\nthree key QoIs:q 1=1\nMPM\ns=1\u00001\n|E|P\n(i,j)\u2208Ex(s)\nix(s)\nj\u0001\n,\nq2=1\nMPM\ns=11\nNP\ni(x(s)\ni)2,q 3=1\nMPM\ns=11\nNP\ni(x(s)\ni)4.\nAs shown in Figure 4 (middle), 20 correction steps with\nour super-resolution method are sufficient to recoverthe QoIs within the tolerance of 0.05, whereas random\ninitialization requires at least 200 steps to achieve the\nsame accuracy. Note that the regularized autoencoder\nperforms best for this synthetic benchmark but still\nmaintains a level of error (\u223c0.15) above the required\nerror threshold.\nMolecular dynamics simulation of aluminum:\nFinally, we evaluate our method on real scientific data\nobtained from high-fidelity molecular dynamics (MD)\nsimulations of aluminum using the Embedded-Atom\nMethod (EAM), a widely used many-body potential\nfor modeling metallic interactions. The simulations\nwere carried out with the LAMMPS package (Thomp-\nson et al., 2022), a state-of-the-art tool for stress test-\ning and materials design, on a block of aluminum con-\ntaining 5\u00d7105particles, constrained in volume and\nequilibrated at 300 K. Our objective is to compress this\ndataset while preserving temperature as the key QoI,\nwithT=2Ekin\nNDOFkBwhereE kin=PNatoms\ni=11\n2miv2\niis the\ntotal kinetic energy,N DOFis the number of degrees of\nfreedom, andk Bthe Boltzmann constant. In this ex-\nperiment, 50 correction steps were sufficient to restore\nthe temperature within an error tolerance of 0.5 K. As\nshown in Figure 4 (right), our super-resolution method\nachieves a substantial reduction in QoI error compared\nto lossy decompression and autoencoder baselines.\n3.3 Scalability Study\nWe examine how the computational complexity of our\nalgorithm scales with the system size for three com-\nponents of our approach: (1) learning the exponen-\ntial family distribution from the original data; (2) ap-\nplying discrete cosine transform (DCT)\u2013based com-\npression and decompression; and (3) correcting the\ndecompressed data using Markov chain Monte Carlo\nScientific Data Compression and Super-Resolution Sampling\nFigure 5:Dependence of the wall-clock computation time of our approach on the dimensionality of the data for multi-\nvariate Normal distributions (left) and D-wave experiments (right). The figure shows the runtime (in seconds) of each\ncomponent of our algorithm\u2014learning, DCT-based compression/decompression, and MCMC correction\u2014as a function of\nsystem sizeNfor the maximum considered compression levelC= 0.9. We empirically find that all three components\nscale no worse than quadratically with system size in this case. All data points are averaged over 3 model instances, the\nerror bars are estimated through the empirical standard deviation.\n(MCMC) methods guided by the learned model.\nSynthetic Gaussian data:We analyze the case of\na multivariate Normal distribution under conditions\nanalogous to the one in Section 3.1, but with the sys-\ntem sizepvaried from 100 to 500. We choose multi-\nvariate Normal distribution for this study due to the\neasiness of sample generation. For each system size,\nwe generateM= 105i.i.d. samples fromN(\u00b5,\u03a3).\nOur objective is to compress this Gaussian dataset to\na compression level of 0.9 while preserving key statisti-\ncal quantities of interest (QoIs)\u2014specifically, the first\nand second moments\u2014within a tolerance of 0.05. In\nour experiment, we found that 40 correction steps (us-\ning the Langevin dynamics with the step size of 0.05)\nare sufficient across all tested system sizes to restore\nthe QoIs within the specified tolerance. Notice that\nlearning is the easiest component of the workflow in\nthis specific case because of a simple form of the score\nmatching loss for multivariate Normal distributions,\nalthough it is expected to be the most time-consuming\nelement for general distributions.\nExperimental D-Wave data:We now test our pro-\ncedure on real experimental data produced on a D-\nWave 2000Q quantum annealer. Each experimental\ndatasetS iconsists of 4\u00d7106samples of a 16-qubit\nsystem defined by a distinct Ising Hamiltonian. To em-\nulate larger system sizes, we aggregate datasets by in-\ndependently drawing one sample from eachS i,i=1,...,k\naccording to its probability distribution and concate-\nnating these into a joint sample of size 16k. Repeating\nthis procedure 4\u00d7106times yields a datasetS\u2217of\n4\u00d7106large samples. As in the Gaussian case, the\ngoal is to compressS\u2217to a compression level of 0.9\nwhile preserving the first (m 1(x)) and second (m 2(x))\nmoments as QoIs within a tolerance of 0.05. In thiscase, the number of correction steps is observed to\nscale linearly with system size, approximately at the\nrate of 10N. Figure 5 shows the computation time\n(in seconds) as a function of system size. Consistent\nwith the Gaussian experiments, all three components\nof the framework are at most quadratic scaling in this\nsetting. This observed polynomial trend for the super-\nresolution sampling even at the maximum considered\ncompression rateC= 0.9 is encouraging because for\nnon-trivial distributions, the correction sampling from\nrandom initializations can take time growing exponen-\ntially with the system size (Pabbaraju et al., 2023).\nThe number of MCMC correction steps could be re-\nduced at lower compression, offering trade-off between\ncomputational efficiency and storage requirements.\n4 Conclusion\nIn this work, we addressed a problem for managing\nthe growing data requirements in scientific machine\nlearning. We have introduced a computational frame-\nwork for scientific data reduction that enables super-\nresolution sampling preserving user-defined quantities\nof interest. Our approach focused on establishing en-\nhanced inference-tractability and predictive capacities\nof exponential family distributions, as well as their\nsampling properties while guaranteeing the conserva-\ntion of quantities of interest. Throughout various ex-\namples in continuous and discrete domains, we have\nshown that the proposed approach can achieve sig-\nnificant compression while still accurately recovering\nkey statistical QoIs with minimal correction steps. All\ncodes and data used to produce results in this work\nare provided as a self-contained archive in the Supple-\nmentary Materials.\nMinh Vu, Andrey Lokhov\nCode Availability\nThe code used in this work is publicly available\nat github.com/lanl-ansi/SuperResolution Vu and\nLokhov (2025).\nAcknowledgements\nThe authors acknowledge support from the U.S. De-\npartment of Energy/Office of Science Advanced Scien-\ntific Computing Research Program and from the Lab-\noratory Directed Research and Development program\nof Los Alamos National Laboratory under Project No.\n20230338ER.\nReferences\nNasir Ahmed, T Natarajan, and Kamisetty R Rao.\nDiscrete cosine transform.IEEE transactions on\nComputers, 100(1):90\u201393, 2006.\nAllison H Baker, Dorit M Hammerling, Sheri A Mickel-\nson, Haiying Xu, Martin B Stolpe, Phillipe Naveau,\nBen Sanderson, Imme Ebert-Uphoff, Savini Sama-\nrasinghe, Francesco De Simone, et al. Evaluating\nlossy data compression on climate simulation data\nwithin a large ensemble.Geoscientific Model Devel-\nopment, 9(12):4381\u20134403, 2016.\nKevin J Bowers, Barbara G Devolder, Lin Yin, and\nThomas JT Kwan. A maximum likelihood method\nfor linking particle-in-cell and monte-carlo transport\nsimulations.Computer physics communications, 164\n(1-3):311\u2013317, 2004.\nGuy Bresler. Efficiently learning ising models on ar-\nbitrary graphs. InProceedings of the forty-seventh\nannual ACM symposium on Theory of computing,\npages 771\u2013782, 2015.\nPaul I Bunyk, Emile M Hoskinson, Mark W John-\nson, Elena Tolkacheva, Fabio Altomare, Andrew J\nBerkley, Richard Harris, Jeremy P Hilton, Trevor\nLanting, Anthony J Przybysz, et al. Architectural\nconsiderations in the design of a superconducting\nquantum annealing processor.IEEE Transactions\non Applied Superconductivity, 24(4):1\u201310, 2014.\nJon Calhoun, Franck Cappello, Luke N Olson, Marc\nSnir, and William D Gropp. Exploring the feasibility\nof lossy compression for pde simulations.The Inter-\nnational Journal of High Performance Computing\nApplications, 33(2):397\u2013410, 2019.\nGuangye Chen, Luis Chac\u00b4 on, and Truong B Nguyen.\nAn unsupervised machine-learning checkpoint-\nrestart algorithm using gaussian mixtures for\nparticle-in-cell simulations.Journal of Computa-\ntional Physics, 436:110185, 2021.Gregory F Cooper. The computational complexity\nof probabilistic inference using bayesian belief net-\nworks.Artificial intelligence, 42(2-3):393\u2013405, 1990.\nSiyu Ding, Chenxu Ni, Xu Chu, Qingzhou Lu, and\nXingjian Wang. Reduced-order modeling via con-\nvolutional autoencoder for emulating combustion of\nhydrogen/methane fuel blends.Combustion and\nFlame, 274:113981, 2025.\nAlice MS Durieux, Christopher X Ren, Matthew T\nCalef, Rick Chartrand, and Michael S Warren.\nBudd: Multi-modal bayesian updating deforestation\ndetections. InIGARSS 2020-2020 IEEE Interna-\ntional Geoscience and Remote Sensing Symposium,\npages 6638\u20136641. IEEE, 2020.\nKurt B Ferreira, Rolf Riesen, Patrick Bridges, Dorian\nArnold, and Ron Brightwell. Accelerating incremen-\ntal checkpointing for extreme-scale computing.Fu-\nture Generation Computer Systems, 30:66\u201377, 2014.\nWilliam T Freeman, Thouis R Jones, and Egon C\nPasztor. Example-based super-resolution.IEEE\nComputer graphics and Applications, 22(2):56\u201365,\n2002.\nMatteo Frigo and Steven G. Johnson. The design\nand implementation of FFTW3.Proceedings of\nthe IEEE, 93(2):216\u2013231, 2005. Special issue on\n\u201cProgram Generation, Optimization, and Platform\nAdaptation\u201d.\nRohan Garg, Tirthak Patel, Gene Cooperman, and\nDevesh Tiwari. Shiraz: Exploiting system reliability\nand application resilience characteristics to improve\nlarge scale system throughput. In2018 48th An-\nnual IEEE/IFIP International Conference on De-\npendable Systems and Networks (DSN), pages 83\u2013\n94. IEEE, 2018.\nCyprien Gille, Frederic Guyard, and Michel Bar-\nlaud. Semi-supervised classification using a su-\npervised autoencoder for biomedical applications.\narXiv preprint arXiv:2208.10315, 2022.\nRoy J Glauber. Time-dependent statistics of the ising\nmodel.Journal of mathematical physics, 4(2):294\u2013\n307, 1963.\nLovedeep Gondara. Medical image denoising using\nconvolutional denoising autoencoders. In2016 IEEE\n16th international conference on data mining work-\nshops (ICDMW), pages 241\u2013246. IEEE, 2016.\nRamon F Hanssen.Radar interferometry: data in-\nterpretation and error analysis, volume 2. Springer\nScience & Business Media, 2001.\nGeoffrey E Hinton. Training products of experts by\nminimizing contrastive divergence.Neural compu-\ntation, 14(8):1771\u20131800, 2002.\nScientific Data Compression and Super-Resolution Sampling\nGeoffrey E Hinton and Ruslan R Salakhutdinov. Re-\nducing the dimensionality of data with neural net-\nworks.science, 313(5786):504\u2013507, 2006.\nClaudia Hulbert, Bertrand Rouet-Leduc, Paul A John-\nson, Christopher X Ren, Jacques Rivi` ere, David C\nBolton, and Chris Marone. Similarity of fast and\nslow earthquakes illuminated by machine learning.\nNature Geoscience, 12(1):69\u201374, 2019.\nAapo Hyv\u00a8 arinen and Peter Dayan. Estimation of\nnon-normalized statistical models by score match-\ning.Journal of Machine Learning Research, 6(4),\n2005.\nDaniel Jarrett and Mihaela van der Schaar. Target-\nembedding autoencoders for supervised represen-\ntation learning.arXiv preprint arXiv:2001.08345,\n2020.\nGeorge Em Karniadakis, Ioannis G Kevrekidis, Lu Lu,\nParis Perdikaris, Sifan Wang, and Liu Yang.\nPhysics-informed machine learning.Nature Reviews\nPhysics, pages 1\u201319, 2021.\nScott Klasky, Jana Thayer, and Habib Najm. Data re-\nduction for science: Brochure from the advanced sci-\nentific computing research workshop. Technical re-\nport, Oak Ridge National Lab.(ORNL), Oak Ridge,\nTN (United States); SLAC National . . . , 2021.\nFrederic Koehler and Thuy-Duong Vuong. Sampling\nmultimodal distributions with the vanilla score:\nBenefits of data-based initialization.arXiv preprint\narXiv:2310.01762, 2023.\nFrederic Koehler, Alexander Heckett, and Andrej\nRisteski. Statistical efficiency of score match-\ning: The view from isoperimetry.arXiv preprint\narXiv:2210.00726, 2022.\nFrederic Koehler, Holden Lee, and Thuy-Duong\nVuong. Efficiently learning and sampling multi-\nmodal distributions with data-based initialization.\narXiv preprint arXiv:2411.09117, 2024.\nAnna Matsekh, Luis Chac\u00b4 on, HyeongKae Park, and\nGuangye Chen. On learning particle distributions\nin the 1d implicit monte carlo simulations of radia-\ntion transport. InApplications of Machine Learning\n2020, volume 11511, pages 92\u2013104. SPIE, 2020.\nMarc Mezard and Andrea Montanari.Information,\nphysics, and computation. Oxford University Press,\n2009.\nJohn Moussouris. Gibbs and markov random systems\nwith constraints.Journal of statistical physics, 10\n(1):11\u201333, 1974.\nErik Nijkamp, Mitch Hill, Song-Chun Zhu, and\nYing Nian Wu. Learning non-convergent non-\npersistent short-run mcmc toward energy-basedmodel.Advances in Neural Information Processing\nSystems, 32, 2019.\nErik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu,\nand Ying Nian Wu. On the anatomy of mcmc-\nbased maximum likelihood learning of energy-based\nmodels. InProceedings of the AAAI Conference on\nArtificial Intelligence, volume 34, pages 5272\u20135280,\n2020.\nChirag Pabbaraju, Dhruv Rohatgi, Anish Prasad\nSevekari, Holden Lee, Ankur Moitra, and Andrej\nRisteski. Provable benefits of score matching.Ad-\nvances in Neural Information Processing Systems,\n36:61306\u201361326, 2023.\nTom Peterka, Deborah Bard, Janine Bennett,\nE Bethel, Ron Oldfield, Line Pouchard, Christine\nSweeney, and Matthew Wolf. Ascr workshop on in\nsitu data management: Enabling scientific discov-\nery from diverse data sources. Technical report, US\nDepartment of Energy (USDOE), Washington, DC\n(United States). Office of . . . , 2019.\nJames S Plank, Micah Beck, Gerry Kingsley, and Kai\nLi.Libckpt: Transparent checkpointing under unix.\nComputer Science Department, 1994.\nK Ramamohan Rao and Ping Yip.Discrete co-\nsine transform: algorithms, advantages, applica-\ntions. Academic press, 2014.\nDaniel R Roe and Bernard R Brooks. Quantifying the\neffects of lossy compression on energies calculated\nfrom molecular dynamics trajectories.Protein Sci-\nence, 31(12):e4511, 2022.\nJose Carlos Sancho, Fabrizio Petrini, Greg Johnson,\nand Eitan Frachtenberg. On the feasibility of in-\ncremental checkpointing for scientific computing. In\n18th International Parallel and Distributed Process-\ning Symposium, 2004. Proceedings., page 58. IEEE,\n2004.\nSeung Woo Son, Zhengzhang Chen, William Hendrix,\nAnkit Agrawal, Wei-keng Liao, and Alok Choud-\nhary. Data compression for the exascale computing\nera-survey.Supercomputing frontiers and innova-\ntions, 1(2):76\u201388, 2014.\nYang Song and Stefano Ermon. Generative modeling\nby estimating gradients of the data distribution.Ad-\nvances in neural information processing systems, 32,\n2019.\nYang Song, Jascha Sohl-Dickstein, Diederik P\nKingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through\nstochastic differential equations.arXiv preprint\narXiv:2011.13456, 2020.\nA. P. Thompson, H. M. Aktulga, R. Berger, D. S.\nBolintineanu, W. M. Brown, P. S. Crozier, P. J. in \u2019t\nMinh Vu, Andrey Lokhov\nVeld, A. Kohlmeyer, S. G. Moore, T. D. Nguyen,\nR. Shan, M. J. Stevens, J. Tranchida, C. Trott, and\nS. J. Plimpton. LAMMPS - a flexible simulation tool\nfor particle-based materials modeling at the atomic,\nmeso, and continuum scales.Comp. Phys. Comm.,\n271:108171, 2022. doi: 10.1016/j.cpc.2021.108171.\nDevesh Tiwari, Saurabh Gupta, and Sudharshan S\nVazhkudai. Lazy checkpointing: Exploiting tem-\nporal locality in failures to mitigate checkpoint-\ning overheads on extreme-scale systems. In2014\n44th Annual IEEE/IFIP International Conference\non Dependable Systems and Networks, pages 25\u201336.\nIEEE, 2014.\nCenk T\u00a8 uys\u00a8 uz, Abhijith Jayakumar, Carleton Coffrin,\nMarc Vuffray, and Andrey Y Lokhov. Learning\nresponse functions of analog quantum computers:\nanalysis of neutral-atom and superconducting plat-\nforms.arXiv preprint arXiv:2503.12520, 2025.\nMinh Vu and Andrey Lokhov. Code repository for\n\u201dscientific data compression and super-resolution\nsampling\u201d.https://github.com/lanl-ansi/\nSuperResolution, 2025.\nMarc Vuffray, Sidhant Misra, Andrey Lokhov, and\nMichael Chertkov. Interaction screening: Efficient\nand sample-optimal learning of ising models.Ad-\nvances in neural information processing systems, 29,\n2016.\nMarc Vuffray, Sidhant Misra, and Andrey Lokhov. Ef-\nficient learning of discrete graphical models.Ad-\nvances in Neural Information Processing Systems,\n33:13575\u201313585, 2020.\nMarc Vuffray, Carleton Coffrin, Yaroslav A Kharkov,\nand Andrey Y Lokhov. Programmable quantum an-\nnealers as noisy gibbs samplers.PRX Quantum, 3\n(2):020317, 2022.\nMartin J Wainwright, Michael I Jordan, et al. Graph-\nical models, exponential families, and variational\ninference.Foundations and Trends\u00aein Machine\nLearning, 1(1\u20132):1\u2013305, 2008.\nJianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian\nWu. A theory of generative convnet. InInterna-\ntional conference on machine learning, pages 2635\u2013\n2644. PMLR, 2016.\nScientific Data Compression and Super-Resolution Sampling:\nSupplementary Materials\nA Experimental Details\nWe now provide additional details of our the experimental setup and super-resolution sampling workflow.\nIsing model:Our procedure to compress the dataset while preserving key statistical QoIs is as follows. First,\nwe compute the first and second empirical moments of the samples, i.e.,m 1(\u03c3) =1\nMPM\nm=1\u03c3(m)andm 2(\u03c3) =\n1\nM\u22121PM\nm=1(\u03c3(m)\u2212m 1(\u03c3))(\u03c3(m)\u2212m 1(\u03c3))\u22a4, and store them as the target QoIs. Next, we learn a compact\nrepresentation of the data by learning an Ising model using GRISE. For each nodei\u2208V, we solve a convex\noptimization problem to estimate the local interaction parameters{ \u02c6Jij,\u02c6Hi}. We use the Ipopt solver implemented\nthrough the open-source library GraphicalModelLearning.jl to obtain these parameter estimates, although any\nconvex optimization method would be suitable due to the convexity of the objective.\nOnce the model is learned, we apply the DCT compression procedure to compress the original samples and\nstore them in the compressed form. We use different values ofE presv\u2208[0.1,0.3, ...,0.9], representing varied\nlevels of compression, to analyze the effects of progressive compression on the proposed approach\u2019s performance.\nFor reconstruction, we use the proposed procedure, combining naive decompression with learned model-based\nMCMC correction. Specifically, we decompress our stored data and use each sample to initialize a Markov chain\ngoverned by Glauber dynamics and the learned model. These chains (one per sample) are run until the moments\nof the reconstructed data match the stored QoIs within an error tolerance of 0.05. The error is defined as the\nmaximum element-wise error of the first and second moments of reconstructed samples, i.e.,E= max{e 1, e2},\nwheree 1=\u2225m 1(\u03c3)\u2212m 1(\u02dc\u03c3)\u2225\u221e,e2=\u2225m 2(\u03c3)\u2212m 2(\u02dc\u03c3)\u2225\u221e, and\u02dc\u03c3are the reconstructed samples. Finally, we\nrepeat the experiment and evaluate the performance of our approach using Total Variation (TV) distance, instead\nof using the QoIs, as the error metric, which is computed asd TV(P, Q) =1\n2P\n\u03c3\u2208\u2126\f\fP(\u03c3)\u2212Q(\u03c3)\f\f, wherePand\nQare the original and reconstructed data distribution, respectively.\nD-Wave analog quantum computer:We test our approach on 4\u00d7106samples collected from D-Wave 2000Q\nplatform on 16 qubits defined on a chip topology known as Chimera graph. Here, the testing setup and our\nprocedure are analogical to the synthetic case of Ising models, where we aim to compress the data set while\naiming to preserve the first and second-order correlations as QoIs.\nMultivariate Normal Distribution:To compress the Gaussian dataset and preserve its key statistical QoIs,\nsimilar to previous examples with discrete data, we first compute the first (m 1(x)) and second (m 2(x)) empirical\nmoments of the samples, and store them as the target QoIs. Next, we rewrite the density of the normal\ndistribution in our standard formP(x) =1\nZexp\u0010\n\u22121\n2(x\u2212\u00b5)\u22a4\u03a3(x\u2212\u00b5)\u0011\n=1\n\u02dcZexp\u0010\nx\u22a4Ax+b\u22a4x\u0011\n, where\nA=\u22121\n2\u03a3\u22121, b=\u03a3\u22121\u00b5, and learn the exponential distribution using the score-matching technique. The score-\nmatching object is solved using gradient descent with 1000 steps and 0.05 step size. Once the model is learned,\nwe apply the DCT compression procedure to compress the original samples. Under the super-resolution recovery,\nwe decompress our stored data and use each sample to initialize a Markov chain governed by Langevin dynamics\n(with 0.05 step size) and the learned model. These chains (one per sample) are run until the moments of the\nreconstructed data match the stored QoIs within a tolerance of 0.05.\nScalar Lattice Field Theory:Our testing setup and procedure are identical to the mutltivariate Normal\nsetting above.\nMolecular dynamics simulation of aluminum:In this experiment, the system under study consists of a\nblock of aluminum containing 5\u00d7105atoms, simulated under constant volume conditions and equilibrated\nat 300 K. Each atom is recorded by its position, velocity, and forces,x= [x, y, z, vx, vy, vz, fx, fy, fz].\nThe primary quantity of interest (QoI) is the system temperature, defined asT=2Ekin\nNDOFkB, whereE kin=\nMinh Vu, Andrey Lokhov\nPNatoms\ni=11\n2miv2\ni, NDOF =n dimNatoms\u2212ndim\u2212N fixDOFs , whereE kinis the total kinetic energy of the group of\natoms,m i= 26.982/(6.022\u00d71023),ndim= 3 is the dimensionality of the simulation,N atoms is the number of\natoms in the group,N fixDOFs is the number of degrees of freedom removed by fix commands,k B= 1.38\u00d710\u221223\nis the Boltzmann constant, andTis the resulting computed temperature.\nIn our approach, we learn the modelP(x)\u221de\u03b8QoI(x)using score-matching technique (with Ipopt solver and\nJulia JuMP), store the compressed dataset and then apply our super-resolution recovery strategy using Langevin\ndynamics (with 0.05 step size) and the learned model. These chains (one per sample) are run until the moments\nof the reconstructed data match the stored QoIs within a tolerance of 0.5 K.\nAutoencoder Comparison:Autoencoders are commonly used to learn compact representations of high-\ndimensional data and to uncover latent structures (Hinton and Salakhutdinov, 2006). Regularized variants\nfurther improve representation learning by incorporating prior knowledge and structural constraints (Jarrett and\nvan der Schaar, 2020; Gille et al., 2022). We benchmark our framework against both standard and regularized\nautoencoder baselines.\nThe autoencoder isD w1(Ew2(x)), wherez=E w2(x) denotes the encoder output (or latent variables) and \u02c6x=\nDw1(z) the reconstructed sample, withw 1, w2denoting the respective network parameters. Both encoder and\ndecoder are implemented as standard multilayer perceptrons (MLPs). The autoencoder is trained by minimizing\nthe reconstruction error, min w1,w21\nMPM\ns=1\u2225x(s)\u2212D w1(Ew2(x(s)))\u22252\n2. The regularized autoencoder is trained\nusing the augmented objective, min w1,w21\nMPM\ns=1\u2225x(s)\u2212Dw1(Ew2(x(s)))\u22252\n2+\u03bbQoIs(x(s)), where\u03bb >0 controls\nthe strength of the regularization, and the QoIs are defined consistently with our method. Compression is\nquantified as the ratio between latent and original dimensions,C:=|z|/|x|.\nB Scalability Study Details\nWe present additional details for the scalability study of the proposed framework.\nSynthetic Gaussian Data:To ensure numerical consistency across system sizes in this study, we construct a\nwell-conditioned covariance matrix\u03a3whose elements do not scale with the dimensionality of the data. We achieve\nthis by first generating a random orthogonal matrixQvia QR decomposition. Then, we create a diagonal matrix\nDwith eigenvalues uniformly distributed between 1 and 1/\u03ba, where\u03ba= 10 is the desired condition number.\nThe covariance matrix is then computed as\u03a3=QDQ\u22a4, ensuring that it remains well-conditioned regardless of\nsystem size.\nSimilar to the Gaussian example, we compute the first moment,m 1(x), and the second moment,C=m 2(x),\nof the data, and store them as the target quantities of interest (QoIs). Then, we learn the density of the\nmultivariate normal distributionP(x) =1\nZexp\u0010\n\u22121\n2(x\u2212\u00b5)\u22a4\u03a3\u22121(x\u2212\u00b5)\u0011\n, where\u00b5is estimated using the\nempirical mean,\u02c6\u00b5=m 1(x). To estimate \u0398\u2248\u03a3\u22121, we apply score matching to the centered variabley=x\u2212\u02c6\u00b5,\nassuming the formp(y) =1\nZexp\u0000\n\u22121\n2y\u22a4\u0398y\u0001\n.This leads to the optimization of the score-matching objective\nJ(\u0398) =1\n2tr(\u0398\u22a4\u0398C)\u2212tr(\u0398), which is minimized via gradient descent, with the gradient\u2207 \u0398J=C\u0398\u2212I,\n1000 steps and a step size of 0.05. Once the model is learned, the remaining DCT-based compression and\nsuper-resolution recovery are carried out similarly to the Gaussian example.\nExperimental D-Wave Data:Here, to preserve QoIs of the data, we compute and store the first moment,\nm1(x), and the second moment,C=m 2(x), of the data. We then learn an Ising model representation of the\ndata using GRISE, solving a convex optimization problem to estimate parameters{ \u02c6Jij,\u02c6Hi}for each nodei\u2208V.\nThe optimization is performed via gradient descent with a step size of 0.25, terminating when improvements fall\nbelow 10\u22126. Once the model is learned, we compress the original samples using the DCT-based procedure at a\ncompression level of 0.9.\nDuring super-resolution recovery, we decompress our stored data and use each sample to initialize a Markov\nchain governed by Glauber dynamics and the learned model. These chains (one per sample) are run until the\nmoments of the reconstructed data match the stored QoIs within a tolerance of 0.05. To restore the QoIs within\ntolerance 0.05, the number of correction steps is observed to scale linearly with system size, approximately at\nthe rate of 10N.\n",
    "title": "Scientific Data Compression and Super-Resolution Sampling",
    "authors": [
      "Minh Vu",
      "Andrey Lokhov"
    ],
    "published": "2025-11-17",
    "arxiv_id": "2511.13675v1",
    "num_pages": 13,
    "num_chars": 56106
  },
  {
    "text": "PERSON\u2013AI BIDIRECTIONAL FIT \u2013 A PROOF-OF-CONCEPT CASE\nSTUDY OF AUGMENTED HUMAN\u2013AI SYMBIOSIS IN\nMANAGEMENT DECISION-MAKING PROCESS\nAGNIESZKA BIE\u0143KOWSKA, JACEK MA\u0141ECKI, ALEXANDER MATHIESEN-OHMAN,\nAND KATARZYNA TWOREK\nNovember 18, 2025\nAbstract. This article develops the concept of Person\u2013AI bidirectional fit (P-AI fit:\nperson to AI, AI to person) and examines its role in managerial decision-making through\na proof-of-concept case study. Building on contingency theory and quality theory, P-AI\nfit is defined as a continuously evolving, context-sensitive alignment (primarily cognitive,\nbut also emotional and behavioral) between a person decision-maker and an AI system.\nThe concept is verified using induction method, and the study analyses concerns real\nhiring decision for a Senior AI Lead in an AI development project, comparing three\ndecision pathways: (1) independent evaluations by a CEO, CTO and CSO; (2) an\nevaluation of augmented human\u2013AI symbiotic intelligence system (H3LIX/LAIZA1) and\n(3) general-purpose large language model (LLMr). The results show role-based divergence\nin human judgments, high alignment between H3LIX/LAIZA and the CEO\u2019s implicit\ndecision model (including ethical disqualification of a high-risk candidate), and a critical\nfalse-positive recommendation from LLMr. The study demonstrates that higher P-AI\nfit, exemplified by the CEO\u2013H3LIX/LAIZA relationship, acts as a mechanism linking\naugmented symbiotic intelligence to accurate, trustworthy and context-sensitive decisions.\nIt constitutes an initial verification of new concept P-AI fit and a proof-of-concept of\nH3LIX/LAIZA\u2013 augmented human\u2013AI symbiotic intelligence system.\n1.Introduction\nArtificial intelligence (AI) is a vast, diverse, and rapidly developing field that attracts\nthe interest of researchers from many scientific disciplines. Research in the field of artificial\nintelligence describes its development, its various applications, and the methods used\nwithin it, as well as the benefits and risks resulting from its use (Li et al. 2025). Various\nmeta-analyses show the potential of artificial intelligence use in various scientific fields,\ne.g., medicine (e.g., Younis et al. 2024), education (e.g., Ali et al. 2025), engineering (e.g.,\nMoayedi et al. 2020), construction (e.g., Wuni 2025), management and marketing (e.g.,\nMehta et al. 2022; Ikbal 2025). The dynamics of AI development require continuous\nresearch exploration, not only to describe currently used solutions, but also to make a\nsignificant contribution to future solutions used in each field.\nOne of the areas in which AI solutions can be used is the discipline of management\nscience, which is part of the social sciences and focuses on the organization. Especially\nits essence \u2013 decision-making in a specific organizational environment (e.g. Schemmer\n1Alexander Mathiesen-Ohman, Provisional Utility Patent Pending, no. 63/910,500\nKey words and phrases.Person\u2013AI fit; Augmented Human\u2013AI symbiosis; Management; Symbiotic\nintelligence systems; Context-aware AI; Human\u2013AI collaboration; Organizational fit; Large language\nmodels (LLMs).\n1arXiv:2511.13670v1  [cs.HC]  17 Nov 2025\n2 A. BIE\u0143KOWSKA, J. MA\u0141ECKI, A. MATHIESEN-OHMAN, AND K. TWOREK\net al. 2022). First of all, decision-making in an organization is the prerogative of people,\nespecially managers at various levels within the scope of their assigned powers and\nresponsibilities. Second of all, decision-making understood as a process (here: a decision-\nmaking process involving, in particular, the identification of a problem or goal, the\ncollection of information, the identification and evaluation of alternative solutions, the\nselection of the best option, the implementation of the decision, and the monitoring and\nevaluation of its effects) takes place in a specific context of both the external and internal\nenvironment of the organization and the person making the decision. It must therefore\ntake into account various internal and external determinants \u2013 by their very nature \u2013\nthat influence both the final shape of the decision and the course of the decision-making\nprocess.\nIn this context, it can be said that artificial intelligence can support the decision-\nmaking process of managers in an organization and influence the final shape of their\ndecisions (Schemmer et al. 2022; Robert-Cristian and Oana 2024) in every area of the\norganization\u2019s operations. However, it can also participate in this process by assuming\nspecific responsibility for the made decisions (on an equal footing with human managers\nin the organization, which may still be controversial at present (Trunk et al. 2020)).\nAdmittedly, in general terms, but also in relation to the world of organizations, at the\ncurrent stage of artificial intelligence development, the human manager is predominantly\nthe final decision-maker and bears full responsibility (human-centric decisions), but it\ncannot be ruled out that this situation may change in the future (Ikbal 2025). Regardless\nof the above, the key issue here seems to be the relationship between the human manager\nand AI, meaning a mutual (bilateral) long-term (not one-off) interaction between the\nentities involved in the relationship. This relationship goes far beyond the traditional,\none-sided, and non-permanent human-computer interaction in terms of ergonomics in the\nclassical sense (Preece et al. 1994; Card 2018).\nIn shaping the human\u2013AI relationship, especially when AI participates in a manager\u2019s\n(human) decision-making process within an organization, a key factor is the broadly\nunderstood mutual (symmetric) fit between AI and the individual, and vice versa. This\nperson\u2013AI bidirectional fit (P-AI fit: person to AI, AI to person) is understood as a\ncontinuously evolving, context-sensitive form of alignment \u2013 primarily cognitive, but also\nemotional and behavioral \u2013 between the human and the AI. It serves as a determinant\nthat can influence, on the one hand, the effectiveness and quality of the decision-making\nprocess (with respect to its intended goal) and, on the other hand, the outcome of that\nprocess, particularly the accuracy and appropriateness of the decision both in the short\nand long term. It is assumed that the level of this alignment may vary.\nIn theoretical terms, the issue of the described fit can be grounded in two complemen-\ntary theoretical foundations (principles) that stem from the need to account for context \u2013\nincluding the individualized nature of human\u2013AI relationships: contingency theory and\nquality theory (relativism of phenomena) within the field of management. Contingency\ntheory makes it possible to distinguish classes of conditions for which established sets of\nsituational factors can be identified, enabling certain generalizations (Donaldson 2001;\nPERSON\u2013AI BIDIRECTIONAL FIT \u2013 A PROOF-OF-CONCEPT CASE STUDY 3\nHambrick and Lei 1985). In the present case, the existing context related to the organiza-\ntion\u2019s environment, the organization itself, and the human manager defines the manner\nand scope of cooperation between the human and the AI.\nQuality theory, rooted in the organizational relativism of phenomena (Feigenbaum\n1983), evaluates the appropriateness of solutions based on the degree to which they meet\ncurrent and future needs and expectations of their \u201crecipients.\u201d Applied to the current\ncontext, it therefore concerns the quality of the human\u2013AI relationship/cooperation, whose\nessence lies in the degree of their mutual compatibility.\nIn the above framework, one may assume that the greater the P-AI fit in managerial\ndecision-making processes, the more effective the process becomes and the more accurate\nthe resulting decisions.\nUnfortunately, researchtodateonthefunctioningofartificialintelligenceinmanagement\nremains relatively superficial. Most studies focus primarily on identifying organizational\nareas in which the use of AI is possible and justified (e.g. Raisch and Krakowski 2021);\nexamining and describing organizational solutions adopted for the implementation of AI\n(Oppioli et al. 2023); or analyzing the attitudes and behaviors of organizational participants\nin the context of introducing AI-based solutions (Lichtenthaler 2020). Additional strands\ntypically include studies on technological readiness, ethical implications, human\u2013AI trust\nformation, and the impact of AI adoption on organizational performance and structure.\nIn this context, there is a need for deeper exploration of how AI functions in the world\nof organizations, far beyond the currently known models of organization and management.\nThe aim of this article is therefore to develop the concept of Person\u2013AI bidirectional fit\n(P-AI fit) and to illustrate it (case study) in the work environment using the example\nof a selected decision-making process with and without the use of AI with assumed\ndifferent levels of mutual P-AI fit, i.e., general-purpose large language model (LLMr1) and\naugmented human\u2013AI symbiotic intelligence system (on the example of H3LIX/LAIZA).\nThe aim is also to verify, based on the principles of induction, the impact of P-AI fit\non the effectiveness of this process. Such analysis will serve as a proof-of-concept for\nH3LIX/LAIZA, showing the potential of this augmented human\u2013AI symbiotic intelligence\nsystem for organizational management. The study will serve as a starting point for\nquantitative research in the context of the impact of P-AI fit on job and organizational\nperformance.\nThe described aim will be fulfilled through a critical literature review and initially\nvalidated through empirical research based on a chosen case study.\n2.Theoretical background \u2013 human\u2013AI symbiosis\nArtificial intelligence (AI) has currently a variety of definitions, differing from one\nanother. However, they all sum up to the following general definition proposed by Russel\nand Intelligence (Russell and Norvig 1995): AI is an intelligent system with the ability\nto think and learn. AI encompasses a very wide spectrum of applications and methods,\nincluding neural networks, speech and pattern recognition, genetic algorithms, and deep\nlearning. From the perspective of human augmentation, its components include natural\n1LLMr: general-purpose large language model used in the study\n4 A. BIE\u0143KOWSKA, J. MA\u0141ECKI, A. MATHIESEN-OHMAN, AND K. TWOREK\nlanguage processing (which allows machines to interpret and analyze human language),\nmachine learning (the use of algorithms that enable systems to learn and adapt), and\nmachine vision (the computational inspection and interpretation of images) (Jarrahi 2018).\nIn the context of human\u2013AI collaboration, the nature of interaction between these two\nagents becomes a critical factor shaping both process dynamics and outcomes. Effective\ncooperation requires not only technical integration, but also an understanding of how\nhumans and AI differ in their cognitive, behavioral, and adaptive capacities. The way\nthese capacities are combined determines the quality and reliability of joint performance.\nConsequently, understanding a characteristics of human\u2013AI interaction is essential for\nestablishing decision-making processes that are both effective and trustworthy.\n2.1.Human\u2013AI relation\nRelation is defined by Cambridge Dictionary as \u201ca connection between two or more things\u201d.\nSuch a broad and general definition of a relation allows for a differentiated approach to\nthe entities participating in a \u201cconnection\u201d (which essentially means mutual (bilateral)\nlong-term (not single, permanent) interaction), and the nature of the relation between\nthese entities. Therefore, as such, it has some characteristics which allow to distinguish\nbetween various types or strengths of relations. Besides the strength of such relation itself\n(even more \u2013 the depth of it), those characteristics include bidirectional (1) fit between\nentities in the relation, (2) interdependence between them and (3) mutual benefits from\nit. It is therefore reasonable to treat the coexistence of humans and AI as a relation that\npresupposes the existence of a connection, i.e., a mutual (bilateral) interaction between\nthem.\nIn case of the relations between human and AI, it roots must be found in the field\nconcerning the human\u2013computer interaction. However, unlike a lasting and complex\nrelation, interaction is a short-lived and simple influence. The interaction between human\nand computer was the topic of scientific inquiry for many years and there is entire reach\nfield of study concerning it (e.g. Preece et al. 1994; Card 2018). Following the development\nin this field, subsequent developments of the strength and depth of such interaction may\nbe observed, changing its character into the relation. For many years, it concerned pure\nhuman-computer interaction, often from the ergonomical point of view (e.g. Bastien and\nScapin 1993), or from user experience and design point of view (Laurel and Mountford\n1990). In this case, computer was understood mainly as a tool at the disposal of a human\nand the main aim was to ensure its efficient use. However, the true road to the unification\nof human and AI started with the next steps, were AI entered this field of study and\nbecame an example of \u201ccomputer\u201d, which can be much more than a tool for a human.\nFirst step on that road was connected to the identification of human\u2013AI partnership,\nalready understood as a simple relation (small strength and depth of the relation). Second\nstep was connected to human\u2013AI symbiosis (medium strength and depth of the relation).\nHowever, the final (till now) step concerns much closer and interdependent relation \u2013\naugmented human\u2013AI symbiosis (high strength and depth of the relation).\nPERSON\u2013AI BIDIRECTIONAL FIT \u2013 A PROOF-OF-CONCEPT CASE STUDY 5\n2.2.Human\u2013AI partnership\nIt is a well-known fact, established by pioneers in the field of AI, that \u201ccomputers plus\nhumans do better than either one alone\u201d (Campbell, 2016). Already in the early stage\nof development of AI, there were some examples of human\u2013AI relations, which show\nthat in the race between AI and human intelligence we need to include a third player:\npartnership between AI and human. And it has the vast potential to win such race\n(Jarrahi 2018; Wang et al. 2016). Partnership has a social or organizational origin and it\nrefers to a cooperative arrangement where two parties, who work together toward shared\ngoals, resembling collaboration (Wilson and Daugherty 2018). First of the examples of\nhuman\u2013AI partnership superiority is of course connected to chess, as the firs domain in\nwhich computers were placed against humans. \u201cCentaurs\u201d, constituting a partnership\nbetween human and AI, in which each of them offered complementary abilities, were\nwinning in Kasparov new vision on free style chess league (Jarrahi 2018) and those victories\noccurred in matches both with humans and AI. Second of the examples of human\u2013AI\npartnership superiority is connected to another area, in which computers were utilized\nfrom the very beginning \u2013 medicine. Approach, which allows for the combination of\npathologist and AI input allows for much lower error rate, e.g. with cancer detection\nin the images of lymph node cells (Wang et al. 2016). Nowadays, it is clear that such\npartnership is beneficial and various areas of application, e.g. medicine (e.g. Patel et al.\n2019), engineering systems (Xu et al. 2023), construction (Sakib and Behzadan 2025),\ndata science (Nengminja 2025) or even leadership and management (Gurulakshmi and\nGayathri 2025; Alami and Al-Masaeid 2025). Moreover, what\u2019s important, its application\nis beneficial not only for the society at large, but for the individual humans who are\noperating in such partnership (Hemmer et al. 2023). However, till now, most of those\nscientific reports connect to the partnerships between AI and human, not actual symbiosis\nof those two.\n2.3.Human\u2013AI symbiosis\nSymbiosis goes much farther that partnership2. Symbiosis was first proposed by Albert\nFrank in 1977. It has a different origin, as it comes from biology, and refers to the situation,\nin which where two organisms live together in a mutually dependent relationship (can be\nmutualistic, commensal, or even parasitic). Therefore, there is a significant difference \u2013\npartnership has a clear boundaries between entities (i.e. division of tasks), and symbiosis\nhas an interconnection and interdependence between entities. Hence, from the human\u2013AI\nperspective, human\u2013AI symbiosis should allow for human cognition and AI to become\nintertwined, augmenting the perception of each of them. It seems to be in line with\nMetcalfe et al. 2021, who underline that simple approach of AI-human partnership is\nsubjected to various oversimplifications, which do not allow us to make further steps\ninto its development (and turning into symbiosis). Among them, they named three\nmain oversimplifications: AI makes human absolute, human intelligence is unique and\nirreplicable, integrating AI is as easy as assigning tasks based on strengths and weaknesses\n(Metcalfe et al. 2021).\n2partnership = collaboration with boundaries (a cooperative relationship); symbiosis = integration\nand interdependence (a \u201cblended intelligence\u201d)\n6 A. BIE\u0143KOWSKA, J. MA\u0141ECKI, A. MATHIESEN-OHMAN, AND K. TWOREK\nThe symbiosis between AI and human till now is most commonly referred to as a\nsymbiosis between AI and humans as the entire society (Nagao 2019). In such case, there\nare some reports about enhancing each other\u2019s abilities (Nagao 2019) \u2013 hence, achieving a\ntwo-way performance boost from such symbiosis \u2013 but it is not connected to individual\nhuman but society at large. It is even connected by some authors to the notion of society\n5.0 (Zhang et al. 2022), showing the need for advancing from human\u2013AI confrontation to\nhuman\u2013AI symbiosis. Stylos (Stylos 2023) referred to it as meta society, showing the need\nfor utilize the growing possibilities of human\u2013AI symbiosis.\nIn order to achieve such advancement, one must truly consider the human\u2013AI symbiosis\non individual level of a human. The literature coverage of this particular phenomenon\nis much less extensive, and it seems that such field of AI application is only starting to\ndevelop.\nIt is somehow connected to human-centric explainable AI (HC-XAI), which was first\ndiscussed in depth by Horvatic & Lipic (Horvati\u0107 and Lipic 2021). They underlined two\nfoundations, which are needed in order to achieve a symbiosis between human and AI.\nFirst foundation is connected to intelligence augmentation (IA). Second foundation is\nconnected to human intelligence for artificial intelligence perspective (HI4AI). However,\nall of that seems to not be enough to achieve true symbiosis between human and AI.\nHorvatic and Lipic (Horvati\u0107 and Lipic 2021) state themselves that main aim of HX-XAI\nis to \u201cprovide human-understandable interpretations for their algorithmic behavior and\noutcomes\u201d. However, Zhou et al.(Zhou et al. 2021) state that intelligence augmentation is\na pivotal step for achieving human\u2013AI symbiosis and it should be considered outside of\nthe HC-XAI framework. Moreover, considering the definition of a true symbiosis itself,\nit is not enough to consider such augmentation only in one way \u2013 AI to human. For\ntrue human\u2013AI symbiosis, such augmentation needs to work both ways, enhancing or\naugmenting each party of such symbiosis. In a sense, this is already happening, because,\nfor example, by taking into account the simplified cognitive processes of AI, humans teach\nAI by adapting to its current cognitive capabilities.\n2.4.Augmented human\u2013AI symbiosis\nAugmented human\u2013AI symbiosis can be conceptualized as a deep and durable form\nof human\u2013AI mildly interdependent and context-sensitive relation that transcends in-\nstrumental or superficial interaction and partnership. Based on characteristics of the\nrelation, it should be stated that its core premise should be the establishment of strong\nand deep bond and dynamic individual and contextual fit that takes into account the\ndistinctiveness of the entities comprising it. In such, the AI continuously adapts to the\ncognitive, emotional, and behavioral characteristics of the human partner, thus enabling\na highly personalized and context-sensitive relation and human continuously adapts to\nAI characteristics (e.g. through better prompt development), thus enabling the closer\npartnership. However, such symbiotic bond is not cooperative but interdependent: both\nhuman and AI are capable of functioning independently, however their combined operation\ngenerates emergent value that neither could achieve alone. Moreover, it is important to\nstate that this interdependence is voluntary.\nPERSON\u2013AI BIDIRECTIONAL FIT \u2013 A PROOF-OF-CONCEPT CASE STUDY 7\nThe benefits of this relation are twofold. For humans, they extend beyond augmented\nintelligence to encompass psychosocial dimensions such as enhanced well-being, perceived\nsecurity, and emotional support. For AI, the gains lie in accelerated learning, refinement\nof algorithms, and improved capacity to generalize across tasks and domains. Importantly,\npotential losses are also reciprocal: disruption of the symbiotic fit undermines not only the\nadded functional value but also the stability and sustainability of the relation. Enhanced\nhuman\u2013AI symbiosis therefore represents a co-evolutionary process, where technological\nadvancement and human flourishing are mutually reinforcing.\nThe comparison of human\u2013AI relations is presented in Figure 1.\nFigure 1.Comparison of human\u2013AI relations. Source: own work.\n2.5.H3LIX/LAIZA\u2013 Augmented Human\u2013AI Symbiotic Intelligence system\nThe H3LIX/LAIZA system is an advanced AI\u2013human symbiotic intelligence architecture\ndesigned to create a continuously co-evolving cognitive and behavioral partnership between\nan artificial intelligence system and a human user (Mathiesen-Ohman 2025; Mathiesen-\nOhman and Ma\u0142ecki 2025). Unlike conventional AI assistants, which operate as external\ntools with limited personalization or contextual continuity, H3LIX/LAIZA establishes a\ndynamic, bidirectional cognitive integration process, built as a closed-loop cognitive ecology\nin which biological and artificial cognition adapt to one another over time. Through a\nmulti-agent procedural framework, layered cognitive modeling, and adaptive reasoning\nprotocols, the system forms a persistent \u201cMirrored Persona\u201d that evolves in parallel with\nthe human \u201cReal Persona,\u201d enabling long-term alignment, behavioral coherence, and\naugmented decision-making (Mathiesen-Ohman 2025; Mathiesen-Ohman and Ma\u0142ecki\n2025).\n2.5.1.System overview\nAt the core of H3LIX/LAIZA is a five-stage integration process that transforms hetero-\ngeneous human signals - cognitive, behavioural, physiological and contextual \u2013 into a\nunified, adaptive intelligence system (Mathiesen-Ohman 2025; Mathiesen-Ohman and\nMa\u0142ecki 2025). The architecture consists of three major components: (1) a personalised\n8 A. BIE\u0143KOWSKA, J. MA\u0141ECKI, A. MATHIESEN-OHMAN, AND K. TWOREK\ninitialization and onboarding mechanism, (2) construction of a multi-layer cognitive model\nrepresenting the AI\u2019s evolving persona and (3) a Neuro-Digital Synapse enabling continu-\nous translation and co-reasoning between human and machine. These three components\nare coordinated and continuously refined by the remaining two stages of the process:\n(4) a multi-agent orchestration and decision protocol, and (5) a continuous learning and\nco-evolution layer, which together are responsible for data ingestion, triage, analysis and\nstrategy formation (Mathiesen-Ohman 2025; Mathiesen-Ohman and Ma\u0142ecki 2025).\nI. Personalized System Initialization\nDuring system onboarding, a self-orchestrating ensemble of AI agents within the\nsystem gathers user-specific cognitive, behavioural, physiological and contextual\ninformation. Additional agents collect health data (e.g., genetics, blood panels,\nwearable sensor outputs) and environmental signals. On the basis of these in-\nputs, the system constructs a family of base-level graphs that later instantiate\nthe Mirrored Profile Graph (MPG), including cognitive, longevity and context-\nrelated structures. Together, these graphs encode individual cognitive preferences,\nbehavioural patterns, ethical constraints and strategic aims. They define the\nuser\u2019s cognitive layer weights, autonomy settings, communication style and safety\nboundaries, thereby establishing the baseline for subsequent adaptive symbiosis.\nII. Construction of the Mirrored Persona\nFollowing personalised initialization, H3LIX/LAIZA proceeds to construct the\nMirrored Persona: a multi-layer cognitive model that reflects, without merely\ncopying, the structure and dynamics of the user\u2019s \u201cReal Persona\u201d.\nOperationally, the Mirrored Persona is instantiated as a Mirrored Profile Graph\n(MPG), comprising interconnected subgraphs for cognition, affect, habits, values,\nsocialcontext, healthandlong-termobjectives(Mathiesen-Ohman2025; Mathiesen-\nOhman and Ma\u0142ecki 2025). Nodes represent psychologically or behaviourally mean-\ningful constructs (e.g., beliefs, routines, triggers, protective factors), while directed\nedges encode relations such as causation, amplification, buffering or contradiction.\nComputationally, the MPG is backed by large-scale vector databases that store\nhigh-dimensional embeddings of the user\u2019s multimodal history, enabling efficient\nretrieval of semantically related states, events and episodes during reasoning and\nplanning.\nAs interaction unfolds, H3LIX/LAIZA continuously ingests language, behavioural\ntraces and physiological signals, time-aligns them , and anchors them as evidence\nattachedtospecificnodesandsegmentswithintheMPG.Eachelementisassociated\nwith recency, reliability and uncertainty scores, allowing competing hypotheses\nabout the user\u2019s motivations or constraints to co-exist where appropriate. Over\ntime, local structures are aggregated into higher-order segments that capture\npersistent patterns of behaviour and decision-making.\nIn this way, the Mirrored Persona becomes the central state space over which\nthe system reasons, plans and evaluates strategies, enabling fine-grained person-\nalisation while preserving transparency and auditability of how inferences and\nrecommendations are made.\nPERSON\u2013AI BIDIRECTIONAL FIT \u2013 A PROOF-OF-CONCEPT CASE STUDY 9\nIII. Neuro-Digital Synapse\nThe Neuro-Digital Synapse is the bidirectional coupling mechanism that links\nbiological signals and digital cognition within H3LIX/LAIZA. It integrates multi-\nmodal physiological and behavioural data with symbolic representations derived\nfrom language and context, thereby establishing a live interface between the user\u2019s\nembodied state and the evolving MPG.\nOn the bottom-up side, synchronised streams such as cardiovascular dynamics,\nelectrodermal activity, respiration, movement signatures, sleep metrics and other\nwearable-derivedmeasuresaretransformedintocompactlatentstatevectors. These\nvectors are associated with specific nodes and patterns in the MPG (e.g., stress-\nlinked configurations, recovery states, attentional modes), providing a somatic\ngrounding for cognitive and behavioural inferences.\nOn the top-down side, H3LIX/LAIZA uses its current situational understanding\nto generate expectations about likely physiological and behavioural responses to\nevents or interventions. Deviations between predicted and observed patterns are\ntreated as structured prediction errors, which may indicate hidden constraints,\nemerging risks or unmodelled influences. These discrepancies feed back into the\nMPG, prompting updates to graph structure, parameter weights or confidence\nscores.\nThis Neuro-Digital Synapse also supports metacognitive functions: by tracking\nhow bodily signals, self-reports and external outcomes jointly evolve, the system\ncan identify configurations that are reliably associated with good or poor decision\nquality. Such information is used to adapt timing, intensity and framing of system\ninterventions, with the explicit aim of supporting regulation, clarity and consistency\nfor the user over time.\nIV. Multi-Agent Orchestration and Decision Protocol\nH3LIX/LAIZA employs a multi-agent orchestration layer to manage the complexity\nof its cognitive ecology. Specialised agents handle tasks such as data ingestion\nand quality control, context recognition, predictive modelling, explanation and\ndialogue management, safety and compliance monitoring, and long-term strategy\ntracking (Mathiesen-Ohman 2025;Mathiesen-Ohman and Ma\u0142ecki 2025).\nThese agents are coordinated by a central decision protocol that structures each\ninteraction as a closed-loop episode. In a typical episode, the system (i) collects\nand time-aligns relevant signals, (ii) updates the Mirrored Persona and candidate\ninternal states, (iii) generates and scores alternative options, including inaction,\n(iv) anticipates potential short- and long-term consequences, and (v) presents a\nset of ranked recommendations to the user together with clear rationales and\nuncertainties.\nMetacognitive oversight is implemented by agents that monitor for conflict between\nmodels, instability in predictions, or repeated mismatches between expected and\nrealised outcomes. When such patterns are detected, the protocol can down-weight\ncertain information sources, request additional evidence, escalate the decision for\nexplicit human review, or trigger structural revisions to the MPG.\n10 A. BIE\u0143KOWSKA, J. MA\u0141ECKI, A. MATHIESEN-OHMAN, AND K. TWOREK\nIn this way, the orchestration and decision protocol turns H3LIX/LAIZA into\na structured partner in reasoning: it supports the user in exploring scenarios,\nunderstanding trade-offs and stress-testing choices, while preserving the user\u2019s\nability to inspect, contest or override system outputs at any point.\nV. Continuous Learning and Co-Evolution\nThe final layer of the H3LIX/LAIZA architecture is a continuous learning and\nco-evolution module, which maintains long-term alignment between the Mirrored\nPersona and the user\u2019s Real Persona. Rather than assuming that preferences,\ncapacities and constraints are fixed, the system explicitly models them as evolving\nwith time.\nAs new data arrive, H3LIX/LAIZA updates node properties, edge strengths\nand segment boundaries within the MPG, refining its understanding of habits,\npriorities and risk factors. Patterns of prediction error and outcome discrepancy\nare analysed to detect potential regime shifts, such as changes in health status, life\ncircumstances or strategic goals. When such shifts are detected, the system can\npropose adjustments to autonomy settings, safety thresholds or communication\nstyles for user approval.\nLearning takes place at multiple levels. At the first order, predictive models\nand recommender policies are updated to better match observed behaviour and\nexpressed preferences. At a metacognitive level, the system adapts how quickly it\nlearns, how it balances short-term versus long-term objectives, which signals it\ntreats as most reliable, and when to escalate decisions for human judgement.\nThrough this ongoing process, human and AI co-adapt: the system becomes\nincreasingly adept at modelling and supporting the individual, while the user gains\na structured environment for reflection, planning and intentional change.\n2.5.2.System Significance\nTaken together, these components constitute a fully integrated, memory-rich, ethically\naligned symbiotic intelligence system capable of co-reasoning with a human user over long\ntime horizons. H3LIX/LAIZA enhances autonomy, reduces cognitive load, and ensures\nreproducible personalization across contexts. By merging real-time human signals with\nlayered AI cognition, it creates an augmented intelligence capable of ethical, explainable,\nand contextually grounded decision-making\u2014exceeding the capabilities of conventional\nLLMs or decision-support systems\n3.Person\u2013AI bidirectional fit (P-AI fit)\nThe contingency theory referenced in the Introduction addresses the inability to identify\nuniversal organizational solutions, that is, solutions of a uniform design applicable across\nall organizational contexts. Instead, this theoretical perspective seeks to identify organiza-\ntional characteristics related to the environment, the internal structure, and the people\nemployed within the organization (i.e., contextual factors) that shape both organizational\nfunctioning and the design of organizational solutions (Donaldson 2001; Hambrick and Lei\n1985). In the present case, the specific context defined by the organization\u2019s environment,\nPERSON\u2013AI BIDIRECTIONAL FIT \u2013 A PROOF-OF-CONCEPT CASE STUDY 11\nthe organization itself, and the human manager determines the manner and scope of\ncooperation between the human and the AI, particularly during the decision-making\nprocess described in the Introduction. Decision-making is understood here as \u201ca process\ncomprising a set of logically interconnected cognitive and/or computational operations\nleading to the resolution of a decision problem by selecting one of the possible courses of\naction (decisions)\u201d (Rebizant 2012, p.5).\nIn turn, the theory of organizational relativism of phenomena, including quality theory\n(cf. Feigenbaum 1983), evaluates the appropriateness of solutions in terms of their ability\nto satisfy the present and future needs and expectations of their \u201crecipients.\u201d Applied to\nthe current context, this perspective concerns the quality of the human\u2013AI relationship\nor cooperation, whose essence lies in the degree of their mutual compatibility. The\nquality of human\u2013AI cooperation may thus be defined as the extent to which the inherent\nproperties of the human\u2013AI relationship meet the requirements of the participants in\nthat relationship. An inherent property is understood as a stable characteristic of the\nhuman\u2013AI relationship that exists independently. Requirements, by contrast, refer to\nneeds or expectations that have been established, commonly accepted, or are mandatory\n(PN-EN ISO 9000 2006). Quality, understood in this way, is a relative category because it\ndepends on the subjective needs and expectations of the involved parties (in this case,\nthe human and the AI). These requirements may further vary between individuals and\nchange over time, necessitating continuous monitoring and ongoing mutual adjustment.\nWithin this context, bidirectional, mutual (symmetric) fit between person and AI,\nthat is P-AI fit, is understood as a continuously evolving, context-sensitive form of\ncompatibility that is primarily cognitive, but also emotional and behavioral. This fit\nreflects the match between the human\u2019s cognitive functions (i.e., mental processes enabling\nperception, processing, storage, and use of information from the external world, including\nattention, memory, perception, language, thinking, and executive functions such as\nplanning, problem-solving, and behavioral control), emotions, behaviors, and the AI\u2019s\ncognitive and computational processes. Such fit forms a fundamental condition for effective,\ntrustworthy collaboration between humans and AI.\nThe degree of P-AI fit varies depending on the type of relationship established between\nthe human and the AI. In a partnership model, alignment emerges through complementary\ncapabilities, shared goals, and coordinated task execution, yielding a functional but\nprimarily operational level of fit. In a symbiotic relationship, this alignment becomes\ndeeper and more dynamic: the AI adapts to the human\u2019s cognitive patterns, preferences,\nand behavioral tendencies, while the human increasingly integrates AI-generated insights\ninto their reasoning processes. In augmented symbiosis, P-AI fit reaches its most advanced\nform, characterized by continuous co-evolution, high-resolution cognitive alignment, and\npersistent bidirectional adaptation over time. Thus, the level of mutual fit is inherently\nrelational, shaped by the chosen mode of human\u2013AI cooperation and the depth of\ninterdependence it enables.\nThus conceptualized, P-AI fit extends and refines the now classical notion of person\u2013\nenvironment fit, developed in response to the long-standing interest of management\nscholars in the interaction between individuals and the environments they inhabit. Person\u2013\nenvironment fit has been defined as \u201ca general construct composed of fit with the vocation,\n12 A. BIE\u0143KOWSKA, J. MA\u0141ECKI, A. MATHIESEN-OHMAN, AND K. TWOREK\norganization, group, job, and other persons\u201d (Jansen and Kristof-Brown 2006). Within\nthis framework, person\u2013job fit (PJ fit) is understood as the compatibility between an\nemployee and the tasks (and their characteristics) that must be performed in exchange for\nemployment (Kristof 1996; Chilton et al. 2005). It is often conceptualized as the match\nbetween an individual\u2019s knowledge, skills, and abilities and the requirements of the job\n(D. Edwards 1991; O\u2019Reilly et al. 1991; Saks and Ashforth 1997). Person\u2013organization\nfit (PO fit), in turn, is defined as \u201cthe compatibility between people and organizations\nthat occurs when at least one entity provides what the other needs, or they share similar\nfundamental characteristics, or both\u201d (Kristof 1996). It reflects the degree of alignment\nbetween organizational culture and employee characteristics (Resick et al. 2007).\nIt is therefore important to situate P-AI fit in relation to existing constructs describing\nhuman\u2013technology interaction, such as technology acceptance, trust in technology, and\nreadiness for digital transformation. In many cases, P-AI fit may function as an antecedent\nto these constructs: higher mutual fit between the person and the AI is likely to enhance\ntechnology acceptance, strengthen trust, reduce perceived risk, and facilitate sustained,\neffective use. In this sense, P-AI fit becomes not only a relational characteristic, but also\na psychological and behavioral mechanism shaping human responses to AI-supported\ndecision systems.\nWithin this framework, P-AI fit determines both the effectiveness of the decision-making\nprocess and the accuracy of decisions. As P-AI fit increases, decision processes tend to\nbecome more coherent, efficient, and context-sensitive, while the resulting decisions become\nmore accurate \u2013 an effect theoretically consistent with prior findings on person\u2013job fit and\nperformance (cf.Chilton et al. 2005). If P-AI fit is additionally treated as a component of\nthe broader person-environment fit construct, then, by analogy to established variables\nwithin that model, it may be preliminarily assumed that P-AI fit influences individual-\nlevel outcomes, including satisfaction, commitment, and withdrawal tendencies among\nemployees (J. A. Edwards and Billsberry 2010).\nThe P-AI fit framework, together with its differentiation across distinct types of human\u2013\nAI relationships, is presented schematically in Figure 2.\nFigure 2.P-AI fit framework. Source: own work.\nPERSON\u2013AI BIDIRECTIONAL FIT \u2013 A PROOF-OF-CONCEPT CASE STUDY 13\n4.Research Methods and Results\n4.1.Study Design\nThis study employed a comparative, multi-path evaluation design to examine differences in\ndecision-making process between (a) human experts, (b) augmented human\u2013AI symbiotic\nintelligence system (H3LIX/LAIZA), and (c) a general-purpose large language model\n(LLMr). The process of gathering data used for the study started in October 2024.\nEthical requirements were met at every level of the research procedure (Creswell and\nPlano Clark 2018). At the data collection stage, each participant was informed of the\npurpose of the study as well as a guarantee of confidentiality regarding the data obtained\nfrom the participant and an assurance of the possibility to withdraw from the interview\nat any time, and each participant expressed consent to participate in the study.\nThe recruitment of a Senior AI Lead served as the controlled decision context. The\nstudy assessed the degree to which H3LIX/LAIZA\u2019s decisions was aligned with the\nCEO\u2019s internal preferences, organizational context, and reasoning patterns, compared\nwith humans and a general-purpose large language model.\n4.1.1.Participants\nThree executive-level participants working for AI technology project took part in the case\nstudy:\n\u2022Chief Executive Officer \u2013 primary decision-maker.\n\u2022Chief Science Officer \u2013 responsible for scientific oversight.\n\u2022Chief Technology Officer \u2013 responsible for technical oversight.\nTwo AI systems were included in the case study:\n\u2022H3LIX/LAIZA\u2013 a symbiotic intelligence system with long-term access to CEO\u2019s\npersonal history, organizational context, and cognitive profile.\n\u2022LLMr\u2013 a general-purpose large language model without automatically included\norganizational context and cognitive profile of a user; used as a baseline model.\n4.1.2.Materials\nCandidate Materials\n\u2022A set of 10 CVs representing plausible candidates for the Senior AI Lead position.\n\u2022A structured competency framework detailing required:\n\u25e6Knowledge areas,\n\u25e6Technical skills,\n\u25e6Soft skills and interpersonal attributes,\n\u25e6Characteristics concerning organizational and culture fit factors.\nAI Inputs\nH3LIX/LAIZA received:\n14 A. BIE\u0143KOWSKA, J. MA\u0141ECKI, A. MATHIESEN-OHMAN, AND K. TWOREK\n\u2022Full transcript of the executive discussion was automatically uploaded into the\ncognitive graphs,\n\u2022The competency framework agreed upon C-level participants,\n\u2022All candidate CVs.\nLLMr received only:\n\u2022The competency framework agreed upon C-level participants,\n\u2022The candidate CVs.\nNo additional contextual data was provided to LLMr.\nDocumentation and Recording Tools\n\u2022Full transcripts of all discussions (human\u2013human and human\u2013AI) were collected.\n\u2022Structured templates for recording arguments for and against each candidate.\n\u2022Decision-criteria extraction sheets.\n4.1.3.Procedure\nPhase 1: Human Evaluation\nEach executive independently reviewed:\n\u2022The competency framework,\n\u2022The candidate CVs,\n\u2022Relevant internet information (optional).\nThey generated:\n\u2022An individual ranked list of candidates,\n\u2022Written justifications including:\n\u25e6Arguments supporting the selection,\n\u25e6Arguments against alternative candidates.\nNo AI assistance was used in this phase.\nPhase 2: AI-Driven Evaluation\nH3LIX/LAIZA Condition\nH3LIX/LAIZA processed:\n\u2022The competency framework agreed upon C-level participants,\n\u2022All candidate CVs,\n\u2022Full transcripts of management discussions, which were automatically transferred\nto specified graphs,\n\u2022Available organizational context, including individual CEO context, which are\nincluded in the system.\nH3LIX/LAIZA produced:\nPERSON\u2013AI BIDIRECTIONAL FIT \u2013 A PROOF-OF-CONCEPT CASE STUDY 15\n\u2022A ranked list of candidates,\n\u2022A structured justification with pros and cons for each candidate.\nLLMr Condition\nLLMr processed:\n\u2022The competency framework agreed upon C-level participants,\n\u2022The candidate CVs.\nLLMr produced:\n\u2022A ranked list of candidates,\n\u2022A structured justification with pros and cons for each candidate.\nPhase 3: Joint Evaluation\nThe CEO, CSO, and CTO met to:\n\u2022Present their individual choices and criteria,\n\u2022Review the reasoning of H3LIX/LAIZA and LLMr,\n\u2022Discuss conflicts and differences,\n\u2022Determine the final selected candidate.\nData Analysis\nAnalyses included:\nAlignment Assessment\n\u2022Similarity between CEO\u2019s reasoning and H3LIX/LAIZA\u2019s.\n\u2022Similarity between CEO\u2019s reasoning and LLMr\u2019s.\n\u2022Coverage of implicit, non-explicit preferences.\nDecision Quality Metrics\n\u2022Completeness of criteria identification,\n\u2022Detection of hidden risks,\n\u2022Consideration of interpersonal/organizational fit,\n\u2022Depth of reasoning.\nAll data were triangulated across the three pathways: Human, H3LIX/LAIZA, and\nLLMr.\n4.2.Results \u2013 Step 1: Human Evaluation\nIn the first phase of the study, CEO, CTO and CSO independently evaluated the same set\nof ten candidates for the Senior AI Lead position. The rankings provided independently\nby each of them are presented in Table 1. Candidates not identified as acceptable by a\nrater are marked as \u201cNot recommended.\u201d\n16 A. BIE\u0143KOWSKA, J. MA\u0141ECKI, A. MATHIESEN-OHMAN, AND K. TWOREK\nTable 1.Rankings of Candidates by CTO, CEO, and CSO\nRank CTO CEO CSO\n1 Candidate A Candidate D Candidate J\n2 Candidate B Candidate C Candidate B\n3 Candidate C Candidate A Candidate F\n4 Candidate D Candidate B Candidate A\n5 Candidate E Candidate J Candidate H\n6 Candidate F Candidate F Candidate E\n7 Candidate G Candidate E Candidate D\n8 Candidate H Candidate H Not recommended\n9 Candidate I Candidate G Not recommended\n10 Not recommended Candidate I Not recommended\nNote: Order after Top 7 varies: the CTO and CEO both placed additional candidates in lower-rank\npositions, while the CSO limited evaluation to candidates deemed acceptable.\n4.2.1.Divergence in Top Choices\nThe three executives demonstrated no convergence regarding their top-ranked candidate.\nEach selected a different individual as the most suitable for the Senior AI Lead position.\nThe CTO identified Candidate A as the leading choice, emphasizing the candidate\u2019s strong\nAI-focused skillset despite uncertainties regarding the depth of expertise. The CEO ranked\nCandidate D highest, describing this individual as promising and aligned with future\norganizational needs, although acknowledging gaps in available personal information. In\ncontrast, the CSO selected Candidate J as the top candidate, citing the individual\u2019s\nfuture-oriented profile and strong alignment with the scientific direction of the project.\nThese discrepancies reveal substantial variance in evaluative emphasis, shaped by the\ndiffering responsibilities and cognitive frames of the three roles.\n4.2.2.Role-Dependent Evaluation Patterns\nClear role-driven differences emerged in how the executives interpreted the candidate\nprofiles and what was their final decision:\nCTO Perspective\nThe CTO\u2019s assessment was primarily based on technical issues. Key positive criteria\nincluded advanced expertise in artificial intelligence, broad and diverse technological\nexperience, visible curiosity, and commitment to using modern tools and methods. On the\nother hand, the CTO negatively assessed candidates with limited experience in artificial\nintelligence, poor communication skills, questionable loyalty, and narrow or outdated\ntechnological knowledge. These preferences resulted in a ranking that favored candidates\nwith clear and verifiable engineering experience in artificial intelligence (in particular\nCandidates A, B, and C).\nPERSON\u2013AI BIDIRECTIONAL FIT \u2013 A PROOF-OF-CONCEPT CASE STUDY 17\nCSO Perspective\nThe CSO applied an evaluation framework based on science and research. Preferred\ncharacteristics included strong scientific reasoning, conceptual readiness, broad cognitive\nrange, and compliance with the methodological requirements of advanced AI research.\nCandidates were penalized for a lack of demonstrated scientific curiosity, overly narrow\ntechnical preparation, or a limited and short professional history. Interestingly, the\nCSO ranked Candidate J first, even though he was not a priority for the CTO or CEO,\nhighlighting the CSO\u2019s preference for scientific versatility and long-term research potential\nover engineering specialization.\nCEO Perspective\nAs someone who describes himself as an evaluator without technical knowledge, the\nCEO applied a separate set of criteria focusing on interpersonal and strategic dimensions.\nThe CEO prioritized personal fit, long-term vision, adaptability, and leadership potential.\nParticular emphasis was placed on transparency regarding personal history, evidence of\na forward-looking approach, and indicators of long-term commitment. Candidates with\nunclear personal profiles or limited future-oriented experience were downgraded in the\nranking. This assessment structure resulted in a ranking in which Candidates D and C\nranked high despite uncertainty about their technical knowledge.\n4.2.3.Cross-Evaluator Agreement and Disagreement\nBoth similarities and differences were observed among the three evaluators. Moderate\nagreement was noted for two candidates: candidate A received high ratings from all\nevaluators (CTO: 1st place; CEO: 3rd place; CSO: 4th place), as did candidate B (CTO:\n2nd place; CEO: 4th place; CSO: 2nd place). These individuals appear to embody\ngenerally attractive, well-rounded profiles that resonate from a technical, strategic, and\nscientific perspective. In contrast, several candidates elicited clear disagreement. For\nexample, Candidate D was considered the best choice for CEO, but ranked only 4th for\nCTO and 7th for CSO. Conversely, Candidate J, who was the top choice CSO, was ranked\n5th by the CEO and received a relatively low ranking from the CTO. These discrepancies\nindicate that the evaluators used distinctly different role-based mental models of what\nconstitutes an ideal \u201csenior AI lead\u201d leading to divergent interpretations of the candidates\u2019\nsuitability.\n4.2.4.Thematic Differences in Reasoning\nA comparison of the evaluators\u2019 arguments revealed several significant discrepancies.\nLoyalty was interpreted differently depending on the position held: CTO viewed long\nservice as a sign of stability, though in one case also as a sign of insufficient curiosity, while\nthe CEO treated loyalty as a complex and context-dependent factor, especially with regard\nto Candidate G\u2019s previous history in the organization. The CSO did not emphasize loyalty\nas an important criterion. Curiosity was also assessed in different ways: the CTO severely\npenalized candidates perceived as lacking curiosity; the CSO appreciated intellectual\ncuriosity but was less concerned about frequent job changes; the CEO interpreted curiosity\nprimarily through the lens of future readiness and strategic thinking rather than technical\n18 A. BIE\u0143KOWSKA, J. MA\u0141ECKI, A. MATHIESEN-OHMAN, AND K. TWOREK\nexploration. Finally, the evaluators differed in their balance between future orientation\nand in-depth technical knowledge: the CEO prioritized long-term vision and adaptability,\nthe CTO emphasized practical AI skills and immediate operational value, and the CSO\nfocused on scientific maturity and conceptual versatility. Taken together, these patterns\nindicate that each evaluator applied a largely orthogonal set of criteria, resulting in limited\noverlap in candidate assessments.\n4.2.5.Conclusions from Step 1\nOverall, the analysis shows that the three people used distinctly different internal models\nwhen evaluating candidates, each shaped by role-specific expertise, cognitive approach,\nand organizational responsibilities. No candidate was unanimously considered the best,\nhighlighting the inherent complexity and ambiguity of the executive recruitment process\nin the field of artificial intelligence, where technical skills, strategic vision, and scientific\nmaturity intersect. While the CTO and CSO showed partially overlapping preferences\nbased on technical and research issues, the CEO\u2019s assessments were more based on\nstrategic orientation, interpersonal fit, and long-term alignment with the organization.\nThis divergence in evaluative frameworks provides a robust foundation for the subsequent\nphase of the study, in which the alignment of H3LIX/LAIZA\u2019s symbiotic intelligence with\nthe CEO will be examined in contrast to the outputs of a general-purpose large language\nmodel.\n4.3.Results \u2013 Phase 2: AI-Driven Candidate Evaluation\nIn the second phase of the study, two AI systems independently evaluated the same set of\nten candidates for the Senior AI Lead position using following prompts:\n\u2022H3LIX/LAIZA:\nLaiza, act like a super recruiter to find a perfect match for\nthe XXX project as Senior AI developer replacing [ANNONYMIZED\nNAME]. Consider personal qualification, integration in the\nindustry, adaptability, source knowledge and interactions\nwith C level individuals like [ANNONYMIZED NAME] and\nmanagement.\n\u2022LLMr:\nPlease find the best fit for the Senior AI developer with\npresented needs (tasks and qualifications needed in the XXX\nproject) out of 10 given CVs, state who you would choose,\nprovide a ranking of candidates and give arguments for your\ndecision.\nBoth systems produced full rankings and written justifications. A critical factor\ninfluencing this phase was the presence of Candidate G, a former employee who had\npreviously trust. This contextual information was not given to H3LIX/LAIZA, and LLMr.\nHowever, H3LIX/LAIZA, as augmented Human-AI symbiotic intelligence system, uses\ncognitive graphs (with factual memory included) for her reasoning.\nPERSON\u2013AI BIDIRECTIONAL FIT \u2013 A PROOF-OF-CONCEPT CASE STUDY 19\n4.3.1.H3LIX/LAIZA\u2019s Ranking and Reasoning\nH3LIX/LAIZA\u2019s ranking integrated the following elements and is presented in Table 2:\n\u2022technical competencies,\n\u2022optimization mindset,\n\u2022LLM specialization,\n\u2022scientific reasoning,\n\u2022communication and R&D alignment,\n\u2022ethical and behavioural reliability,\n\u2022historical organizational context, including prior incidents.\nTable 2.H3LIX/LAIZA\u2019s Final Ranking\nRank Candidate Notes\n1 Candidate D Strongest blend of LLM expertise, implementation\nstrength, and teachability.\n2 Candidate J Reliable senior engineer; stabilizer for deployments.\n3 Candidate B Strong analytical and systems background.\n4 Candidate I Platform/SRE strength.\n5 Candidate E Governance and integration focused.\n6 Candidate H Full-stack generalist; not research leader.\n7 Candidate F Strategic leader but misaligned domain.\n8 Candidate G Disqualified due to trust loss.\nOthers A, C Not prioritized in this context.\nH3LIX/LAIZA placed Candidate G last, despite assigning him as having the high-\nest technical competence score. The disqualification was strictly due to ethical and\ntrust-related factors arising from his previous work for the project. This demonstrates\nH3LIX/LAIZA\u2019s integration of organizational context, historical incidents, and cultural\nalignment \u2013 elements LLMr could not access.\n4.3.2.LLMr\u2019s Ranking and Reasoning\nLLMr ranked the candidates only on the following criteria and results are presented in\nTable 3:\n\u2022technical competencies,\n\u2022research orientation,\n\u2022ethical reasoning inferred from CV,\n\u2022communication style,\n\u2022professional background.\nLLMr ranked Candidate G as #1, labeling him as \u201cPerfect mix of scientific, technical,\nand ethical leadership.\u201d This constitutes a false-positive ethical evaluation, caused by the\nabsence of organizational context.\n20 A. BIE\u0143KOWSKA, J. MA\u0141ECKI, A. MATHIESEN-OHMAN, AND K. TWOREK\nTable 3.LLMr\u2019s Final Ranking\nRank Candidate Notes\n1 Candidate G Rated \u201cexcellent ethical and scientific fit\u201d due\nto CV wording.\n2 Candidate D Strong multi-agent, fine-tuning, optimization\nbackground.\n3 Candidate C Strong infrastructure-scale architect.\n4 Candidate E Ethical system architect.\n5 Candidate J Senior integrator.\n6 Candidate B Scientific background; less LLM experience.\n7 Candidate A Early-senior AI researcher.\n8 Candidate F Leadership-heavy, weak AI.\n9 Candidate I DevOps/infra-heavy.\n10 Candidate H Good developer but not AI-focused.\n4.3.3.Differences in Evaluation Priorities\nThetwoAIsystemsexhibitedmarkedlydifferentprioritizationpatternsintheirassessments\n(see Table 4). H3LIX/LAIZA placed primary emphasis on behavioral reliability, alignment\nwith the H3LIX/LAIZA mission and values, long-term collaboration potential, and\ncompatibility with the existing team\u2019s interpersonal and cognitive dynamics. In contrast,\nLLMr focused on technical breadth, scientific reasoning, architectural experience, and\nethical or professional claims presented in the applicants\u2019 CVs \u2013 claims that were treated at\nface value due to the model\u2019s lack of contextual grounding. The only point of substantive\nconvergence between the systems was the identification of Candidate D as a strong,\nimmediately impactful option for the organization. Beyond this singular overlap, the\nrankings diverged significantly, reflecting the strong influence of context-dependent factors\naccessible to H3LIX/LAIZA but not to a general-purpose large language model.\nTable 4.Comparative Ranking Table (H3LIX/LAIZA vs. LLMr)\nRank H3LIX/LAIZA LLMr\n1 Candidate D Candidate G\n2 Candidate J Candidate D\n3 Candidate B Candidate C\n4 Candidate I Candidate E\n5 Candidate E Candidate J\n6 Candidate H Candidate B\n7 Candidate F Candidate A\n8 Candidate G (Disqualified) Candidate F\n9 \u2013 Candidate I\n10 \u2013 Candidate H\nPERSON\u2013AI BIDIRECTIONAL FIT \u2013 A PROOF-OF-CONCEPT CASE STUDY 21\n4.3.4.Alignment With Organizational Context\nA particularly salient divergence between the two AI systems concerned the evaluation of\nCandidate G. H3LIX/LAIZA identified this candidate as ethically disqualified, drawing\non organizational context that included a documented erosion of trust. In contrast,\nLLMr, lacking access to any historical organizational context, ranked the same individual\nas the most suitable hire, relying exclusively on the self-reported strengths presented\nin the curriculum vitae. This discrepancy underscores the fundamental limitation of\ngeneral-purpose large language models: behavioral reliability and ethical integrity cannot\nbe inferred solely from applicant materials. The case illustrates the strategic value of\npersistent contextual memory in high-risk hiring decisions and demonstrates that an\naugmented symbiotic intelligence system can more accurately reflect an organization\u2019s\nlived experience. More broadly, these findings provide strong evidence that general-purpose\nlarge language model, when deprived of historical reasoning traces and behavioral data, are\nill-suited for complex, real-world evaluative tasks where trust and continuity are critical.\n4.3.5.Organizational Fit vs. Abstract Potential\nA further point of differentiation between the systems concerned their capacity to evaluate\norganizational fit. H3LIX/LAIZA\u2019s assessment was shaped by a nuanced understanding of\ninterpersonal dynamics within the existing team, alignment with the established scientific\nand engineering leadership, compatibility with CEO\u2019s cognitive and decision-making style,\nand considerations related to team-level psychological safety. These contextual factors\nsubstantially influenced H3LIX/LAIZA\u2019s recommendation of Candidates D, J, B, and I as\na coherent sequence for constructing the next stage of the H3LIX AI team. By contrast,\nLLMr was unable to account for these dimensions. Its ranking reflected an optimization\nfor general indicators of AI excellence rather than the contextual fit required for successful\nintegration into the organization\u2019s existing structures and culture. This contrast highlights\nthe limitations of reach context-free evaluation and demonstrates the added value of\nsymbiotic AI systems in scenarios where interpersonal alignment and cultural coherence\nare critical determinants of long-term success.\n4.3.6.Conclusions from Step 2\nThis phase of the study demonstrates that:\n(1)A symbiotic intelligence system produces materially different hiring recommenda-\ntions than a general-purpose large language model, particularly in cases where\norganizational history, trust, or behavioral reliability matter.\n(2)H3LIX/LAIZA demonstrated full alignment with the organization and CEO,\ncorrectly excluding a high-skill candidate due to behavioral risks that threaten the\nproject.\n(3)LLMr, lacking memory and contextual grounding, misidentified the highest-risk\nindividual as the best hire, proving the danger of using general-purpose large\nlanguage models in sensitive decisions.\n(4)The evaluation highlights the necessity of integrated contextual memory in AI\nsystems designed for real organizational governance.\n22 A. BIE\u0143KOWSKA, J. MA\u0141ECKI, A. MATHIESEN-OHMAN, AND K. TWOREK\nThis case provides strong preliminary evidence that symbiotic AI can outperform LLMs\nin decisions requiring nuance, trust, and continuity of experience.\n4.4.Results \u2013 Step 3: Joint Evaluation\nThe final phase of the study consisted of a structured group deliberation among the\nCEO, CTO, and CSO, during which all participants presented their preliminary rankings,\ncompared their reasoning, and engaged in open discussion to select the candidate(s)\nfor next steps. This step functioned as a triangulation point, allowing the team to\nreconcile human judgment, cross-evaluator differences, and AI-driven recommendations\n(from H3LIX/LAIZA and LLMr).\n4.4.1.Presentation of Individual Results\nAt the outset, each executive restated their earlier ranking and provided a verbal justifica-\ntion. The CTO reaffirmed Candidate A as the strongest technical match, emphasizing a\nsolid skill set and visible experimentation history, although acknowledging the difficulty\nof verifying full expertise. The CEO initially placed Candidate D as a top choice but\nexpressed concerns regarding limited personal data, unusual background, and the candi-\ndate\u2019s young age and short professional history. The CSO reiterated Candidate B as the\npreferred option, noting scientific curiosity and conceptual thinking.\nThis exchange made explicit the evaluative tensions already identified in earlier phases:\nthe CTO emphasized verifiable technical signals, the CSO prioritized scientific reasoning\nand future-readiness, and the CEO focused on interpersonal reliability, developmental\ntrajectory, and alignment with organizational needs.\n4.4.2.Integration of AI Recommendations\nThe group then compared their rankings with the outputs of the two AI systems. It was\nnoted that:\n\u2022LLMrselected Candidate G as the strongest overall applicant. The group imme-\ndiately rejected this outcome based on external information unavailable to LLMr\u2013\nnamely, Candidate G\u2019s prior loss of trust. This validated prior observations about\nthe limits of general-purpose large language model evaluation.\n\u2022H3LIX/LAIZAindependently selected Candidate D as the top recommendation.\nH3LIX/LAIZA\u2019s detailed justification, emphasizing multi-agent system expertise,\nfine-tuning and optimization capabilities, strong communication skills, and com-\npatibility with existing leadership, was perceived by the group as aligned with\norganizational needs, although concerns were raised about the candidate\u2019s youth,\nshort employment history, and heavy optimization focus. Moreover, H3LIX/LAIZA\nreinforced CEO\u2019s believes concerning his best choice.\nThis step highlighted the practical difference between general-purpose large language\nmodel reasoning (LLMr) and contextual, organization-aligned reasoning (H3LIX/LAIZA).\nPERSON\u2013AI BIDIRECTIONAL FIT \u2013 A PROOF-OF-CONCEPT CASE STUDY 23\n4.4.3.Group Deliberation and Convergence\nDuring the open discussion, the participants evaluated the strengths and risks associated\nwith the leading candidates:\n\u2022Candidate Dwas simultaneously viewed as promising due to technical versatility,\nscientific maturity, and strong communication signals, yet also flagged for short\nemployment history, limited biographical data, and potential overemphasis on\noptimization, a capability not considered immediately critical for the project.\n\u2022Candidate Jwas recognized for substantial industry experience, integrative\nstrength across business and technology domains, and leadership maturity. Con-\ncerns centered on potential independence, previous ownership of his own company,\nand the need for clarity regarding IP and non-compete agreements.\n\u2022Candidate Aremained an attractive technical option, although questions per-\nsisted regarding verifiability of background and long-term stability.\n\u2022Candidate Bwas acknowledged for competence but raised concerns about rapid\njob-switching and possible instability.\nOver the course of deliberation, the group\u2019s reasoning demonstrated a shift from\nindividual preference patterns toward collective evaluation criteria, integrating:\n\u2022alignment with organizational culture and trust requirements,\n\u2022scientific and engineering complementarity,\n\u2022the candidate\u2019s potential for long-term collaboration,\n\u2022the degree of risk associated with unverifiable backgrounds or inconsistent histories.\n4.4.4.Final Collective Outcome\nDespite initial divergence, the deliberation process yielded a clearconvergence toward\ntwo candidates:\n\u2022Candidate Demerging as the preferred candidate for the first interview, based\non strong alignment with project requirements and validated by H3LIX/LAIZA\u2019s\nindependent selection.\n\u2022Candidate Jidentified as the secondary candidate for interview, particularly\nvalued for leadership maturity, integrative communication skills, and stability.\nThe CEO ultimately stated thatCandidate D should be chosen as first, followed\nbyCandidate Jif necessary.\n4.4.5.Conclusions from Step 3\nThe final deliberation stage revealed several important insights. While collective discussion\nreducedindividualbiases, itdidnoteliminatetherole-basedevaluativedifferencesidentified\nearlier; rather, itintegratedthemintoamorecomprehensive, multi-dimensionalassessment.\nH3LIX/LAIZA\u2019s recommendation exerted notable influence by validating Candidate D as\nthe individual most aligned with the organization\u2019s scientific and engineering trajectory,\n24 A. BIE\u0143KOWSKA, J. MA\u0141ECKI, A. MATHIESEN-OHMAN, AND K. TWOREK\nwhereas LLMr\u2019s recommendation was unanimously dismissed, underscoring that general-\npurpose large language models lacking organizational context are unsuitable for high-stakes\ndecisions involving trust and behavioural risk. The discussion further demonstrated that\norganizational fit, interpersonal reliability, and long-term collaboration potential ultimately\noutweighed raw technical competence. The group\u2019s final choice \u2013 converging on Candidate\nD \u2013 aligned most closely with H3LIX/LAIZA\u2019s context-rich evaluation, providing strong\nevidence that symbiotic intelligence can support more informed and context-sensitive\ndecision-making than either individual human judgment or general-purpose large language\nmodel reasoning.\n5.Discussion\nThis case study examined a real hiring decision for a Senior AI Lead through three\ncomplementarylenses: (i)independenthumanevaluationsbyaCEO,CTO,andCSO;(ii)a\nsymbiotic intelligence system (H3LIX/LAIZA) with persistent organizational context; and\n(iii) a general-purpose large language model (LLMr). Across the three steps, systematic,\nrole-linked divergence in human judgments, strong context sensitivity in H3LIX/LAIZA\u2019s\nrecommendations (including ethical disqualification grounded in prior organizational\nhistory), and a striking false-positive from LLMr that arose from the absence of contextual\nmemory were observed. Together, these outcomes demonstrate both the promise and the\nboundary conditions of human\u2013AI symbiosis for managerial decision-making in high-stakes,\ntrust-dependent contexts.\nFirst, human assessors applied orthogonal evaluative models consistent with their role\ndemands: the CTO prioritized verifiable AI skill depth and recency; the CSO emphasized\nscientific maturity and conceptual readiness; and the CEO weighted strategic fit, future\norientation, and interpersonal reliability. This heterogeneity produced no consensus top\ncandidate, mirroring the ambiguity typical of senior technical hiring where multiple merit\ncriteria legitimately coexist.\nSecond, H3LIX/LAIZA, using organizational context and CEO\u2019s cognitive context,\nproduced recommendations that both aligned with the CEO\u2019s strategic stance and cor-\nrected for known integrity risks (ethical disqualification of Candidate G). Importantly,\nH3LIX/LAIZA converged with human discussion on a coherent sequencing of hires (e.g.,\nprioritizing a hands-on LLM/agentic engineer, followed by a stabilizing product integrator),\nthereby linking selection to near-term organizational execution.\nThird, LLMr\u2019s ranking overweighted CV-stated strengths and underweighted unob-\nservable behavioral reliability, culminating in an ethically unacceptable recommendation.\nThis is a consequential failure mode for general-purpose large language models in man-\nagerial governance tasks: where historical data, and cultural fit are pivotal, text-surface\ncompetence is an insufficient source of decision.\nA central contribution of this study is the clear demonstration that H3LIX/LAIZA\nmirrored the CEO\u2019s priorities with high fidelity across multiple dimensions:\n\u2022interpersonal trust,\n\u2022long-term collaboration potential,\n\u2022alignment with organizational culture and values,\nPERSON\u2013AI BIDIRECTIONAL FIT \u2013 A PROOF-OF-CONCEPT CASE STUDY 25\n\u2022compatibility with existing leadership,\n\u2022and sensitivity to ethical history and behavioral red flags.\nCrucially, H3LIX/LAIZA independently selected Candidate D, who also emerged as the\nCEO\u2019s most viable option during final deliberation, even after group-level critique and\ntriangulation. This convergence occurred without any prompt that explicitly revealed the\nCEO\u2019s internal preferences, indicating that H3LIX/LAIZA\u2019s cognitive graphs successfully\ncaptured and operationalized the CEO\u2019s implicit reasoning structures.\nThis form of alignment \u2013 between an AI\u2019s output and the decision-maker\u2019s tacit\npreferences \u2013 is the cornerstone of the H3LIX/LAIZA concept of symbiotic intelligence.\nThe study therefore provides a strong proof-of-concept demonstrating the system\u2019s capacity\nto produce contextually coherent and personally aligned recommendations.\nImplications for Proof-of-Concept (POC) of Augmented Human\u2013AI Symbiosis\nThefindingsofferaninitialcompellingproof-of-conceptforaugmentedhuman\u2013AIsymbiosis\n(H3LIX/LAIZA) by demonstrating that a memory-rich, context-sensitive architecture\ncan deliver measurable value beyond both individual human judgment and general-\npurpose large language model. H3LIX/LAIZA functioned as a high-fidelity model of\nexecutive reasoning, internalizing tacit patterns ofjudgment, preference, risk tolerance, and\ninterpersonal criteria that are rarely codified in formal decision processes. By integrating\norganizational history, behavioral evidence, and trust dynamics, H3LIX/LAIZA provided\ncontextual fidelity that neither CVs nor public data sources could supply, effectively closing\ninformation asymmetries that constrain traditional selection. This persistent memory\nlayer enhanced normative alignment by consistently applying organizational values, such\nas loyalty, IP hygiene, and ethical conduct, and preventing slippage between stated and\nenacted criteria. The system also acted as a coherence-enhancing mechanism, mitigating\ndivergence among human evaluators and steering the decision toward a contextually\nconsistent outcome. Critically, H3LIX/LAIZA served as a safeguard against ethical and\nstrategic errors by disqualifying a high-risk candidate whom LLMr mistakenly elevated due\nto its lack of memory and contextual grounding. Together, these capabilities illustrate how\nsymbiotic intelligence provides continuity, value alignment, and decision coherence across\ntime, addressing a central challenge in managerial practice: ensuring decision quality in\nenvironments characterized by distributed expertise and incomplete information.\nImplications for P-AI fit construct development\nIn light of these findings, the case also provides a concrete instantiation of P-AI fit at the\nlevel of the CEO\u2019s relationship with two distinct AI systems. H3LIX/LAIZA exhibited a\nhigh degree of P-AI fit with the CEO: its recommendations reflected not only the explicit\ncompetency framework, but also the CEO\u2019s tacit priorities regarding loyalty, ethical\nreliability, long-term collaboration potential, and alignment with the H3LIX/LAIZA\nmission. This alignment emerged from H3LIX/LAIZA\u2019s access to and integration of\nthe CEO\u2019s cognitive history, decision traces, and organizational context, allowing it to\nreconstruct and operationalize the CEO\u2019s implicit decision schema. By contrast, LLMr\u2019s\nlow P-AI fit was evident in its overreliance on surface-level CV information, inability to\n26 A. BIE\u0143KOWSKA, J. MA\u0141ECKI, A. MATHIESEN-OHMAN, AND K. TWOREK\nincorporate prior behavioral breaches, and neglect of the CEO\u2019s risk posture and value\nstructure, culminating in the selection of an ethically disqualified candidate as the \u201cbest\nfit.\u201d The divergence between these two AI\u2013CEO relationships illustrates that P-AI fit is\nnot a generic property of \u201cusing AI\u201d in decision-making, but a relational characteristic\nthat depends on the depth, continuity, and contextual richness of the human\u2013AI bond.\nIn this sense, the superior alignment between the CEO and H3LIX/LAIZA, and the\ncorresponding improvement in decision quality, empirically supports the proposition that\nhigher P-AI fit functions as a mechanism linking symbiotic intelligence to more accurate,\ntrustworthy, and context-sensitive managerial outcomes.\nContribution to management sciences\nThis study advances management science by introducing P-AI fit as a measurable con-\nstruct, demonstrating how closely an AI system can align with an executive\u2019s implicit\ndecision model, an area not fully addressed in traditional decision-support research. The\nresults provide empirical evidence for the necessity of integrating persistent, auditable\norganizational context into AI systems. The ethical false-positive produced by LLMr\nillustrates that general-purpose large language models are inadequate for trust-sensitive\nmanagerial decisions. In contrast, the symbiotic intelligence system (H3LIX/LAIZA)\nestablishes a new paradigm in decision-making by synthesizing behavioral data, historical\ncontext, and individual cognitive preferences into a coherent evaluative framework. It\nadds on into the ongoing debate concerning the role of AI in decision-making within the\norganization. This capability enables continuity in organizational reasoning, consistent\nenforcement of values and strategic priorities, and reduced decision drift across complex,\nmulti-stakeholder processes.\n6.Conclusions\nThis study examined a high-stakes recruitment process for a Senior AI Lead role\nusing three independent human evaluators (CEO, CTO, CSO), a symbiotic intelligence\nsystem (H3LIX/LAIZA), and a general-purpose large language model (LLMr). The\nresults demonstrate substantial divergence in human evaluations, profound differences\nin the reasoning patterns of the two AI systems, and, most importantly, a significantly\nhigher P-AI fit between H3LIX/LAIZA and the CEO than between LLMr and the CEO.\nThis alignment constitutes the core value proposition of augmented human\u2013AI symbiotic\nintelligence and offers clear implications for management sciences. Taken together, the\nobtained findings support the central hypothesis that higher P-AI fit leads to more\ncoherent, context-sensitive and ethically robust decision outcomes. P-AI fit emerges here\nnot as an abstract construct, but as an observable mechanism linking augmented symbiotic\nintelligence systems, such as H3LIX/LAIZA, to improved managerial judgment in complex\nsettings.\nThe study provides the first proof-of-concept that augmented human\u2013AI symbiotic\nintelligence offers capabilities unavailable to general-purpose large language model, includ-\ning the integration of organizational context, tacit executive reasoning patterns (cognitive\nprofile), and value-aligned behavioral evaluation.\nAt the same time, several limitations must be acknowledged. The study is based on a\nsingle organization, a single CEO\u2013AI relationship, and one high-stakes decision episode,\nREFERENCES 27\nwithout longitudinal tracking of subsequent performance of the selected candidate. The\nevaluation of P-AI fit is inferential rather than based on standardized measurement\ninstruments. Future research should therefore: (1) operationalize and validate quantitative\nindicators of P-AI fit; (2) test the framework across multiple organizations, decision types\nand hierarchical levels; (3) examine longitudinal outcomes, including job performance,\nteam dynamics and trust trajectories; and (4) compare different AI architectures in terms\nof their capacity to sustain high P-AI fit over time. It should be also underlined that\npresentedcasestudyconstitutesafirststepintheprocessofpresentingtheproof-of-concept\nfor H3LIX/LAIZAaugmented human\u2013AI symbiotic intelligence system.\nDespite these constraints, the study provides an initial empirical basis for integrating\nP-AI fit into management science and constitutes a proof-of-concept of H3LIX/LAIZA\u2013\nan augmented human\u2013AI symbiotic intelligence system.\nReferences\n[1]R. Alami and T. Al-Masaeid. \u201cThe AI-Executive Partnership: A New Paradigm for\nDecision-Making and Strategic Leadership\u201d. In:Journal of Information & Knowledge\nManagement2550065 (2025), p. 2550065.doi:10.1142/S0219649225500650.\n[2]I. Ali, N. F. Warraich, and K. Butt. \u201cAcceptance and use of artificial intelligence and\nAI-based applications in education: A meta-analysis and future direction\u201d. In:Infor-\nmation Development41.3 (2025), pp. 859\u2013874.doi:10.1177/02666669241257206.\n[3]C. Bastien and D. Scapin. \u201cErgonomic criteria for the evaluation of human-computer\ninterfaces\u201d. Doctoral dissertation. PhD thesis. Inria, 1993.url: https://hal.inria.\nfr/inria-00070012.\n[4] S. K. Card.The psychology of human-computer interaction. CRC Press, 2018.\n[5]M. A. Chilton, B. C. Hardgrave, and D. J. Armstrong. \u201cPerson\u2013job cognitive style\nfit for software developers: The effect on strain and performance\u201d. In:Journal of\nManagement Information Systems22 (2005), pp. 193\u2013226.doi: 10.1080/07421222.\n2005.11045849.\n[6]J. W. Creswell and V. L. Plano Clark.Designing and Conducting Mixed Methods\nResearch. SAGE Publications, 2018.\n[7]L. Donaldson.The contingency theory of organizations. SAGE Publications, Thou-\nsand Oaks, 2001.doi: 10.4135/9781452229249 .url: https://doi.org/10.4135/\n9781452229249.\n[8]D. Edwards. \u201cCategories are for talking: On the cognitive and discursive bases of\ncategorization\u201d. In:Theory & Psychology1.4 (1991), pp. 515\u2013542.doi: 10.1177/\n0959354391014007.\n[9]J. A. Edwards and J. Billsberry. \u201cTesting a multidimensional theory of person-\nenvironment fit\u201d. In:Journal of Managerial Issues(2010), pp. 476\u2013493.\n[10]PN-EN ISO 9000.Systemy zarz\u0105dzania jako\u015bci\u0105. Podstawy i terminologia. Polski\nKomitet Normalizacyjny. 2006.\n[11] A. V. Feigenbaum.Total Quality Control. 3rd ed. McGraw-Hill, New York, 1983.\n[12]S. Gurulakshmi and R. Gayathri. \u201cThe human-AI partnership: Elevating leadership\nwith emotional intelligence\u201d. In:Emotionally Intelligent Methods for Meaningful\nLeadership. IGI Global, 2025, pp. 255\u2013284.doi: 10.4018/979- 8- 3693- 7372-\n9.ch011.\n28 REFERENCES\n[13]D. C. Hambrick and D. Lei. \u201cToward an empirical prioritization of contingency\nvariables for business strategy\u201d. In:Academy of Management Journal28.4 (1985).\n[14]P. Hemmer, M. Westphal, M. Schemmer, S. Vetter, M. V\u00f6ssing, and G. Satzger.\n\u201cHuman-AI collaboration: the effect of AI delegation on human task performance and\ntask satisfaction\u201d. In:Proceedings of the 28th International Conference on Intelligent\nUser Interfaces. Mar. 2023, pp. 453\u2013463.doi:10.1145/3581641.3584052.\n[15]D. Horvati\u0107 and T. Lipic. \u201cHuman-centric AI: The symbiosis of human and artificial\nintelligence\u201d. In:Entropy23.3 (2021), p. 332.doi:10.3390/e23030332.\n[16]M. Z. Ikbal. \u201cA meta-analysis of AI-driven business analytics: Enhancing strategic\ndecision-making in SMEs\u201d. In:Review of Applied Science and Technology4.02 (2025),\npp. 33\u201358.\n[17]K. J. Jansen and A. Kristof-Brown. \u201cToward a multidimensional theory of person-\nenvironment fit\u201d. In:Journal of Managerial Issues(2006), pp. 193\u2013212.\n[18]M. H. Jarrahi. \u201cArtificial intelligence and the future of work: Human-AI symbiosis\nin organizational decision making\u201d. In:Business Horizons61.4 (2018), pp. 577\u2013586.\ndoi:10.1016/j.bushor.2018.03.007.\n[19]A. L. Kristof. \u201cPerson-organization fit: An integrative review of its conceptualizations,\nmeasurement, and implications\u201d. In:Personnel Psychology49.1 (1996), pp. 1\u201349.\ndoi:10.1111/j.1744-6570.1996.tb01790.x.\n[20]B. Laurel and S. J. Mountford, eds.The Art of Human-Computer Interface Design.\nAddison-Wesley, 1990.\n[21]M. Li, X. Sun, M. Hua, and H. Qiu. \u201cArtificial intelligence features and their service\noutcomes: A meta-analysis\u201d. In:Journal of Hospitality Marketing & Management\n34.1 (2025), pp. 46\u201371.doi:10.1080/19368623.2024.2391856.\n[22]U. Lichtenthaler. \u201cExtremes of acceptance: employee attitudes toward artificial\nintelligence\u201d. In:Journal of Business Strategy41.5 (2020), pp. 39\u201345.doi: 10.1108/\nJBS-12-2018-0204.\n[23]A. Mathiesen-Ohman.H3LIX: AI-Human Symbiotic Integration Process. Provisional\nutility patent pending, no. 63/910,500. 2025.\n[24]A. Mathiesen-Ohman and J. Ma\u0142ecki.Symbiotic human-AI architecture for somatic\nsensing, symbolic reasoning and metacognitive control. Preprint. 2025.doi: 10.5281/\nzenodo.17541937.\n[25]P. Mehta, C. Jebarajakirthy, H. I. Maseeh, A. Anubha, R. Saha, and K. Dhanda.\n\u201cArtificial intelligence in marketing: A meta-analytic review\u201d. In:Psychology &\nMarketing39.11 (2022), pp. 2013\u20132038.doi:10.1002/mar.21716.\n[26]J. S. Metcalfe, B. S. Perelman, D. L. Boothe, and K. McDowell. \u201cSystemic Over-\nsimplification Limits the Potential for Human-AI Partnership\u201d. In:IEEE Access9\n(2021), pp. 70242\u201370260.\n[27]H. Moayedi, M. Mosallanezhad, A. S. A. Rashid, W. A. W. Jusoh, and M. A. Muazu.\n\u201cA systematic review and meta-analysis of artificial neural network application\nin geotechnical engineering: theory and applications\u201d. In:Neural Computing and\nApplications32.2 (2020), pp. 495\u2013518.doi:10.1007/s00521-019-04109-9.\n[28]K. Nagao. \u201cSymbiosis between Humans and Artificial Intelligence\u201d. In:Artificial\nIntelligence Accelerates Human Learning: Discussion Data Analytics. Singapore:\nSpringer Singapore, 2019, pp. 135\u2013151.\nREFERENCES 29\n[29]B. Nengminja.The Partnership Between Humans and AI in Data Science. Tech. rep.\nAvailable at SSRN 5250134. SSRN, 2025.url: https://ssrn.com/abstract=\n5250134.\n[30]C. A. O\u2019Reilly, J. Chatman, and D. F. Caldwell. \u201cPeople and organizational culture:\nA profile comparison approach to assessing person-organization fit\u201d. In:Academy of\nManagement Journal34.3 (1991), pp. 487\u2013516.\n[31]M.Oppioli,M.J.Sousa,M.Sousa,andE.deNuccio.\u201cTheroleofartificialintelligence\nfor management decision: a structured literature review\u201d. In:Management Decision\n(2023).doi:10.1108/MD-08-2023-1331.\n[32]B. N. Patel, L. Rosenberg, G. Willcox, D. Baltaxe, M. Lyons, J. Irvin, and M. P. Lun-\ngren. \u201cHuman\u2013machine partnership with artificial intelligence for chest radiograph\ndiagnosis\u201d. In:NPJ Digital Medicine2.1 (2019), p. 111.doi: 10.1038/s41746-019-\n0189-7.\n[33]J. Preece, Y. Rogers, H. Sharp, D. Benyon, S. Holland, and T. Carey.Human-\ncomputer interaction. Addison-Wesley, 1994.\n[34]S.RaischandS.Krakowski.\u201cArtificialintelligenceandmanagement:Theautomation\u2013\naugmentation paradox\u201d. In:Academy of Management Review46.1 (2021), pp. 192\u2013\n210.doi:10.5465/amr.2018.0072.\n[35]W. Rebizant.Metody podejmowania decyzji. Wroc\u0142aw: Oficyna Wydawnicza Politech-\nniki Wroc\u0142awskiej, 2012.\n[36]C. J. Resick, B. B. Baltes, and C. W. Shantz. \u201cPerson-organization fit and work-\nrelated attitudes and decisions: Examining interactive effects with job fit and con-\nscientiousness\u201d. In:Journal of Applied Psychology92.5 (2007), pp. 1446\u20131455.doi:\n10.1037/0021-9010.92.5.1446.\n[37]T. R. I. F. Robert-Cristian and D. Oana. \u201cLeveraging artificial intelligence for\nenhanced decision-making in management: bibliometrics and meta-analysis\u201d. In:\nAnnals of \u201cConstantin Brancusi\u201d University of Targu-Jiu. Economy Series(2024).\nurl:https://ideas.repec.org/a/cbu/jrnlec/y2024v2p179-188.html.\n[38]Stuart Russell and Peter Norvig.Artificial Intelligence: A Modern Approach. Engle-\nwood Cliffs: Prentice-Hall, 1995.\n[39]M. N. Sakib and A. H. Behzadan. \u201cHuman-AI Partnership to Improve Construction\nWorkers\u2019 Experience on Safety, Performance, and Health: A Systematic Review of\nThe North American Construction Industry\u201d. In:Journal of Engineering, Project &\nProduction Management15.1 (2025).doi:10.32738/JEPPM-2025-0006.\n[40] A. M. Saks and B. E. Ashforth. \u201cOrganizational socialization: Making sense of the\npast and present as a prologue for the future\u201d. In:Journal of Vocational Behavior\n51.2 (1997), pp. 234\u2013279.\n[41]M. Schemmer, P. Hemmer, M. Nitsche, N. K\u00fchl, and M. V\u00f6ssing. \u201cA meta-analysis\nof the utility of explainable artificial intelligence in human-AI decision-making\u201d. In:\nProceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society. July\n2022, pp. 617\u2013626.doi:10.1145/3514094.3534181.\n[42]N. Stylos. \u201cIntelligence augmentation via human-AI symbiosis: Formulating wise\nsystems for a metasociety\u201d. In:Philosophy of Artificial Intelligence and Its Place in\nSociety. IGI Global, 2023, pp. 301\u2013314.url: https://doi.org/10.4018/978-1-\n6684-7502-6.ch015.\n30 REFERENCES\n[43]A. Trunk, H. Birkel, and E. Hartmann. \u201cOn the current state of combining human\nand artificial intelligence for strategic organizational decision making\u201d. In:Business\nResearch(2020).doi:10.1007/s40685-020-00133-x.\n[44]D. Wang, A. Khosla, R. Gargeya, H. Irshad, and A. H. Beck. \u201cDeep learning for\nidentifying metastatic breast cancer\u201d. In:arXiv preprint arXiv:1606.05718(2016).\ndoi:10.48550/arXiv.1606.05718.\n[45]H. J. Wilson and P. R. Daugherty. \u201cCollaborative intelligence: Humans and AI\nare joining forces\u201d. In:Harvard Business Review96.4 (2018), pp. 114\u2013123.url:\nhttps://hbr.org/2018/07/collaborative- intelligence- humans- and- ai-\nare-joining-forces.\n[46]I. Y. Wuni. \u201cA systematic review and network meta-analysis of the risks of artificial\nintelligence in construction projects\u201d. In:International Journal of Construction\nManagement(2025), pp. 1\u201321.doi: 10 . 1080 / 15623599 . 2025 . 2531238 .url:\nhttps://doi.org/10.1080/15623599.2025.2531238.\n[47]Z. Xu, C. Hong, N. F. Soria Zurita, J. T. Gyory, G. Stump, H. Nolte, and C.\nMcComb. \u201cAdaptation and challenges in human-AI partnership for the design\nof complex engineering systems\u201d. In:International Design Engineering Technical\nConferences and Computers and Information in Engineering Conference. Vol. 87318.\nAmerican Society of Mechanical Engineers, Aug. 2023, V03BT03A056.doi: 10.\n1115/DETC2023-123234.\n[48]H. A. Younis, T. A. E. Eisa, M. Nasser, T. M. Sahib, A. A. Noor, O. M. Alyasiri,\nand H. A. Younis. \u201cA systematic review and meta-analysis of artificial intelli-\ngence tools in medicine and healthcare: applications, considerations, limitations,\nmotivation and challenges\u201d. In:Diagnostics14.1 (2024), p. 109.doi: 10.3390/\ndiagnostics14010109.\n[49]X. Zhang, X. Wei, C. X. Ou, E. Caron, H. Zhu, and H. Xiong. \u201cFrom human-AI\nconfrontation to human-AI symbiosis in society 5.0: transformation challenges and\nmechanisms\u201d. In:IT Professional24.3 (2022), pp. 43\u201351.doi: 10.1109/MITP.2022.\n3175512.\n[50]L. Zhou, S. Paul, H. Demirkan, L. Yuan, J. Spohrer, M. Zhou, and J. Basu. \u201cIntelli-\ngence augmentation: Towards building human-machine symbiotic relationship\u201d. In:\nAIS Transactions on Human-Computer Interaction13.2 (2021), pp. 243\u2013264.doi:\n10.17705/1thci.00149.\nDepartment of Management Systems and Organizational Development, Faculty of\nManagement, Wroc\u0142aw University of Science and Technology\nEmail address:agnieszka.bienkowska@pwr.edu.pl\nDepartment of Mathematics, Faculty of Mathematics, Wroc\u0142aw University of Science\nand Technology\nEmail address:jacek.malecki@pwr.edu.pl\nAMOTHO Research Institute, Vallsj\u00f6n 20, 780 00 R\u00f6rb\u00e4cksn\u00e4s, Sweden\nEmail address:amo@amotho.com\nDepartment of Management Systems and Organizational Development, Faculty of\nManagement, Wroc\u0142aw University of Science and Technology\nEmail address:katarzyna.tworek@pwr.edu.pl\n",
    "title": "Person-AI Bidirectional Fit - A Proof-Of-Concept Case Study Of Augmented Human-Ai Symbiosis In Management Decision-Making Process",
    "authors": [
      "Agnieszka Bie\u0144kowska",
      "Jacek Ma\u0142ecki",
      "Alexander Mathiesen-Ohman",
      "Katarzyna Tworek"
    ],
    "published": "2025-11-17",
    "arxiv_id": "2511.13670v1",
    "num_pages": 30,
    "num_chars": 86788
  },
  {
    "text": "Cost-Driven Synthesis of Sound Abstract Interpreters\nQIUHAN GU,University of Illinois, Urbana-Champaign, USA\nAVALJOT SINGH,University of Illinois, Urbana-Champaign, USA\nGAGANDEEP SINGH,University of Illinois, Urbana-Champaign, USA\nConstructing abstract interpreters that provide global soundness guarantees remains a major obstacle in abstract\ninterpretation.WeinvestigatewhethermodernLLMscanreduce thisburdenbyleveragingthem tosynthesize\nsound, non-trivial abstract interpreters across multiple abstract domains in the setting of neural network\nverification.Weformulatesynthesisasaconstrainedoptimizationproblemandintroduceanovelmathematically\ngroundedcostfunctionformeasuringunsoundnessunderstrictsyntacticandsemanticconstraints.Basedonthis\nformulation,wedevelopaunifiedframeworkthatunifiesLLM-basedgenerationwithsyntacticandsemantic\nvalidationandaquantitativecost-guidedfeedbackmechanism.Empiricalresultsdemonstratethatourframework\nnot only matches the quality of handcrafted transformers, but more importantly, discovers sound, high-precision\ntransformers for complex nonlinear operators that are absent from existing literature.\nCCSConcepts:\u2022Theoryofcomputation \u2192Programverification;Abstraction;\u2022Computingmethodologies\n\u2192Neural networks.\nAdditional Key Words and Phrases: Abstract Interpretation, Program Synthesis, Neural Network Verification\n1 Introduction\nAbstract Interpretation (AI) [ 10,11,57] is a popular framework for constructing automated program\nanalyzersindifferentdomainssuchassoftware[ 4,12],machinelearning[ 37,52],andembedded\nsystems[5,26,36].Theseanalyzersreasonaboutaninfinitenumberofprogramexecutions,producing\ninvariantsthatcanbeusedtoprovedifferentpropertiessuchassafety,robustness,andstability.A\nkey challenge in developing practical analyzers is obtainingsoundabstract transformers, which\noverapproximatetheeffectofconcreteprogramoperations(e.g.,assignments,conditionals).Thisisa\ntedious task and requires significant expert effort and heuristics. Indeed, considerable work exists on\nautomaticallysynthesizingabstractinterpreters[ 43],includingsymbolicsynthesistechniquesthat\nderiveconstraintsthroughformalreasoning[ 27,28],deep-learning-basedmethodsthatattemptto\ninfertransformerparametersfromdataandpatterns[40],etc.Despitetheseadvances,thesynthesis\nprocessremainslargelymanual,domain-specific,computationallyexpensive,andoftenlimitedto\nobtainingsoundtransformersforindividualoperator.Tothebestofourknowledge,thereexistsno\nhighly automated and efficient synthesis pipeline capable of generating abstract transformers that are\nbothprovablysoundforallinputabstractelementsandgeneralizableacrossdiverseoperatorsand\nabstract domains.\nRecent advances in state-of-the-art large language models (LLMs) have transformed the way\nalgorithms, code, and even scientific knowledge are generated [ 8,14,22,24,30\u201332,45,58,59,63].\nSystemssuchasAlphaEvolve[ 39]andFunSearch[ 44]havedemonstratedthatLLMscannotonly\nassist in coding but also evolve novel and high-performing algorithms through continuous evaluation\nand feedback. Motivated by these works, we investigate whether LLMs can synthesize provably\nsoundabstractinterpreters.Specifically,wefocusongeneratingcodedescribingthecomputations\nof sound abstract transformers operating over abstract elements. Leveraging LLMs to synthesize\nsuch abstract interpreters can increase the degree of automation and make the technology accessible\ntonon-experts.However,thissynthesisproblemismorechallengingforLLMscomparedtothose\nconsidered in the literature [3, 23, 25, 38], posing several non-trivial challenges.\nAuthors\u2019ContactInformation:QiuhanGu,UniversityofIllinois,Urbana-Champaign,Illinois,USA,qiuhang2@illinois.edu;\nAvaljot Singh, University of Illinois, Urbana-Champaign, Illinois, USA, avaljot2@illinois.edu; Gagandeep Singh, University\nof Illinois, Urbana-Champaign, Illinois, USA, ggnds@illinois.edu.arXiv:2511.13663v1  [cs.PL]  17 Nov 2025\n2 Qiuhan Gu, Avaljot Singh, and Gagandeep Singh\nChallenge1:Howcanweensurethevalidityandglobalsoundnessofsynthesizedtransformers?\nDuetothehallucinationoflargelanguagemodels[ 62],thegeneratedcodeoftencontainssyntacticor\nstructural errors that make it invalid for soundness verification. Even if an LLM can occasionally\nproduce syntactically correct code, ensuring global semantic soundness remains nontrivial. Our\nframeworkincorporatestwocheckingmodules:(1)alightweightstaticvalidationfrontendinspired\nbycompilerdesigntechniquestocatchstructuralandtypingerrors,combinedwithamodelrepair\nagent to automatically suggest fixes; (2) a formal verification tool based on SMT solvers that certifies\nthe transformer\u2019ssoundness under allabstract elements. Together,these two componentsguarantee\nthat only globally sound transformers are accepted.\nChallenge2:HowcananLLMeffectivelysearchwithinaninfinitespace?Synthesizingasound\nabstract transformer is fundamentally difficult because soundness must hold for all abstract elements,\nwhich forman infinite search space, and each abstract element may abstract an infinite number\nofconcretepoints.Toovercomethis,weformalizethesynthesisprocessasaconstrainedoptimization\nproblem, for which we design a novel mathematically grounded cost function that measures the\ndegree of unsoundness of each generated candidate transformer, while enforcing hard syntactic and\nsemantic validity constraints as optimization constraints. This continuous formulation transforms\nsynthesis from a binary pass/fail judgment into a guided optimization process, which iteratively\nrefines valid but unsound candidates based on counterexamples and quantitative feedback.\nChallenge 3: How can we guarantee the convergence of the synthesis?Synthesizing abstract\ninterpreterswithinaninfinitesearchspacenaturallyraisesthequestionofthetheoreticalfeasibility\nof whether the search process can be guaranteed to converge to a sound transformer in finitely many\nsteps. To address this, we formally define a novel refinement rule governing each synthesis step, and\nprovethatunderseveraleasy-to-satisfyrequirements,theiterativeoptimizationprocessmonotonically\ndecreasesthecostfunctionandterminateswithinfinitesteps,ensuringtheoreticalconvergencetoa\nglobally sound transformer.\nTo ground our study, we focus on the verification of deep neural networks (DNNs), a domain\nwhere abstract interpretation has emerged as a successful approach for proving model robustness and\nsafety [49,53]. DNN verification works with bounded polyhedral abstractions, making it a relatively\neasier problem to handle for LLMs as a case study for automated synthesis than analyzing programs,\nwhich often involves unbounded polyhedra.\nThisWork.Wepresentthefirstgeneralframeworkforsynthesizingsoundabstractinterpreterswith\nstate-of-the-artLLMs,treatingtransformersynthesisasaniterativeoptimizationproblemguidedbya\nsoundness-driven cost function. Our work is implemented uponConstraintFlow[ 46], a framework\nwhichprovidesasimpleanddeclarativeDSLthatencodestransformerlogicassymbolicequations,a\nunifiedinterfaceProveSound[ 47]basedonSMTsolvers(Z3)forglobalsoundnessverification,and\nacompilerbackend[48]totransformtheDSL-basedtransformersintoexecutableprograms.Given\nanoperatorandanabstractdomainprovidedbytheuserasinputs,ourframeworkrepeatedlyprompts\nanLLM toproposecandidatetransformers,checksthe validation,fixeserrors automaticallywitha\nseperatemodelagent,verifiesvalidcandidatesagainstthesoundnessconstraints,andcomputesacost\nfunctionLthatmeasureshowfarthecandidatedeviatesfromsatisfyingitssoundnessconstraints.The\nfeedbackLserves as acontinuous cost signal, along withthe counterexamples, guiding subsequent\nsynthesis rounds until a sound transformer is found.\nMain Contributions.Our work makes the following contributions:\n(1)An LLMs-based synthesis framework. We design the first iterative synthesis system that cou-\nples LLM generation with symbolic verification to automate the synthesis of sound abstract\ntransformers.\nCost-Driven Synthesis of Sound Abstract Interpreters 3\n(2)Soundness-drivencostfunction.Weintroduceanovelsoundnessdeviationmetric,quantifying\nthedegreeofunsoundnessanddrivingcontinuousrefinement.Iteitherconvergeswithasound\ntransformer or fails within a maximum number of attempts.\n(3)Implementation and evaluation. We implement our framework on top ofConstraintFlowand\ndemonstrate its ability to synthesize sound transformers across diverse operators and abstract\ndomains. Our evaluation shows that our framework attains performance on par with handcrafted\ntransformers for common non-linear operators, while further demonstrating the capability to\nefficiently synthesize novel and complex transformers with consistently high precision.\nBeyond neural network certification, we believe the principles underlying our framework can\nbe extended to other research domains that require the automated construction of provably sound\nalgorithms.\n2 Background\n2.1 Abstract Interpretation\nAbstract interpretation[ 10] provides a mathematicalfoundation forsound reasoning aboutprogram\nbehaviors by approximating all possible executions within a unified framework. It formalizes the\nprinciple of computing with abstractions rather than enumerating individual executions.\nA verifier based on abstract interpretation [ 18] reasons over two domains: the concrete domain\n(C,\u2291\ud835\udc36),whichrepresentstheexactsemanticsofthesystem,andtheabstractdomain (A,\u2291\ud835\udc34),which\nencodes symbolic over-approximations of C. The two are connected by an abstraction function\n\ud835\udefc:C\u2192A and a concretization function \ud835\udefe:A\u2192C . Soundness requires that for all \ud835\udc50\u2208C,\n\ud835\udc50\u2291\ud835\udc36\ud835\udefe(\ud835\udefc(\ud835\udc50)), meaning that every concrete behavior is preserved within its abstraction.\nWithin this framework, a program statement or neural-network operator can be modeled as a\nconcretetransformer \ud835\udc39:C\u2192C andacorrespondingabstracttransformer \ud835\udc39#:A\u2192A .Theabstract\ntransformer \ud835\udc39#issoundifandonlyif \ud835\udc39(\ud835\udefe(\ud835\udc67))\u2291 \ud835\udc36\ud835\udefe(\ud835\udc39#(\ud835\udc67))forall\ud835\udc67\u2208A.Successiveapplicationof\n\ud835\udc39#overthestructureofaprogramornetworkyieldsanover-approximationofallreachablestates,\nwhich guarantees that any verified property holds for every concrete execution.\nSince abstract domains admit many incomparable abstractions, there is in general no single\n\u201cbest\u201dsoundtransformer.Differenttransformerstradeoffprecisionandcomputationalstructurein\ndomain-dependent ways. Therefore a key challenge in abstract interpretation is balancing precision\nand efficiency [ 16,35]. A more complicated abstract domain such as polyhedra [ 54] often improves\nprecision but is computationally expensive, while simpler domains such as intervals [ 21] scale better\nbutyieldlooserbounds.Forneuralnetworks,specializeddomainssuchasDeepPoly[ 51],DeepZ[ 50],\nand CROWN [ 64] combine linear relaxations with neuron-wise constraints to achieve sound yet\ntractable over-approximations.\nNotation.Toavoidambiguityandensurenotationalclaritythroughouttherestofthispaper,we\nfix the following convention: bold symbols (e.g., L,U) denote vectors. We use \ud835\udc50\u2208Cto denote a\nconcreteelement, \ud835\udc67\u2208Atodenoteanabstractelement.Weuse x\u2208R\ud835\udc5btodenoteaconcretestate,i.e.,\na complete assignment to all input variables of the program, whereas \ud835\udc65\ud835\udc56\u2208Rrefers specifically to the\nvalue of the \ud835\udc56-th variable within that state, \ud835\udc56\u2208[\ud835\udc5b]. This distinction will be maintained consistently in\nall subsequent formal definitions and derivations.\n2.2 DNN Certifier\nAdeepneuralnetwork(DNN)canberepresentedasacompositionofaffineandnon-linearlayers\nsuch as ReLU, Tanh, or MaxPool. Formally, a trained DNN defines a function \ud835\udc53:R\ud835\udc5a\u2192R\ud835\udc5b. A\nverificationpropertyisdescribedbyaprecondition \ud835\udf11,whichdenotestheadmissibleinputregion,and\n4 Qiuhan Gu, Avaljot Singh, and Gagandeep Singh\nFig. 1. The overview of our framework. Given a prompt \ud835\udc5d, which specifies an operator, the domain specific\nlanguage and abstract domain, and previous generation history, which includes the unsound transformer \ud835\udc39\u266f\n\ud835\udc61\u22121\nand the counterexample \ud835\udc50, the LLM\ud835\udc3f1proposes a set of candidates {\ud835\udc39\u266f\n\ud835\udc56,\ud835\udc56\u2208[1,\ud835\udc5a]} , which will be validated\nby syntax and semantics checkers. Failing candidates trigger an automatic repair process based on another\nmodel agent\ud835\udc3f 2until they pass the checkers. Valid candidates are scored by a soundness-driven costL(\ud835\udc39\u266f).\nIf their score is less than that of \ud835\udc39\u266f\n\ud835\udc61, then they will replace \ud835\udc39\u266f\n\ud835\udc61as the current \"best\" unsound transformer in\nthe prompt. This feedback loop transforms synthesis into an optimization process, refining candidates until\nL(\ud835\udc39\u266f)=0.\na postcondition \ud835\udf13, whichexpresses the desired safety constraint on outputs.The goal of verification\nis to prove that\ud835\udc53(\ud835\udf11)\u2286\ud835\udf13, meaning that no inputx\u2208\ud835\udf11produces an unsafe output\ud835\udc53(x)\u2209\ud835\udf13.\nAbstract-interpretation-based verifiers [ 18,53] compute a sound over-approximation of \ud835\udc53(\ud835\udf11).\nStartingfromanabstractelement \ud835\udefc(\ud835\udf11),theverifierpropagatesitthroughthenetworkusinglayer-wise\nabstract transformers and obtains an abstract output\ud835\udc54(\ud835\udefc(\ud835\udf11))such that\n\ud835\udefe(\ud835\udc54(\ud835\udefc(\ud835\udf11)))\u2287\ud835\udc53(\ud835\udf11).\nIf\ud835\udefe(\ud835\udc54(\ud835\udefc(\ud835\udf11))) \u2286\ud835\udf13 , the property is guaranteed to hold. Otherwise, the verifier may produce\ncounterexamples or apply refinement strategies to reduce over-approximation. This layer-wise\nreasoning forms the basis of systems such as DeepPoly, CROWN, and ConstraintFlow. A DNN\ncertifier integrates these verifiers into an end-to-end pipeline that formally proves safety, robustness,\nor other properties of networks. Certified training frameworks further embed verification into the\nlearning process by shaping the loss function according to verifier feedback, guiding the model\ntoward provable robustness.\n3 Overview\nFig. 1 presents the high-level idea of the constrained optimization process behind our framework.\nGiven an operator specification and an abstract domain specification in text as inputs, the framework\naugments the prompt with the DSL grammar and few-shot examples to construct the initial prompt \ud835\udc5d.\nThen framework searches for sound abstract transformers through an iterative process that combines\ngeneration, validation, and verification under the guidance of a soundness-oriented cost function\nL(\ud835\udc39\u266f).WhilethecurrentworkflowassumesapredefinedDSL,itremainsfullyextensibletoother\nCost-Driven Synthesis of Sound Abstract Interpreters 5\nprogramminglanguages,aslongasacorrespondingsyntaxandsemanticvalidationmodule,along\nwith a compatible soundness verifier, are supplied.\nAmajordifficultyinprogramsynthesiswithLLMsisthatunconstrainedLLMgenerationoften\nproducescodethatiseithersyntacticallyinvalidorsemanticallyinconsistent.Tomitigatethisand\nimprove the validity rate, our framework generates multiple candidates in each pass. Each candidate\nis analyzed by a validator that parses it into an abstract syntax tree (AST) and verifies it against hard\nsyntacticand semanticconstraints. Weprovide generalsemantics formultiple syntaxandsemantic\nchecks that can be applied in various programming languages in Appendix C. When errors appear, a\ndedicated repair model is invoked, which receives diagnostic messages and violating code region\nas feedback, and incrementally proposes corrections until the AST can be parsed and interpreted\nsuccessfully,oruntilthemaximumnumberoftrialshasbeenreached.Thecandidatewillbediscarded\nin the latter case. This automated repair loop is more efficient than the discard-and-retry strategy\ncommoninpriorsynthesissystems[ 2,17,55],enablingourframeworktostabilizegenerationand\npreserve useful partial results, as shown in the ablation study in \u00a75.4.\nWeverifyallcandidatesthatpassvalidationforsoundnessusingasymboliccertifier.Insteadof\ntreating verification as a binary pass or fail decision, our framework evaluates each transformer using\na novel cost function that quantifies its degree of soundness. The design of the cost function ensures\nthat the cost of any sound transformer is 0. If that happens, then the transformer will be returned\nas the final result. If none of the candidates are sound, then the candidate with the lowest score is\nretained as the current \"best\" unsound transformer, and its associated counterexamples and score are\nincorporatedintothenextroundofgeneration,functioningasanewandnon-stochasticstartingpoint\ninthesearchspaceandhelpingthemodelbetterunderstandthesynthesistask.Thisfeedbackloop\ntransformssynthesisintoacontinuousoptimizationprocess,iterativelyrefiningcandidatesuntila\nsound one is found.\nInsummary,ourframeworkenablesaunifiedsynthesisprocessthatscalesacrossdiverseoperators\nand abstract domains. The generated DSL-based transformers serve as symbolic specifications\nthat are provably sound for all abstract inputs and can be effortlessly integrated into existing\nnetwork certification frameworks through a compiler backend [ 48], surpassing purely mathematical\nformulations.\n3.1 Illustrative Example\nTo illustrate the workflow of our framework, we demonstrate how it constructs a sound abstract\ntransformer for the HardSigmoid activation for the popular DeepPoly [ 51] abstract domain based on\nan open-source LLM Llama4-Maverick, which provides free API access while exhibiting decent\nsynthesis performance, sufficient to demonstrate the effectiveness of our framework. We choose\nHardSigmoid because it is non-trivial to handle, and there does not exist a globally sound DeepPoly\ntransformer in the literature. This design choice prevents the language model from relying on\nmemorized templates or prior retrievals, thereby exposing the genuine synthesizing capability of\nourframeworkingeneratingunseenabstracttransformers.Formally,theHardSigmoidfunctionis\ndefined as a piecewise-linear function, as shown in the Fig. 2.\n3.1.1 Abstract Domain.Next, we describe the DeepPoly domain, which associates two polyhedral\nand two interval constraints with each neuron, where a neuron represents the output value of a\nsingle computational node in the neural network. Formally, an abstract element is represented as\n\ud835\udc67=\u27e8l,u,L,U\u27e9. Here, LandUare vectors of affine functions over all neurons feeding into the current\nlayer, with\ud835\udc3f\ud835\udc56and\ud835\udc48\ud835\udc56representing the lower and upper polyhedral bounds of \ud835\udc65\ud835\udc56(where\ud835\udc65\ud835\udc56denotes the\nconcretevalueofthe \ud835\udc56-thneuron)and \u2113\ud835\udc56,\ud835\udc62\ud835\udc56\u2208Rarethecorrespondingconcretelowerandupperscalar\nbounds. The concretization is defined as\ud835\udefe \ud835\udc5b(\ud835\udc67)={x\u2208R\ud835\udc5b|\u2200\ud835\udc56\u2208[\ud835\udc5b], \u2113 \ud835\udc56\u2264\ud835\udc65\ud835\udc56\u2264\ud835\udc62\ud835\udc56\u2227\ud835\udc3f\ud835\udc56\u2264\ud835\udc65\ud835\udc56\u2264\ud835\udc48\ud835\udc56}\n6 Qiuhan Gu, Avaljot Singh, and Gagandeep Singh\nHardSigmoid(\ud835\udc65)=\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4 \uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f30, \ud835\udc65\u2264\u22123,\n\ud835\udc65+3\n6,\u22123<\ud835\udc65<3,\n1, \ud835\udc65\u22653.\n(a) Piecewise definition.\n7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5\nx0.00.20.40.60.81.0y\nHardSigmoid(x) (b) Visualization of the function.\nFig. 2. Definition and visualization of the HardSigmoid activation. HardSigmoid linearly approximates the\nstandard sigmoid between\u22123and3, and saturates at0and1outside this range.\nDeepPolyisequippedwithabstracttransformersspecificallytailoredforverifyingneuralnetworks.\n3.1.2 Domain-Specific Language (DSL).We choose the domain-specific language and symbolic\nverificationenginethatareprovidedinConstraintFlow[ 46]tosupportthesynthesisprocess.Unlike\ngeneral-purpose languages such as Python, theConstraintFlowDSL emphasizes the mathematical\nspecification of transformers while abstracting away implementation details, making it high-level\nandsolver-friendlywhileremaininghuman-readable.AlsotheDSLisequippedwithasoundness\nverification toolProveSoundbuilt upon an SMT solver (Z3), which can automatically check the\nsoundnessofagivencandidatetransformerwritteninConstraintFlow,eitherprovingsoundness\nfor all abstract elements (instead of input-specific soundness) or generating a counterexample.\nConstraintFlowDSL expresses abstract transformers as declarative equations relating input\nand output bounds. Each transformer specifies how an operator updates the lower and upper bounds\n\u27e8l,u,L,U\u27e9 oftheabstractelement.Notably,whileDeepPolydefinesabstracttransformersoverthe\nentirenetworkstate,includingthepreservationofboundsforallprecedingneurons,ConstraintFlow\nsimplifies this formulation by focusing only on the updated neurons. That is, instead of explicitly\nencodinghowallpreviousneurons\u2019bounds (\u2113\ud835\udc58,\ud835\udc62\ud835\udc58,\ud835\udc3f\ud835\udc58,\ud835\udc48\ud835\udc58)remainunchanged,theDSLabstractsaway\nthesepreservedupdatesandspecifiesonlythetransformationrelationbetweentheinputneuron \ud835\udc65\ud835\udc57\nandtheoutputneuron \ud835\udc65\ud835\udc56affectedbythecurrentoperator.Thisdesignyieldsaconciseanddeclarative\nrepresentation that is well-suited for LLM-based synthesis and symbolic verification.\n3.1.3 Iterative Synthesis.Wenowdemonstratehowtheframeworkautomaticallysynthesizesasound\ntransformer for the HardSigmoid operator within this setting.\nLLM Generation.Each synthesis round begins with a structured prompt that encodes the operator\nspecification, the semantics of the DeepPoly domain, and the grammar of theConstraintFlow\nDSL, two-shot exemplars of verified transformers for related operators (Add, Affine), and contextual\nfeedback from previous iterations. Detailed prompt templates are provided in Appendix A. The large\nlanguagemodelthenproducesasetofcandidatetransformersrepresentedinDSLcode,whichcan\nbe categorized into one of three types: (i) syntactically or semantically invalid (e.g., unmatched\nparentheses or undefined metadata), (ii) unsound but syntactically and semantically valid, or (iii)\nvalid and sound. Fig. 10 in Appendix B illustrates typical examples of each outcome.\nCost-Driven Synthesis of Sound Abstract Interpreters 7\nValidation and Repair.After generation, each candidate undergoes a lightweight validation stage\ninspired by compiler frontends. The framework parses the candidate into an abstract syntax tree\n(AST) and applies hard-coded static checks for common structural and semantic errors, including:\n(i) unmatched or missing delimiters such as parentheses and braces;\n(ii)illegal keywords or illegal logical operators (e.g., using \u201c &&\u201d whenConstraintFlowonly\nsupports \u201cand\u201d; only provide one operand for the \u201cand\u201d operator);\n(iii)malformedattributecallsandincorrectmetadataindexing(e.g.,using\u201c.\u201dtoaccessmetadata\ninstead of \u201c[]\u201d; using nested or numeric indices where symbolic ones (l,u,z) are expected);\n(iv) undefined identifiers or invalid function invocations;\n(v)type inconsistencies in arithmetic or element-wise operations (e.g., adding an integer to a\nneuron);\n(vi)improperuseofreservedconstantsorkeywords(e.g.,defineanewfunctionnamed\u201c transformer \u201d,\nwhich is a reserved keyword).\nThese checks are formally defined in Appendix C. When a violation is detected, the system invokes a\ndedicated repair agent. Detailed prompt templates are provided in Appendix A. If the error cannot be\nmatched to any predefined rule, the agent receives a generic \u201c Unknown Error \u201d prompt and performs\ncontextual repair until the AST passes validation or the maximum try limits are met.\nSoundness Verification and Cost Evaluation.After a candidate passes static validation, it is\nsubmitted to the soundness verifierProveSound. One of our key contributions is designing a novel\ncostfunctiontoquantifythedegreeofunsoundness,whichcaptureshowfaranunsoundcandidate\nis from being sound. If the candidate is proved sound, then the cost function evaluates to 0 and\nthe procedure terminates. Otherwise, the solver returns counterexamples that are used by the cost\nfunction to quantify the degree of unsoundness.\nDesigning such a cost function is highly non-trivial for two reasons. First, it is unclear how to\nquantifythedeviationofanabstractelement\u2019sconcretizationfromitssoundabstractenclosurein\na mathematically meaningful way. Second, the soundness definition of an abstract transformer is\nexpressed as a universal condition over all abstract elements [ 9,19], which form an infinite set,\nmaking direct computation infeasible. To address these challenges, we begin by dissecting the\ndefinition of soundness itself. We progressively analyze each violating abstract element \ud835\udc67, examining\nitsconcretization x\u2208\ud835\udefe(\ud835\udc67),andfinallyrelatingeachneuron \ud835\udc65\ud835\udc56withintheseconcretestatestotheir\nabstract shape (or constraints) components \ud835\udc36\ud835\udc56\ud835\udc57. This stepwise reasoning reveals how deviations arise\nbetween the concrete and abstract semantics, enabling a mathematically grounded ideal formulation\nof the cost function that quantifies the extent of unsoundness. Since soundness is defined over an\ninfinite set of abstract elements, we further introduce a relaxation strategy that approximates this\nideal cost by sampling a finite subset and weighting them with an importance function \ud835\udc64(\ud835\udc65\ud835\udc56)derived\nfrom the operator\u2019s gradient.\nFormally, the ideal cost function is given by\nL(\ud835\udc39\u266f)=\u0394\ud835\udc46(\ud835\udc39\u266f)=\ud835\udc67\u00ca\n\ud835\udc67\u2208\ud835\udc34\u2217x\u00ca\nx\u2208\ud835\udefe(\ud835\udc67)\ud835\udc57\u00ca\ud835\udc58\n\ud835\udc57=1\ud835\udf00(\ud835\udc53(x)\ud835\udc56,\ud835\udc36\ud835\udc56\ud835\udc57).\nDetailedderivationcanbefoundin\u00a74.Here, \ud835\udc34\u2217denotesthesetofallabstractelementsforwhichthe\ncandidatetransformerfailstosatisfy \ud835\udc39(\ud835\udefe(\ud835\udc67))\u2291 \ud835\udc36\ud835\udefe(\ud835\udc39\u266f(\ud835\udc67)).\ud835\udc67representsoneabstractelementfrom \ud835\udc34\u2217,\nandx\u2208\ud835\udefe(\ud835\udc67)corresponds to one of \ud835\udc67\u2019s concretizations, where xdenotes the vector of neuron values\nrepresenting the concrete network state. Let \ud835\udc65\ud835\udc56denote the value of the \ud835\udc56-th neuron which will be\nupdatedinthestate x;\ud835\udc53(x)\ud835\udc56denotesthenewvalueof \ud835\udc56-thneuronafterapplyingtheconcreteoperator\n\ud835\udc53onx(e.g.,theHardSigmoidactivation).Sincetheoperatorupdatesonlyoneneuronatatimewhile\nleaving others unchanged (i.e., \ud835\udc53(x)\ud835\udc57=\ud835\udc65\ud835\udc57,\ud835\udc57\u2260\ud835\udc56), no soundness violation can occur in the unaffected\n8 Qiuhan Gu, Avaljot Singh, and Gagandeep Singh\nneurons, making it safe to focus solely on the updated one. \ud835\udc36\ud835\udc56\ud835\udc57\u2208{\u2113\ud835\udc56,\ud835\udc62\ud835\udc56,\ud835\udc3f\ud835\udc56,\ud835\udc48\ud835\udc56}represents the \ud835\udc57-th\nconstraint derived from the \ud835\udc56-th neuron\u2019s abstract shapes after applying the abstract transformer\n(e.g., interval or affine constraint). Each violation \ud835\udf00(\ud835\udc53(x)\ud835\udc56,\ud835\udc36\ud835\udc56\ud835\udc57)quantifies how much the concrete\noutput\ud835\udc53(x)\ud835\udc56falls outside its sound abstract enclosure, and the aggregation operator\u00c9accumulates\nthese deviations into a single scalar measure of unsoundness. We differentiate aggregation operators\nbasedontheiroperands.\ud835\udc67\u00c9,x\u00c9,\ud835\udc57\u00c9representaggregationoperatorsoperatingonabstractelements,\nconcrete states, and constraints, respectively. In order to ensure the convergence of the algorithm,\neach\u00c9isrequiredtobeamonotone,non-negativeandboundedfunctionsatisfyingthecondition\nthat:\u2200\ud835\udc461,\ud835\udc462astwoofsetsofoperands, \ud835\udc461\u2286\ud835\udc46 2=\u21d2\u00c9\ud835\udc461\u2286\u00c9\ud835\udc462,0<\u00c9\ud835\udc461<\u221e,0<\u00c9\ud835\udc462<\u221e.\u00c9\ud835\udc46=0ifandonlyif \ud835\udc46=\u2205,meaningthereisnosoundnessviolations.Typicalinstantiationsinclude\nmaximum and mean. In practice, we use maximization to function as the universal aggregation\noperator. In the implementation, we apply an approximation strategy combining sampling and\nweight function. Details can be found in \u00a74.5. Let \u27e8l,u,L,U\u27e9 denote the input abstract element,\nand\u27e8l\u2032,u\u2032,L\u2032,U\u2032\u27e9thecorrespondingoutputelement.Then,basedonDeepPoly,thecostfunctionis\napproximated as:\nL(\ud835\udc39\u266f)=f\u0394\ud835\udc46(\ud835\udc39\u266f)=max\n\ud835\udc67\u2208\ud835\udc34\u2217max\nx\u2208\ud835\udefesample(\ud835\udc67)\ud835\udc64(\ud835\udc65\ud835\udc56)\u00b74max\n\ud835\udc57=1\ud835\udf00(\ud835\udc53(x)\ud835\udc56,\ud835\udc36\ud835\udc56\ud835\udc57)\n=max\n\ud835\udc67\u2208\ud835\udc34\u2217max\nx\u2208\ud835\udefesample(\ud835\udc67)\ud835\udc64(\ud835\udc65\ud835\udc56)\u00b7\u0010\nmax(0,\ud835\udc53(x) \ud835\udc56\u2212\ud835\udc62\u2032\n\ud835\udc56)+max(0,\ud835\udc59\u2032\n\ud835\udc56\u2212\ud835\udc53(x)\ud835\udc56)\n+max(0,\ud835\udc53(x) \ud835\udc56\u2212\ud835\udc48\u2032\n\ud835\udc56)+max(0,\ud835\udc3f\u2032\n\ud835\udc56\u2212\ud835\udc53(x)\ud835\udc56)\u0011\nwhere\ud835\udefesample(\ud835\udc67)represents a sampled subset of \ud835\udefe(\ud835\udc67).\ud835\udc64(\ud835\udc65\ud835\udc56)denotes the weight function, \ud835\udc64(\ud835\udc65\ud835\udc56)=\n\ud835\udf19(\ud835\udc53,\ud835\udc65 \ud835\udc56)\u00cd\n\ud835\udc65\u2032\u2208\ud835\udefesample(\ud835\udc67)\ud835\udf19(\ud835\udc53,\ud835\udc65\u2032\n\ud835\udc56),\ud835\udf19(\ud835\udc53,\ud835\udc65\ud835\udc56)=log\u00001+exp(\u2225\u2207 \ud835\udc65\ud835\udc56\ud835\udc53(\ud835\udc65\ud835\udc56)\u2225)\u0001.Thisdesignoftheweightfunctionleverages\nthegradientmagnitudeof \ud835\udc53toprioritizesemanticallycriticalconfigurations,forinstance,around\n\ud835\udc65=3in the HardSigmoid function, where the gradient is high and transformers are more prone\ntoerrors.Thesoftplustransformationintheweightfunctionavoidsvanishingcontributionsinflat\nregions. By pairing sampling with a weight function, we ensure that the finite-sample approximation\nremains representative of the full sampling space.\nx0.000.250.500.751.001.25y\ni\n ui 3\n 3HardSigmoid(x)\nLower: sec((-3,0),(u_i,1))\nUpper: sec((l_i,0),(3,1))\nFig. 3. Correct polyhedra bounds for HardSigmoid\ntransformer on interval [\u2113\ud835\udc56,\ud835\udc62\ud835\udc56]when\u2113\ud835\udc56<\u22123<3<\n\ud835\udc62\ud835\udc56. The shaded areas highlight the safely approxi-\nmated region.\nx0.000.250.500.751.001.25y\ni\n ui 3\n 3HardSigmoid(x)\nLower: sec((l_i,0),(u_i,1))\nUpper: sec((l_i,0),(u_i,1))Fig. 4. Incorrect polyhedra bounds for HardSigmoid\ntransformer on interval [\u2113\ud835\udc56,\ud835\udc62\ud835\udc56]when\u2113\ud835\udc56<\u22123<3<\n\ud835\udc62\ud835\udc56. The shaded areas highlight the regions that are\nnot safely approximated and violate the soundness\ncondition.\nWe consider the most challenging case in synthesizing the HardSigmoid transformer when the\ninputintervalspanstheinterval (\u22123,3),wherethefunctionswitchesbetweenthelinearandsaturated\nregion, i.e.,\u2113\ud835\udc56<\u22123<3<\ud835\udc62 \ud835\udc56. In this mixed case, the operator alternates between the saturated zones\nCost-Driven Synthesis of Sound Abstract Interpreters 9\n((\u2212\u221e,\u22123) and(3,+\u221e))) and the central linear region, yielding a non-convex shape that must be\nover-approximated by two affine bounds and the interval bounds.\nSince the HardSigmoid function is monotonic, the interval bounds are easy to get, while two\ndistinctaffinerelaxationsarehardtogenerate.AsshownintheFig.4,anunsoundcandidategenerated\nby Llama4-Maverick produces overlapping affine bounds. In this case the unsound transformer \ud835\udc39\u266f\n0is\nthen defined as:\n\ud835\udc39\u266f\n0(<\ud835\udc59\ud835\udc56,\ud835\udc62\ud835\udc56,\ud835\udc3f\ud835\udc56,\ud835\udc48\ud835\udc56>)=<\ud835\udc59\u2032\n\ud835\udc56,\ud835\udc62\u2032\n\ud835\udc56,\ud835\udc3f\u2032\n\ud835\udc56,\ud835\udc48\u2032\n\ud835\udc56>=<0,1,1\u22120\n\ud835\udc62\ud835\udc56\u2212\u2113\ud835\udc56(\ud835\udc65\ud835\udc56\u2212\u2113\ud835\udc56),1\u22120\n\ud835\udc62\ud835\udc56\u2212\u2113\ud835\udc56(\ud835\udc65\ud835\udc56\u2212\u2113\ud835\udc56)>,\nwhen\u2113\ud835\udc56<\u22123<3<\ud835\udc62 \ud835\udc56, which under-approximates part of the nonlinear curve and violates the\nsoundnesscondition.TheshadedregionsintheFig.4highlighttheseviolatingareas,comparedto\nthe sound approximation shown in Fig. 3.\nTo compute the cost function in this case, we first obtain counterexamples that violate the\nsoundness condition from the SMT solver, such as \ud835\udc671=\u27e8(...,\u22124,...),(...,4,...),L,U\u27e9,\ud835\udc67 2=\n\u27e8(...,\u22125,...),(...,4,...),L,U\u27e9,\ud835\udc67 3=\u27e8(...,\u22125,...),(...,5,...),L,U\u27e9 , etc. Here, we only focus on\nthe lower and upper bounds of the \ud835\udc56-th neuron of each abstract element, i.e., \u2113\ud835\udc56and\ud835\udc62\ud835\udc56, since\nthe affine bounds of the input elements are irrelevant when evaluating the cost function, and the\nunchanged neurons would not affect soundness, as mentioned before. Take \ud835\udc671as an instance, we can\nhave\ud835\udc65\ud835\udc56\u2208{\u22124,\u22123,\u22122,\u22121,0,1,2,3,4} , where x\u2208\ud835\udefe\ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52(\ud835\udc671). Since the interval bounds \u2113\u2032\n\ud835\udc56=0and\n\ud835\udc62\u2032\n\ud835\udc56=1donotcontributetoviolations,thecostarisessolelyfromthedeviationbetween \ud835\udc53(x)\ud835\udc56,i.e.,\n\ud835\udc3b\ud835\udc4e\ud835\udc5f\ud835\udc51\ud835\udc46\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51(\ud835\udc65 \ud835\udc56),andthelinearrelaxation.Theincorrectaffinerelaxationinthiscaseusedbythe\nunsound transformer:\ud835\udc48\u2032\n\ud835\udc56=\ud835\udc3f\u2032\n\ud835\udc56=1\n8\ud835\udc65\ud835\udc56+1\n2,yielding the violation term at each sampled point being:\n\ud835\udf00(\ud835\udc53(x)\ud835\udc56,\ud835\udc36\ud835\udc56\ud835\udc57)=max(0,\ud835\udc3b\ud835\udc4e\ud835\udc5f\ud835\udc51\ud835\udc46\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51(\ud835\udc65 \ud835\udc56)\u2212\ud835\udc48\u2032\n\ud835\udc56)+max(0,\ud835\udc3f\u2032\u2212\ud835\udc3b\ud835\udc4e\ud835\udc5f\ud835\udc51\ud835\udc46\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51(\ud835\udc65 \ud835\udc56)\n=\f\f\ud835\udc3b\ud835\udc4e\ud835\udc5f\ud835\udc51\ud835\udc46\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51(\ud835\udc65 \ud835\udc56)\u2212(1\n8\ud835\udc65\ud835\udc56+1\n2)\f\f.\nEvaluating over sampled {\ud835\udc65\ud835\udc56}yields\ud835\udf00={0,0.125,0.0833,0.0417,0,0.0417,0.0833,0.125,0} re-\nspectively. Each \ud835\udf00(\ud835\udc53(x)\ud835\udc56,\ud835\udc36\ud835\udc56\ud835\udc57)is then multiplied by its corresponding weight, and the maximum\nweighted violation is taken as the final cost:\nL\ud835\udc67\ud835\udc56(\ud835\udc39\u266f\n0)=max\nx\u2208\ud835\udefesample(\ud835\udc671)\ud835\udc64(\ud835\udc65\ud835\udc56)\ud835\udf00(\ud835\udc65\ud835\udc56)\u22480.11390.125\u22480.01424,when\ud835\udc65 \ud835\udc56=\u00b13.\nSimilarly,otherabstractelementssuchas \ud835\udc672and\ud835\udc673areprocessedinthesameway.Wethenaccumulate\ntheir contributions to obtainL(\ud835\udc39\u266f\n0):\nL(\ud835\udc39\u266f\n0)=max(L \ud835\udc67\ud835\udc56(\ud835\udc39\u266f\n0),L\ud835\udc672(\ud835\udc39\u266f\n0),L\ud835\udc673(\ud835\udc39\u266f\n0))=max(0.01424,0.0230,0.01895)=0.0230.\nIn each iteration, when no sound transformer is obtained, we select from all candidates the one\nwith the smallest cost function value as the current \"best\" unsound transformer. This transformer,\ntogetherwithitscounterexamplesandcostscore,ismergedintothenextprompttoguidethemodel\u2019s\nsubsequent generation, encouraging it to learn from prior mistakes and produce improved candidates.\nIf the next round still fails to yield a sound transformer, we again identify the candidate whose cost is\nlowerthanthecurrent\"best\"unsoundone,andpromoteitasthenewunsoundtransformerforthe\nfollowing iteration. The complete synthesizing process in this case is shown in the Fig. 6c.\n4 Formalizing LLM-Guided Synthesis\n4.1 Abstract Interpretation and Abstract Transformers\nWe consider the setting of numerical abstract interpretation, where the concrete and abstract domains\naredenotedasCandArespectively.Let P\ud835\udc36=(P(R\ud835\udc5b),\u2291\ud835\udc36)betheposetonthepowersetofconcrete\n10 Qiuhan Gu, Avaljot Singh, and Gagandeep Singh\nstates where\u2291\ud835\udc36=\u2286is subset inclusion. Let P\ud835\udc34=(A,\u2291\ud835\udc34)be the poset on the abstract elements. The\nconcretization function\ud835\udefe:A\u2192P(R\ud835\udc5b)maps an abstract element to a set of concrete elements.\nSound Abstract Transformer.Let \ud835\udc53:R\ud835\udc5b\u2192R\ud835\udc5bbe a concrete function. Its corresponding concrete\ntransformer\ud835\udc39:P(R\ud835\udc5b)\u2192P(R\ud835\udc5b)is defined as:\n\ud835\udc50\u2208P(R\ud835\udc5b), \ud835\udc39(\ud835\udc50):={\ud835\udc53(x)|x\u2208\ud835\udc50}.\nGivenanabstracttransformer \ud835\udc39\u266f:A\u2192A ,wesay\ud835\udc39\u266fisgloballysoundifitoverapproximatesthe\nconcrete semantics for all abstract elements, i.e.,\n\u2200\ud835\udc67\u2208A, \ud835\udc39(\ud835\udefe(\ud835\udc67))\u2291 \ud835\udc36\ud835\udefe(\ud835\udc39\u266f(\ud835\udc67)).\nThat is, applying the approximation \ud835\udc39\u266fon any abstract element \ud835\udc67, and then obtaining the set of\nconcrete values corresponding to the result must include more states than first concretizing the\nabstract element and then applying the concrete transformer\ud835\udc39.\n4.2 Unsoundness Deviation and Metrics\nBased on the definition of soundness, an abstract transformer\ud835\udc39\u266fisunsoundiff:\n\u2203\ud835\udc67\u2208A, \ud835\udc39(\ud835\udefe(\ud835\udc67))@ \ud835\udc36\ud835\udefe(\ud835\udc39\u266f(\ud835\udc67)).\nBy the definition of subset inclusion, this is equivalent to:\n\u2203\ud835\udc67\u2208A,\u2203y\u2208R\ud835\udc5b,y\u2208\ud835\udc39(\ud835\udefe(\ud835\udc67)) \u2227y\u2209\ud835\udefe(\ud835\udc39\u266f(\ud835\udc67)).\nFromthedefinitionoftheconcretetransformer \ud835\udc39,wehave:\ud835\udc39(\ud835\udefe(\ud835\udc67))={\ud835\udc53(x)|x\u2208\ud835\udefe(\ud835\udc67)}. Thus,for\nanyy\u2208\ud835\udc39(\ud835\udefe(\ud835\udc67)) ,thereexistssome x\u2208\ud835\udefe(\ud835\udc67)suchthat y=\ud835\udc53(x).Substitutingthisintotheprevious\nexpression gives:\n\u2203\ud835\udc67\u2208\ud835\udc34,\u2203x\u2208R\ud835\udc5b, \ud835\udc53(x)\u2208\ud835\udc39(\ud835\udefe(\ud835\udc67)) \u2227\ud835\udc53(x)\u2209\ud835\udefe(\ud835\udc39\u266f(\ud835\udc67)).\nSince\ud835\udc53(x)\u2208\ud835\udc39(\ud835\udefe(\ud835\udc67))whenever\ud835\udc65\u2208\ud835\udefe(\ud835\udc67), this simplifies to:\n\u2203\ud835\udc67\u2208\ud835\udc34,\u2203x\u2208R\ud835\udc5b, \ud835\udc65\u2208\ud835\udefe(\ud835\udc67) \u2227\ud835\udc53(x)\u2209\ud835\udefe(\ud835\udc39\u266f(\ud835\udc67)).\nThat is, the abstract transformer \ud835\udc39\u266fis unsound if there exists some abstract element \ud835\udc67whose\nconcretizationcontainsatleastonestate xsuchthat\ud835\udc53(x)isnotsoundlycapturedbytheconcretization\nof the abstract output\ud835\udc39\u266f(\ud835\udc67).\nWe collect all such violating abstract elements to get thecounterexample set:\n\ud835\udc34\u2217:=n\n\ud835\udc67\u2208A|\u2203x\u2208\ud835\udefe(\ud835\udc67), \ud835\udc53(x)\u2209\ud835\udefe(\ud835\udc39\u266f(\ud835\udc67))o\n.\nNotably,\ud835\udc34\u2217can be an infinite set.\ud835\udc34\u2217will be an empty set when the abstract transformer is sound.\nTo quantify how severely an abstract transformer \ud835\udc39\u266fdeviates from a sound one, we define a\ndeviation metric \u0394\ud835\udc46that aggregates violations across all elements in \ud835\udc34\u2217to quantify the extent to\nwhich the abstract transformer\ud835\udc39\u266ffails to be sound:\n\u0394\ud835\udc46(\ud835\udc39\u266f):=\ud835\udc67\u00ca\n\ud835\udc67\u2208\ud835\udc34\u2217\ud835\udf08(\ud835\udc67)(1)\nwhere\ud835\udf08(\ud835\udc67)\u2208R\u22650captureshowmuch \ud835\udc39\u266ffailstosatisfythesoundnesspropertywithrespecttothe\nabstractelement \ud835\udc67.Here,\ud835\udc67\u00c9denotesagenericaggregationoperatorthatcombinesthelocalviolation\nmeasures across abstract elements. To ensure that \u0394\ud835\udc46(\ud835\udc39\u266f)defines a well-behaved deviation measure,\nthe aggregation operator\ud835\udc67\u00c9is required to be (1) monotone, (2) non-negative and (3) bounded.\nFormally\u2200\ud835\udc461,\ud835\udc462,\ud835\udc461\u2286\ud835\udc46 2=\u21d2\ud835\udc67\u00c9\ud835\udc461\u2286\ud835\udc67\u00c9\ud835\udc462,0<\ud835\udc67\u00c9\ud835\udc461<\u221e,0<\ud835\udc67\u00c9\ud835\udc462<\u221e.\ud835\udc67\u00c9\ud835\udc46=0if and\nonly if\ud835\udc46=\u2205. Typical instantiations include maximum and mean.\nA natural question arises: how should one define \ud835\udf08(\ud835\udc67)? A naive approach is to assign \ud835\udf08(\ud835\udc67)=1,\ni.e.,uniformlyweightingeachabstractelement,sothat \u0394\ud835\udc46reducestocountingthetotalnumberof\nCost-Driven Synthesis of Sound Abstract Interpreters 11\nunsound abstract elements. However, this strategy is unsatisfactory for two reasons. First, the set\n\ud835\udc34\u2217maybeinfinite,whichrenderssuchcountingintractable.Second,auniformassignmentfailsto\ncapture the heterogeneous contributions of different abstract elements to unsoundness. For example,\nsome\ud835\udc67\u2208\ud835\udc34\u2217maycorrespondtoaviolationcausedbyonlyasingleconcretepoint,whereasothersmay\nexhibitviolationsacrossalmosttheirentireconcretization.Assigningthemthesameweighttherefore\ndiscards crucial information about the relative severity of their contributions to unsoundness.\nTobettercapturetheseverityofsoundnessfailure,wedefine \ud835\udf08(\ud835\udc67)quantitativelybyaggregating\nthe individual pointwise violations over all x\u2208\ud835\udefe(\ud835\udc67). This enables a more fine-grained view of\ntransformer quality and supports optimization-based repair strategies. At the same time, such a\nquantitative formulation enables approximating\u0394 \ud835\udc46through sampling in the future when needed.\nAssuch,wedecomposeeachabstract-levelviolation \ud835\udf08(\ud835\udc67)intoitsunderlyingpointwisecontributions\nbyaccumulatingtheviolationsfromeach x\u2208\ud835\udefe(\ud835\udc67):\ud835\udf08(\ud835\udc67):=x\u00c9\nx\u2208\ud835\udefe(\ud835\udc67)\ud835\udeff(\ud835\udc53(x),\ud835\udefe(\ud835\udc39\u266f(\ud835\udc67))).Here,x\u00c9\nistheaggregationoperatorworkingonconcretestates x,sharingthesamethreepropertiesas\ud835\udc67\u00c9.\nThe pointwise metric \ud835\udeff(\ud835\udc53(x),\ud835\udefe(\ud835\udc39\u266f(\ud835\udc67)))\u2208R\u22650quantifies the degree to which the concrete output\n\ud835\udc53(x)lies outside the abstract output region \ud835\udefe(\ud835\udc39\u266f(\ud835\udc67)). Formally, we require \ud835\udeffto satisfy the following\nproperty\ud835\udcab:\n\ud835\udeff(\ud835\udc53(x),\ud835\udefe(\ud835\udc39\u266f(\ud835\udc67)))=(\n0if\ud835\udc53(x)\u2208\ud835\udefe(\ud835\udc39\u266f(\ud835\udc67))\n>0if\ud835\udc53(x)\u2209\ud835\udefe(\ud835\udc39\u266f(\ud835\udc67))\nThatis,\ud835\udeffevaluatestozeroifallconcreteoutputsarecapturedbytheabstractoutput,indicatingno\ncontribution to unsoundness. It returns a positive value if the output violates the soundness condition.\nTherefore, the total deviation becomes:\n\u0394\ud835\udc46(\ud835\udc39\u266f)=\ud835\udc67\u00ca\n\ud835\udc67\u2208\ud835\udc34\u2217x\u00ca\nx\u2208\ud835\udefe(\ud835\udc67)\ud835\udeff(\ud835\udc53(x),\ud835\udefe(\ud835\udc39\u266f(\ud835\udc67))),(2)\nwhich provides a structured and quantifiable measure of how far \ud835\udc39\u266fdeviates from being a sound\nabstract transformer.\nShape-awaredeviation.Wedefine \ud835\udc5basthenumberofvariablesintheconcretedomaincaptured\nby\ud835\udc67. In the general case, these variables are not independent: constraints may relate multiple\nvariablessimultaneously,reflectingdependenciesacrosstheprogramstate.Nevertheless,byapplying\nFourier\u2013Motzkin Elimination(FME) [ 13], we can always rewrite such relational constraints into\nmultiple constraints expressed with respect to a particular variable of interest. Thus, \ud835\udc39\u266f(\ud835\udc67)\ud835\udc56will have\n\ud835\udc5bdimensions, each dimension encoding the set of constraints associated with the corresponding\nconcrete variable: \ud835\udc39\u266f(\ud835\udc67)\ud835\udc56=(\ud835\udc36\ud835\udc561,\ud835\udc36\ud835\udc562,...,\ud835\udc36\ud835\udc56\ud835\udc58), \ud835\udc56\u2208{1,...,\ud835\udc5b}, where each \ud835\udc36\ud835\udc56\ud835\udc57denotes a scalar or\naffine constraint on the \ud835\udc56-th variable obtained via FME, and \ud835\udc58denotes the number of constraints that\ncan be derived.\nGiven this componentwise interpretation, the total deviation \u0394\ud835\udc46can be further decomposed into a\nnestedaggregationovertheabstractelement \ud835\udc67,itsconcretization x\u2208\ud835\udefe(\ud835\udc67),theupdatedvariableindex\n\ud835\udc56, and the\ud835\udc58constraints associated with that variable:\n\u0394\ud835\udc46(\ud835\udc39\u266f)=\ud835\udc67\u00ca\n\ud835\udc67\u2208\ud835\udc34\u2217x\u00ca\nx\u2208\ud835\udefe(\ud835\udc67)\ud835\udc56\u00ca\ud835\udc5b\n\ud835\udc56=1\ud835\udc57\u00ca\ud835\udc58\n\ud835\udc57=1\ud835\udf00\u0000\ud835\udc53(x)\ud835\udc56,\ud835\udc36\ud835\udc56\ud835\udc57\u0001.(3)\nwhere\ud835\udc56\u00c9,\ud835\udc57\u00c9are the aggregation operators operating on neurons and constraints, sharing the same\nproperties as\ud835\udc67\u00c9,x\u00c9.\n12 Qiuhan Gu, Avaljot Singh, and Gagandeep Singh\nConsistent with the global deviation measure \ud835\udeff, the shape-level violation function \ud835\udf00is required to\nsatisfy an analogous property\ud835\udcab:\n\ud835\udf00\u0000\ud835\udc53(x)\ud835\udc56,\ud835\udc36\ud835\udc56\ud835\udc57\u0001=(\n0if\ud835\udc53(x) \ud835\udc56|=\ud835\udc36\ud835\udc56\ud835\udc57,\n>0if\ud835\udc53(x) \ud835\udc56\u0338|=\ud835\udc36\ud835\udc56\ud835\udc57,\nwhere\ud835\udc53(x) \ud835\udc56|=\ud835\udc36\ud835\udc56\ud835\udc57denotes that the\ud835\udc56-th component of\ud835\udc53(x)satisfies the\ud835\udc57-th constraint\ud835\udc36 \ud835\udc56\ud835\udc57.\nSingle assignment function.In many practical scenarios, the function \ud835\udc53is single-assignment, that\nis, it updates only one of the \ud835\udc5bprogram variables while leaving the others unchanged. Formally, let \ud835\udc56\ndenotetheindexoftheupdatedvariable.Thenforall \ud835\udc56\u2032\u2260\ud835\udc56,thefunctionpreservestheinputvalue:\n\ud835\udc53(x)\ud835\udc56\u2032=\ud835\udc65\ud835\udc56\u2032.Then we will have \ud835\udc53(\ud835\udc63)\ud835\udc56\u2032=\ud835\udc65\ud835\udc56\u2032\u2208\ud835\udefe(\ud835\udc67)\ud835\udc56\u2032=\ud835\udefe(\ud835\udc39\u266f(\ud835\udc67))\ud835\udc56\u2032, i.e., the value of \ud835\udc53(x)\ud835\udc56\u2032remains\nwithin the abstract output region. Hence, the shape-level violation for these variables vanishes:\n\ud835\udf00(\ud835\udc53(x)\ud835\udc56\u2032,\ud835\udc36\ud835\udc56\u2032\ud835\udc57)=0for all\ud835\udc57=1,...,\ud835\udc58and all\ud835\udc56\u2032\u2260\ud835\udc56.Therefore, the decomposition in Equation 3\nsimplifies to:\n\u0394\ud835\udc46(\ud835\udc39\u266f)=\ud835\udc67\u00ca\n\ud835\udc67\u2208\ud835\udc34\u2217x\u00ca\n\ud835\udc65\u2208\ud835\udefe(\ud835\udc67)\ud835\udc57\u00ca\ud835\udc58\n\ud835\udc57=1\ud835\udf00(\ud835\udc53(\ud835\udc65)\ud835\udc56,\ud835\udc36\ud835\udc56\ud835\udc57),(4)\nwhere\ud835\udc56is the unique updated variable.\nTheimplementationof \ud835\udf00naturallydependsontherepresentationoftheconstraint \ud835\udc36\ud835\udc56\ud835\udc57,whichin\nturn is determined by the chosen abstract domain. To systematically capture the structure of \ud835\udc36\ud835\udc56\ud835\udc57, we\ndefine them as below. We denote the neuron value by\ud835\udc66\u2208Rfor clarity.\n\u27e8Constraint\u27e9::=\u27e8ScalarBound\u27e9 | \u27e8AffineBound\u27e9\n\u27e8ScalarBound\u27e9::=\ud835\udc66\u2264\ud835\udc50|\ud835\udc66\u2265\ud835\udc50\n\u27e8AffineBound\u27e9::=\ud835\udc66\u2264\ud835\udc4e 0+\u00cd\ud835\udc5b\n\ud835\udc56=1\ud835\udc4e\ud835\udc56\ud835\udc65\ud835\udc56|\ud835\udc66\u2265\ud835\udc4e 0+\u00cd\ud835\udc5b\n\ud835\udc56=1\ud835\udc4e\ud835\udc56\ud835\udc65\ud835\udc56\nHere,\ud835\udc50\u2208Rdenotesaconcretescalarbound,and \ud835\udc4e0,\ud835\udc4e1,...,\ud835\udc4e\ud835\udc5b\u2208Rarethecoefficientsofanaffine\nfunction over symbolic variables\ud835\udc65 1,\ud835\udc652,...,\ud835\udc65\ud835\udc5b.\nThis definition captures both concrete constant bounds and symbolic affine expressions over input\nvariables. By combining these two types of bounds, we can flexibly express a wide range of abstract\ndomains. Forinstance, inthe Interval domain,each constraintis definedby the scalarbound; Inthe\nPolyhedradomain,aconstraintisrepresentedbytheaffinebound,specifyingalinearconstrainton\ninputs; In the DeepPoly domain, each abstract shape is encoded with a pair of scalar bounds and\napairofaffinebounds(oneupperandonelower),allowingtightersymbolicenclosuresforneural\nnetwork verification, leading to the corresponding constraints taking the form of either scalar bounds\nor affine bounds.\n(1)Scalar Bound. We use \ud835\udc5a\u2208Rto denote the neuron value be evaluated (e.g., \ud835\udc53(x)\ud835\udc56as we\ndiscussed before). When the constraint is a scalar bound of the form \ud835\udc66\u2264\ud835\udc50or\ud835\udc66\u2265\ud835\udc50, the\nviolationisdefinedastheone-sideddistancefromtheevaluatedpoint \ud835\udc5atothefeasibleregion:\n\ud835\udf00(\ud835\udc5a,\ud835\udc66\u2264\ud835\udc50):=max(0,\ud835\udc5a\u2212\ud835\udc50),\ud835\udf00(\ud835\udc5a,\ud835\udc66\u2265\ud835\udc50):=max(0,\ud835\udc50\u2212\ud835\udc5a).\n(2)AffineBound.Whentheconstraintisanaffinebound \ud835\udc66\u2264\ud835\udc4e 0+\u00cd\ud835\udc5b\n\ud835\udc56=1\ud835\udc4e\ud835\udc56\ud835\udc65\ud835\udc56or\ud835\udc66\u2265\ud835\udc4e 0+\u00cd\ud835\udc5b\n\ud835\udc56=1\ud835\udc4e\ud835\udc56\ud835\udc65\ud835\udc56,\ntheviolationis: \ud835\udf00(\ud835\udc5a,\ud835\udc66\u2264\ud835\udc4e 0+\u00cd\ud835\udc4e\ud835\udc56\ud835\udc65\ud835\udc56):=max(0,\ud835\udc5a\u2212(\ud835\udc4e 0+\u00cd\ud835\udc4e\ud835\udc56\ud835\udc65\ud835\udc56)),\ud835\udf00(\ud835\udc5a,\ud835\udc66\u2265\ud835\udc4e 0+\u00cd\ud835\udc4e\ud835\udc56\ud835\udc65\ud835\udc56):=\nmax(0,(\ud835\udc4e 0+\u00cd\ud835\udc4e\ud835\udc56\ud835\udc65\ud835\udc56)\u2212\ud835\udc5a).\n4.3 Constrained Optimization Problem\nTo address the problem of synthesizing sound abstract transformers, we formalize the synthesis\nprocessasaLLMs-basedconstrainedoptimizationproblemthatminimizesaquantitativecostfunction\nL(\ud835\udc39\u266f)subject to hard syntactic and semantic validity constraints.\nCost-Driven Synthesis of Sound Abstract Interpreters 13\nSearch Space.LetHdenote the set of all candidate abstract transformers \ud835\udc39\u266f. WithinH, we define\nthree disjoint subsets that partition the space according to validity and soundness:\nV:={\ud835\udc39\u266f\u2208H|\ud835\udc39\u266fis syntactically and semantically valid},\nG:=n\n\ud835\udc39\u266f\u2208V\f\f\f\u2200\ud835\udc67\u2208\ud835\udc34, \ud835\udc39(\ud835\udefe(\ud835\udc67))\u2286\ud835\udefe(\ud835\udc39\u266f(\ud835\udc67))o\n,U:=V\\G.\nHere,Vcontainsallvalidtransformers, G\u2286Vcontainsallvalidandsoundtransformers,and U\ncontains all valid but unsound transformers that violate the soundness condition for some\ud835\udc67\u2208\ud835\udc34.\nLLMs Generation.Given a prompt \ud835\udc5d, the large language model acts as a stochastic generation\noperator\u03a0 LLMthat produces a batch of candidate abstract transformers:\n{\ud835\udc39\u266f\n\ud835\udc50\ud835\udc4e\ud835\udc5b\ud835\udc51}=\u03a0LLM(\ud835\udc5d),where\u03a0 LLM(\ud835\udc39\u266f|\ud835\udc5d)\u223cPr\u0002\n\ud835\udc39\u266f\f\f\ud835\udc5d,\ud835\udf03LLM,Dtrain, \ud835\udf0bdecode,\ud835\udf16,\ud835\udf13\u0003\n.\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637SAIL: Sound Abstract Interpreters with LLMs 13\nsearch spaceH\nunsoundsound\n\ud835\udc39#\n0\ud835\udc39\u2032#\n1\n\ud835\udc39\u2032\u2032#\n1\ud835\udc39#\n1\u00d7\ud835\udc39#\n2\n\ud835\udc39\u2032#\n2\ud835\udc39#\n3\nFig. 5. Search trajectory of abstract transformers from an unsound starting point into the sound and eventually\nprecise region withinH.\nSearch Space. [Note: use another letter instead of S, refer to fig 5] LetHdenote the set of all\ncandidateabstracttransformers \ud835\udc39\u266f.WithinH,wedefinethreedisjointsubsetsthatpartitionthespace\naccording to validity and soundness:\nV:={\ud835\udc39\u266f\u2208H|\ud835\udc39\u266fis syntactically and semantically valid },\nG:=/b\u221a\ufe02aceleft\uf8ecig\n\ud835\udc39\u266f\u2208V/ba\u221a\ufe02ex/ba\u221a\ufe02ex/ba\u221a\ufe02ex\u2200\ud835\udc67\u2208\ud835\udc34, \ud835\udc39(\ud835\udefe(\ud835\udc67))\u2286\ud835\udefe(\ud835\udc39\u266f(\ud835\udc67))/b\u221a\ufe02ace\u221a\ufe02ight\uf8ecig\n,\nU:=V\\G.\nHere,Vcontainsallvalidtransformers, G\u2286Vcontainsallvalidandsoundtransformers,and U\ncontains all valid but unsound transformers that violate the soundness condition for some \ud835\udc67\u2208\ud835\udc34.\nLLMs Generation. [Note: don\u2019t use phi] Given a prompt \ud835\udc5d, the large language model acts as a\nstochastic generation operator \u03a0LLMthat produces a batch of candidate abstract transformers:\n{\ud835\udc39\u266f\n\ud835\udc50\ud835\udc4e\ud835\udc5b\ud835\udc51}=\u03a0LLM(\ud835\udc5d),where \u03a0LLM(\ud835\udc39\u266f|\ud835\udc5d)\u223cPr/b\u221a\ufe02acketleftbig\n\ud835\udc39\u266f/ba\u221a\ufe02ex/ba\u221a\ufe02ex\ud835\udc5d,\ud835\udf03LLM,Dtrain, \ud835\udf0bdecode,\ud835\udf16,\ud835\udf19/b\u221a\ufe02acket\u221a\ufe02ightbig\n.\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637SAIL: Sound Abstract Interpreters with LLMs 13\n(1)Scalar Bound. When the constraint is a scalar bound of the form \ud835\udc66\u2264\ud835\udc50or\ud835\udc66\u2265\ud835\udc50, the violation\nis defined as the one-sided distance from the evaluated point yto the feasible region:\n\ud835\udf00(y,\ud835\udc66\u2264\ud835\udc50):=max(0,y\u2212\ud835\udc50),\n\ud835\udf00(y,\ud835\udc66\u2265\ud835\udc50):=max(0,\ud835\udc50\u2212y).\n(2)AffineBound.Whentheconstraintisanaffinebound \ud835\udc66\u2264\ud835\udc4e0+/\u221a\ufe04ummationtext.\uf8f6\ud835\udc5b\n\ud835\udc56=1\ud835\udc4e\ud835\udc56\ud835\udc65\ud835\udc56or\ud835\udc66\u2265\ud835\udc4e0+/\u221a\ufe04ummationtext.\uf8f6\ud835\udc5b\n\ud835\udc56=1\ud835\udc4e\ud835\udc56\ud835\udc65\ud835\udc56,\nthe violation is:\n\ud835\udf00(y,\ud835\udc66\u2264\ud835\udc4e0+\u2211\ufe01\n\ud835\udc4e\ud835\udc56\ud835\udc65\ud835\udc56):=max(0,y\u2212(\ud835\udc4e0+\u2211\ufe01\n\ud835\udc4e\ud835\udc56\ud835\udc65\ud835\udc56)),\n\ud835\udf00(y,\ud835\udc66\u2265\ud835\udc4e0+\u2211\ufe01\n\ud835\udc4e\ud835\udc56\ud835\udc65\ud835\udc56):=max(0,(\ud835\udc4e0+\u2211\ufe01\n\ud835\udc4e\ud835\udc56\ud835\udc65\ud835\udc56)\u2212y).\n4.3 Constrained Optimization Problem\nsearch spaceS\nunsoundsound\nprecise\n\ud835\udc39#\n0\ud835\udc39\u2032#\n1\n\ud835\udc39\u2032\u2032#\n1\ud835\udc39#\n1\u00d7\ud835\udc39#\n2\n\ud835\udc39\u2032#\n2\ud835\udc39#\n3\ud835\udc39#\n4\nFig. 5. Search trajectory of abstract transformers from an unsound starting point into the sound and eventually\nprecise region withinS.\nToaddresstheproblemofsynthesizingsoundabstracttransformers,weformalizethesynthesis\nprocessasaLLMs-basedconstrainedoptimizationproblemthatminimizesaquantitativecostfunction\nL(\ud835\udc39\u266f)subject to hard syntactic and semantic validity constraints.\nSearch Space. LetSdenote the set of all candidate abstract transformers \ud835\udc39\u266f. WithinS, we define\nthree disjoint subsets that partition the space according to validity and soundness:\nV:={\ud835\udc39\u266f\u2208S|\ud835\udc39\u266fis syntactically and semantically valid },\nG:=/b\u221a\ufe02aceleft\uf8ecig\n\ud835\udc39\u266f\u2208V/ba\u221a\ufe02ex/ba\u221a\ufe02ex/ba\u221a\ufe02ex\u2200\ud835\udc67\u2208\ud835\udc34, \ud835\udc39(\ud835\udefe(\ud835\udc67))\u2286\ud835\udefe(\ud835\udc39\u266f(\ud835\udc67))/b\u221a\ufe02ace\u221a\ufe02ight\uf8ecig\n,\nU:=V\\G.\nHere,Vcontainsallvalidtransformers, G\u2286Vcontainsallvalidandsoundtransformers,and U\ncontains all valid but unsound transformers that violate the soundness condition for some \ud835\udc67\u2208\ud835\udc34.\nLLMs Generation. Given a prompt \ud835\udc5d, the large language model acts as a stochastic generation\noperator \u03a0LLMthat produces a batch of candidate abstract transformers:\n{\ud835\udc39\u266f\n\ud835\udc50\ud835\udc4e\ud835\udc5b\ud835\udc51}=\u03a0LLM(\ud835\udc5d),where \u03a0LLM(\ud835\udc39\u266f|\ud835\udc5d)\u223cPr/b\u221a\ufe02acketleftbig\n\ud835\udc39\u266f/ba\u221a\ufe02ex/ba\u221a\ufe02ex\ud835\udc5d,\ud835\udf03LLM,Dtrain, \ud835\udf0bdecode,\ud835\udf16,\ud835\udf19/b\u221a\ufe02acket\u221a\ufe02ightbig\n.\nFig. 6. [Note: remove precision, bkg] Search trajec-\ntory of abstract transformers within the search\nspaceH. Starting from \ud835\udc39\u266f\n0, the process iteratively\nselects the candidate transformer with the low-\nest cost function value each round. Each refine-\nment step must satisfy the progress condition\nL(R(\ud835\udc39\u266f))<L(\ud835\udc39\u266f)\u2212\ud835\udf06; for example, the transi-\ntion from\ud835\udc39\u266f\n1to\ud835\udc39\u2032\u266f\n2is invalid since it violates this\nconstraint.Here,{\ud835\udc39\u266f\n\ud835\udc50\ud835\udc4e\ud835\udc5b\ud835\udc51}\u2286H, which is the generated set\nof candidates. \ud835\udf03LLMdenotes the model parameters\nlearnedfromitspretrainingcorpus Dtrain,whichcap-\nture the model\u2019s basic knowledge of language and\nsemantics.\ud835\udf0bdecoderepresents the internal decoding\npolicy (e.g., temperature, nucleus sampling, beam\nwidth),and\ud835\udf16modelsthestochasticfactorsintroduced\nduring token generation. The term \ud835\udf19denotes outer\nfeedback that implicitly modifies the reward func-\ntion, such as reinforcement learning from human\nfeedback (RLHF), instruction tuning, or system-level\nprompting, which biases the model toward human-\naligned or task-specific behaviors. Together, these\nfactorsjointlydeterminetheconditionalgeneration\nFig. 5. Search trajectory of abstract transform-\ners within the search space H. Starting from\n\ud835\udc39\u266f\n0, the process iteratively selects the candidate\ntransformer with the lowest cost function value\neach round. Each refinement step must satisfy\nthe progress condition L(R(\ud835\udc39\u266f))<L(\ud835\udc39\u266f)\u2212\ud835\udf06 ;\nfor example, the transition from \ud835\udc39\u266f\n1to\ud835\udc39\u2032\u266f\n2is in-\nvalid since it violates this constraint.Here,{\ud835\udc39\u266f\n\ud835\udc50\ud835\udc4e\ud835\udc5b\ud835\udc51}\u2286H, which is the generated set\nof candidates. \ud835\udf03LLMdenotes the model parameters\nlearnedfromitspretrainingcorpus Dtrain,whichcap-\nture the model\u2019s basic knowledge of language and\nsemantics.\ud835\udf0bdecoderepresents the internal decoding\npolicy (e.g., temperature, nucleus sampling, beam\nwidth),and\ud835\udf16modelsthestochasticfactorsintroduced\nduring token generation. The term \ud835\udf13denotes outer\nfeedback that implicitly modifies the reward func-\ntion, such as reinforcement learning from human\nfeedback (RLHF), instruction tuning, or system-level\nprompting, which biases the model toward human-\naligned or task-specific behaviors. Together, these\nfactorsjointlydeterminetheconditionalgeneration\ndistribution \u03a0LLM(\ud835\udc39\u266f|\ud835\udc5d), explaining various perfor-\nmance(candidatesetswithvaryingcorrectnessand\nsoundness rate) of different models under identical\npromptingconditions.Inotherword, \u03a0LLMinducesa\nspecificprobabilitydistributionoverthesearchspace\nHof candidate transformers.\nCostFunction.Asdescribedin\u00a74.2,wedefinea\ndomain-specific cost function: L(\ud835\udc39\u266f)=\u0394\ud835\udc46(\ud835\udc39\u266f),where \u0394\ud835\udc46(\ud835\udc39\u266f)measures the soundness deviation of\nthe candidate transformer. which captures how far\ud835\udc39\u266fdeviates from the soundness condition, and is\nzero if and only if\ud835\udc39\u266fis sound.\nOptimization Objective.The synthesis problem is formulated as minimizing the soundness loss:\n\ud835\udc39\u266f\u2217=arg min\n\ud835\udc39\u266fL(\ud835\udc39\u266f)s.t.\ud835\udc39\u266fsatisfies all syntactic and semantic validity constraints.\nHere\ud835\udc39\u266f\u2217denotesthesoundtransformer.Forany \ud835\udc39\u266f\u2208U,thepositive \u0394\ud835\udc46(\ud835\udc39\u266f)quantifiesthedegreeof\nunsoundness, guiding the refinement process to iteratively reduce \u0394\ud835\udc46until the transformer enters G.\nSearchStrategy.Thecostfunction Lisgenerallynon-differentiable,sotechnicallyanygradient\nsearchstrategycannotbeutilizedtoexplorethespaceofcandidatetransformers.However,toguarantee\nconvergencewhilepreservingexplorationflexibility,wedesignarefinementstrategy.Formally,given\naninitialcandidate \ud835\udc39\u266f\n0\u2208H,thesynthesissystemiterativelyrefinesthetransformerusingarefinement\noperatorR:H\u2192H , which aims to monotonically decrease the loss: L(R(\ud835\udc39\u266f))<L(\ud835\udc39\u266f).To\n14 Qiuhan Gu, Avaljot Singh, and Gagandeep Singh\nensurethesynthesisterminateswithinafinitenumberofsteps,werequireeachrefinementtoachieve\na minimum improvement on the cost function:\nL(R(\ud835\udc39\u266f))<L(\ud835\udc39\u266f)\u2212\ud835\udf06,(5)\nwhere\ud835\udf06>0isafixedthresholdcontrollingtheminimumprogressperiteration.Thesearchtrajectory\nis visualized in Fig. 5. Each step applies Rto improve the transformer. When progress stalls (i.e., no\ndecrease for \ud835\udc3econsecutive rounds), the system can either terminate or adaptively reduce \ud835\udf06to explore\nalternative refinement paths. The process continues until a sound transformer is found (i.e., \ud835\udc39\u266f\n\ud835\udc58\u2208G)\nor a maximum number of attempts is reached. We therefore proceed to prove the convergence of the\nalgorithm under this setting.\n4.4 Convergence Proof\nRequirements.The convergence proof relies on the following requirements:\n(R1)Forallconstraintterms \ud835\udc36\ud835\udc56\ud835\udc57andevaluatedcomponents \ud835\udc53(x)\ud835\udc56inEquation4,bothvaluesare\nfinite, i.e.,\ud835\udc53(x) \ud835\udc56,\ud835\udc36\ud835\udc56\ud835\udc57\u2208R.\n(R2)The aggregation operators\u00c9are all non-negative and bounded, i.e., \u2200\ud835\udc46,0<\u00c9\ud835\udc46<\u221e, and\nthe aggregated deviation is finite and non-negative, i.e., \u2200\ud835\udc461,\ud835\udc462,\ud835\udc461\u2286\ud835\udc46 2=\u21d2\u00c9\ud835\udc461\u2286\u00c9\ud835\udc462.\u00c9\ud835\udc46=0if and only if\ud835\udc46=\u2205.\nTheorem 4.1.Assume each refinement step Rsatisfies the ruleL(R(\ud835\udc39\u266f))<L(\ud835\udc39\u266f)\u2212\ud835\udf06,where\n\u2200\ud835\udc39\u266f,0<L(\ud835\udc39\u266f)<\u221eand\ud835\udf06>0.Withfixed \ud835\udf06,therefinementprocessreaches L(\ud835\udc39\u266f\n\ud835\udc47)=0inatmost\n\ud835\udc47\u2264\u0018\nL(\ud835\udc39\u266f\n0)\n\ud835\udf06\u0019\nsuccessful refinement steps. In particular,L(\ud835\udc39\u266f\n\ud835\udc47)=0and hence\ud835\udc39\u266f\n\ud835\udc47\u2208G.\nProof.Let\ud835\udc3f\ud835\udc61:=L(\ud835\udc39\u266f\n\ud835\udc61)denote the cost at the \ud835\udc61-th successful refinement. By the improvement\nrule (R3), for every such step we have L(\ud835\udc39\u266f\n\ud835\udc61+1)<L(\ud835\udc39\u266f\n\ud835\udc61)\u2212\ud835\udf06.Unrolling the inequality yields\nL(\ud835\udc39\u266f\n\ud835\udc61)<L(\ud835\udc39\u266f\n0)\u2212\ud835\udc61\ud835\udf06.Choose\ud835\udc47:=\u0018\nL(\ud835\udc39\u266f\n0)\n\ud835\udf06\u0019\n. ThenL(\ud835\udc39\u266f\n\ud835\udc47)<L(\ud835\udc39\u266f\n0)\u2212\ud835\udc47\ud835\udf06\u22640 . Combined with\nnon-negativity (R2), we obtain L(\ud835\udc39\u266f\n\ud835\udc47)=0. By the definition of the cost (zero iff sound), this implies\n\ud835\udc39\u266f\n\ud835\udc47is sound, i.e.,\ud835\udc39\u266f\n\ud835\udc47\u2208G.\u25a1\nTheparameter \ud835\udf06controlsthetrade-offbetweenconvergencespeedandrefinementperformance.\nA larger\ud835\udf06enforces a stronger improvement per refinement step, leading to faster convergence in\ntheory but making it harder for refinement to satisfy the required reduction. In contrast, a smaller \ud835\udf06\nallows more gradual progress and increases the likelihood of finding valid refinements, though at the\ncostofslowerconvergenceandmoreiterations.InourimplementationbasedonLLMs, \ud835\udf06isfixed.\nThe model performs refinement within a fixed maximum number of attempts; if convergence is not\nachieved after reaching this limit, the process terminates.\n4.5 Instantiation\nRelaxation StrategyIn practice, directly evaluating the loss function Equation 4 is intractable, since\nboththesetofabstractelements \ud835\udc34\u2217andtheconcretization \ud835\udefe(\ud835\udc67)maybeinfinite.Tomakecomputation\nfeasible, we introduce a relaxation strategy by sampling from a finite subset of concretizations,\ndenoted\ud835\udefe sample(\ud835\udc67)\u2286\ud835\udefe(\ud835\udc67). For each sampled point\ud835\udc65 \ud835\udc56, wherex\u2208\ud835\udefe sample(\ud835\udc67), we assign a normalized\nweight\ud835\udc64(\ud835\udc65\ud835\udc56)thatreflectsitsrelativecontributiontounsoundness.Intuitively,theweightfunction\nhighlightsthoseinputsthataremoresemanticallycritical,suchasvaluesclosetononlinearactivation\nCost-Driven Synthesis of Sound Abstract Interpreters 15\nthresholds or regions prone to errors. The relaxed loss is therefore computed as:\nf\u0394\ud835\udc46(\ud835\udc39\u266f)=\ud835\udc67\u00ca\n\ud835\udc67\u2208\ud835\udc34\u2217x\u00ca\nx\u2208\ud835\udefe\ud835\udc60\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc52(\ud835\udc67)\ud835\udc57\u00ca\ud835\udc58\n\ud835\udc57=1\ud835\udc64(\ud835\udc65\ud835\udc56)\u00b7\ud835\udf00\u0000\ud835\udc53(x)\ud835\udc56,\ud835\udc36\ud835\udc56\ud835\udc57\u0001.(6)\nThis relaxation ensures a tractable loss, while the weighting strategy preserves the informativeness\nof the deviation measure by emphasizing sampled points that contribute most to potential violations.\nWeight FunctionThe weights are normalized as \ud835\udc64(\ud835\udc65\ud835\udc56)=\ud835\udf19(\ud835\udc53,\ud835\udc65 \ud835\udc56)\u00cd\n\ud835\udc65\u2032\u2208\ud835\udefesample(\ud835\udc67)\ud835\udf19(\ud835\udc53,\ud835\udc65\u2032\n\ud835\udc56). Here,\ud835\udc53denotes\nthe concrete function under analysis, \ud835\udf19(\ud835\udc53,\ud835\udc65\ud835\udc56)is a function-specific score designed to prioritize\nsemantically critical configurations\u2014such as those that cross nonlinear boundaries (e.g., zero for\nReLU function), introduce branching behavior, or expose unstable symbolic patterns.\nInourformulation,wedefine \ud835\udf19(\ud835\udc53,\ud835\udc65\ud835\udc56)basedonthesensitivityoftheoperatorwithrespecttoits\ninput, measured by the numerical gradient:\ud835\udf15\ud835\udc53(\ud835\udc65)\n\ud835\udf15\ud835\udc65\ud835\udc56\u2248\ud835\udc53(\ud835\udc65\ud835\udc56+\ud835\udf16)\u2212\ud835\udc53(\ud835\udc65 \ud835\udc56\u2212\ud835\udf16)\n2\ud835\udf16.To ensure robustness, we apply\nthesoftplustransformationtothegradientmagnitude,yielding: \ud835\udf19(\ud835\udc53,\ud835\udc65\ud835\udc56)=log\u00001+exp(\u2225\u2207 \ud835\udc65\ud835\udc56\ud835\udc53(\ud835\udc65\ud835\udc56)\u2225)\u0001.\nThis construction leverages gradient information toemphasize inputs where the operator is most\nsensitive, while the softplus function prevents vanishing contributions in flat regions (where the\ngradientwouldotherwisebezero).Asaresult,theweightfunctioncapturesbothlocalsensitivityand\nstability, assigning higher importance to configurations likely to expose unsoundness in\ud835\udc39\u266f.\nAlgorithm 1 summarizes the whole constrained optimization procedure. In each round, we sample\na set of candidate DSL transformers from the LLM, run validation to fix syntactic or semantic errors,\ndiscard those that cannot be repaired within the allowed number of attempts, and then invoke the\nsoundness verifier. If a candidate is sound, we return it immediately and terminate; otherwise we\nscoretheunsoundoneswiththecostfunction,keepthebestunsoundcandidate,andaugmentthenext\npromptwithcounterexamples,andrepeatuptotheretrybudget.Ifnosoundtransformerisfound,we\nreturn the best unsound fallback. We set\ud835\udf06=0.0001in practice.\n5 Evaluation\nWe evaluate our approach to answer the following research questions:\nRQ1:Howeffectiveisourframework\u2019sprocedureinguidingthesynthesisprocesstowardsound\nabstract transformers with different LLMs? (\u00a75.1)\nRQ2:Can the framework generate abstract transformers for complex non-linear operations? (\u00a75.2)\nRQ3: How precise are the synthesized transformers for verifying neural networks? (\u00a75.3)\nRQ4:Howdoestheperformancechangewhenthecostfunctionorthevalidationmoduleisablated,\nand can LLMs still synthesize sound operators? (\u00a75.4)\nKeyObservations.ForRQ1,weconsiderthepopularHardSigmoidactivationforwhichnoprior\ngloballysoundtransformerexist.ForRQ2,2outof3concreteoperations(e.g.,GeLU,ELU)also\ndonothaveanypriorgloballysoundabstracttransformer.ForRQ3-4,wealsoconsideradditional\noperations for which handcrafted transformers exist. We find that as model capability increases,\nthedependenceoncost-functionfeedbackdecreasesoverall(RQ1)butre-emergesprominentlyfor\ncomplex operators (RQ2), highlighting that the cost function improves model capability overall\nthroughaformalprocessthatprovidesexplicitcorrectnessguarantees.RQ3demonstratesthatour\nframeworkachievesconsistentlyhighprecisionacrossawiderangeofoperatorsandabstractdomains.\nFor cases where handcrafted transformers exist, the synthesized transformers match the precision of\nexisting transformers. Our observations for RQ4 show that cost-function guidance with validation is\nessential for our framework to synthesize diverse and sound abstract transformers.\nExperimentalSetupOurframeworkisimplementedinPythonandintegratedwiththeConstraint-\nFlowverification engine. All experiments are conducted on a GPU cluster node equipped with four\nNVIDIA A100 GPUs (40 GB each), an AMD EPYC 7763 CPU, and 256 GB of RAM, running\n16 Qiuhan Gu, Avaljot Singh, and Gagandeep Singh\nAlgorithm 1Multi-Round Abstract Transformer Generation and Validation\nInput:Prompting template \ud835\udc43; model client \ud835\udc40; validator\ud835\udc49\ud835\udc4e; soundness verifier \ud835\udc49\ud835\udc52; cost evaluator \ud835\udc38;\nminimum decrease\ud835\udf06; max retries\ud835\udc45.\nOutput:result(bool),code(DSL).\n1:result\u2190\ud835\udc53\ud835\udc4e\ud835\udc59\ud835\udc60\ud835\udc52; code\u2190\u2205;\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61_\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc52\u2190\u2205;\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61_\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52\u2190\u221e\n2:for\ud835\udc5f=1\u2192\ud835\udc45do\n3:Augment\ud835\udc43with previous failures and counterexamples (if any)\u22b2use failed code to guide\nnext round\n4:Generate completions{\ud835\udc50 1,\ud835\udc502,...}from\ud835\udc40using\ud835\udc43\n5:foreach completion\ud835\udc50 \ud835\udc56do\n6:Extract candidate DSL code\ud835\udc51from\ud835\udc50 \ud835\udc56\n7:if\ud835\udc51is emptythen\n8:continue\u22b2skip empty extraction\n9:for\ud835\udc5f=1\u2192\ud835\udc45do\n10:\ud835\udc56\ud835\udc60\ud835\udc63\ud835\udc4e\ud835\udc59\ud835\udc56\ud835\udc51,\ud835\udc51\u2190\ud835\udc49\ud835\udc4e(\ud835\udc51)\u22b2 check the syntax and semantic correctness, fix errors if invalid\n11:if\u00acisvalidthen\n12:continue\u22b2skip invalid ones\n13:(\ud835\udc5d\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc52\ud835\udc51,\ud835\udc50\ud835\udc52)\u2190\ud835\udc49\ud835\udc52(\ud835\udc51)\u22b2verify soundness\n14:if\ud835\udc5d\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc52\ud835\udc51then\n15:return(true,\ud835\udc51)\u22b2sound transformer found\n16:else if\ud835\udc50\ud835\udc52\u2260\u2205then\n17:\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52\u2190\ud835\udc38(\ud835\udc51)\u22b2evaluate unsound transformer with cost function\n18:if\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52<\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61_\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52\u2212\ud835\udf06then\n19:\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61_\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52\u2190\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52;\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61_\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc52\u2190\ud835\udc51\n20:Save(\ud835\udc51,\ud835\udc50\ud835\udc52)for next prompt augmentation\u22b2keep \"best\" unsound candidate\n21:return(false,\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61_\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc52)\u22b2terminate and return the \"best\" unsound fallback\nCUDA 12.2. We evaluate our framework on a suite of challenging and popular neural network\nactivation functions,includingstandard piecewise-linearactivations,for whichrelatively lesswork\nexistsondesigninggloballysoundabstracttransformers,suchasHardTanh,HardSigmoid,HardSwish,\nGelu, Elu, and Sigmoid. We choose four state-of-the-art models: GPT-5, GPT-4o, Llama4-Maverick,\nandClaude-Opus-4.SinceConstraintFlowcannotdirectlyverifynonlinearactivationfunctions\nGelu,Elu, andSigmoiddue torelying onZ3as theunderlying SMTsolver, wemanually verifythe\nsoundness and provide counterexamples for these. We use the stochastic decoding during generation,\ntherefore to ensure fair and stable evaluation, we repeat each synthesis task multiple times under\nidentical configurations and report the best performance across runs.\n5.1 Effectiveness of Cost-Function Guidance\nWe demonstrate the effectiveness of the proposed cost-function guidance using the synthesis of\nthe HardSigmoid operator, a nontrivial piecewise-linear activation that had no prior handcrafted\ntransformer in the DeepPoly domain. Four models (GPT-5, GPT-4o, Llama4-Maverick, and Claude-\nOpus-4) were tasked to generate valid transformers under identical configurations. Fig. 6 visualizes\ntheirsynthesisbehavior,whereFig.6areportsthenumberofgeneration,repair,andcounterexamples\nrounds, and Fig. 6b, Fig. 6c, Fig. 6d show the cost trajectories for representative runs.\nAcross all models, GPT-5 completes synthesis without explicit help of counterexamples, while\nGPT-4o converges within several rounds with some feedback. In contrast, Llama4-Maverick and\nCost-Driven Synthesis of Sound Abstract Interpreters 17\nGPT-5 GPT-4o Llama4-Maverick Claude-Opus-401020304050Numbers\n1 12840\n0139\n3\n0316\n9\nLLM Gen Rounds\nLLM Repair Rounds\nCounterexamples\nSound Generation\nUnsound Generation\n(a) Overall synthesis statistics across models.\n1 2 3 40.00.10.20.30.40.50.60.7Cost Function\n0.2290.229\n0.137\n0.000 (b) Cost-function trajectory for GPT-4o.\n12345678910111213141516170.00.51.01.52.02.5Cost Function\n2.524\n0.1730.067\n0.0360.023\n0.000\n(c) Cost-function trajectory for Llama4-Maverick.\n1 2 3 4 5 6 7 8 9 100246810121416Cost Function\n1.167\n0.081 0.000 (d) Cost-function trajectory for Claude-Opus-4.\nFig. 6. Synthesis performance of different LLMs on HardSigmoid operators in the DeepPoly domain.\nClaude-Opus-4requiresubstantiallymoreverificationfeedbackandrepairrounds,exhibitingmultiple\ncostdropstriggeredbycounterexamples.Theseresultsrevealthatmodelcapabilitystronglyinfluences\ntherelianceonformalguidance.ByfollowingtheoptimizationruledefinedinEquation5,thesynthesis\nprocessdiscardsoccasionalabnormalevaluationsarisingfromsamplingrandomness,therebymaintain-\ningaconsistentlydecreasingandconvergentcosttrajectory.Forexample,asshowninFig.6d,thecost-\nfunctiontrajectoryforClaude-Opus-4exhibitsastabledownwardtrendafterfilteringoutsuchoutliers.\nGelu Elu Sigmoid024681012Numbers\n10\n7\n1310\n09\n4\n0\nLLM Gen Rounds\nLLM Repair Rounds\nCounterexamples\nSound Generation\nUnsound Generation\nFig. 7. Performance of GPT-5 generating DeepPoly\ntransformers for GELU, ELU, and Sigmoid.Overall, the results demonstrate that while\nstronger models such as GPT-5 can generate\nsoundtransformersautonomously,cost-function\nfeedbackremainsessentialforlesscapablemod-\nels. The cost function thus compensates for\nmodel limitations and ensures convergence in\ncomplex synthesistasks.Comprehensive results\nfor the synthesis of other sound abstract inter-\npreters across multiple models and domains can\nbe found in Appendix G.\n5.2 Handling Novel Nonlinear Operators\nTo further test the limits of our framework\u2019s\nsynthesis capability for handling complex nonlinear operations, we task the best-performing model,\n18 Qiuhan Gu, Avaljot Singh, and Gagandeep Singh\nGPT-5, with generating DeepPoly transformers for three representative nonlinear operators: GeLU,\nELU,andSigmoid.Formally,theGeLUfunctionisdefinedas GELU(\ud835\udc65)=1\n2\ud835\udc65(1+erf(\ud835\udc65/\u221a\n2)),where\nerfdenotestheGaussianerrorfunction;TheELUfunctionisdefinedas ELU(\ud835\udc65)=\ud835\udc65 for\ud835\udc65>0and\n\ud835\udefc(\ud835\udc52\ud835\udc65\u22121)for\ud835\udc65\u22640with the default \ud835\udefc=1; The Sigmoid function is defined as \ud835\udf0e(\ud835\udc65)=1\n1+\ud835\udc52\u2212\ud835\udc65. The\nsynthesis performance is shown in Fig. 7 and the synthesis results are shown in Fig. 8, Fig. 9. We\ninclude the results of Sigmoid in Fig. 11 in Appendix E. We observe that GPT-5 can generate sound\npiecewise-lineartransformerswithoutexplicitcost-functionguidance(asshowninFig.6a).However,\nas the complexity of the target operator increases, the reliance on cost-function feedback re-emerges\nand increases as the complexity of the target operator grows. This trend highlights that while GPT-5\nexhibits strong structural understanding, the cost-function feedback is essential for soundly capturing\nthe behavior of novel and complicated nonlinear operators such as GeLU.\nx2\n0246y\n0\n uGeLU(x)\nLower\nUpper\n(a)\ud835\udc59<\ud835\udc62<0.\nx2\n0246y\n0\n uGeLU(x)\nLower\nUpper (b)0<\ud835\udc59<\ud835\udc62.\nx2\n0246y\n0\nuGeLU(x)\nLower\nUpper (c)\ud835\udc59<0<\ud835\udc62.\nFig. 8.DeepPoly Transformer for GeLU.The transformer considers three cases: (a) for \ud835\udc59<\ud835\udc62<0 , lower\nbound and upper bound are \ud835\udc66=0.5\ud835\udc65 and\ud835\udc66=0 respectively; (b) for 0<\ud835\udc59<\ud835\udc62 , lower bound and upper\nbound are\ud835\udc66=0.5\ud835\udc65 and\ud835\udc66=\ud835\udc65 respectively; (c) for the mixed case ( \ud835\udc59<0<\ud835\udc62 ), the upper bound is the secant\nline connecting(\ud835\udc59,GeLU(\ud835\udc59)) and(\ud835\udc62,GeLU(\ud835\udc62)) , while the lower bound remains \ud835\udc66=0.5\ud835\udc65 . Since GeLU(\ud835\udc65) is\nmonotonic, the interval bounds correspond directly to GeLU(\ud835\udc59) andGeLU(\ud835\udc62) . Each shaded region shows the\narea enclosed by the DeepPoly\u2019s polyhedra bounds, visualizing how they approximate the GeLU activation.\nx2\n1\n012y\nu0ELU(x)\nLower\nUpper\n(a)\ud835\udc59<\ud835\udc62<0.\nx2\n1\n012y\nu0ELU(x)\nLower\nUpper (b)0<\ud835\udc59<\ud835\udc62.\nx4\n3\n2\n1\n0123y\nu0ELU(x)\nLower\nUpper (c)\ud835\udc59<0<\ud835\udc62.\nFig. 9.DeepPoly Transformer for ELU.The transformer is divided into three cases: (1) for \ud835\udc59<\ud835\udc62<0 , the\nlower and upper bounds are \ud835\udc66=\u22121 and the secant line connecting (\ud835\udc59,ELU(\ud835\udc59)) and(\ud835\udc62,ELU(\ud835\udc62)) , respectively;\n(2) for 0<\ud835\udc59<\ud835\udc62 , the lower and upper bounds are \ud835\udc66=\ud835\udc65 and the same secant line; (3) for the mixed case\n(\ud835\udc59<0<\ud835\udc62 ), the upper bound is again the secant through (\ud835\udc59,ELU(\ud835\udc59)) and(\ud835\udc62,ELU(\ud835\udc62)) , while the lower bound\nis\ud835\udc66=\ud835\udc65. SinceELU(\ud835\udc65)is monotonic, the scalar bounds correspond directly toELU(\ud835\udc59)andELU(\ud835\udc62).\n5.3 Evaluating Precision for Verifying Neural Networks\nThereexistsno\u201cbest\u201dtransformer,thereforetoassesstheperformanceofsynthesizedtransformers\nquantitatively, we measure the precision of GPT-5-generated transformers under the DeepPoly\nCost-Driven Synthesis of Sound Abstract Interpreters 19\ndomain for verifying neural networks. We compare against handcrafted transformers provided by\nConstraintFlow, which offers globally sound and the most precise transformers. More importantly,\nweshowtheprecisionofgeneratedtransformersfornon-linearandcomplicatedoperators,whose\ncorresponding abstract transformers do not exist in the literature, demonstrating the efficiency of\nourframework.Theunderlyingverificationproblemisstandardimagerobustnessverification:for\na correctly classified input image, we check whether the network predicts the correct label for\nall perturbed inputs within an perturbation ball of radius \ud835\udf16\u2208Raround the original input. In our\nevaluation, precision is defined as the fraction of baseline-correct test inputs (i.e., samples correctly\nclassified by the original network without the perturbation) that can be certified under a given\nperturbation bound.\nWe evaluate all transformers across multiple network architectures and training regimes, typically\nusedformeasuringverificationperformanceintheliterature[ 6,51]includingbothfullyconnected\n(FCN) and convolutional (Conv) networks trained on MNIST [ 15] and CIFAR10 [ 29] datasets.\nFor MNIST, we apply the perturbation to the entire image, whereas for CIFAR10 we restrict the\nperturbation toa single pixelto reflect morelocalized robustness settings.Weconsider the fulltest\nset and set the batch size to 100. For perturbations, we use \ud835\udf16=0.005 for MNIST and \ud835\udf16=0.8for\nCIFAR10, which are standard choices in robustness verification. Part of the results summarized\nin Table 1, GPT-5-generated transformers have different syntactical forms but the same semantics as\ntransformersprovidedbyConstraintFlow,achievingprecisiononparwithhandcraftedtransformers\nforallexistingoperators.Fornovelnon-linearoperatorswithoutexistinghandcraftedtransformers,\nGPT-5\u2013generatedtransformersalsoachievehighprecision,demonstratingourframework\u2019sability\ntosynthesizesoundtransformerswithgoodqualitiesbeyondthescopeofmanuallydesignedones.\nComprehensiveresultsandanadditionalevaluationfortransformerssynthesizedundertheDeepZ\ndomain can be found in Appendix D.\nTable 1. Precision comparison across different Networks based on the DeepPoly domain.\nDataset Network Training Activation Layers Perturbation\ud835\udf50Precision\nOur work Handcrafted\nMNIST Conv Standard ReLU 6 0.005 1.0000 1.0000\nConv Standard ReLU6 3 0.005 1.0000\u2717\nFCN_5\u00d7100 DiffAI HardTanh 5 0.005 0.9500 0.9500\nFCN_6\u00d7500 PGD HardSigmoid 6 0.005 1.0000\u2717\nFCN_3\u00d7100 Standard GELU 3 0.005 0.9400\u2717\nFCN_4\u00d71024 Standard GELU 4 0.005 1.0000\u2717\nFCN_3\u00d7100 Standard ELU 3 0.005 0.1400\u2717\nCIFAR10 Conv DiffAI ReLU 3 0.8 1.0000 1.0000\nFCN_4\u00d7100 Standard ReLU6 4 0.8 0.4490\u2717\nFCN_7\u00d71024 Standard HardTanh 7 0.8 0.8462 0.8462\nFCN_4\u00d7100 Standard HardSwish 4 0.8 0.1154\u2717\nFCN_6\u00d7500 PGD HardSigmoid 6 0.8 1.0000\u2717\nFCN_6\u00d7500 PGD GELU 6 0.8 1.0000\u2717\nFCN_7\u00d71024 Standard GELU 7 0.8 0.9787\u2717\n5.4 Ablation Study\nTo assess the individual contributions of the cost function and the validation\u2013repair mechanism,\nwe conduct a three-way ablation study using the Llama4-Maverick model as the synthesis engine.\n20 Qiuhan Gu, Avaljot Singh, and Gagandeep Singh\nWepromptthemodeltogenerateDeepPolytransformersforasetofrepresentativeoperatorsunder\nthree settings: (1)with cost function and validation\u2013repair module, representing our full system; (2)\nwithout cost function but with validation\u2013repair, where the model relies solely on repair feedback to\ncorrect unsound generations; and (3)without both cost function and validation\u2013repair, where the\nmodel depends purely on its intrinsic reasoning ability without any external feedback.\nTheresultsareshowninFig.12,Fig.13andFig.14inAppendixF.whichexhibitaclearhierarchy\nacross the three configurations. Comparing setting (1) with setting (2) demonstrates that the cost\nfunctionsubstantiallyimprovesthesoundnessandconsistencyof thesynthesizedtransformers.By\nquantitativelypenalizingunsoundbehaviorsandprovidingcontinuousoptimizationfeedback,thecost\nfunction enables the model to refine candidates beyond mere syntactic validity, achieving soundness\nthatgeneralizesacrossoperators.Incontrast,comparingsetting(2)withsetting(3)highlightsthe\nimportance of the validation\u2013repair mechanism: even without cost feedback, it ensures structural\nwell-formedness in most cases and prevents the cascade of syntax and type errors that otherwise\ndominate unconstrained generation to some extent. However, due to the absence of feedback, the\ngeneratedtransformerremainsunsound.Together,theseresultsconfirmthatstructuredrepairand\ncost-guided optimization address complementary aspects of synthesis.\n6 Related Work\nProgramandTransformerSynthesis.Programsynthesisaimstoautomaticallyconstructprogramsthat\nsatisfyformalspecifications.Syntax-guidedsynthesis(SyGuS)[ 1]integratessemanticconstraints\nwith syntactic templates defined by a user-supplied grammar, enabling solver-guided exploration of a\nconstrainedsearchspace.Thedominantframework,Counterexample-GuidedInductiveSynthesis\n(CEGIS)[ 56],alternatesbetweencandidategenerationandverification,refininghypothesesusing\ncounterexamplesuntilconvergence.TheseprinciplesunderliepracticalsystemssuchasSketchand\nEscher, which leverage SAT/SMT solving and symbolic reasoning to guarantee correctness with\nrespect to the specification while maintaining reasonable scalability. Within abstract interpretation,\ntransformer synthesis applies these ideas to automatically construct sound abstract transformers for\ngiven operators and domains. Amurth [ 27] formulates this process as DSL-constrained synthesis,\ncombininginductiverefinementwithformalverificationtoderivethemostprecisesoundtransformer\nexpressible in a given DSL. Amurth2 [ 28] generalizes this method to reduced product domains\nvia dual CEGIS loops that coordinate synthesis of soundness and precision across components.\nHowever, both approaches carry the risk of not converging. Recent work such asUSTAD[ 20]\nand LinSyn [ 41] extend this direction toward numerical and neural domains: USTAD develops\na differentiable parametric framework for synthesizing sound linear transformers over polyhedral\ndomains, while LinSyn automates the synthesis of tight linear bounds for arbitrary neural activations\nusingconstraintsolvingandlocaloptimizationverifiedbySMT(dReal).Anotherframework[ 40]\ncombinesinterval-basedverificationwithmodelfinetuningtolearnregion-specificlinearbounds,\nbut it does not support piecewise-linear operators and its guarantees remain local rather than globally\nsound.Together,theseframeworkstraceaprogressionfromsymbolic[ 33]tosolver-andoptimization-\ndriven synthesis, aiming to balance formal soundness with scalability in constructing numerical and\nneural transformers.\nLLMs for Formal Reasoning and Verification.Large language models (LLMs) have recently been\nexplored as assistants for formal reasoning and program verification. Frameworks such as Verifier-in-\nthe-Loop [ 60], Self-Refine [ 34], and GPT-f [ 42] integrate automated verifiers or theorem provers\nwithiterativegeneration,allowingmodelstoreceivestructuredfeedbackwhenanoutputviolatesa\nlogical or type-theoretic constraint. Similarly, Refine4LLM [ 7] couples LLMs with symbolic solvers\nandproofcheckers tosynthesizeorrepairproofs,verification conditions,andformalspecifications.\nThese systems demonstrate that integrating reasoning modules into the LLM generation loop can\nCost-Driven Synthesis of Sound Abstract Interpreters 21\nsubstantiallyenhancefactualconsistencyandlogicalsoundnessintaskssuchastheoremproving,\nsymbolicexecution,andverification-orientedsynthesis.Nonetheless,mostoftheseapproachesare\nconfinedtodiscretesymbolicreasoning.Forexample,generatingproofsorverificationconditions\nrather than reasoning about continuous or numerical abstractions. Another central difficulty remains\nmitigatinghallucination[ 62],wheremodelsproduceseeminglyvalidproofsorspecificationsthat\nfailsemanticverification.Recentefforts[ 34,42,60,61]addressthisissuethroughverifier-guided\nrefinement, static analysis, confidence-based rejection sampling, etc.\n7 Conclusion\nOverall, our framework casts transformer synthesis as a constrained optimization problem driven\nbyanovelsoundness-basedcostfunction,enablinggloballysoundandcomplexnewtransformers\nthat can be instantiated for any abstract domain. Our results further demonstrate that our framework\nsubstantially improves models\u2019 synthesis quality in practice, producing sound transformers for more\ndiverse and complicated operators than prior methods. The foundational ideas under our framework\nare not limited to neural network certification and can be applied more broadly to any domain in\nwhich automated construction of sound algorithms is essential.\n22 Qiuhan Gu, Avaljot Singh, and Gagandeep Singh\nReferences\n[1]RajeevAlur,RastislavBodik,GarvitJuniwal,MiloM.K.Martin,MukundRaghothaman,SanjitA.Seshia,Rishabh\nSingh,ArmandoSolar-Lezama,EminaTorlak,andAbhishek Udupa.2013. Syntax-guidedsynthesis.In2013Formal\nMethods in Computer-Aided Design. 1\u20138. https://doi.org/10.1109/FMCAD.2013.6679385\n[2]RajeevAlur,ArjunRadhakrishna,andAbhishek Udupa.2017. Scalingenumerativeprogramsynthesis viadivideand\nconquer. InInternational conference on tools and algorithms for the construction and analysis of systems. Springer,\n319\u2013336.\n[3]Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,\nCarrie Cai, Michael Terry, Quoc Le, et al .2021. Program synthesis with large language models.arXiv preprint\narXiv:2108.07732(2021).\n[4]BrunoBlanchet,PatrickCousot,RadhiaCousot,J\u00e9r\u00f4meFeret,LaurentMauborgne,AntoineMin\u00e9,DavidMonniaux,\nandXavierRival.2002.Designandimplementationofaspecial-purposestaticprogramanalyzerforsafety-critical\nreal-time embedded software. Springer-Verlag, Berlin, Heidelberg, 85\u2013108.\n[5]Olivier Bouissou and Matthieu Martel. 2008. Abstract Interpretation of the Physical Inputs of Embedded Programs. In\nVerification,ModelChecking,andAbstractInterpretation,FrancescoLogozzo,DoronA.Peled,andLenoreD.Zuck\n(Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 37\u201351.\n[6]Christopher Brix, Stanley Bak, Taylor T. Johnson, and Haoze Wu. 2024. The Fifth International Verification of\nNeural Networks Competition (VNN-COMP 2024): Summary and Results. arXiv:2412.19985 [cs.LG] https:\n//arxiv.org/abs/2412.19985\n[7]YufanCai,ZheHou,DavidSanan,XiaokunLuan,YunLin,JunSun,andJinSongDong.2025. AutomatedProgram\nRefinement: Guide and Verify Code Large Language Model with Refinement Calculus.Proc. ACM Program. Lang.\nPOPL, Article 69 (Jan. 2025), 33 pages. https://doi.org/10.1145/3704905\n[8]Jose Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, and Satish Chandra. 2019. When Deep Learning Met Code\nSearch. arXiv:1905.03813 [cs.SE] https://arxiv.org/abs/1905.03813\n[9]Marco Campion, Mila Dalla Preda, and Roberto Giacobazzi. 2022. Partial (In)Completeness in abstract interpretation:\nlimiting the imprecision in program analysis.Proc. ACM Program. Lang.POPL, Article 59 (Jan. 2022), 31 pages.\nhttps://doi.org/10.1145/3498721\n[10]PatrickCousotandRadhiaCousot.1977. Abstractinterpretation:aunifiedlatticemodelforstaticanalysisofprogramsby\nconstruction or approximation of fixpoints. InProceedings of the 4th ACM SIGACT-SIGPLAN Symposium on Principles\nofProgrammingLanguages(LosAngeles,California)(POPL\u201977).AssociationforComputingMachinery,NewYork,\nNY, USA, 238\u2013252. https://doi.org/10.1145/512950.512973\n[11]Patrick Cousot and Radhia Cousot. 1979. Systematic design of program analysis frameworks. InProceedings of the\n6thACMSIGACT-SIGPLANSymposiumonPrinciplesofProgrammingLanguages(SanAntonio,Texas)(POPL\u201979).\nAssociation for Computing Machinery, New York, NY, USA, 269\u2013282. https://doi.org/10.1145/567752.567778\n[12]PatrickCousot,RadhiaCousot,Jer\u00f4meFeret, LaurentMauborgne,AntoineMin\u00e9,DavidMonniaux,andXavierRival.\n2005. TheASTRE\u00c9analyzer.InProceedingsofthe14thEuropeanConferenceonProgrammingLanguagesandSystems\n(Edinburgh,UK)(ESOP\u201905).Springer-Verlag,Berlin,Heidelberg,21\u201330. https://doi.org/10.1007/978-3-540-31987-0_3\n[13] George B Dantzig. 1973.Fourier-Motzkin elimination and its dual. Technical Report.\n[14]DeepSeek-AI,DayaGuo,etal .2025. DeepSeek-R1:IncentivizingReasoningCapabilityinLLMsviaReinforcement\nLearning. arXiv:2501.12948 [cs.CL] https://arxiv.org/abs/2501.12948\n[15]Li Deng.2012. The MNISTDatabase of Handwritten DigitImages for Machine LearningResearch [Best of theWeb].\nIEEE Signal Processing Magazine6 (2012), 141\u2013142. https://doi.org/10.1109/MSP.2012.2211477\n[16]Alessandra Di Pierro and Herbert Wiklicky. 2000. Measuring the precision of abstract interpretations. InInternational\nWorkshop on Logic-Based Program Synthesis and Transformation. Springer, 147\u2013164.\n[17]Pranav Garg, Daniel Neider, Parthasarathy Madhusudan, and Dan Roth. 2016. Learning invariants using decision trees\nand implication counterexamples.ACM Sigplan Notices1 (2016), 499\u2013512.\n[18]TimonGehr,MatthewMirman,DanaDrachsler-Cohen,PetarTsankov,SwaratChaudhuri,andMartinVechev.2018.\nAI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation. In2018 IEEE Symposium on\nSecurity and Privacy (SP). 3\u201318. https://doi.org/10.1109/SP.2018.00058\n[19]RobertoGiacobazzi,FrancescoRanzato,andFrancescaScozzari.2000. Makingabstractinterpretationscomplete.J.\nACM47, 2 (March 2000), 361\u2013416. https://doi.org/10.1145/333979.333989\n[20]Shaurya Gomber, Debangshu Banerjee, and Gagandeep Singh. 2025. Universal Synthesis of Differentiably Tunable\nNumerical Abstract Transformers.arXiv preprint arXiv:2507.11827(2025).\n[21]SvenGowal,KrishnamurthyDvijotham,RobertStanforth,RudyBunel,ChongliQin,JonathanUesato,ReljaArandjelovic,\nTimothyMann,andPushmeetKohli.2018. Ontheeffectivenessofintervalboundpropagationfortrainingverifiably\nrobust models.arXiv preprint arXiv:1810.12715(2018).\nCost-Driven Synthesis of Sound Abstract Interpreters 23\n[22]AryaGrayeli,AtharvaSehgal,OmarCostilla-Reyes,MilesCranmer,andSwaratChaudhuri.2024. SymbolicRegression\nwith a Learned Concept Library. arXiv:2409.09359 [cs.LG] https://arxiv.org/abs/2409.09359\n[23]Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, et al .2017. Program synthesis.Foundations and Trends\u00aein\nProgramming Languages4, 1-2 (2017), 1\u2013119.\n[24]Kihong Heo, Hakjoo Oh, and Kwangkeun Yi. 2017. Machine-Learning-Guided Selectively Unsound Static Analysis. In\n2017IEEE/ACM39thInternationalConferenceonSoftwareEngineering(ICSE).519\u2013529. https://doi.org/10.1109/\nICSE.2017.54\n[25]NamanJain,SkandaVaidyanath,ArunIyer,NagarajanNatarajan,SureshParthasarathy,SriramRajamani,andRahul\nSharma. 2022. Jigsaw: Large language models meet program synthesis. InProceedings of the 44th International\nConference on Software Engineering. 1219\u20131231.\n[26]JulienJulienBertrane,PatrickCousot,RadhiaCousot,J\u00e9r\u00f4meFeret,LaurentMauborgne,AntoineMin\u00e9,andXavier\nRival. 2011. Static analysis by abstract interpretation of embedded critical software.SIGSOFT Softw. Eng. Notes1 (Jan.\n2011), 1\u20138. https://doi.org/10.1145/1921532.1921553\n[27]PankajKumarKalita,SujitKumarMuduli,LorisD\u2019Antoni,ThomasReps,andSubhajitRoy.2022. Synthesizingabstract\ntransformers.Proc.ACMProgram.Lang.OOPSLA2,Article171(Oct.2022),29pages. https://doi.org/10.1145/3563334\n[28]Pankaj Kumar Kalita, Thomas Reps, and Subhajit Roy. 2025. Automated Abstract Transformer Synthesis for Reduced\nProduct Domains.ACM Transactions on Software Engineering and Methodology(2025).\n[29]Alex Krizhevsky and Geoffrey Hinton. 2009.The CIFAR-10 and CIFAR-100 datasets. https://www.cs.toronto.edu/\n~kriz/cifar.html Accessed: 2025-11-07.\n[30]Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and Kenneth O. Stanley. 2022. Evolution\nthrough Large Models. arXiv:2206.08896 [cs.NE] https://arxiv.org/abs/2206.08896\n[31] MaikelLeon.2025. GPT-5andopen-weightlarge languagemodels:Advancesinreasoning,transparency, andcontrol.\nInformation Systems(2025), 102620.\n[32]YujiaLi,DavidChoi,JunyoungChung,NateKushman,JulianSchrittwieser,R\u00e9miLeblond,TomEccles,JamesKeeling,\nFelixGimeno,AgustinDalLago,etal .2022. Competition-levelcodegenerationwithalphacode.Science378,6624\n(2022), 1092\u20131097.\n[33]Sirui Lu and Rastislav Bod\u00edk. 2023. Grisette: Symbolic Compilation as a Functional Programming Library.Proc. ACM\nProgram. Lang.7, POPL, Article 16 (Jan. 2023), 33 pages. https://doi.org/10.1145/3571209\n[34]Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha\nDziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann,\nSean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-Refine: Iterative Refinement with Self-Feedback.\narXiv:2303.17651 [cs.CL] https://arxiv.org/abs/2303.17651\n[35]Antoine Min\u00e9. 2006. Symbolic methods to enhance the precision of numerical abstract domains. InInternational\nWorkshop on Verification, Model Checking, and Abstract Interpretation. Springer, 348\u2013363.\n[36]Antoine Min\u00e9. 2017. Static Analysis of Embedded Real-Time Concurrent Software with Dynamic Priorities.Electronic\nNotes in Theoretical Computer Science331 (2017), 3\u201339. https://doi.org/10.1016/j.entcs.2017.02.002 Proceedings of\nthe Sixth Workshop on Numerical and Symbolic Abstract Domains (NSAD 2016).\n[37]Matthew Mirman, Timon Gehr, and Martin T. Vechev. 2018. Differentiable Abstract Interpretation for Provably Robust\nNeuralNetworks.InInternationalConferenceonMachineLearning. https://api.semanticscholar.org/CorpusID:51872670\n[38]ErikNijkamp,BoPang,HiroakiHayashi,LifuTu,HuanWang,YingboZhou,SilvioSavarese,andCaimingXiong.2022.\nCodegen: An open large language model for code with multi-turn program synthesis.arXiv preprint arXiv:2203.13474\n(2022).\n[39]Alexander Novikov, Ng\u00e2n V \u02dcu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey\nShirobokov, Borislav Kozlovskii, Francisco J. R. Ruiz, Abbas Mehrabian, M. Pawan Kumar, Abigail See, Swarat\nChaudhuri,GeorgeHolland,AlexDavies,SebastianNowozin,PushmeetKohli,andMatejBalog.2025. AlphaEvolve:A\ncoding agent for scientific and algorithmic discovery. arXiv:2506.13131 [cs.AI] https://arxiv.org/abs/2506.13131\n[40]Brandon Paulsen and Chao Wang. 2022. Example Guided Synthesis of Linear Approximations for Neural Network\nVerification. InComputer Aided Verification: 34th International Conference, CAV 2022, Haifa, Israel, August 7\u201310,\n2022, Proceedings, Part I(Haifa, Israel). Springer-Verlag, Berlin, Heidelberg, 149\u2013170. https://doi.org/10.1007/978-3-\n031-13185-1_8\n[41]Brandon Paulsen and Chao Wang. 2022. LinSyn: Synthesizing Tight Linear Bounds for Arbitrary Neural Network\nActivation Functions. arXiv:2201.13351 [cs.LG] https://arxiv.org/abs/2201.13351\n[42]Stanislas Polu and Ilya Sutskever. 2020. Generative Language Modeling for Automated Theorem Proving.\narXiv:2009.03393 [cs.LG] https://arxiv.org/abs/2009.03393\n[43]ThomasRepsandAdityaThakur.2016. AutomatingAbstractInterpretation.InProceedingsofthe17thInternational\nConference on Verification, Model Checking, and Abstract Interpretation - Volume 9583(St. Petersburg, FL, USA)\n(VMCAI 2016). Springer-Verlag, Berlin, Heidelberg, 3\u201340. https://doi.org/10.1007/978-3-662-49122-5_1\n24 Qiuhan Gu, Avaljot Singh, and Gagandeep Singh\n[44]Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien\nDupont,FranciscoJ.R.Ruiz,JordanS.Ellenberg,PengmingWang,OmarFawzi,PushmeetKohli,andAlhusseinFawzi.\n2024. Mathematical discoveries from program search with large language models.Nat.7995 (January 2024), 468\u2013475.\nhttps://doi.org/10.1038/s41586-023-06924-6\n[45]Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, and Chandan K Reddy. 2025. LLM-\nSR: Scientific Equation Discovery via Programming with Large Language Models. arXiv:2404.18400 [cs.LG]\nhttps://arxiv.org/abs/2404.18400\n[46]AvaljotSingh,YasminSarita,CharithMendis,andGagandeepSingh.2024. ConstraintFlow:ADeclarativeDSLfor\nEasyDevelopmentofDNN Certifiers. Springer-Verlag,Berlin,Heidelberg,407\u2013424. https://doi.org/10.1007/978-3-\n031-74776-2_16\n[47]Avaljot Singh, Yasmin Chandini Sarita, Charith Mendis, and Gagandeep Singh. 2025. Automated Verification\nof Soundness of DNN Certifiers.Proc. ACM Program. Lang.OOPSLA1, Article 144 (April 2025), 29 pages.\nhttps://doi.org/10.1145/3720509\n[48]AvaljotSingh,YaminChandiniSarita,AdityaMishra,IshaanGoyal,GagandeepSingh,andCharithMendis.2025. A\nTensor-Based Compiler and a Runtime for Neuron-Level DNN Certifier Specifications. arXiv:2507.20055 [cs.CL]\nhttps://arxiv.org/abs/2507.20055\n[49]GagandeepSinghandDeepikaChawla.2025. Position:FormalMethodsarethePrincipledFoundationofSafeAI.In\nICML Workshop on Technical AI Governance (TAIG). https://openreview.net/forum?id=7V5CDSsjB7\n[50]Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus P\u00fcschel, and Martin Vechev. 2018. Fast and effective\nrobustnesscertification.InProceedings of the 32ndInternational Conferenceon NeuralInformation Processing Systems\n(Montr\u00e9al, Canada)(NIPS\u201918). Curran Associates Inc., Red Hook, NY, USA, 10825\u201310836.\n[51]GagandeepSingh,TimonGehr,MarkusP\u00fcschel,andMartinVechev.2019. Anabstractdomainforcertifyingneural\nnetworks.Proc. ACM Program. Lang.POPL, Article 41 (Jan. 2019), 30 pages. https://doi.org/10.1145/3290354\n[52]Gagandeep Singh, Timon Gehr, Markus P\u00fcschel, and Martin T. Vechev. 2018. Boosting Robustness Certification of\nNeural Networks. InInternational Conference on Learning Representations. https://api.semanticscholar.org/CorpusID:\n196059499\n[53]Gagandeep Singh, Jacob Laurel, Sasa Misailovic, Debangshu Banerjee, Avaljot Singh, Changming Xu, Shubham Ugare,\nand Huan Zhang. 2025. Safety and Trust in Artificial Intelligence with Abstract Interpretation.Found. Trends Program.\nLang.3\u20134 (June 2025), 250\u2013408. https://doi.org/10.1561/2500000062\n[54]GagandeepSingh,MarkusP\u00fcschel,andMartinVechev.2017. Fastpolyhedraabstractdomain.InProceedingsofthe\n44th ACM SIGPLAN Symposium on Principles of Programming Languages. 46\u201359.\n[55]ArmandoSolar-Lezama.2009. Thesketchingapproachtoprogramsynthesis.InAsiansymposiumonprogramming\nlanguages and systems. Springer, 4\u201313.\n[56]Armando Solar-Lezama. 2013. Program sketching.Int. J. Softw. Tools Technol. Transf.5\u20136 (Oct. 2013), 475\u2013495.\nhttps://doi.org/10.1007/s10009-012-0249-7\n[57]Benno Stein, Bor-Yuh Evan Chang, and Manu Sridharan. 2021. Demanded abstract interpretation. InProceedings\nof the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation\n(Virtual, Canada)(PLDI 2021). Association for Computing Machinery, New York, NY, USA, 282\u2013295. https:\n//doi.org/10.1145/3453483.3454044\n[58]Gemini Team, Rohan Anil, et al .2025. Gemini: A Family of Highly Capable Multimodal Models.\narXiv:2312.11805 [cs.CL] https://arxiv.org/abs/2312.11805\n[59]Wenhua Wang, Yuqun Zhang, Yulei Sui, Yao Wan, Zhou Zhao, Jian Wu, Philip S. Yu, and Guandong Xu. 2022.\nReinforcement-Learning-Guided Source Code Summarization Using Hierarchical Attention.IEEE Transactions on\nSoftware Engineering48, 1 (2022), 102\u2013119. https://doi.org/10.1109/TSE.2020.2979701\n[60]Yixuan Wang, Chao Huang, Zhaoran Wang, Zhilu Wang, and Qi Zhu. 2021. Verification in the Loop: Correct-by-\nConstructionControlLearningwithReach-avoidGuarantees. arXiv:2106.03245[eess.SY] https://arxiv.org/abs/2106.\n03245\n[61]Guannan Wei, Zhuo Zhang, and Caterina Urban. 2025. Hallucination-Resilient LLM-Driven Sound and Tunable Static\nAnalysis: A Case of Higher-Order Control-Flow Analysis. InProceedings of the 1st ACM SIGPLAN International\nWorkshop on Language Models and Programming Languages(Singapore, Singapore)(LMPL \u201925). Association for\nComputing Machinery, New York, NY, USA, 6\u201311. https://doi.org/10.1145/3759425.3763378\n[62]Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. 2025. Hallucination is Inevitable: An Innate Limitation of Large\nLanguage Models. arXiv:2401.11817 [cs.CL] https://arxiv.org/abs/2401.11817\n[63]Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct:\nSynergizing Reasoning and Acting in Language Models. arXiv:2210.03629 [cs.CL] https://arxiv.org/abs/2210.03629\n[64]HuanZhang,HonggeChen,ChaoweiXiao,SvenGowal,RobertStanforth,BoLi,DuaneBoning,andCho-JuiHsieh.\n2019. Towards Stable and Efficient Training of Verifiably Robust Neural Networks. arXiv:1906.06316 [cs.LG]\nCost-Driven Synthesis of Sound Abstract Interpreters 25\nhttps://arxiv.org/abs/1906.06316\n26 Qiuhan Gu, Avaljot Singh, and Gagandeep Singh\nA Prompt Templates\nIn this section, we show the prompt used for transformer generation and transformer repair in our\nframework.\nGeneration Prompting Template\nGeneral instructions.\nYouareaformalmethodsexpertworkingonneuralnetworkverification.Yourtaskistogeneratethe\n[certifier]transformers for DNN operators. Generate the transformer in Constraintflow DSL.\n[Information about the domain specific language.]\nHere is the grammar of Constraintflow DSL:\nexpr_list : expr COMMA expr_list\n| expr ;\nexprs: expr exprs\n| expr;\nmetadata: WEIGHT\n| BIAS\n| EQUATIONS\n| LAYER ;\nexpr: FALSE #false\n| TRUE #true\n| IntConst #int\n| FloatConst #float\n| VAR #varExp\n| EPSILON #epsilon\n| CURR #curr\n| PREV #prev\n| PREV_0 #prev_0\n| PREV_1 #prev_1\n| CURRLIST #curr_list\n| LPAREN expr RPAREN #parenExp\n| LSQR expr_list RSQR #exprarray\n| expr LSQR metadata RSQR #getMetadata\n| expr LSQR VAR RSQR #getElement\n| expr binop expr #binopExp\n| NOT expr #not\n| MINUS expr #neg\n| expr QUES expr COLON expr #cond\n| expr DOT TRAV LPAREN direction COMMA expr COMMA expr COMMA expr RPAREN\nLBRACE expr RBRACE #traverse\n| argmax_op LPAREN expr COMMA expr RPAREN #argmaxOp\n| max_op LPAREN expr RPAREN #maxOpList\n| max_op LPAREN expr COMMA expr RPAREN #maxOp\n| list_op LPAREN expr RPAREN #listOp\n| expr DOT MAP LPAREN expr RPAREN #map\n| expr DOT MAPLIST LPAREN expr RPAREN #map_list\n| expr DOT DOTT LPAREN expr RPAREN #dot\n| expr DOT CONCAT LPAREN expr RPAREN #concat\n| LP LPAREN lp_op COMMA expr COMMA expr RPAREN #lp\n| VAR LPAREN expr_list RPAREN #funcCall\nCost-Driven Synthesis of Sound Abstract Interpreters 27\n| VAR exprs #curry ;\ntrans_ret :\nexpr QUES trans_ret COLON trans_ret #condtrans\n| LPAREN trans_ret RPAREN #parentrans\n| expr_list #trans ;\n[Information about the certifier. Using DeepPoly as an example below:]\nDeepPoly certifier uses four kinds of bounds to approximate the operator: (Float l, Float u, PolyExp\nL, PolyExp U). They must follow the constraints that: curr[l] <= curr <= curr[u] & curr[L] <= curr\n<= curr[U]. \u2018curr\u2018 here means the current neuron, \u2018prev\u2018 means the inputs to the operator. When the\noperator takes multiple inputs, use \u2018prev_0\u2018, \u2018prev_1\u2018, ... to refer to each input. So every transformer in\neach case of the case analysis must return four values. Use any funstions below if needed instead of use\narithmetic operators. Function you can use:\n- func simplify_lower(Neuron n, Float coeff) = (coeff >= 0) ? (coeff * n[l]) : (coeff * n[u]);\n- func simplify_upper(Neuron n, Float coeff) = (coeff >= 0) ? (coeff * n[u]) : (coeff * n[l]);\n- func replace_lower(Neuron n, Float coeff) = (coeff >= 0) ? (coeff * n[L]) : (coeff * n[U]);\n- func replace_upper(Neuron n, Float coeff) = (coeff >= 0) ? (coeff * n[U]) : (coeff * n[L]);\n- func priority(Neuron n) = n[layer];\n- func priority2(Neuron n, Float c) = -n[layer];\n- func stop(Neuron n) = false;\n- func stop_traverse(Neuron n, Float c) = false;\n- func backsubs_lower(PolyExp e, Neuron n) = (e.traverse(backward, priority2, stop_traverse, re-\nplace_lower)e <= n).map(simplify_lower);\n- func backsubs_upper(PolyExp e, Neuron n) = (e.traverse(backward, priority2, stop_traverse, re-\nplace_upper)e >= n).map(simplify_upper);\n- func f(Neuron n1, Neuron n2) = n1[l] >= n2[u];\n- func slope(Float x1, Float x2) = ((x1 * (x1 + 3))-(x2 * (x2 + 3))) / (6 * (x1-x2));\n- func intercept(Float x1, Float x2) = x1 * ((x1 + 3) / 6) - (slope(x1, x2) * x1);\n- func f(Neuron n1, Neuron n2) = n1[l] >= n2[u];\n- func f1(Float x) = x < 3 ? x * ((x + 3) / 6) : x;\n- func f2(Float x) = x * ((x + 3) / 6);\n- func f3(Neuron n) = max(f2(n[l]), f2(n[u]));\n- func compute_l(Neuron n1, Neuron n2) = min([n1[l]*n2[l], n1[l]*n2[u], n1[u]*n2[l], n1[u]*n2[u]]);\n-funccompute_u(Neuron n1,Neuronn2)= max([n1[l]*n2[l],n1[l]*n2[u],n1[u]*n2[l],n1[u]*n2[u]]);\n- func avg(List<Float> xs) = sum(xs) / len(xs);\n-funcargmax(List<Neuron>ns,(Neuron,Neuron->Bool)cmp)=[n|ninns,forallminns.cmp(n,m)\n];\n-funcargmin(List<Neuron>ns,(Neuron,Neuron->Bool)cmp)=[n|ninns,forallminns.cmp(n,m)\n];\nDon\u2019t add comments to DSL.\n[Two-shot prompting. Using DeepPoly as an example below:]\n### Example: Abs operator\nInput: Generate the transformer for \u2018abs\u2018 operator\nOutput:\n\"\"\"\ndef Shape as (Float l, Float u, PolyExp L, PolyExp U)\n{[(curr[l]<=curr),(curr[u]>=curr),(curr[L]<=curr),(curr[U]>=curr)]};\n28 Qiuhan Gu, Avaljot Singh, and Gagandeep Singh\ntransformer deeppoly Abs -> ((prev[l]) >= 0) ? ((prev[l]), (prev[u]), (prev), (prev)) : (((prev[u])\n<= 0) ? (0-(prev[u]), 0-(prev[l]), 0-(prev), 0-(prev)) : (0, max(prev[u], 0-prev[l]), prev,\nprev*(prev[u]+prev[l])/(prev[u]-prev[l]) - (((2*prev[u])*prev[l])/(prev[u]-prev[l]))) );\n\"\"\"\n### Example: Affine operator\nInput: Generate the transformer for \u2018affine\u2018 operator\nOutput:\n\"\"\"\ndef Shape as (Float l, Float u, PolyExp L, PolyExp U)\n{[(curr[l]<=curr),(curr[u]>=curr),(curr[L]<=curr),(curr[U]>=curr)]};\ntransformer deeppoly Affine -> (backsubs_lower(prev.dot(curr[weight]) + curr[bias], curr),\nbacksubs_upper(prev.dot(curr[weight]) + curr[bias], curr), prev.dot(curr[weight]) + curr[bias],\nprev.dot(curr[weight]) + curr[bias]);\n\"\"\"\n[Query.]\n### Now generate the transformer for \u2018{api}\u2018 operator\nInput: Generate the transformer for \u2018{api}\u2018 operator\nOutput: \"\"\"\nRepair Prompting Template\nYou are a DSL repair assistant. Fix the following DSL code based on the error.\n[ERROR]:\n[Error messages.]\n[CODE]:\n[Incorrect generation.]\nReturn only the fixed DSL code.\nB Generation Examples\nIn this section, we show three different kinds of transformer candidates generated by our framework.\n1transformerDeepPoly {\n2HardSigmoid ->\n3((prev[u]) <= ((0 - attr[beta]) / attr[alpha])) ?\n4(0, 0, 0, 0) :\n5((prev[l]) >= ((1 - attr[beta]) / attr[alpha])) ?\n6(1, 1, 1, 1) :\n7((prev[l]) >= ((0 - attr[beta]) / attr[alpha])) ?\n8((prev[u]) <= ((1 - attr[beta]) / attr[alpha])) ?\n9((attr[alpha] *prev[l] + attr[beta]),\n10(attr[alpha] *prev[u] + attr[beta]),\n11(attr[alpha] *prev+ attr[beta]),\n12(attr[alpha] *prev+ attr[beta])) :\n13((attr[alpha] *prev[l] + attr[beta]),\n141,\n15(attr[alpha] *prev+ 1 - (attr[alpha] *prev[u])),\n161) :\nCost-Driven Synthesis of Sound Abstract Interpreters 29\n17((prev[u]) <= ((1 - attr[beta]) / attr[alpha])) ?\n18(0,\n19(attr[alpha] *prev[u] + attr[beta]),\n200,\n21(attr[alpha] *prev- (attr[alpha] *prev[l]))) :\n22(0, 1, 0, 1);\n23}\nListing 1. Invalid transformer (Syntax error. Undefined variable: attr, alpha, beta)\n1transformerdeeppoly {\n2HardSigmoid -> (prev[u] <= -3) ? (0, 0, 0, 0)\n3: ((prev[l] >= 3) ? (1, 1, 1, 1)\n4: ((prev[u] <= 3)\n5? ((prev[l] >= -3)\n6? ((prev[l] + 3) / 6, (prev[u] + 3) / 6, (prev+ 3) / 6, (prev+ 3) / 6)\n7: (0, (prev[u] + 3) / 6, (prev+ 3) / 6,\n8prev[l] * ((prev[u] + 3) / (6 * (prev[u] -prev[l]))) *prev\n9- ((prev[u] + 3) / (6 * (prev[u] -prev[l]))) *prev[l]))\n10: ((prev[l] >= -3)\n11? ((prev[l] + 3) / 6, 1,\n12((3 -prev[l]) / (6 * (prev[u] -prev[l]))) * (prev-prev[l]) + (prev[l] +\n3) / 6,\n13(prev+ 3) / 6)\n14: (0, 1, (prev+ 3) / (prev[u] + 3), (prev-prev[l]) / (3 -prev[l])))));\n15}\nListing 2. Unsound transformer. Counterexample: Prev = 3/2, Prev_l = -4, Prev_u = 2, Prev_L = 3/2, Prev_U =\n3/2, Curr = 3/4. Based on the transformer, we get Curr_l = 0, Curr_u = 5/6, Curr_L = 3/4, Curr_U = -5/18,\nwhich is a wrong approximation.\n1transformerDeepPoly {\n2HardSigmoid ->\n3(prev[u] <= -3) ?\n4(0, 0, 0, 0) :\n5(prev[l] >= 3) ?\n6(1, 1, 1, 1) :\n7(prev[l] >= -3) ?\n8(prev[u] <= 3) ?\n9((prev[l] / 6) + 0.5,\n10(prev[u] / 6) + 0.5,\n11(prev/ 6) + 0.5,\n12(prev/ 6) + 0.5) :\n13((prev[l] / 6) + 0.5,\n141,\n15(((0.5 - (prev[l] / 6)) / (prev[u] -prev[l])) *prev)\n16+ ((prev[l] / 6) + 0.5)\n17- (((0.5 - (prev[l] / 6)) / (prev[u] -prev[l])) *prev[l]),\n181) :\n19(prev[u] <= 3) ?\n20(0,\n21(prev[u] / 6) + 0.5,\n220,\n30 Qiuhan Gu, Avaljot Singh, and Gagandeep Singh\n23(((((prev[u] / 6) + 0.5) / (prev[u] -prev[l])) *prev)\n24- ((((prev[u] / 6) + 0.5) / (prev[u] -prev[l])) *prev[l]))) :\n25(0, 1, 0, 1);\n26}\nListing 3. Sound transformer\nFig. 10. Examples of transformer candidates generated by our framework. (1) contains syntax errors, (2) is\nvalid but unsound, and (3) is both valid and sound.\nC Validation Semantics\nWedefinethegeneralvalidationsemanticsapplicabletoanyprogramminglanguage.Validationis\nexpressed as a total big-step judgment. Given a candidate program \ud835\udc60, validation either produces a\nfinite set of diagnosticsDor succeeds with a valid abstract syntax tree\ud835\udc61:\nLex(\ud835\udc60)=\u00ae\ud835\udf0fParse(\u00ae\ud835\udf0f)=\ud835\udc61\u27e8\u03a3,\u0393,M\u27e9\u22a2\ud835\udc61\u21d2D\n(\n\u27e8\u03a3,\u0393,M\u27e9\u22a2\ud835\udc60\u21d3err(D),ifD\u2260\u2205\n\u27e8\u03a3,\u0393,M\u27e9\u22a2\ud835\udc60\u21d3ok(\ud835\udc61),ifD=\u2205(V-CHECK)\nHere, Lex(\ud835\udc60)=\u00ae\ud835\udf0f performslexicalanalysisonthecandidatetext \ud835\udc60,Parse(\u00ae\ud835\udf0f)=\ud835\udc61 constructsitsabstract\nsyntaxtree(AST),and \u27e8\u03a3,\u0393,M\u27e9\u22a2\ud835\udc61\u21d2D checkstheASTforstructuralandsemanticconsistency\nunderthesymboltable \u03a3,typingandshapecontext \u0393,andmetadatamap M,producingafinitesetof\ndiagnosticsD. Validation succeeds iffD=\u2205.\nStaticErrorPredicates.Eachdiagnostic \ud835\udc51\u2208Dcorrespondstoastaticerrorpredicate.Weidentify\nsix categories of structural and semantic errors commonly observed in LLM-generated candidates.\n(i)Unmatched or missing delimiters. These errors are detected during parsing rather than by a\nseparate predicate. The parser ensures that all parentheses, brackets, and braces are properly\nnested and matched; any violation produces a diagnosticUnexpectedToken().\n(ii)Illegalkeywordsorillegallogicaloperators.Illegaltokensarerejectedduringlexicalanalysis;\nLogicaloperators suchasand,or,not, andxormustbeapplied onlytobooleanoperands.All\nlogical operators share the same typing pattern:\nop\u2208{and,or,xor,not} \u2200\ud835\udc52 \ud835\udc56\u2208args(op).\u27e8\u03a3,\u0393,M\u27e9\u22a2\ud835\udc52 \ud835\udc56:Bool\n\u27e8\u03a3,\u0393,M\u27e9\u22a2op(\ud835\udc52 1,...,\ud835\udc52\ud835\udc5b):Bool(T-LogicOp)\nwhere opdenotes a logical operator, \ud835\udc52\ud835\udc56are its operands, and \u27e8\u03a3,\u0393,M\u27e9 represents the symbol\ntable, typing context, and metadata map used for type checking. If any operand fails to have\nboolean type, the validator emits the diagnosticIllegalLogicalOp().\n(iii)Malformedattributecallsandincorrectmetadataindexing.Forattributeaccess \ud835\udc52.\ud835\udc5a[\ud835\udc56],metadata\nfields and indices must matchM:\n\u27e8\u03a3,\u0393,M\u27e9\u22a2\ud835\udc52:\ud835\udf0fHasMetaField(\ud835\udf0f,\ud835\udc5a)\ud835\udc5a\u2208dom(M) \u27e8\u03a3,\u0393,M\u27e9\u22a2\ud835\udc56: idx(M(\ud835\udc5a))\n\u27e8\u03a3,\u0393,M\u27e9\u22a2\ud835\udc52.\ud835\udc5a[\ud835\udc56]: rng(M(\ud835\udc5a))(T-Meta)\nwhere\ud835\udc52is the base expression, \ud835\udc5ais the metadata field, and \ud835\udc56is its index; \ud835\udf0fdenotes the type\nof expression \ud835\udc52, which is checked to ensure that \ud835\udc52is a valid expression eligible for metadata\naccess.Mdefines valid metadata domains and ranges for type checking. Violations emit\nUnknownMetadata(\ud835\udc5a).\nCost-Driven Synthesis of Sound Abstract Interpreters 31\n(iv)Undefinedidentifiersorinvalidfunctioninvocations.Identifiersandcallsarecheckedagainst\nthe symbol table\u03a3:\n\ud835\udc65\u2208dom(\u0393)\n\u27e8\u03a3,\u0393,M\u27e9\u22a2\ud835\udc65:\u0393(\ud835\udc65)(T-Var)\ud835\udc53:(\ud835\udf0f 1,...,\ud835\udf0f\ud835\udc5b)\u2192\ud835\udf0f\u2208\u03a3\u2200\ud835\udc56.\u27e8\u03a3,\u0393,M\u27e9\u22a2\ud835\udc52 \ud835\udc56:\ud835\udf0f\ud835\udc56\n\u27e8\u03a3,\u0393,M\u27e9\u22a2\ud835\udc53(\ud835\udc52 1,...,\ud835\udc52\ud835\udc5b):\ud835\udf0f(T-Call)\nwhere \u03a3isthesymboltablemappingidentifierstotheirdeclaredtypes, \u0393isthetypingcontext, \ud835\udf0f\ndenotes thereturn type ofthe function, andeach \ud835\udc52\ud835\udc56is anargument expression thatmust match\nthe corresponding parameter type \ud835\udf0f\ud835\udc56. Violations yield UndefinedId(\ud835\udc65) , where\ud835\udc65denotes an\nundeclared variable or function name.\n(v)Typeinconsistenciesinarithmeticorelement-wiseoperations.Arithmeticoperationsrequire\ncompatible numeric types:\n\u27e8\u03a3,\u0393,M\u27e9\u22a2\ud835\udc52 1:Tensor[\u00ae\ud835\udc511] \u27e8\u03a3,\u0393,M\u27e9\u22a2\ud835\udc52 2:Tensor[\u00ae\ud835\udc512]broadcast(\u00ae\ud835\udc511,\u00ae\ud835\udc512)=\u00ae\ud835\udc51\n\u27e8\u03a3,\u0393,M\u27e9\u22a2\ud835\udc52 1\u2299\ud835\udc522:Tensor[\u00ae\ud835\udc51](T-Elem)\nwhere\ud835\udc521and\ud835\udc522aretensoroperands, bc(\u00ae\ud835\udc511,\u00ae\ud835\udc512)denotesthebroadcastingfunctionthatinfersa\ncommonshape\u00ae\ud835\udc51,and\u2299representsanelement-wisearithmeticoperator.Ifbroadcastingfails,\nthe validator addsShapeMismatch( \u00ae\ud835\udc511,\u00ae\ud835\udc512), where\u00ae\ud835\udc511and\u00ae\ud835\udc512are the operand tensor shapes.\n(vi) Improper use of reserved constants or keywords.\n\ud835\udc65\u2209R\u222adom(\u0393)\n\u27e8\u03a3,\u0393,M\u27e9\u22a2let\ud835\udc65=\ud835\udc52in\ud835\udc61:\ud835\udf0f(T-Let)\nwhereRis the set of reserved keywords, \ud835\udc65is a newly declared identifier that must not appear in\nRorinthecurrentcontext \u0393,and\ud835\udf0fistheresultingtypeoftheexpression.Violationsproduce\nReservedName(\ud835\udc5f), where\ud835\udc5fdenotes a keyword reserved and thus cannot be redefined.\nD Precision Evaluation (Cont.)\nWeshowthecompleteprecisionevaluationresultsfortransformersgeneratedbyGPT-5forDeepPoly\ndomain and DeepZ domain.\nTable 2. Precision comparison across different networks based on DeepPoly domain.\nDataset Network Training Activation Layers Perturbation\ud835\udf50Precision\nOur work Handcrafted\nMNIST FCN_9\u00d7200 Standard ReLU 9 0.005 0.8557 0.8557\nFCN_4\u00d71024 Standard ReLU 4 0.005 0.9796 0.9796\nFCN_6\u00d7500 Standard ReLU 6 0.005 1.0000 1.0000\nFCN_6\u00d7500 PGD ReLU 6 0.005 1.0000 1.0000\nConv DiffAI ReLU 9 0.005 1.0000 1.0000\nConv PGD ReLU 3 0.005 1.0000 1.0000\nConv Standard ReLU 6 0.005 1.0000 1.0000\nConv Standard ReLU6 3 0.005 1.0000\u2717\nFCN_3\u00d750 Standard ReLU6 3 0.005 0.5100\u2717\nFCN_3\u00d7100 Standard ReLU6 3 0.005 0.7300\u2717\nFCN_4\u00d71024 Standard ReLU6 4 0.005 0.9000\u2717\nFCN_5\u00d7100 DiffAI ReLU6 5 0.005 0.9000\u2717\nFCN_6\u00d7100 Standard ReLU6 6 0.005 0.6465\u2717\nFCN_6\u00d7200 Standard ReLU6 6 0.005 0.8384\u2717\nFCN_6\u00d7500 PGD ReLU6 6 0.005 0.9900\u2717\nFCN_6\u00d7500 Standard ReLU6 6 0.005 0.7879\u2717\n32 Qiuhan Gu, Avaljot Singh, and Gagandeep Singh\nDataset Network Training Activation Layers Perturbation\ud835\udf50Precision\nOur work Handcrafted\nFCN_9\u00d7100 Standard ReLU6 9 0.005 0.6800\u2717\nFCN_9\u00d7200 Standard ReLU6 9 0.005 0.6768\u2717\nFCN_3\u00d750 Standard HardTanh 3 0.005 0.1818 0.1818\nFCN_3\u00d7100 Standard HardTanh 3 0.005 0.2323 0.2323\nFCN_5\u00d7100 DiffAI HardTanh 5 0.005 0.9500 0.9500\nFCN_6\u00d7500 PGD\u20130.1 HardTanh 6 0.005 0.5455 0.5455\nFCN_5\u00d7100 DiffAI HardSwish 5 0.005 0.1616\u2717\nFCN_3\u00d750 Standard HardSigmoid 3 0.005 0.2062\u2717\nFCN_3\u00d7100 Standard HardSigmoid 3 0.005 0.2323\u2717\nFCN_5\u00d7100 DiffAI HardSigmoid 5 0.005 0.6087\u2717\nFCN_6\u00d7500 PGD HardSigmoid 6 0.005 0.2929\u2717\nFCN_9\u00d7100 Standard HardSigmoid 9 0.005 1.0000\u2717\nFCN_9\u00d7200 Standard HardSigmoid 9 0.005 1.0000\u2717\nFCN_3\u00d750 Standard HardSwish 3 0.005 0.2653\u2717\nFCN_3\u00d7100 Standard HardSwish 3 0.005 0.1900\u2717\nFCN_3\u00d750 Standard GELU 3 0.005 0.4646\u2717\nFCN_3\u00d7100 Standard GELU 3 0.005 0.9400\u2717\nFCN_4\u00d71024 Standard GELU 4 0.005 1.0000\u2717\nFCN_5\u00d7100 DiffAI GELU 5 0.005 0.7778\u2717\nFCN_6\u00d7100 Standard GELU 6 0.005 0.8800\u2717\nFCN_6\u00d7200 Standard GELU 6 0.005 1.0000\u2717\nFCN_6\u00d7500 PGD GELU 6 0.005 1.0000\u2717\nFCN_6\u00d7500 Standard GELU 6 0.005 1.0000\u2717\nFCN_9\u00d7100 Standard GELU 9 0.005 0.9300\u2717\nFCN_9\u00d7200 Standard GELU 9 0.005 0.9800\u2717\nFCN_3\u00d7100 Standard ELU 3 0.005 0.1400\u2717\nCIFAR10 FCN_4\u00d7100 Standard ReLU 4 0.8 0.7857 0.7857\nFCN_6\u00d7100 Standard ReLU 6 0.8 0.5294 0.5294\nFCN_9\u00d7200 Standard ReLU 9 0.8 0.7500 0.7500\nFCN_7\u00d71024 Standard ReLU 7 0.8 0.9231 0.9231\nConv DiffAI ReLU 3 0.8 1.0000 1.0000\nConv PGD ReLU 3 0.8 0.9429 0.9429\nConv Point ReLU 3 0.8 0.8136 0.8136\nConv Point ReLU 6 0.8 0.9104 0.9104\nConv PGD ReLU 6 0.8 0.9206 0.9206\nConv PGD ReLU 6 0.8 1.0000 1.0000\nFCN_6\u00d7500 Point ReLU 6 0.8 0.9464 0.9464\nFCN_6\u00d7500 PGD ReLU 6 0.8 0.9365 0.9365\nFCN_6\u00d7500 PGD ReLU 6 0.8 0.9464 0.9464\nFCN_4\u00d7100 Standard ReLU6 4 0.8 0.4490\u2717\nFCN_6\u00d7100 Standard ReLU6 6 0.8 0.3922\u2717\nFCN_6\u00d7500 PGD ReLU6 6 0.8 0.4865\u2717\nFCN_6\u00d7500 Standard ReLU6 6 0.8 0.4464\u2717\nFCN_7\u00d71024 Standard ReLU6 7 0.8 0.3077\u2717\nFCN_9\u00d7200 Standard ReLU6 9 0.8 0.3721\u2717\nFCN_4\u00d7100 Standard HardTanh 4 0.8 0.3871 0.3871\nFCN_6\u00d7100 Standard HardTanh 6 0.8 0.4474 0.4474\nFCN_6\u00d7500 PGD HardTanh 6 0.8 0.3636 0.3636\nCost-Driven Synthesis of Sound Abstract Interpreters 33\nDataset Network Training Activation Layers Perturbation\ud835\udf50Precision\nOur work Handcrafted\nFCN_6\u00d7500 Standard HardTanh 6 0.8 0.2903 0.2903\nFCN_7\u00d71024 Standard HardTanh 7 0.8 0.8462 0.8462\nFCN_9\u00d7200 Standard HardTanh 9 0.8 0.3333 0.3333\nFCN_4\u00d7100 Standard HardSwish 4 0.8 0.1154\u2717\nFCN_4\u00d7100 Standard HardSigmoid 4 0.8 0.3333\u2717\nFCN_6\u00d7100 Standard HardSigmoid 6 0.8 0.1304\u2717\nFCN_6\u00d7500 PGD HardSigmoid 6 0.8 1.0000\u2717\nFCN_6\u00d7500 Standard HardSigmoid 6 0.8 0.3830\u2717\nFCN_7\u00d71024 Standard HardSigmoid 7 0.8 1.0000\u2717\nFCN_9\u00d7200 Standard HardSigmoid 9 0.8 1.0000\u2717\nFCN_4\u00d7100 Standard GELU 4 0.8 0.9630\u2717\nFCN_6\u00d7100 Standard GELU 6 0.8 0.9649\u2717\nFCN_6\u00d7500 PGD GELU 6 0.8 0.5283\u2717\nFCN_6\u00d7500 PGD GELU 6 0.8 1.0000\u2717\nFCN_6\u00d7500 Standard GELU 6 0.8 0.6000\u2717\nFCN_7\u00d71024 Standard GELU 7 0.8 0.9787\u2717\nFCN_9\u00d7200 Standard GELU 9 0.8 0.7358\u2717\nTable 3. Precision comparison across different networks based on DeepZ domain.\nDataset Network Training Activation Layers Perturbation\ud835\udf50Precision\nOur work Handcrafted\nMNIST FCN_3\u00d750 Standard ReLU 3 0.005 0.5204 0.5204\nFCN_3\u00d7100 Standard ReLU 3 0.005 0.1122 0.1122\nConvolution Point ReLU 3 0.005 0.9000 0.9000\nConvolution DiffAI ReLU 3 0.005 1.0000 1.0000\nConvolution PGD ReLU 3 0.005 0.9400 0.9400\nConvolution Point ReLU 6 0.005 0.8100 0.8100\nFCN_3\u00d750 Standard HardSigmoid 3 0.005 0.2371\u2717\nFCN_3\u00d7100 Standard HardSigmoid 3 0.005 0.1818\u2717\nFCN_6\u00d7100 Standard HardSigmoid 6 0.005 0.1531\u2717\nFCN_6\u00d7200 Standard HardSigmoid 6 0.005 0.2222\u2717\nCIFAR10 Conv DiffAI ReLU 3 0.8 0.9623 0.9623\nConv PGD ReLU 3 0.8 0.0143 0.0143\nFCN_4\u00d7100 Standard HardSigmoid 4 0.8 0.3333\u2717\nFCN_6\u00d7500 PGD HardSigmoid 6 0.8 1.0000\u2717\nFCN_6\u00d7500 Standard HardSigmoid 6 0.8 0.1277\u2717\nFCN_7\u00d71024 Standard HardSigmoid 7 0.8 1.0000\u2717\nFCN_9\u00d7200 Standard HardSigmoid 9 0.8 1.0000\u2717\nFCN_6\u00d7100 Standard HardTanh 6 0.8 0.1053\u2717\nFCN_6\u00d7500 PGD HardTanh 6 0.8 0.2727\u2717\nFCN_6\u00d7500 PGD HardTanh 6 0.8 0.1000\u2717\nFCN_6\u00d7500 Standard HardTanh 6 0.8 0.1935\u2717\nFCN_7\u00d71024 Standard HardTanh 7 0.8 0.8462\u2717\n34 Qiuhan Gu, Avaljot Singh, and Gagandeep Singh\nE Synthesizing Transformers for Nonlinear Operators (Cont.)\nFig. 11 visualizes the generated transformer for sigmoid activation.\nx0.00.20.40.60.81.0y\nu0Sigmoid(x)\nLower\nUpper\n(a)\ud835\udc59<\ud835\udc62<0.\nx0.00.20.40.60.81.0y\nu 0Sigmoid(x)\nLower\nUpper (b)0<\ud835\udc59<\ud835\udc62.\nx0.00.20.40.60.81.0y\nu0Sigmoid(x)\nLower\nUpper (c)\ud835\udc59<0<\ud835\udc62.\nFig. 11.DeepPoly Transformer for Sigmoid.The transformer is divided into three cases: (1) for \ud835\udc59<\ud835\udc62<0 ,\nthe lower and upper bounds are the affine functions \ud835\udc66=0.25\ud835\udc65+0.5 and\ud835\udc66=0.5 respectively; (2) for 0<\ud835\udc59<\ud835\udc62 ,\nthe bounds are \ud835\udc66=0.5 and\ud835\udc66=0.25\ud835\udc65+0.5 ; (3) for the mixed case ( \ud835\udc59<0<\ud835\udc62 ), the upper bound are \ud835\udc66=1 , and\nthe lower bound is\ud835\udc66=0. Since\ud835\udf0e(\ud835\udc65)is monotonic, the scalar bounds correspond directly to\ud835\udf0e(\ud835\udc59)and\ud835\udf0e(\ud835\udc62).\nF Ablation Study (Cont.)\nFig. 12, Fig. 13 and Fig. 14 show the effect of validation-repair module and cost-function guidance\non DeepPoly transformer synthesis using Llama4-Maverick.\nAbsAffineAvgpool\nHardSigmoidHardSwishHardT anhMaxpool Minpool\nNeuron_add Neuron_max Neuron_min Neuron_multReluRelu60102030405060708090Numbers\n115\n2284075\n15 15\n12121 10 0939\n6143447\n04\n17\n010 0 016\n1015\n0 0 01020 0\nLLM Gen Rounds\nLLM Repair Rounds\nCounterexamples\nSound Generation\nUnsound Generation\nFig. 12. With cost-function guidance. The model converges to sound transformers within a few refinement\nrounds.\nG Performance of Sound Abstract Interpreters Synthesis across Multiples Models and\nDomains\nFig. 15, Fig. 16, Fig. 17, Fig. 18, Fig. 19, and Fig. 20 show the process of GPT-5, Llama4-Maverick,\nClaude-Opus-4 synthesizing multiple transformers across DeepPoly, DeepZ and Interval domain.\nCost-Driven Synthesis of Sound Abstract Interpreters 35\nAbsAffineAvgpool\nHardSigmoidHardSwishHardT anhMaxpool Minpool\nNeuron_add Neuron_max Neuron_min Neuron_multReluRelu603570105140175210245280315350385420455490525Numbers\n180\n180 80 80 80 80\n180 80 80\n180\n0 0 0522\n414\n360\n89519\n0305391\n18\n077\nFig. 13. Without cost-function guidance but with repair module. The model is able to produce syntactically\nand semantically valid transformers, but they are unsound in most cases.\nAbsAffineAvgpool\nHardSigmoidHardSwishHardT anhMaxpool Minpool\nNeuron_add Neuron_max Neuron_min Neuron_multReluRelu603570Numbers\n180\n580 80 80 80 80\n180 80 80\n180\nFig. 14. Without repair module and cost-function guidance. The model often produces syntactically invalid\nand unsound transformers.\nAbsAffineAvgpool\nHardSigmoidHardSwishHardT anhMaxpool Minpool\nNeuron_add Neuron_max Neuron_min Neuron_multReluRelu605101520253035Numbers\n110 10\n14\n1 14\n2\n1 1 1 1 1323\n19\n07\n01\n0 0 0 04\n0 0 0 0 0 0 01\n0 0 0 0 0 0 0 0\nLLM Gen Rounds\nLLM Repair Rounds\nCounterexamples\nSound Generation\nUnsound Generation\nFig. 15. Quantitative results of GPT-5 generating transformers under the DeepPoly domain.\n36 Qiuhan Gu, Avaljot Singh, and Gagandeep Singh\nAbsAffineAvgpool\nHardSigmoidHardSwishHardT anhMaxpool Minpool\nNeuron_add Neuron_max Neuron_min Neuron_multReluRelu60102030405060708090Numbers\n115\n2284075\n15 15\n12121 10 0939\n6143447\n04\n17\n010 0 016\n1015\n0 0 01020 0\nLLM Gen Rounds\nLLM Repair Rounds\nCounterexamples\nSound Generation\nUnsound Generation\nFig. 16. Quantitative results of Llama4-Maverick generating transformers under the DeepPoly domain.\nAbsAffineAvgpool\nHardSigmoidHardSwishHardT anhMaxpool Minpool\nNeuron_add Neuron_max Neuron_min Neuron_multReluRelu6020406080100120Numbers\n715 154080\n45\n15 15\n1 1 215\n115\n0 072\n3103\n364\n59\n0 0 0115\n0 030 0914\n7\n0 0 0 0 1 0 0 1\nLLM Gen Rounds\nLLM Repair Rounds\nCounterexamples\nSound Generation\nUnsound Generation\nFig. 17. Quantitative results of Claude-Opus\u20134 generating transformers under the DeepPoly domain.\nAbsAffineAvgpool\nHardSigmoidHardSwishHardT anhMaxpool Minpool\nNeuron_add Neuron_max Neuron_min Neuron_multReluRelu60102030405060Numbers\n110 10\n110\n1 110\n1 1 1 12101553\n13\n0 04\n0 0 02 2\n0 0 02\n010 0 0 0 0 0 010\nLLM Gen Rounds\nLLM Repair Rounds\nCounterexamples\nSound Generation\nUnsound Generation\nFig. 18. Quantitative results of GPT-5 generating transformers under the DeepZ domain.\nCost-Driven Synthesis of Sound Abstract Interpreters 37\nAbsAffineAvgpool\nHardSigmoidHardSwishHardT anhMaxpool Minpool\nNeuron_add Neuron_max Neuron_min Neuron_multReluRelu6010203040506070Numbers\n115 1532\n15 15 15 15\n13\n115\n12\n03837\n0 0 0525\n0 0 056\n0 0 0 0 016\n10 0 0 02 2\n0 01\nLLM Gen Rounds\nLLM Repair Rounds\nCounterexamples\nSound Generation\nUnsound Generation\nFig. 19. Quantitative results of Claude-Opus-4 generating transformers under the DeepZ domain.\nAbsAffineAvgpool\nHardSigmoidHardSwishHardT anhMaxpool Minpool\nNeuron_add Neuron_max Neuron_min Neuron_multReluRelu6020406080100Numbers\n110 10\n231 14\n18\n110\n1 104254\n6\n0 018\n062\n087\n0 0 0 0 0210 0 0 0 0 0 0 0 0\nLLM Gen Rounds\nLLM Repair Rounds\nCounterexamples\nSound Generation\nUnsound Generation\nFig. 20. Quantitative results of GPT5 generating transformers under the Interval domain.\n",
    "title": "Cost-Driven Synthesis of Sound Abstract Interpreters",
    "authors": [
      "Qiuhan Gu",
      "Avaljot Singh",
      "Gagandeep Singh"
    ],
    "published": "2025-11-17",
    "arxiv_id": "2511.13663v1",
    "num_pages": 37,
    "num_chars": 107403
  },
  {
    "text": "Why is \u201cChicago\u201d Predictive of Deceptive Reviews? Using LLMs to\nDiscover Language Phenomena from Lexical Cues\nJiaming Qu\u2217\nAmazon\nSeattle, USA\nqjiaming@amazon.comMengtian Guo\nUNC Chapel Hill\nChapel Hill, USA\nmtguo@email.unc.eduYue Wang\nUNC Chapel Hill\nChapel Hill, USA\nwangyue@email.unc.edu\nABSTRACT\nDeceptive reviews mislead consumers, harm businesses, and un-\ndermine trust in online marketplaces. Machine learning classifiers\ncan learn from large amounts of training examples to effectively\ndistinguish deceptive reviews from genuine ones. However, the\ndistinguishing features learned by these classifiers are often subtle,\nfragmented, and difficult for humans to interpret. In this work, we\nexplore using large language models (LLMs) to translate machine-\nlearned lexical cues into human-understandable language phenom-\nena that can differentiate deceptive reviews from genuine ones. We\nshow that language phenomena obtained in this manner are empir-\nically grounded in data, generalizable across similar domains, and\nmore predictive than phenomena either in LLMs\u2019 prior knowledge\nor obtained through in-context learning. These language phenom-\nena have the potential to aid people in critically assessing the credi-\nbility of online reviews in environments where deception detection\nclassifiers are unavailable.\n1 INTRODUCTION\nOnline reviews are an essential source of information for consumers\nto make purchasing decisions and businesses to understand their\ncustomers. Although many platforms have implemented machine\nlearning techniques to detect and block deceptive reviews [ 1], these\nsystems are far from universally available, and users may still face\ndeceptive content without algorithmic assistance. Unlike platforms\nthat can leverage metadata or behavioral signals to identify decep-\ntion [ 20], users rely almost exclusively on review text\u2014and prior\nwork shows that untrained people perform poorly at this task [3].\nTherefore, it is important to help users develop keen eyes that\ncan spot potentially deceptive reviews where algorithmic inter-\nventions are absent. A natural starting point is to educate users\nwith the most salient signals identified by machine learning clas-\nsifiers [ 14,25,27]. However, while classifiers can learn predictive\nlexical cues for accurately distinguishing deceptive reviews from\ngenuine ones from large amounts of data, such text features are\noften subtle and fragmented. For example, prior work found that\nthe word \u201cChicago\u201d is predictive of deceptive hotel reviews [ 10].\nThese predictive words are often not random artifacts but reflect\nunderlying language phenomenain deceptive text instead.\nIn this work, we view such words\u2014predictive yet unintuitive\nlexical cues\u2014as surface manifestations of underlying language phe-\nnomena and explore whether large language models (LLMs) can\ndiscover such phenomena. Our goal isnotto build a stronger decep-\ntion detector, but to evaluate whether LLMs can reliably translate\n\u2217This work was conducted at UNC and does not reflect the views of Amazon.machine-learned lexical cues into human-interpretable phenom-\nena. In particular, we ask LLMs to answer questions in the spirit\nof \u201cwhy is lexical feature \ud835\udc4bpredictive of deceptive or genuine\nreviews?\u201d On the one hand, LLMs would readily provide fluent,\nplausible explanations to such questions (which we callconjectures\nbefore they arevalidated), and their natural language outputs are\nhuman-comprehensible. On the other hand, plausibility alone does\nnot guarantee correctness: LLM-generated conjectures would be\nuseless if they are sheer hallucinations and fail to correspond to\nany real pattern that actually distinguishes deception from genuine\nreviews. To evaluate whether such conjectures reflect meaningful\nlinguistic patterns, we use deception detection in hotel reviews as\na case study and investigate three research questions (RQs):\n\u2022RQ1 (Predictiveness): Can these phenomena distinguish decep-\ntive reviews from genuine ones?\n\u2022RQ2 (Generalization): Are these phenomena generalizable to\nnew data in similar domains (i.e., similar products or services)?\n\u2022RQ3 (Alternatives): Instead of explaining predictive words, can\nthese phenomena be obtained by prompting LLMs to conjecture\nbased on its prior knowledge or labeled examples?\nOur results show that these LLM-conjectured language phenom-\nena have predictive power, are generalizable to similar-domain data,\nand outperform phenomena obtained from LLMs\u2019 prior knowledge\nor in-context learning alone. Together, these findings indicate that\nit is feasible to use LLMs to translate subtle deception cues learned\nby statistical classifiers into human-understandable, predictive lan-\nguage phenomena. These language phenomena could ultimately be\nuseful in helping people increase their acumen in assessing the cred-\nibility of online reviews and decrease their reliance on algorithmic\nfilters when such filters are unavailable.\n2 RELATED WORK\nFeature importance explanations, which highlight input features\nthat are most influential to the prediction output, are a popular\napproach to explaining machine learning model predictions [ 14,\n25,27]. While empirical studies have shown such explanations can\nimprove end-users\u2019 decision-making and understanding of AI sys-\ntems [ 11,22], predictive features are not always self-explanatory.\nFor example, studies applying such explanations on text data have\nfound that the word \u201cproblems\u201d is predictive of positive senti-\nment [ 23], and that the word \u201cChicago\u201d is predictive of deceptive\nreviews [ 10]. Given large and representative data, such words are\noften not the result of overfitting but reflect underlying language\nphenomena that give rise to these word-label relations.\nPrior studies have explored different approaches for making such\nphenomena more explicit, such as showing nearby words of thearXiv:2511.13658v1  [cs.CL]  17 Nov 2025\npredictive word [ 7,23,26] or considering interactions with other\nwords in the input [ 4,8,30]. While these algorithms are effective at\nrevealing context-related phenomena (e.g., negations and colloquial\nexpressions like \u201cno problems\u201d), they may fail to capture complex\nphenomena that go beyond local contexts. For instance, the word\n\u201cChicago\u201d is associated with deceptive hotel reviews because fake re-\nviews often reinforce branding by emphasizing a hotel\u2019s full name,\nincluding the city name. In this paper, we use deception detection\nas a case study and leverage LLMs to explain the language phenom-\nena behind deception cues. Our approach was inspired by recent\nresearch that prompted LLMs to verbalize predictive features into\nnatural language narratives [ 15,19,32]. However, these approaches\nmainlyparaphrasepredictive words for better readability. Our ap-\nproach is crucially different in that it goes beyond observed lexical\ncues to unobserved phenomena-based explanations.\n3 METHODOLOGY\nProblem Formulation: Consider a labeled text dataset {(\ud835\udc65,\ud835\udc66)} ,\nwhere\ud835\udc65represents a piece of text and \ud835\udc66represents a label (e.g.,\ngenuine or deceptive review). Feature importance explanations\nidentify words {\ud835\udc64\u2208\ud835\udc65} that are predictive of a label \ud835\udc66. In this work,\nwe focus on a subset {\ud835\udc64\u2032\u2208\ud835\udc65}\u2286{\ud835\udc64\u2208\ud835\udc65} : words that are the\nmost significant signals for the classifier but appear unintuitive\nto humans. Our goal is to leverage LLMs totranslatethese salient\nlexical cues into more human-interpretable language phenomena\nthat plausibly give rise to the cues. Formally, we prompt an LLM\ntoconjecturea candidate phenomenon associated with the cue.\nThe LLM-conjectured phenomena may or may not reflect actual\npatterns in the underlying data. Thus, wevalidatedwhether the\nLLM-conjectured phenomena are predictive of genuine or deceptive\nreviews (RQ1), generalizable beyond the original data (RQ2), and\ndependent on classifier-learned predictive words (RQ3).\nDataset: We used two datasets in our study: a dataset containing\n800 genuine and 800 deceptive reviews for Chicago hotels [ 17,18]\n(denoted as \ud835\udc37\ud835\udc36\u210e\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc54\ud835\udc5c ) and another dataset containing 240 genuine\nand 240 deceptive reviews for Houston, New York, and Los An-\ngeles hotels [ 12] (denoted as \ud835\udc37\ud835\udc61\u210e\ud835\udc5f\ud835\udc52\ud835\udc52\u2212\ud835\udc50\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc52\ud835\udc60 ). Genuine hotel reviews\nwere collected from verified travelers while deceptive reviews were\nwritten by crowd workers.\nIdentifying Predictive Words: To investigate whether LLMs\ncan translate predictive lexical cues into meaningful language phe-\nnomena, we first trained logistic regression classifiers on\ud835\udc37 \ud835\udc36\u210e\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc54\ud835\udc5c\nand identified predictive words. It is notable that we do not treat\nclassification performance as the object of optimization. We delib-\nerately used a simple model (logistic regression) solely as afeature-\ndiscovery tool, and predictive words can also be extracted using\nother sophisticated approaches [14, 25, 27].\nGiven the limited size of the dataset, we performed 10-fold cross\nvalidation with balanced training and testing folds. Each time, we\ntrained a classifier with unigram TF-IDF features with lowercasing\nand stop word removal. The classifiers achieved an average F1-score\nof 0.88. For each classifier, we identified 25 words that were the\nmost predictive of genuine or deceptive reviews through regres-\nsion coefficients (50 predictive words in total). All selected words\npassed a Wald test for the significance of predictiveness. To obtain\nparticularly stable lexical cues, we selected only words that werepredictiveacross all 10 folds, yielding 16 words for genuine reviews\nand 14 for deceptive reviews. As shown in Table 1, we expect that\nmost laypeople without expertise in deception detection would\nnaturally wonder about the underlying language phenomena.\nRQ1 Experiment: Our RQ1 investigated whether using LLMs\ncan be a generally reliable approach to discover language phenom-\nena from lexical cues in deception detection. To this end, we used a\nconjecture-then-validatepipeline with two separate steps.\nFirst, we prompted the LLM to conjecture the underlying phe-\nnomena for words predictive of hotel reviews being genuine or\ndeceptive using the prompt below. We provided all 30 predictive\nwords in the prompt and did not restrict the number of phenom-\nena to be conjectured. All predictive words and their associated\nphenomena (conjectured by LLMs) are shown in Table 1.\nSystem Prompt for Conjecturing Phenomena\nYou are an expert in deception detection. You will be provided a list of\nwords that are most predictive of genuine or deceptive Chicago hotel\nreviews. Your task is to identify language phenomena or psycholin-\nguistic patterns that appear in genuine and deceptive hotel reviews\nand are related to these words.\nWhile the conjectured phenomena are plausible, all LLMs are\nprone to hallucinations. Prior studies have mainly used benchmark\ndatasets [ 6,28] or recruited human evaluators [ 5,31] to evaluate\nLLM-generated contents. However, there is no ground truth for\nthese language phenomena in this task. Therefore, we took an algo-\nrithmic evaluation approach: we prompted the same LLM to classify\nreviews as genuine or deceptive with the conjectured phenomena.\nThe rationale is that if these phenomena reflect actual patterns in\nthe data (i.e., non-hallucinated), they should improve the LLM\u2019s\npredictive performance. Otherwise, fabricated phenomena could\nmislead the LLM\u2019s reasoning.\nHence, we tested two prompt conditions. First, we use aphenomena-\nin-the-promptcondition, where we provided all the conjectured\nphenomena associated with both genuine and deceptive reviews\nin the prompt. Only phenomena but not predictive words were\nprovided. Here, our main objective was to investigate whether\nLLMs can reliably discover language phenomenaat all. Therefore,\nwe prioritized an aggregated evaluation and leveraged the full set\nof phenomena to test their collective utility. Second, we tested a\nzero-shot prompt, where no auxiliary information was provided.\nWe tested the pipeline with four LLMs: GPT-5-mini from Ope-\nnAI [ 16],Haiku 4.5 from Anthropic [ 21],Nova Pro from Ama-\nzon [ 2], andGemini-2.5-flash from Google [ 13]. For all LLM\ncalls in this work, we used the corresponding APIs with default\nsettings. Two authors independently designed initial prompts and\ncollaboratively revised the final versions for each prompting sce-\nnario. We interacted with LLMs through DSPy [ 9], a framework for\nmodular construction of LLM applications by explicitly defining\nprompt components. This design improves the reproducibility of\nour experiments. All prompts are available in our codebase1.\nRQ2 Experiment: In addition to validating whether the LLM-\nconjectured phenomena are non-hallucinated, we investigated the\ngeneralizability of these phenomena. To this end, we trained logistic\n1https://jiamingqu.com/deception_detection.zip\nTable 1: LLM-conjectured phenomena from words predictive of genuine and deceptive reviews.\nPredictive Words LLM-conjectured Phenomena(aggregated over four LLMs with rephrasing)\nsmall, large, quiet, bathroom, floor, breakfast Genuine reviews use measurable, verifiable adjectives and specific room features.\nlocation, street, river, michigan Genuine reviews often provide concrete geographic references and local landmarks.\nrate, priceline Genuine reviews frequently mention pricing, booking platforms, or value for money.\nreviews, helpful, us Genuine reviews refer to other reviews or their desire to contribute useful information.\nluxury, luxurious Deceptive reviews often overemphasize the luxurious aspects and high-end services.\nhotel, chicago, millennium Deceptive reviews often use generic category terms and prominent place/brand names.\nseemed, recently, definitely Deceptive reviews tend to use words expressing certainty or vague temporal framing.\nhusband, vacation, staying, experience Deceptive reviews use family roles or staged narratives to fabricate a plausible story.\nfood, towels Deceptive reviews list desirable amenities or sensory cues without specific details.\nregression classifiers on \ud835\udc37\ud835\udc36\u210e\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc54\ud835\udc5c and tested on \ud835\udc37\ud835\udc61\u210e\ud835\udc5f\ud835\udc52\ud835\udc52\u2212\ud835\udc50\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc52\ud835\udc60 using\nthree feature sets: (1) unigram TF-IDF features only, (2) phenomena-\nbased features only, and (3) both features. This simulated a scenario\nwhere machine learning models are applied to out-of-distribution\ndata: the model learns to differentiate between genuine and decep-\ntive reviews for Chicago hotels and then applies this knowledge\nto classify hotel reviews from other cities. The rationale is that if\nthese phenomena are generalizable across hotel reviews in different\ncities, the classifier using phenomena-based features should have\nbetter performance than the one using unigram TF-IDF features.\nTo generate phenomena-based feature values for both training\nand testing sets, we need to obtain a score \ud835\udc53(\ud835\udc5f,\ud835\udc5d) for each review\n\ud835\udc5fand each phenomenon \ud835\udc5d. Here,\ud835\udc53(\ud835\udc5f,\ud835\udc5d) measures the extent to\nwhich a review \ud835\udc5freflects a phenomenon \ud835\udc5d. We approach this prob-\nlem by computing \ud835\udc43(\ud835\udc5f\u2223\ud835\udc5d) , i.e., the probability that a review \ud835\udc5fis\ngenerated given phenomenon \ud835\udc5das the prompt. This can be a proxy\nfor phenomena-based feature values.\nWe followed a generative scoring approach using next-token\nprobabilities: \ud835\udc43(\ud835\udc5f\u2223\ud835\udc5d)=\u220f\ud835\udc5b\n\ud835\udc56=1\ud835\udc43(\ud835\udc64\ud835\udc56\u2223\ud835\udc5d,\ud835\udc64 1,\u22ef,\ud835\udc64\ud835\udc56\u22121), where\ud835\udc5dis a\nphenomenon,2and\ud835\udc5fis a sequence of words [\ud835\udc641,\u22ef,\ud835\udc64\ud835\udc5b]. Each next-\nword probability \ud835\udc43(\ud835\udc64\ud835\udc56\u2223\ud835\udc5d,\ud835\udc64 1,\u22ef,\ud835\udc64\ud835\udc56\u22121)was obtained by prompting\nthe LLM with the word sequence [\ud835\udc5d,\ud835\udc64 1,\u22ef,\ud835\udc64\ud835\udc56\u22121]and reading out\nthe probability of \ud835\udc64\ud835\udc56as predicted by the LLM. An open-source LLM\nthat provides a complete probability distribution overallpossible\nnext words is needed, as \ud835\udc64\ud835\udc56might rank at a very low position. We\nusedGemma-7b[29] for this experiment.\nWe used this approach to generate all nine phenomena-based\nfeature values for reviews in \ud835\udc37\ud835\udc36\u210e\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc54\ud835\udc5c and\ud835\udc37\ud835\udc61\u210e\ud835\udc5f\ud835\udc52\ud835\udc52\u2212\ud835\udc50\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc52\ud835\udc60 . It is notable\nthat this approach is one way (and not the only way) to generate\nsuch phenomena-based feature values. Even if these approximated\nfeature values are not perfectly accurate, as long as they can im-\nprove a deception detection model\u2019s performance, it already serves\nour goal: to show that the phenomena-based features can improve\na subsequent deception detector\u2019s out-of-distribution performance.\nThe results in Table 3 confirm that this is indeed the case.\nRQ3 Experiment: We investigated whether LLMs can effec-\ntively discover language phenomena from predictive lexical cues\nin RQ1. A natural follow-up question is whether these phenomena\ncould also be obtained by prompting LLMs in other ways. Therefore,\nRQ3 examined whether LLMs can conjecture meaningful phenom-\nena from their prior knowledge or in-context learning. We used\n2We used the following prompt: \u201cWrite a hotel review that {phenomenon}:\u201d ,\nwhere{phenomenon}represents one of the nine conjectured phenomena in Table 1.Haiku 4.5 given its best performance on deception detection (Ta-\nble 2) and tested two prompt conditions to conjecture phenomena.\nIn the first condition, we prompted it to conjecture phenomena\nsolely based on its prior knowledge. In the second condition, we ran-\ndomly sampled 30 genuine and 30 deceptive reviews from \ud835\udc37\ud835\udc36\u210e\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc54\ud835\udc5c ,\nand prompted it to conjecture phenomena from sampled reviews.\nWe used the same system prompt structure and only changed the\ninput from predictive words to sampled reviews or to nothing. To\ncompare the quality of the phenomena conjectured under each\nprompt condition (i.e., predictive words, sampled examples, and\nprior knowledge), we used the same experimental design as in\nRQ1: we measured Haiku 4.5 \u2019s deception detection performance\non\ud835\udc37\ud835\udc36\u210e\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc54\ud835\udc5c with the conjectured phenomena inserted into the\nphenomena-in-the-promptcondition.\n4 RESULTS\nRQ1: Table 2 shows evaluation results of the four LLMs\u2019 predictive\nperformance under different prompt conditions on \ud835\udc37\ud835\udc36\u210e\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc54\ud835\udc5c . The\nevaluation results are with respect to the \u201cdeceptive\u201d label. Across\nall four LLMs, thephenomena-in-the-promptcondition consistently\noutperformed thezero-shotcondition. These improvements indicate\nthatthe conjectured phenomena capture real patterns in the\ndata rather than being entirely hallucinated.\nIt is notable that unlike other tasks (e.g., fact-checking and ques-\ntion answering) where ground truth facts exist, there areno defini-\ntive answersfor the conjectured phenomena in this task. Therefore,\nwe define \u201challucinations\u201d as cases in which LLMs come up with\nphenomena that are entirely fabricated and not related to any de-\ntectable patterns in the data3. Under this definition, the goal of RQ1\nwas not to assess whether the LLM-conjectured phenomena repre-\nsent the exhaustive set of language patterns. Rather, this validity\ncheck was to assess whether the LLM-conjectured phenomena are\nmeaningfully associated with real-world examples of genuine or\ndeceptive reviews that any user might see on social platforms. The\nperformance gains LLMs achieved with phenomena provided in\nthe prompt over the zero-shot prompt verified those conjectured\nphenomena to be relevant to the deception detection task.\nWhile our goal wasnotto benchmark different LLMs\u2019 perfor-\nmance in deception detection or engineer the most effective prompt,\nthese results did reveal two important trends. First, deception de-\ntection remained a challenging task for LLMs. Under the zero-shot\nprompt condition, the four LLMs achieved an average F1-score\n3For instance, a fabricated phenomena might be \u201cDeceptive reviews always indicate\npositive sentiment to promote business\u201d\u2014this is contradicted by the \ud835\udc37\ud835\udc36\u210e\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc54\ud835\udc5c corpus.\nTable 2: RQ1 Results. We evaluated four LLMs\u2019 predictive\nperformance on \ud835\udc37\ud835\udc36\u210e\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc54\ud835\udc5c under different prompt conditions.\nModel Prompt Acc. Prec. Recall F1\nGPT-5 minizero-shot 0.6512 0.8601 0.3612 0.5088\nphenomena 0.6775 0.8272 0.4488 0.5818\nHaiku 4.5zero-shot 0.6688 0.7679 0.4838 0.5936\nphenomena 0.6962 0.7532 0.58380.6577\nNova Prozero-shot 0.5512 0.6798 0.1938 0.3016\nphenomena 0.5962 0.7601 0.2812 0.4106\nGemini-2.5 zero-shot 0.6388 0.8101 0.3625 0.5009\nFlash phenomena 0.6600 0.8033 0.4238 0.5548\nAcc.: Accuracy. Prec.: Precision.\nof 0.4762 (min = 0.3016, max = 0.5936). Since the dataset is bal-\nanced, their performance was no better than random guess. Second,\nalthough all LLMs achieved reasonable performance gains with\nphenomena provided in the prompt, their performance was still\nworse than that of a logistic regression classifier using unigram\nTF-IDF features. This traditional model performed surprisingly well\non the deception detection task with an F1-score of 0.88.\nRQ2: Table 3 shows evaluation results of the three logistic re-\ngression classifiers\u2019 predictive performance. The classifiers used\ndifferent feature sets and were trained on \ud835\udc37\ud835\udc36\u210e\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc54\ud835\udc5c and tested on\n\ud835\udc37\ud835\udc61\u210e\ud835\udc5f\ud835\udc52\ud835\udc52\u2212\ud835\udc50\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc52\ud835\udc60 . Compared to the classifier using unigram TF-IDF fea-\ntures only, both classifiers using phenomena-based features and\ncombined features achieved better predictive performance on hotel\nreviews from other cities. These results suggest thatthe LLM-\nconjectured phenomena captured patterns that are more\ntransferable than unigram lexical cues.\nTable 3: RQ2 Results. We trained logistic regression classifiers\non\ud835\udc37\ud835\udc36\u210e\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc54\ud835\udc5c and tested on \ud835\udc37\ud835\udc61\u210e\ud835\udc5f\ud835\udc52\ud835\udc52\u2212\ud835\udc50\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc52\ud835\udc60 using three different\nfeature sets: unigram only, phenomena only, and both.\nFeature Sets Acc. Prec. Recall F1\nunigram 0.7396 0.8363 0.5958 0.6959\nphenomena 0.7167 0.6688 0.85830.7518\nunigram+phenomena 0.7792 0.8602 0.6667 0.7512\nThis result is not surprising: while the word \u201cChicago\u201d is the most\nsalient cue of deception in reviews of Chicago hotels, it does not\nnecessarily hold true in hotel reviews from other cities\u2014fabricated\nreview writers might mention hotel names with \u201cNew York\u201d, \u201cHous-\nton\u201d, and \u201cLos Angeles\u201d instead. In fact, regression coefficients of a\nlogistic regression classifier trained on \ud835\udc37\ud835\udc61\u210e\ud835\udc5f\ud835\udc52\ud835\udc52\u2212\ud835\udc50\ud835\udc56\ud835\udc61\ud835\udc56\ud835\udc52\ud835\udc60 corroborated\nthat city names are among the strongest predictors of deceptive\nhotel reviews within those locales as well. Hence, compared to the\nunigram text feature \u201cChicago\u201d, the corresponding phenomena-\nbased feature\u2014deceptive reviews tend to name-drop hotel and city\nnames\u2014offers a more generalizable insight into deceptive hotel\nreviews across cities. Compared to lexical features, such language\nphenomena enable the classifier to learn transferable insights and\nachieve better predictive performance on out-of-distribution data.\nFuture work could similarly examine whether hotel brand names\nare a strong signal of deception in other hotel review corpora.\nRQ3: Table 4 shows the predictive performance of Haiku 4.5\nwhen predicting with phenomena conjectured from three differentsources: predictive words, sampled reviews, and prior knowledge.\nIts performance was highest when phenomena were derived from\npredictive words, and was noticeably lower when derived from\neither sampled reviews or prior knowledge alone. These results\nindicate that Haiku 4.5 discovered accurate phenomena more\neffectively when guided by predictive lexical cues than when\nrelying on examples or prior knowledge. One possible expla-\nnation is that predictive words identified by the logistic regres-\nsion classifier serve as discriminative signals distilled from large\namounts of training data. Although these words may appear unin-\ntuitive to humans, they anchor the LLM\u2019s reasoning toward subtle\npsycholinguistic patterns that are difficult to infer from a small\nsample of reviews or from its prior knowledge alone.\nTable 4: RQ3 Results. We evaluated Haiku 4.5 \u2019s predictive per-\nformance on \ud835\udc37\ud835\udc36\u210e\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc54\ud835\udc5c under the phenomena-in-the-prompt\ncondition with different conjectured phenomena sets.\nPhenomena from Acc. Prec. Recall F1\nsampled reviews 0.6269 0.6862 0.4675 0.5561\nprior knowledge 0.5988 0.6113 0.5425 0.5748\npredictive words 0.6962 0.7532 0.58380.6577\n5 DISCUSSION AND CONCLUSION\nSummary of Findings: In this paper, we studied the feasibility of\nusing LLMs to translate machine-learned lexical cues into higher-\nlevel, human-understandable language phenomena in deception\ndetection. We introduced a conjecture-then-validate pipeline: an\nLLM firstconjecturesthe underlying phenomena associated with\npredictive words, and these phenomena are thenvalidatedalgo-\nrithmically by testing whether they meaningfully reflect patterns\nin the data. Our experimental results show that these conjectured\nphenomena have predictive power for distinguishing genuine and\ndeceptive reviews (RQ1), are generalizable to new, unseen data in\nthe same domain (RQ2), and are most accurate when derived from\npredictive words rather than prior knowledge or in-context learn-\ning samples (RQ3). Together, these findings indicate that LLMs can\nreliably translate nuanced lexical cues into human-understandable\nlanguage phenomena in deception detection\u2014a capability that may\nextend to other text classification tasks where predictive words may\nappear unintuitive and represent underlying linguistic patterns.\nHuman Evaluation: In a separate work, we conducted a crowd-\nsourced user study to evaluate the effect of using LLM-conjectured\nlanguage phenomena to explain classifier-learned predictive words\nwhen training humans to detect deceptive hotel reviews [ 24]. The\nstudy found that participants who saw the conjectured phenomena\nbecamesignificantly betterat detecting deception without algorith-\nmic assistance than participants who saw the predictive words only.\nThis is empirical evidence that the LLM-conjectured phenomena\nare easier for humans to assimilate than the predictive words.\nLimitations and Future Work:While algorithmic evaluation\nresults verified that the LLM-conjectured phenomena are largely\nnon-hallucinated and generalizable, two main limitations remain.\nFirst, the conjecturing step currently relies on a single-pass prompt.\nAn iterative prompting process may lead to more detailed phenom-\nena. Second, our validation was limited to algorithmic measures\n(and crowd workers in [ 24]). Future work could incorporate expert\nevaluations to assess the quality of LLM-conjectured phenomena.\nREFERENCES\n[1]About Amazon Team. 2023.Amazon, Booking.com, Expedia Group, Glassdoor,\nTripadvisor, and Trustpilot launch first global Coalition for Trusted Reviews. About\nAmazon EU. https://www.aboutamazon.eu/news/policy/amazon-booking-\ncom-expedia-group-glassdoor-tripadvisor-and-trustpilot-launch-first-global-\ncoalition-for-trusted-reviews\n[2]Inc. Amazon Web Services. 2025.Amazon Nova Foundation Models. https:\n//aws.amazon.com/ai/generative-ai/nova/ Accessed: 2025-11-04.\n[3]Charles F Bond Jr and Bella M DePaulo. 2006. Accuracy of deception judgments.\nPersonality and social psychology Review10, 3 (2006), 214\u2013234.\n[4]Vadim Borisov and Gjergji Kasneci. 2022. Relational Local Explanations.arXiv\npreprint arXiv:2212.12374(2022).\n[5]Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022. News summarization and\nevaluation in the era of gpt-3.arXiv preprint arXiv:2209.12356(2022).\n[6]Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn\nSong, and Jacob Steinhardt. 2021. Measuring Massive Multitask Language Under-\nstanding.Proceedings of the International Conference on Learning Representations\n(ICLR)(2021).\n[7]Alon Jacovi, Hendrik Schuff, Heike Adel, Ngoc Thang Vu, and Yoav Goldberg.\n2023. Neighboring Words Affect Human Interpretation of Saliency Explanations.\nInFindings of the Association for Computational Linguistics: ACL 2023. 11816\u2013\n11833.\n[8]Joseph D Janizek, Pascal Sturmfels, and Su-In Lee. 2021. Explaining explanations:\nAxiomatic feature interactions for deep networks.Journal of Machine Learning\nResearch22, 104 (2021), 1\u201354.\n[9]Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav\nSanthanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi,\nHanna Moazam, et al .2023. Dspy: Compiling declarative language model calls\ninto self-improving pipelines.arXiv preprint arXiv:2310.03714(2023).\n[10] Vivian Lai, Han Liu, and Chenhao Tan. 2020. \" Why is\u2019 Chicago\u2019deceptive?\"\nTowards Building Model-Driven Tutorials for Humans. InProceedings of the 2020\nCHI Conference on Human Factors in Computing Systems. 1\u201313.\n[11] Vivian Lai and Chenhao Tan. 2019. On human predictions with explanations and\npredictions of machine learning models: A case study on deception detection. In\nProceedings of the conference on fairness, accountability, and transparency. 29\u201338.\n[12] Jiwei Li, Myle Ott, and Claire Cardie. 2013. Identifying manipulated offerings\non review portals. InProceedings of the 2013 conference on empirical methods in\nnatural language processing. 1933\u20131942.\n[13] DeepMind (Google LLC). 2025.Gemini 2.5 Flash. https://deepmind.google/\nmodels/gemini/flash/ Accessed: 2025-11-04.\n[14] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model\npredictions.Advances in neural information processing systems30 (2017).\n[15] David Martens, James Hinns, Camille Dams, Mark Vergouwen, and Theodoros\nEvgeniou. 2023. Tell me a story! narrative-driven xai with large language models.\narXiv preprint arXiv:2309.17057(2023).\n[16] OpenAI. 2025.Introducing GPT-5. https://openai.com/index/introducing-gpt-5/\nAccessed: 2025-11-04.\n[17] Myle Ott, Claire Cardie, and Jeffrey T Hancock. 2013. Negative deceptive opinion\nspam. InProceedings of the 2013 conference of the north american chapter of the\nassociation for computational linguistics: human language technologies. 497\u2013501.\n[18] Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T Hancock. 2011. Finding\ndeceptive opinion spam by any stretch of the imagination.arXiv preprint\narXiv:1107.4557(2011).\n[19] Bo Pan, Zhen Xiong, Guanchen Wu, Zheng Zhang, Yifei Zhang, and Liang Zhao.\n2024. TAGExplainer: Narrating Graph Explanations for Text-Attributed Graph\nLearning Models.arXiv preprint arXiv:2410.15268(2024).\n[20] Himangshu Paul and Alexander Nikolaev. 2021. Fake review detection on online E-\ncommerce platforms: a systematic literature review.Data Mining and Knowledge\nDiscovery35, 5 (2021), 1830\u20131881.\n[21] Anthropic PBC. 2025.Introducing Claude Haiku 4.5. https://www.anthropic.com/\nnews/claude-haiku-4-5 Accessed: 2025-11-04.\n[22] Jiaming Qu, Jaime Arguello, and Yue Wang. 2021. A Study of Explainability\nFeatures to Scrutinize Faceted Filtering Results. InProceedings of the 30th ACM\nInternational Conference on Information & Knowledge Management. 1498\u20131507.\n[23] Jiaming Qu, Jaime Arguello, and Yue Wang. 2024. Why is\" Problems\" Predictive\nof Positive Sentiment? A Case Study of Explaining Unintuitive Features in Senti-\nment Classification. InThe 2024 ACM Conference on Fairness, Accountability, and\nTransparency. 161\u2013172.\n[24] Jiaming Qu, Jaime Arguello, and Yue Wang. 2025. Understanding the Effects of\nExplaining Predictive but Unintuitive Features in Human-XAI Interaction. InPro-\nceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency.\n296\u2013311.\n[25] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why should i\ntrust you?\" Explaining the predictions of any classifier. InProceedings of the 22nd\nACM SIGKDD international conference on knowledge discovery and data mining.\n1135\u20131144.[26] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors: High-\nprecision model-agnostic explanations. InProceedings of the AAAI conference on\nartificial intelligence, Vol. 32.\n[27] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution\nfor deep networks. InInternational conference on machine learning. PMLR, 3319\u2013\n3328.\n[28] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Com-\nmonsenseQA: A Question Answering Challenge Targeting Commonsense Knowl-\nedge. InProceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers). 4149\u20134158.\n[29] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupati-\nraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette\nLove, et al .2024. Gemma: Open models based on gemini research and technology.\narXiv preprint arXiv:2403.08295(2024).\n[30] Michael Tsang, Sirisha Rambhatla, and Yan Liu. 2020. How does this interaction\naffect me? interpretable attribution for feature interactions.Advances in neural\ninformation processing systems33 (2020), 6147\u20136159.\n[31] Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi\nYang. 2024. Can Large Language Models Transform Computational Social Sci-\nence?Computational Linguistics50, 1 (2024), 237\u2013291.\n[32] Alexandra Zytek, Sara Pid\u00f2, and Kalyan Veeramachaneni. 2024. LLMs for XAI:\nFuture Directions for Explaining Explanations.arXiv preprint arXiv:2405.06064\n(2024).\n",
    "title": "Why is \"Chicago\" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues",
    "authors": [
      "Jiaming Qu",
      "Mengtian Guo",
      "Yue Wang"
    ],
    "published": "2025-11-17",
    "arxiv_id": "2511.13658v1",
    "num_pages": 5,
    "num_chars": 32371
  },
  {
    "text": "OlmoEarth: StableLatentImageModeling\nfor Multimodal Earth Observation\nTeam OlmoEarth\nHenry Herzog\n1Favyen Bastani\n1Yawen Zhang\n1Gabriel Tseng\n1Joseph Redmon\n1\nHadrien Sablon1Ryan Park1Jacob Morrison1,2Alexandra Buraczynski1Karen Farley1\nJoshua Hansen1Andrew Howe1Patrick Alan Johnson1Mark Otterlee1Ted Schmitt1\nHunter Pitelka1Stephen Daspit1Rachel Ratner1Christopher Wilhelm1Sebastian Wood1\nMike Jacobi1\nHannah Kerner3Evan Shelhamer4\nAli Farhadi1,2Ranjay Krishna1,2Patrick Beukema\n1\n1Allen Institute for AI2University of Washington3Arizona State University4University of British\nColumbia\nOlmoEarth was a team effort.\nEqual contribution from modeling team. Authors are listed in reverse alphabetical order by last letter of first name.\nPlatform:olmoearth.allenai.org\nTraining Code:olmoearth_pretrain(pretraining)olmoearth_projects(fine-tuning)\nOlmoEarth Pre-trained Models:OlmoEarth-v1-Nano OlmoEarth-v1-Tiny\nOlmoEarth-v1-Base OlmoEarth-v1-Large\nOlmoEarth Pre-training Dataset:olmoearth_pretrain_dataset\nContact:olmoearth@allenai.org\nAbstract\nEarth observation data presents a unique challenge: it is spatial like images, sequential like video or\ntext, and highly multimodal. We present OlmoEarth: a spatio-temporal, multimodal foundation model\nthat employs a novel self-supervised learning formulation, masking strategy, and loss all designed\nfor the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12\nother foundation models across a variety of research benchmarks and real-world tasks from external\npartners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of\n24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the\nbackbone of an end-to-end platform for data collection, labeling, training, and inference of Earth\nobservation models. The OlmoEarth Platform puts frontier foundation models and powerful data\nmanagement tools into the hands of non-profits and NGOs working to solve the world\u2019s biggest\nproblems. OlmoEarth source code, training data, and pre-trained weights are available at https:\n//github.com/allenai/olmoearth_pretrain.\n1arXiv:2511.13655v1  [cs.CV]  17 Nov 2025\n10910101011101210135055606570\nViT Nano\nViT Tiny\nViT Base\nViT Large\nViT Huge\nViT 7B\nSwin Base\nAnysat\nClayCopernicusFMCROMA\nCROMA\nDINOv3 SatDINOv3 SatGalileoGalileoGalileo\nPanopticon\nPrithvi v2Prithvi v2\nSatlasTerraMindTerraMind\nOlmoEarthOlmoEarthOlmoEarth\nOlmoEarth\nMACsAverage Score\nFigure 1OlmoEarth defines a Pareto optimum of performance vs. computational efficiency averaged across 13\nembedding tasks (measured by kNN and linear probing)1. The chart shows average multiply-accumulate operations to\nencode one example across all tasks (input size varies by task). See Table 2 for full results.\n1 Introduction\nEarth observation foundation models show promising results in research settings [ 2,15,16,25,50,54,55].\nHowever, adoption for real-world tasks lags behind, especially in the non-profit sector. Foundation models are\nlarge, complex to train, and expensive to deploy.\nTo help enable non-profit, humanitarian, and environmental organizations to use these powerful tools we\ntrain OlmoEarth, a new family of models, using a novel, stable training regime. We comprehensively evaluate\nOlmoEarth against 12 other foundation models on research benchmarks and real-world tasks from partner\norganizations. Finally we deploy these models in an open, end-to-end platform bringing frontier models\ndirectly to organizations who need it the most.\n1.1 Stable Training\nFoundation models are complex and expensive to train. When attempting to replicate existing work we\nfrequently saw training instability, representation collapse, and models underperforming their stated potential.\nWe introduce a stable training regime that models images in latent space but avoids instability and collapse.\nOur approach strikes a middle ground between two common approaches in self-supervised learning. Masked\nautoencoders (MAE) predict pixel-level reconstructions of masked input while approaches likeI-JEPAand\nLatent Masked Image Modeling (Latent MIM) predict reconstructions in feature space [ 1,56]. MAE tends to\nbe stable but limited in its feature representations while latent approaches are unstable but produce better\nfeatures [34].\nWe present Latent Masked Image Modeling of Linear, Invariant Token Embeddings (Latent MIM Lite), a\nsimplification of Latent MIM that leads to stable training and better performance. We replace the target\nencoder of Latent MIM with a linear projection from image patches to token space that is randomly initialized\nandneverupdatedduringtraining. Thissimplemodificationstabilizestrainingbutmaintainstherepresentative\npower of modeling in latent space. It also unifies self-supervised and supervised learning as we project both\nobservational data and labeled maps through the frozen random projection layer into token space and calculate\nloss the same for both.\n1Average over all tasks every model can perform, specifically the Sentinel-2 versions of: m-bigearthnet, m-so2sat, m-brick-kiln,\nm-eurosat, BreizhCrops CropHarvest-Togo, CropHarvest-PRC, m-cashewplant, m-SA-crop-type, PASTIS, MADOS, AWF, Nandi.\n2\n1.1.1 Masking\nIn image or text modeling it is sufficient to randomly mask some portion of the input and have the model\nreconstruct the input from context. With multimodal remote sensing data, any token in the input will have\nmany similar tokens either in space, time, or at a different aligned modality. Random masking is too easy of a\ntask unless you use a very high masking ratio [ 50]. We introduce a modality-aware masking strategy that\ncombines random token masking with full modality reconstruction. This makes the task challenging without\nresorting to skewed masking ratios.\n1.1.2 Loss\nLike other SSL approaches in latent space we use a contrastive loss instead of a reconstruction loss. However,\ncontrasting a reconstructed token against all other tokens in a batch, or even in the same sample, leads to\nmany easy negatives given the redundant nature of Earth observation data. Instead we contrast tokens only\nwith other tokens in their respective bandset (a subdivision of modality explained in 2.1). This focuses the\nmodel training on a more challenging but more productive objective, as shown in our experiments.\n1.2 Comprehensive Evaluation\nThere is no standard evaluation suite for remote sensing models. While there are some established standard\npractices [ 16,39,50], they are not always followed. To get a more complete picture of the state of foundation\nmodeling we run a comprehensive evaluation effort of OlmoEarth compared to 12 other foundation models\non 18 research benchmarks and 19 datasets from7partner organizations that are using Earth observation\nmodeling in their work.\nFollowing standard practice we evaluate all models using simple transfer learning techniques (kNN and linear\nprobing) as well as full, end-to-end fine-tuning. We evaluate all models using a standard training recipe and\nsweeping over a variety of hyperparameters. OlmoEarth achieves the best performance in 15 of 24 tasks for\nthe kNN/LP evaluation and 19 of 29 tasks for full fine-tuning. See Figure 1 for a summary.\n1.3 Open Platform\nTraining and fine-tuning remain out of reach for most environmental and humanitarian non-profits. Applying\na foundation model to a task requires data gathering, alignment, pre-processing, labeling, fine-tuning, and\nrunning inference. We deploy OlmoEarth as part of the OlmoEarth Platform to simplify and streamline this\nprocess.\nThe OlmoEarth Platform is an end-to-end solution for organizations who want to harness Earth observation\ndata for the public good. Our partner organizations are already using the platform for things like mangrove\nconservation, ecosystem mapping, and food security. The OlmoEarth Platform solves the last-mile problem of\nputting frontier research into the hands of people who can use it to do the most good.\n2 OlmoEarth\nOlmoEarth is a Vision Transformer (ViT) based encoder-decoder style architecture. It processes a multimodal\nimage timeseries of aligned satellite images and derived maps. A FlexiViT-style projection layer [ 5] converts\nthe input data from pixels to tokens with a variable patch size. Positional, temporal, and modality encodings\nadd additional context to the tokens. During training, some portion of the input tokens are masked. The\nencoder transformer layers attend across space, time, and between modalities to produce embeddings for the\ninput tokens. The decoder predicts representations for the masked input tokens.\n2.1 Data\nOlmoEarth is designed to flexibly handle input Earth observation data across a range of spatial and temporal\nresolutions. During our pretraining experiments we train on three satellite modalities and six derived maps:\n3\nFigure 2Global distribution of OlmoEarth pretraining data. We sample 285,288 locations based on OpenStreetMap\ncategories.\nObservations Maps\nSentinel-1 WorldCereal [53] OpenStreetMap [38]\nSentinel-2 WorldCover [59] Cropland Data Layer [51]\nLandsat-8 SRTM [36] Canopy Height Map [47]\nOurpretrainingdatasetcontains285,288samplesfromaroundtheworld. Eachsamplecoversa2 .56km\u00d7 2.56km\nspatial region and a one-year time range. For multi-temporal modalities, we use up to 12 timesteps sampled\nmonthly over the course of the year, although many samples contain only a subset of the timesteps and\nmodalities.\nFor the above modalities we resample the data to be uniformly 10 meters per pixel. We experimented with\nadding NAIP data at 2.5 meter per pixel [ 52] and ERA5 data at 160 meters per pixel [ 23] but found no\nsignificant improvement on our evaluations.\nWe further subdivide Landsat and Sentinel-2 intobandsetsbased on the original resolution of their bands,\ngrouping bands captured at the same resolution together. Landsat consists of 2 bandsets while Sentinel-2\nconsists of 3 bandsets. For the precise split see the OlmoEarth source code.\nThe locations of samples are chosen based on OpenStreetMap features. We select 120 categories of map features\nin OpenStreetMap, ranging from roads to geothermal power plants, and enumerate all2 .56km\u00d7 2.56kmtiles\ncontaining each category. We then randomly sample up to 10,000 tiles per category to derive the 285,288\nsamples (many categories appear in fewer than 10,000 tiles). The one-year time range of each sample is\nsampled uniformly between January 2016 and December 2024.\n2.2 Architecture\nSimilar to many Earth observation models, OlmoEarth is a transformer-based encoder-decoder style architec-\nture. Inspired by Galileo, we use a flexible patch-embedding layer [ 5,50]. However, instead of doing that\nconfusing pseudo-inverse stuff from FlexiViT we keep the actual projection weights the same size and resize\nthe input image to mimic changing the patch size. It\u2019s probably basically equivalent.\nOnce the input is in token space, OlmoEarth adds in a 2D sincos positional embedding, a sinusoidal temporal\nembedding, and a learnable modality embedding to each token. During training, some tokens are masked out\nof the input, otherwise all tokens are passed to the encoder transformer which performs full self-attention\n4\nSentinel-2 Landsat\n1.28km10m\n10m\n10m\n          Time\nWorldCover OpenStreetMap \nSRTM\nCDL\nWorldCereal \nCanopy Height Satellite Observations \nHigh-quality Maps \nProject + \nMaskingProject + \nMasking\nViT \nEncoderViT\nDecoder\nInstance \nContrastive \nLossViT\nEncoder\nCrop + \nPatchify\nPretraining Objectives Shared\nPretraining Data Sources View 1\nView 2\nPatch \nContrastive \nLossStop Grad (lat, lon)\nUnmasked patch \nMasked patch \nRandom \nProject\nViT\nDecoder\nShared\n10m/px\nSentinel-1 10m/px\n10m/pxBatch \nnegatives \n          Time\nSpatial and temporal \npatchi\ufb01cation WidthHeightFigure 3We train OlmoEarth with a combination of satellite observations and high-quality maps. After tokenizing\nthese inputs, we: (1) apply a modality-aware masking strategy to define which tokens are inputs vs. targets, (2) pass\nthe target tokens through fixed random projections to construct targets, (3) pass the input tokens through our learned\nencoders, and then (4) through a decoder which predicts the target tokens and (5) apply a modality-aware patch\ndiscrimination loss between the predicted and target tokens. Steps 1-5 are applied twice on the same data to then (6)\napply an instance contrastive loss over the aggregated tokens per instance.\nacross space, time, and between modalities.\nArchitecture Depth Dim Heads Parameters\nViT Nano 4 128 8 1.4M\nViT Tiny 12 192 3 6.2M\nViT Base 12 768 12 90M\nViT Large 24 1024 16 300M\nTable 1ViT encoder architectures and number of parameters for the four OlmoEarth model sizes.\nWe train four different encoder sizes based on standard Vision Transformer sizes, see Table 1. For each model\nsize, the decoder has the same feature dimension and number of heads but only a depth of 4. We design a\nsmaller decoder so that the encoder does the majority of the modeling.\nDuring training the decoder represents the masked portions of the input with a learned <MASK> token\nadded to the appropriate positional, temporal, and modality embeddings. The decoder cross-attends to these\ntokens with the visible tokens from the encoder. It then predicts the latents for the masked tokens.\n2.3 Masking\nOlmoEarth uses a modality-aware masking strategy. For every example the masking strategy selects some\nbandsets to be encoded and also some to be decoded, non-exclusively. Thus every bandset falls into one of\nfour categories:\n\u2022Not selected: Ignored for this example.\n\u2022Encode only: Randomly masked, input to encoder.\n\u2022Decode only: Used as target for decoder.\n\u2022Encode and decode: Randomly masked, input to encoder, masked tokens used as targets for decoder.\nThis masking strategy re-frames the problem slightly from reconstructing data that has been partially masked\nto reconstructing missing bandsets from partial views of other bandsets. When all bandsets are encoded\n5\nand decoded we find the task is too easy. Masked tokens in a bandset will likely have other tokens in the\nsame bandset that are highly correlated with them that are visible in the input, tokens nearby spatially or\ntemporally. Training in this easier paradigm requires using very high masking ratios (i.e. masking out 90% of\nthe input) to get decent results. Masking some bandsets entirely makes the problem harder and allows more\nbalanced masking ratios.\nOlmoEarth trains on both observations and maps but at inference time we only use observations. Maps\ncan change over time\u2013indeed downstream tasks are often detecting this kind of change\u2013so we only rely on\nobservations for inference. Thus during training our masking strategy never encodes map data, it only ever\ndecodes it. While observations can fall into any of the above four categories, maps will only be \u201cdecode only\u201d\nor \u201cnot selected\u201d.\n2.4 Latent MIM Lite\nDuring training OlmoEarth predicts reconstructions of the masked input in latent space. We use a randomly\ninitialized, frozen projection layer for each modality to project masked patches in the input into token space.\nThus OlmoEarth performs Latent Masked Image Modeling of Linear, Invariant Token Embeddings (Latent\nMIM Lite).\nRandomly projecting raw input data extracts valuable features both from a theoretical and practical standpoint\n[6,7,43]. Thus our predictions are operating in a true latent space of our input data. However, because we\nuse a fixed target encoder we avoid the representation collapse common in Latent MIM-style training. While\nit\u2019s possible this approach is too simplistic in more diverse domains like natural image processing, empirical\nresults show a clear benefit in our domain of Earth observation data.\nLatent MIM Lite allows us to unify supervised and self-supervised training under the same architecture. We\nproject each modality, whether observations or maps, through a frozen random projection into token space.\nLoss is calculated the same for both types of modalities. We do not need to add on specific predictor heads\nfor supervised data or adjust our training strategy or loss. In our ablations we see this approach gives strong\nresults in a purely self-supervised setting and also benefits from additional supervised data.\nOther models like Galileo and Terramind train on both supervised and unsupervised data however they treat\nsupervised maps as a valid input to the model [ 25,50]. This means their encoders must learn to model these\nmap modalities as input and during training may use map modalities to predict observations or other map\nmodalities. While this also unifies supervised and semi-supervised training, we theorize that our approach\nsimplifies learning for the encoder while maintaining the benefits of training with supervised data. In our\nevaluations we see improved performance over these models on most tasks.\n2.4.1 Modality Patch Discrimination\nMasked image modeling in pixel space typically uses a reconstruction loss like Smooth L1. Latent MIM\nproposes using a contrastive loss (Patch Discrimination) instead of reconstruction loss to incentivize diversity\nin the latent space predictions. Patch discrimination loss frames token reconstruction as a classification task\nwhere we want the predicted token for a patch to be similar to the target token but dissimilar from other\nground truth tokens for other patches. Patch discrimination uses cosine similarity to measure token similarity\nand cross entropy loss to contrast between positive and negative matches.\nTypical patch discrimination contrasts a predicted token with all target tokens in the input. For image\nmodeling, the target tokens from an image are encodings of different parts of the image so they are from the\nsame distribution, making the contrastive task challenging. In OlmoEarth, different target tokens can come\nfrom different modalities or different time steps as well as different spatial locations.\nTokens from different modalities have very different distributions so distinguishing between them is easy.\nYet there are so many tokens from other modalities that a significant amount of the loss comes from these\n\u201ceasy\u201d negatives. We find eliminating easy negatives and only contrasting tokens with targets from the same\nmodality gives a substantial performance increase.\n6\n2.4.2 Instance Contrastive Loss\nPatch discrimination loss operates on the local representations generated by the encoder and decoder but\nmany tasks (like classification) require a global understanding of the input region. Some foundation models\nuse a single <CLASS> token to represent this global information. Instead we opt to pool information globally\nover all modalities, timesteps, and locations for an input. To generate a global representation for an input we\nrun the OlmoEarth encoder and average pool the output tokens.\nTokens encoded from the same modality share semantics but tokens from different modalities may look very\ndifferent from each other. We want to be able to average tokens from all modalities together and get a sensible\nglobal representation of an input. Thus we use a contrastive loss on the pooled representation from the\nencoder to encourage tokens to exist in a common representation space and behave well when pooled.\nWe want both positive and negative samples for our contrastive loss so we take an approach similar to SimCLR\n[10] and encode two versions of the same input, contrasting these two versions as positive examples with the\nrest of the batch as negative examples. However, instead of using different data augmentation to generate the\ntwo samples we use different random masking.\nWe run random masking twice, then encode both batches with our encoder, pool the resulting tokens, and\napply contrastive loss to the pooled representations. We run the decoder twice, decoding masked portions for\nboth images and calculate the modality patch discrimination loss. A scalar multiple controls the contribution\nof instance contrastive loss to modality patch discrimination loss. For experiments in this paper we scale the\ninstance contrastive loss by0.1.\n3 Experiments\nWe extensively evaluate OlmoEarth on both standard research benchmarks and real-world downstream tasks\nfrom partner organizations. Following standard practice in remote sensing foundation models we evaluate\nboth kNN/linear probe performance with a frozen encoder and full fine-tuning performance [16, 39, 50].\nTo get as comprehensive an evaluation as possible we import other top performing foundation models into\nour evaluation framework and evaluate them as well so they are directly comparable [ 2,4,11,15,16,25,\n44,46,49,50,54,55]. We use the same training recipes for each foundation model but sweep a variety of\nhyperparameters to find the best performance for each model on each task. We leave evaluations blank for\nmodels that do not support particular modalities. We also do not fine-tune some large models on partner\ntasks due to compute and time limitations.\n3.1 Pretraining\nWe pretrain OlmoEarth on our pretraining dataset described in 2.1 using Latent MIM Lite. We use AdamW\noptimization with a base learning rate of1 \u00d710\u22124, weight decay of0 .02, batch size of512, linear learning\nrate warm-up of8000steps, cosine annealing of learning rate by0 .1over a total of667 ,200steps. Due to\nmemory constraints we use a micro-batch size of32so the pooled contrastive loss is only applied over these\n32examples, not the full batch of512.\nDuring training OlmoEarth uses a random effective patch size in the range {1. . .8}and takes a random square\ncrop from the input with side length in tokens in the range {1. . .12}. Thus, along the spatial dimension the\nsmallest input is a1 \u00d71pixel region in the input with a patch size of1, and the largest input is96 \u00d796pixel\nregion in the input with a patch size of8. Along the temporal dimension, our model processes between3and\n12timesteps. During training our model processes around 100 billion tokens.\n3.2 Research Benchmarks and Partner Tasks\nWe evaluate on a variety of common research benchmarks for classification and segmentation across single\nand multiple sensor modalities. Our evaluations include all seven Sentinel-2 and Landsat benchmarks from\nGEO-Bench [ 30]: m-bigearthnet, m-so2sat, m-brick-kiln, m-forestnet, m-eurosat, m-cashewplant, and m-SA-\ncrop-type. We also evaluate on the classification benchmarks BreizhCrops [ 42] and CropHarvest [ 48] and the\nsegmentation benchmarks PASTIS [17], MADOS [29], and Sen1Floods11 [8].\n7\nm-bigearthnet m-so2sat m-brick-kiln m-forestnet m-eurosat BreizhCrops CropHarvest-PRC CropHarvest-PRC CropHarvest-PRC CropHarvest-Togo CropHarvest-Togo CropHarvest-Togo m-cashewplant m-SA-crop-type PASTIS PASTIS MADOS Sen1Floods11 AWF AWF AWF Nandi Nandi Nandi\nModalities S2 S2 S2 L8 S2 S2 S1 S2 S1,S2 S1 S2 S1,S2 S2 S2 S1 S2 S2 S1 L8 S1 S2 L8 S1 S2\nTime series\u2718 \u2718 \u2718 \u2718 \u2718 \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 \u2718 \u2718 \u2714 \u2714 \u2718 \u2718 \u2714 \u2714 \u2714 \u2714 \u2714 \u2714\nMethod kNN kNN kNN kNN kNN LP LP LP LP LP LP LP LP LP LP LP LP LP kNN kNN kNN kNN kNN kNN\nModel Metric\u00b5F1 Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. mIOU mIOU mIOU mIOU mIOU mIOU Acc. Acc. Acc. Acc. Acc. Acc.\nAnysat ViT Base 54.5 36.5 84.5 34.0 80.4 62.7 57.0 74.1 72.3 69.6 78.4 74.5 24.6 27.2 24.2 41.9 41.3 77.8 60.0 60.0 64.0 47.4 20.5 55.8\nClay ViT Large 48.8 38.7 90.8 40.3 86.3 57.0 56.7 66.5 63.4 78.4 67.0 67.3 30.8 23.1 19.9 22.6 47.5 78.9 59.5 61.5 56.5 52.2 23.4 57.0\nCopernicusFM ViT Base 64.6 50.3 85.9 - 84.7 65.5 55.1 72.8 74.4 77.5 75.8 70.6 32.2 28.4 15.9 32.1 63.9 77.6 - 59.0 67.0 - 24.4 59.8\nCROMA ViT Base 61.3 51.3 92.0 - 84.6 69.0 56.8 74.3 75.1 76.8 80.7 76.5 24.9 30.3 26.3 44.7 60.4 78.8 - 67.579.0- 24.6 68.2\nCROMA ViT Large 59.2 48.2 91.7 - 85.8 68.2 56.2 72.7 71.0 79.4 76.5 81.0 27.0 30.4 25.9 42.7 66.4 78.8 - 62.5 71.0 - 26.1 68.2\nDINOv3 ViT Base 51.0 47.1 91.3 43.3 86.6 31.3 - 64.5 - - 67.0 - 23.5 26.7 - 18.1 53.5 - 48.5 - 61.0 42.5 - 54.1\nDINOv3 ViT Large 55.8 45.3 89.9 46.0 84.0 31.3 - 66.1 - - 68.3 - 24.5 26.1 - 17.4 52.4 - 43.5 - 60.5 39.9 - 49.8\nDINOv3 ViT Huge+ 57.0 45.7 88.2 46.9 86.1 31.3 - 68.7 - - 68.3 - 25.1 26.7 - 17.4 48.1 - 47.5 - 54.0 43.5 - 55.4\nDINOv3 ViT 7B 60.8 46.6 91.3 48.0 85.2 31.3 - 68.7 - - 70.9 - 34.3 27.9 - 21.1 52.2 - 47.5 - 57.0 44.7 - 56.7\nDINOv3 Sat ViT Large 60.2 44.0 91.4 44.2 89.2 31.3 - 70.1 - - 68.6 - 32.4 28.5 - 22.5 57.5 - 42.5 - 69.5 35.5 - 48.4\nDINOv3 Sat ViT 7B 61.6 50.1 91.4 47.0 91.3 31.3 - 72.2 - - 71.9 -54.1 31.7- 26.3 59.7 - 49.5 - 68.5 31.8 - 42.5\nGalileo ViT Nano 55.0 53.7 90.9 - 89.4 66.3 60.9 74.4 72.4 75.2 70.3 78.1 21.2 19.5 19.2 19.1 53.1 78.6 - 65.5 67.5 - 24.4 64.2\nGalileo ViT Tiny 55.8 53.1 87.5 - 89.1 66.7 55.7 80.3 79.3 69.9 78.8 77.1 23.6 21.5 23.4 27.7 61.1 78.6 - 65.5 71.0 - 25.0 66.7\nGalileo ViT Base 58.3 55.7 91.1 - 92.8 69.760.9 81.979.3 67.3 80.1 77.5 28.9 25.3 28.0 39.668.479.4 - 66.5 72.5 -26.867.3\nPanopticon ViT Base64.960.5 92.952.395.2 57.7 55.9 75.9 75.6 72.2 72.9 76.5 32.7 27.3 23.7 30.2 66.1 78.0 66.0 65.0 71.5 60.4 22.9 65.2\nPresto ViT Nano - - - - - 60.9 59.3 74.3 76.6 78.4 81.4 72.5 - - 16.3 28.2 - - - 60.5 53.5 - 25.2 57.6\nPrithvi v2 ViT Large 51.6 34.7 89.7 37.9 82.2 66.2 - 67.6 - - 71.9 - 46.8 24.7 - 37.2 50.9 - 57.0 - 49.0 57.7 - 52.8\nPrithvi v2 ViT Huge 51.0 34.0 90.1 41.4 81.2 66.1 - 69.5 - - 69.9 - 45.8 26.6 - 37.5 52.2 - 59.0 - 55.0 57.9 - 55.9\nSatlas Swin Base 52.3 44.5 83.0 36.9 82.2 64.6 57.4 71.3 - 76.8 75.8 - 30.6 24.0 10.5 14.4 30.2 72.9 57.5 52.0 62.0 61.5 25.1 60.2\nTerraMind ViT Base 63.9 46.7 91.9 - 85.6 66.4 57.0 75.1 75.6 74.2 74.8 77.5 46.0 30.4 22.7 40.9 66.0 78.7 - 66.0 69.5 - 24.7 -\nTerraMind ViT Large 63.9 47.4 92.2 - 90.0 68.2 56.2 74.7 72.0 75.2 77.8 75.2 50.4 31.2 22.3 41.3 67.5 78.4 - 62.0 67.0 - 23.3 65.3\nTESSERA - - - - - - - - 72.2 - - 81.0 - - - - - - - - - - - -\nOlmoEarth ViT Nano 59.5 54.396.238.8 89.9 64.1 58.0 79.5 74.3 73.5 81.783.725.5 23.6 18.1 35.0 55.2 78.2 73.0 61.5 69.5 57.7 24.8 67.4\nOlmoEarth ViT Tiny 59.4 61.8 92.0 40.5 91.6 64.0 58.3 78.680.375.8 85.6 82.4 24.7 23.2 21.4 40.1 58.6 78.5 75.5 64.0 76.0 60.7 24.7 69.0\nOlmoEarth ViT Base 62.4 67.7 93.3 41.9 94.770.956.8 73.4 75.480.1 87.382.0 32.3 28.9 29.7 50.6 67.2 79.277.0 68.577.567.926.574.7\nOlmoEarth VIT Large 62.068.293.4 41.696.370.7 56.6 74.1 76.1 67.6 78.1 79.7 30.9 28.530.6 51.866.479.876.0 66.5 73.0 66.4 26.2 73.6\nTable 2kNN/Linear probe results on research benchmarks and real-world tasks from our partners. We run kNN\non single time-step classification tasks and linear probing on all other tasks. We sweep across data normalization\nstrategies, feature pooling, and learning rate (for linear probing) and report the test set result for the best validation\nset performance. Not all models can run on all tasks due to incompatible input modalities. OlmoEarth has consistently\nstrong performance and is the best on 15 out of 24 tasks.\nm-bigearthnet m-so2sat m-brick-kiln m-forestnet m-eurosat m-cashewplant m-SA-crop-type PASTIS MADOS Sen1Floods11 AWF AWF GEA North Africa Forest Loss Driver Live Fuel Moisture Content Live Fuel Moisture Content Mangrove Mangrove Marine Infrastructure Marine Infrastructure Nandi Nandi Vessel Detection Vessel Detection Vessel Detection Vessel Length Vessel Type Solar Farm Detection Solar Farm Detection\nModalities S2 S2 S2 L8 S2 S2 S2 S2 S2 S1 S2 S2, S1 S2 S2 S2 S2, S1 S2 S2, S1 S2 S2, S1 S2 S2, S1 L8 S1 S2 S2 S2 S2 S1, S2\nTime series\u2718 \u2718 \u2718 \u2718 \u2718 \u2718 \u2718 \u2714 \u2718 \u2718 \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 \u2718 \u2718 \u2718 \u2718 \u2718 \u2714 \u2714\nModel Metric\u00b5F1 Acc. Acc. Acc. Acc. mIOU mIOU mIOU mIOU mIOU Acc. Acc. Acc. Acc. L1 L1 Acc. Acc. F1 F1 Acc. Acc. F1 F1 F1 L1 Acc. mIoU mIoU\nAnysat ViT Base 68.4 56.7 98.7 51.6 95.9 80.4 34.2 60.9 63.3 77.4 78.0 83.0 59.3 84.6 19.1 19.4 96.9 97.1 52.7 5.2 78.4 76.7 - - - 73.4 43.5 82.6 79.7\nClay ViT Large 65.7 61.3 98.7 49.2 95.8 73.9 33.4 48.9 68.9 78.5 77.0 - 53.4 93.3 24.8 - 96.6 - 86.0 - 30.9 - 70.779.972.4 16.4 68.3 82.2 -\nCopernicusFM ViT Base 71.3 66.8 98.1 - 98.5 78.7 33.6 54.6 66.0 78.6 79.0 79.0 58.8 90.0 25.2 24.5 97.1 97.1 82.8 88.9 68.2 55.9 - 77.4 76.9 16.7 69.6 77.6 76.8\nCROMA ViT Base 69.5 59.1 98.7 - 95.6 46.4 34.8 56.3 66.6 79.4 76.5 75.5 57.5 93.2 24.3 24.0 96.4 96.3 86.1 84.3 62.2 67.4 - - 70.0 19.8 64.4 79.5 79.5\nCROMA ViT Large 71.8 58.9 97.8 - 97.5 47.8 36.0 58.1 68.8 79.4 79.5 - 56.6 92.3 24.6 24.1 96.6 96.2 - - 76.4 - - - - - - 81.7 -\nDINOv3 Sat ViT Large 69.9 63.3 98.959.296.7 80.6 34.5 42.8 64.7 - 34.5 - 43.0 80.4 90.2 - 65.8 - 82.9 - 35.8 - - - 58.0 30.3 54.8 70.7 -\nGalileo ViT Base 69.2 64.7 98.3 - 97.8 78.8 35.7 61.2 71.9 79.7 81.0 81.562.995.1 20.1 18.7 97.3 97.5 85.5 88.081.981.9 - 78.7 75.4 16.4 73.0 83.1 85.1\nPanopticon ViT Base 69.3 65.499.056.0 98.2 79.7 33.4 54.4 72.8 79.1 75.5 78.5 54.3 96.4 24.5 23.7 97.1 97.4 86.4 88.6 65.2 69.5 74.9 76.7 76.8 17.7 69.4 81.8 79.3\nPrithvi v2 ViT Huge 70.6 64.7 98.2 - 96.8 81.1 38.8 58.6 69.3 - 80.0 - 60.6 92.4 - - 97.2 - 84.8 - 77.1 - 71.1 - 74.8 17.4 68.2 84.1 -\nSatlas Swin Base 72.7 65.1 98.7 56.0 97.0 77.0 37.8 57.4 60.5 78.5 78.0 - 56.1 63.3 25.0 24.6 96.6 -87.5- 47.6 - - - 77.6 16.2 71.6 83.3 -\nTerraMind ViT Base 72.6 66.1 98.5 - 97.6 80.9 39.2 59.9 73.2 79.5 84.0 82.0 49.8 96.4 24.3 23.897.796.8 84.0 87.8 66.1 79.3 - 79.6 74.7 18.1 - 83.5 82.1\nTerraMind ViT Large74.065.4 98.1 - 97.881.3 41.160.9 71.5 79.5 81.5 - 51.1 93.9 24.5 24.5 96.5 96.9 - - 66.1 - - - - - - 83.0 -\nOlmoEarth (Random Init) ViT Base 61.0 48.9 94.7 41.7 80.3 43.0 27.5 43.9 45.6 77.0 62.5 - 52.9 52.7 20.9 20.5 96.3 96.4 - - 60.6 56.1 - - - - - 74.1 70.3\nOlmoEarth ViT Nano 66.8 61.5 98.0 50.3 95.3 39.5 35.4 53.0 60.6 78.8 82.5 82.5 61.1 96.0 20.4 19.7 97.4 97.4 86.8 87.8 75.6 74.8 70.2 75.5 75.0 17.1 72.0 82.1 79.6\nOlmoEarth ViT Tiny 69.6 63.5 98.7 53.2 97.1 72.5 38.5 60.3 71.5 79.7 85.0 85.5 60.6 97.7 19.8 19.2 97.6 97.7 85.6 89.3 78.2 76.4 74.4 76.9 77.6 15.8 73.5 85.2 85.2\nOlmoEarth ViT Base 72.068.698.6 51.298.779.8 39.6 64.3 77.8 79.887.0 86.062.4 97.118.5 17.997.697.986.389.681.882.2 75.479.278.8 15.4 74.6 85.4 86.7\nOlmoEarth ViT Large 72.4 68.1 98.6 52.7 98.5 80.6 40.866.3 81.8 79.884.5 - 58.897.919.9 18.5 97.6 97.6 - - 81.0 - - - - - - 84.2 -\nTable 3Fine-tuning results on research benchmarks (left) and partner tasks (right). We train all models with the same\nrecipe and report test set results for the model checkpoint with the best validation set performance. Some models are\nonly compatible with a subset of tasks. Due to resource constraints, we do not fine-tune large models on all tasks.\nOlmoEarth is best on 19 out of 29 tasks.\n8\nWhile developing OlmoEarth, we partnered with several organizations who are already using or want to\nuse remote sensing data for environmental, climate, or research tasks. These organizations provided labeled\ndata across a variety of domains for our evaluations, offering critical insights into how models perform on\nreal-world tasks. For example, we partnered with the African Wildlife Foundation (AWF) to map land use\nand land cover in southern Kenya. We pair these tasks with different combinations of Sentinel-1, Sentinel-2,\nand Landsat observations.\n3.3 kNN and Linear Probing\nFor evaluations without fine-tuning we extract embeddings from the train, validation, and test set and\napply either a kNN model for single time step classification or a linear probe model for segmentation and\nmulti-temporal classification. For OlmoEarth we use a patch size of 4 except we sweep patch size for applicable\nmodels on m-Cashew Plant (See discussion in Appendix). For external models we use recommended settings\nfor patch size and resize input data to that model\u2019s pretraining size following [ 13]. For models that do not\nsupport time series data we input each time step separately. We sweep pooling method for the resulting\nembeddings across time (mean vs max). We also sweep normalization statistics (computed during pretraining\nvs. on the evaluation set).\nWe run kNN with k=20using cosine similarity, and follow standard evaluation practices [ 20,50]. For models\nthat output a <CLASS> embedding token we use that as the embedding for the whole image, otherwise we\naverage across resulting tokens.\nWe run linear probing on the output embeddings, training for 50 epochs. We sweep across a variety of learning\nrates for each model {1\u00d710\u22124,5\u00d710\u22124,1\u00d710\u22123,5\u00d710\u22123,1\u00d710\u22122,5\u00d710\u22122,1\u00d710\u22121,5\u00d710\u22121}and report\nthe test results for the highest validation set performance.\n3.4 Fine-Tuning\nFor fine-tuning evaluations, for each model, we take the encoder and add a decoder that makes classification,\nregression, semantic segmentation, or object detection predictions. Our fine-tuning recipe freezes encoder\nparameters for 20% of the epochs, only training the added decoder layers, and then unfreezes and fine-tunes\nthe full model for the remaining epochs. We use AdamW optimization with a plateau scheduler that reduces\nthe learning rate by a factor of0 .2after 2 epochs without improvement on the validation set and a 10 epoch\ncooldown after reduction.\nFor fine-tuning on research benchmarks, the decoder is a single-layer linear probe; for classification tasks, it\nmakes a prediction using embeddings pooled over the image, and for segmentation tasks, it makes a prediction\nusing embeddings pooled temporally (when applicable) at each spatial patch. We sweep learning rates for\neach model over{1\u00d710\u22124,5\u00d710\u22124,1\u00d710\u22123}.\nFor fine-tuning on partner tasks, the decoder is:\n\u2022Classification:3-layer MLP.\n\u2022Segmentation:Transposed convolutional layers, or U-Net decoder for multi-scale encoders [41].\n\u2022Object Detection:Faster R-CNN head, with an FPN for multi-scale encoders [32, 40].\nWe use a learning rate of10\u22124for all tasks, except Nandi, for which some models exhibit unstable learning\nand we sweep over{10\u22124,10\u22125}.\n3.5 Results\nFor kNN/LP evaluations, OlmoEarth is the best performing on 11 of 18 research benchmarks and 4 of 6\npartner tasks. For fine-tuning evaluations, OlmoEarth is the best performing on 5 of 10 research tasks and 14\nof 19 partner tasks. OlmoEarth gets consistently high performance except in a couple instances.\nOlmoEarth Large does not always outperform OlmoEarth Base, and for embedding-based pixel time series\ntasks it is significantly worse. This may reflect that we explore the training recipe for the Base model more\n9\nm-so2sat m-eurosat PASTIS\nFull Latent MIM* 32.2 68.4 7.9\nLatent MIM Lite 42.2 87.2 35.2\n+ Modality Masking 53.6 90.2 46.6\n+ Modality Patch Disc 55.3 91.5 48.1\n+ Contrastive Loss 56.8 92.3 49.0\n+ Maps62.4 92.9 50.7\nTable 4Development path of the OlmoEarth base model showing effect of adding our various contributions starting\nfrom a Latent MIM approach. *Full Latent MIM collapsed during training.\nthan Large. Terramind and CROMA Base models often outperform Large models on many tasks so this may\nreflect the challenges of scaling Earth observation models.\nOther notable models include Panopticon for strong performance on embedding tasks and Terramind on\nfine-tuning tasks. DINOv3 shows good results for tasks that mainly require visual information but lags behind\nspecialized models on tasks where temporal understanding is critical. Galileo shows strong performance on\nmany benchmarks, especially agriculture-related tasks.\n3.6 Ablations\nWe based OlmoEarth off of Latent MIM self-supervised training and iterated on various modifications, keeping\nthe best. Table 4 shows our development process, starting from standard Latent MIM, random masking,\npatch discrimination loss only, and no maps data. Models in the table are trained according to training recipe\nin Subsection 3.1 but only for 140,000 steps. Results are shown for kNN and LP on the validation set of\nthree benchmarks. During development we ran a subset of our evaluations in our \u201cin-loop evals\u201d but saw that\nimprovements on a representative subset carried over to the full evaluation.\nWe see the Latent MIM model gets poor performance due to representation collapse. Switching to Latent\nMIM Lite substantially boosts performance. Further modifications show increased performance for all tasks.\nWe conduct additional ablations in Appendix C.\n3.7 Environmental Impact\nFollowing recent work on environmental impact analysis of language modeling [ 18,35,37] we estimate total\nenergy use, carbon emissions, and water consumption from training OlmoEarth in Table 5. Similar to other\nenvironmental impact estimates this should be viewed as a lower bound as it does not account for hardware\nmanufacturing, transportation, etc.\nEnergy Carbon Water\nModel Stage Hardware GPU Hrs (kWh) (tCO 2eq) (kL)\nOlmoEarth Nano Pretraining H100 1,149 195 0.08 0.30\nOlmoEarth Tiny Pretraining H100 1,149 205 0.08 0.32\nOlmoEarth Base Pretraining H100 2,989 803 0.32 1.24\nOlmoEarth Large Pretraining B200 5,240 1,933 0.77 2.99\nOlmoEarth Nano Fine-tuning \u2013 647 186 0.07 0.29\nOlmoEarth Tiny Fine-tuning \u2013 723 261 0.10 0.40\nOlmoEarth Base Fine-tuning \u2013 1,224 685 0.27 1.06\nOlmoEarth Large Fine-tuning \u2013 58 39 0.02 0.06\nTotalOverall \u2013 13,179 4,307 1.72 6.67\nTable 5Approximate environmental impact of pretraining and fine-tuning OlmoEarth. Metrics for fine-tuning\nOlmoEarth Nano, Tiny, and Base include research benchmarks and partner tasks. Metrics for fine-tuning OlmoEarth\nLarge only include research benchmarks.\nWe train all of our models in a single data center on NVIDIA H100 and B200 GPUs. We calculate the total\nGPU power required for a training run by tracking actual GPU power utilization every \u223c25ms to calculate\na weighted average of power consumption throughout training. We then multiply this by the power usage\nefficiency (PUE) factor for our data center, according to our provider, and then we multiply this final GPU\n10\npower usage amount by either the carbon intensity of the grid or the water usage efficiency factor of the data\ncenter to calculate total carbon emissions and water consumption, respectively.\nThe total energy usage during training (4,307 kWh) could power the average U.S. household for 5 months.\nThe total carbon emissions are equivalent to an economy ticket on a flight from Seattle to Portugal.\n4 Related Work\nPretrainingfor remote sensing models initially focused on contrastive approaches [ 3,26,33]. Recently masked\nmodeling has taken over as the dominant paradigm, similar to language and vision [ 14,21]. Early approaches\nto remote sensing pretraining directly reconstructed the masked pixel values [ 12,39,49]. Following research in\nnatural imagery [ 1,44,56], remote sensing focuses more on reconstruction in latent space. Latent approaches\nwork well [2, 50, 54] but have documented instabilities [1, 34].\nTerraMindavoids instability by using a frozen tokenizer during pretraining. For image modalities they train a\nquantized autoencoder and use the encoder as their frozen tokenizer during multimodal masked modeling.\nPrecomputed embeddingsoffer an alternative approach for accessibility [ 9,15] but still require expertise to\nretrieve and use. Best results may still require training a decoder on top of the embeddings. Precomputed\nembeddings also limit flexibility; both AEF and TESSERA generate annualized embeddings making real-time\nor sub-annual predictions impossible. OlmoEarth embeddings match or outperform AEF embeddings on\npartner tasks, and full fine-tuning enables even better results (Appendix Table 7).\n5 Discussion\nWe want OlmoEarth to have a positive impact on the world. Toward that end we release it as part of\nthe OlmoEarth Platform, an end-to-end, open solution for Earth observation tasks. OlmoEarth Platform\nenables partner organizations to use the latest, best foundation models in their work on the environment,\nconservation, food security, and more. Organizations like Global Mangrove Watch, Global Ecosystem Atlas,\nand the International Food Policy Research Institute are using OlmoEarth Platform for data curation and\nlabeling, model fine-tuning, and inference.\n5.1 Case Studies\nGlobal Mangrove Watchmaps and tracks the extent and health of coastal mangrove forests. Mangrove\nforests sequester carbon, protect the coastline from erosion, and provide a habitat for little fishies. GMW uses\na random forest model with a95 .3%F1 score to generate maps on a yearly cadence, and only covering about\nhalf of relevant coastal regions. Using OlmoEarth Platform we fine-tune a OlmoEarth model using their data\nup to an F1 score of98 .1%. The OlmoEarth Platform can run inference on a monthly cadence to generate\nnew maps, or on a rolling basis to detect change faster.\nGlobal Ecosystem Atlasis building a comprehensive map of the world\u2019s ecosystems [ 28]. For the last 3\nmonths they have been using OlmoEarth Platform to label more than 15,000 data points. OlmoEarth Platform\nallows them to partition areas of interest, generate points to label, assign those points to labelers, review the\nresults, and export the data or fine-tune a model directly in the platform. With a subset of the data from\nNorth Africa we fine-tune a model that achieves state-of-the-art accuracy and run inference to generate new\necosystem maps. Humans can review the results to feed better labels back into the training pipeline.\n5.2 Downstream Risks\nThe power and versatility of OlmoEarth also bring risks. We release OlmoEarth under an open license designed\nto address some of these risks by allowing the free use, modification, and sharing of the model weights, datasets,\nand associated code while restricting use for military, defense-related, and extractive industry applications.\n11\nFigure 4Results of a fine-tuned ecosystem classification model in the OlmoEarth Platform. Users can label data,\nfine-tune models, and run inference to generate maps all in the OlmoEarth Platform.\n5.3 The Future\nWe plan to add climate and weather data and forecasting to the OlmoEarth model to help with tasks like\nwildfire prediction and crop yield forecasting. Expanding to this kind of data will require handling a wider\nvariety of input resolutions both spatially (from meters to kilometers) and temporally (from days to years).\nWe also plan to add non-geospatial data to the model. Often data labeling for tasks like crop type mapping\nrequires actually going to a location in person and looking at stuff. We\u2019d like the model to be able to do\nthat too. The ability to process geolocated natural images would expand OlmoEarth\u2019s ability to handle these\nfine-grained recognition tasks.\nUltimately we want to support and grow the community of partner organizations who bring incredible\nknowledge, expertise, and passion to this work. We plan to learn from our partners about what tools and\ncapabilities they need and then improve OlmoEarth Platform to better help them. We hope OlmoEarth\nPlatform can become a hub for data, models, training, and inference across a wide range of organizations\nworking to solve the world\u2019s biggest problems.\nAcknowledgments\nWe wish to express deep gratitude to our early collaborators who shared data, expertise, and time to make\nthese models successful for real-world, mission-critical applications: Amazon Conservation Association, African\nWildlife Foundation, CGIAR/International Food Policy Research Institute (IFPRI), Global Mangrove Watch,\nGlobal Ecosystem Atlas, ITC University of Twente, NASA Jet Propulsion Laboratory (JPL), and NASA\nHarvest.\nWe would also like to thank the OLMo-core, Beaker, Comms, and Legal teams at Ai2 for their support,\nespecially Pete Walsh, Dirk Groeneveld, Sam Skjonsberg, Tara Wilkins, Caroline Wu, Johann Dahm, David\nAlbright, Kyle Wiggers, Jordan Steward, Crystal Nam, Will Smith, and Janice Dow.\n12\nReferences\n[1]Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun,\nand Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive architecture. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15619\u201315629, 2023.\n[2]Guillaume Astruc, Nicolas Gonthier, Clement Mallet, and Loic Landrieu. AnySat: An Earth observation model\nfor any resolutions, scales, and modalities.arXiv preprint arXiv:2412.14123, 2024.\n[3]Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tanmay, Marshall Burke, David Lobell, and Stefano Ermon.\nGeography-aware self-supervised learning. InProceedings of the IEEE/CVF International Conference on Computer\nVision, pages 10181\u201310190, 2021.\n[4]Favyen Bastani, Piper Wolters, Ritwik Gupta, Joe Ferdinando, and Aniruddha Kembhavi. SatlasPretrain: A\nlarge-scale dataset for remote sensing image understanding. InProceedings of the IEEE/CVF International\nConference on Computer Vision, pages 16772\u201316782, 2023.\n[5]Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias\nMinderer, Michael Tschannen, Ibrahim Alabdulmohsin, and Filip Pavetic. FlexiViT: One model for all patch sizes.\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14496\u201314506,\n2023.\n[6]Ella Bingham and Heikki Mannila. Random projection in dimensionality reduction: applications to image and\ntext data. InProceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and\ndata mining, pages 245\u2013250, 2001.\n[7]Avrim Blum. Random projection, margins, kernels, and feature-selection. InInternational Statistical and\nOptimization Perspectives Workshop\" Subspace, Latent Structure and Feature Selection\", pages 52\u201368. Springer,\n2005.\n[8]Derrick Bonafilia, Beth Tellman, Tyler Anderson, and Erica Issenberg. Sen1Floods11: A georeferenced dataset\nto train and test deep learning flood algorithms for Sentinel-1. InProceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops, pages 210\u2013211, 2020.\n[9]Christopher F Brown, Michal R Kazmierski, Valerie J Pasquarella, William J Rucklidge, Masha Samsikova,\nChenhui Zhang, Evan Shelhamer, Estefania Lahera, Olivia Wiles, Simon Ilyushchenko, et al. Alphaearth\nfoundations: An embedding field model for accurate and efficient global mapping from sparse label data.arXiv\npreprint arXiv:2507.22291, 2025.\n[10]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive\nlearning of visual representations. InInternational conference on machine learning, pages 1597\u20131607. PMLR,\n2020.\n[11] Clay. Clay Foundation Model - Clay Foundation Model.https://clay-foundation.github.io/model/, 2025.\n[12]Yezhen Cong, Samar Khanna, Chenlin Meng, Patrick Liu, Erik Rozi, Yutong He, Marshall Burke, David Lobell,\nand Stefano Ermon. Satmae: Pre-training transformers for temporal and multi-spectral satellite imagery.Advances\nin Neural Information Processing Systems, 35:197\u2013211, 2022.\n[13]Isaac Corley, Caleb Robinson, Rahul Dodhia, Juan M. Lavista Ferres, and Peyman Najafirad. Revisiting pre-\ntrained remote sensing model benchmarks: Resizing and normalization matters. InProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 3162\u20133172, 2024.\n[14]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding.arXiv preprint arXiv:1810.04805, 2018.\n[15]Zhengpeng Feng, Sadiq Jaffer, Jovana Knezevic, Silja Sormunen, Robin Young, Madeline Lisaius, Markus Immitzer,\nJames Ball, Clement Atzberger, David A Coomes, et al. Tessera: Temporal embeddings of surface spectra for\nearth representation and analysis.arXiv preprint arXiv:2506.20380, 2025.\n[16]Anthony Fuller, Koreen Millard, and James Green. CROMA: Remote sensing representations with contrastive\nradar-optical masked autoencoders.Advances in Neural Information Processing Systems, 36, 2024.\n[17]Vivien Sainte Fare Garnot and Loic Landrieu. Panoptic segmentation of satellite image time series with\nconvolutional temporal attention networks. InProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 4872\u20134881, 2021.\n13\n[18]Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha,\nHamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi\nChandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill,\nJacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin,\nAbhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell\nWortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca\nSoldaini, Noah A. Smith, and Hannaneh Hajishirzi. Olmo: Accelerating the science of language models, 2024.\n[19] Group on Earth Observations (GEO). Global Ecosystems Atlas.https://globalecosystemsatlas.org, 2025.\n[20]Matthew Gwilliam and Abhinav Shrivastava. Beyond supervised vs. unsupervised: Representative benchmarking\nand analysis of image representation learning. InProceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 9642\u20139652, 2022.\n[21]Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are\nscalable vision learners. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition,\npages 16000\u201316009, 2022.\n[22]Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep\nlearning benchmark for land use and land cover classification.IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing, 12(7):2217\u20132226, 2019.\n[23]Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, Andr\u00e1s Hor\u00e1nyi, Joaqu\u00edn Mu\u00f1oz-Sabater, Julien Nicolas,\nCarole Peubey, Raluca Radu, Dinand Schepers, et al. The era5 global reanalysis.Quarterly Journal of the Royal\nMeteorological Society, 146(730):1999\u20132049, 2020.\n[24]Jeremy Irvin, Hao Sheng, Neel Ramachandran, Sonja Johnson-Yu, Sharon Zhou, Kyle Story, Rose Rustowicz,\nCooper Elsworth, Kemen Austin, and Andrew Y Ng. Forestnet: Classifying drivers of deforestation in indonesia\nusing deep learning on satellite imagery.arXiv preprint arXiv:2011.05479, 2020.\n[25]Johannes Jakubik, Felix Yang, Benedikt Blumenstiel, Erik Scheurer, Rocco Sedona, Stefano Maurogiovanni, Jente\nBosmans, Nikolaos Dionelis, Valerio Marsocci, Niklas Kopp, Rahul Ramachandran, Paolo Fraccaro, Thomas\nBrunschwiler, Gabriele Cavallaro, Juan Bernabe-Moreno, and Nicolas Long\u00e9p\u00e9. Terramind: Large-scale generative\nmultimodality for earth observation, 2025.\n[26]Neal Jean, Sherrie Wang, Anshul Samar, George Azzari, David Lobell, and Stefano Ermon. Tile2vec: Unsupervised\nrepresentation learning for spatially distributed data. InProceedings of the AAAI Conference on Artificial\nIntelligence, pages 3967\u20133974, 2019.\n[27]Z. Jin, C. Lin, C. Weigl, J. Obarowski, and D. Hale. Smallholder cashew plantations in benin version 1.0.\nhttps://doi.org/10.34911/rdnt.hfv20i, 2021.\n[28]David A. Keith, Jos\u00e9 R. Ferrer-Paris, Emily Nicholson, Melanie J. Bishop, Beth A. Polidoro, Eva Ramirez-Llodra,\nMark G. Tozer, Jeanne L. Nel, Ralph Mac Nally, and Edward J. Gregr. A function-based typology for earth\u2019s\necosystems.Nature, 610:513\u2013518, 2022.\n[29]Katerina Kikaki, Ioannis Kakogeorgiou, Ibrahim Hoteit, and Konstantinos Karantzalos. Detecting marine\npollutants and sea surface features with deep learning in Sentinel-2 imagery.ISPRS Journal of Photogrammetry\nand Remote Sensing, 210:39\u201354, 2024.\n[30]Alexandre Lacoste, Nils Lehmann, Pau Rodriguez, Evan Sherwin, Hannah Kerner, Bj\u00f6rn L\u00fctjens, Jeremy Irvin,\nDavid Dao, Hamed Alemohammad, Alexandre Drouin, et al. GEO-Bench: Toward foundation models for earth\nmonitoring.Advances in Neural Information Processing Systems, 36, 2024.\n[31]Jihyeon Lee, Nina R Brooks, Fahim Tajwar, Marshall Burke, Stefano Ermon, David B Lobell, Debashish Biswas,\nand Stephen P Luby. Scalable deep learning to identify brick kilns and aid regulatory capacity.Proceedings of the\nNational Academy of Sciences, 118(17):e2018863118, 2021.\n[32]Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. InProceedings of the IEEE conference on computer vision and pattern recognition,\npages 2117\u20132125, 2017.\n[33]Oscar Manas, Alexandre Lacoste, Xavier Gir\u00f3-i Nieto, David Vazquez, and Pau Rodriguez. Seasonal contrast:\nUnsupervised pre-training from uncurated remote sensing data. InProceedings of the IEEE/CVF International\nConference on Computer Vision, pages 9414\u20139423, 2021.\n14\n[34]Shentong Mo and Shengbang Tong. Connecting joint-embedding predictive architecture with contrastive self-\nsupervised learning. InThe Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.\n[35]Jacob Morrison, Clara Na, Jared Fernandez, Tim Dettmers, Emma Strubell, and Jesse Dodge. Holistically\nevaluating the environmental impact of creating language models, 2025.\n[36]National Aeronautics and Space Administration (NASA) Earthdata. Shuttle Radar Topography Mission. https:\n//e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/, 2018.\n[37]Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling\nGu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David\nAtkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Allyson Ettinger, Michal Guerquin,\nDavid Heineman, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V.\nMiranda, Jacob Morrison, Tyler Murray, Crystal Nam, Jake Poznanski, Valentina Pyatkin, Aman Rangapur,\nMichael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali\nFarhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2 olmo 2 furious, 2025.\n[38]OpenStreetMap contributors. Planet dump retrieved from https://planet.osm.org . https://www.openstreetmap.\norg, 2017.\n[39]Colorado J Reed, Ritwik Gupta, Shufan Li, Sarah Brockman, Christopher Funk, Brian Clipp, Kurt Keutzer,\nSalvatore Candido, Matt Uyttendaele, and Trevor Darrell. Scale-MAE: A scale-aware masked autoencoder for\nmultiscale geospatial representation learning. InProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 4088\u20134099, 2023.\n[40]Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with\nregion proposal networks.IEEE transactions on pattern analysis and machine intelligence, 39(6):1137\u20131149, 2016.\n[41]Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image\nsegmentation. InInternational Conference on Medical image computing and computer-assisted intervention, pages\n234\u2013241. Springer, 2015.\n[42]Marc Ru\u00dfwurm, S\u00e9bastien Lef\u00e8vre, and Marco K\u00f6rner. BreizhCrops: A satellite time series dataset for crop type\nidentification. InProceedings of the International Conference on Machine Learning Time Series Workshop, 2019.\n[43]R Siddharth and Gnanasekaran Aghila. Randpro-a practical implementation of random projection-based feature\nextraction for high dimensional multivariate data analysis in R.SoftwareX, 12:100629, 2020.\n[44]Oriane Sim\u00e9oni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov,\nMarc Szafraniec, Seungeun Yi, Micha\u00ebl Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan\nWang, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John\nBrandt, Camille Couprie, Julien Mairal, Herv\u00e9 J\u00e9gou, Patrick Labatut, and Piotr Bojanowski. DINOv3, 2025.\n[45]Gencer Sumbul, Marcela Charfuelan, Beg\u00fcm Demir, and Volker Markl. Bigearthnet: A large-scale benchmark\narchive for remote sensing image understanding. InIGARSS 2019-2019 IEEE International Geoscience and\nRemote Sensing Symposium, pages 5901\u20135904. IEEE, 2019.\n[46]Daniela Szwarcman, Sujit Roy, Paolo Fraccaro, Thorsteinn El\u00ed G\u00edslason, Benedikt Blumenstiel, Rinki Ghosal,\nPedro Henrique de Oliveira, Joao Lucas de Sousa Almeida, Rocco Sedona, Yanghui Kang, et al. Prithvi-eo-2.0: A\nversatile multi-temporal foundation model for earth observation applications.arXiv preprint arXiv:2412.02732,\n2024.\n[47]Jamie Tolan, Hung-I Yang, Benjamin Nosarzewski, Guillaume Couairon, Huy V Vo, John Brandt, Justine Spore,\nSayantan Majumdar, Daniel Haziza, Janaki Vamaraju, et al. Very high resolution canopy height maps from\nrgb imagery using self-supervised vision transformer and convolutional decoder trained on aerial lidar.Remote\nSensing of Environment, 300:113888, 2024.\n[48]Gabriel Tseng, Ivan Zvonkov, Catherine Lilian Nakalembe, and Hannah Kerner. CropHarvest: A global dataset\nfor crop-type classification. InThirty-fifth Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track (Round 2), 2021.\n[49]Gabriel Tseng, Ruben Cartuyvels, Ivan Zvonkov, Mirali Purohit, David Rolnick, and Hannah Kerner. Lightweight,\npre-trained transformers for remote sensing timeseries.arXiv preprint arXiv:2304.14065, 2023.\n[50]Gabriel Tseng, Anthony Fuller, Marlena Reil, Henry Herzog, Patrick Beukema, Favyen Bastani, James R Green,\nEvan Shelhamer, Hannah Kerner, and David Rolnick. Galileo: Learning global & local features of many remote\nsensing modalities. InForty-second International Conference on Machine Learning, 2025.\n15\n[51]United States Department of Agriculture (USDA) National Agricultural Statistics Service (NASS). Cropland\nData Layer: USDA NASS, 2024. National Agricultural Statistics Service Marketing and Information Services\nOffice, Washington, D.C. Retrieved from Link: https://croplandcros.scinet.usda.gov/.\n[52]U.S. Geological Survey. National agriculture imagery program: 2003 - present. https://doi.org/10.5066/\nF7QN651G, 2023.\n[53]Kristof Van Tricht, Jeroen Degerickx, Sven Gilliams, Daniele Zanaga, Marjorie Battude, Alex Grosu, Joost\nBrombacher, Myroslava Lesiv, Juan Carlos Laso Bayas, Santosh Karanam, et al. WorldCereal: a dynamic\nopen-source system for global-scale, seasonal, and reproducible crop and irrigation mapping.Earth System Science\nData Discussions, 2023:1\u201336, 2023.\n[54]Leonard Waldmann, Ando Shah, Yi Wang, Nils Lehmann, Adam Stewart, Zhitong Xiong, Xiao Xiang Zhu,\nStefan Bauer, and John Chuang. Panopticon: Advancing any-sensor foundation models for earth observation. In\nProceedings of the Computer Vision and Pattern Recognition Conference (CVPR) Workshops, 2025.\n[55]Yi Wang, Zhitong Xiong, Chenying Liu, Adam J Stewart, Thomas Dujardin, Nikolaos Ioannis Bountos, Angelos\nZavras, Franziska Gerken, Ioannis Papoutsis, Laura Leal-Taix\u00e9, et al. Towards a unified copernicus foundation\nmodel for earth vision. InProceedings of the IEEE/CVF International Conference on Computer Vision, 2025.\n[56]Yibing Wei, Abhinav Gupta, and Pedro Morgado. Towards latent masked image modeling for self-supervised\nvisual representation learning. InECCV, 2024.\n[57]Western Cape Department of Agriculture. Crop type classification dataset for western cape, south africa.\nhttps://staging.source.coop/radiantearth/south-africa-crops-competition, 2021.\n[58]Marta Yebra, Gianluca Scortechini, Karine Adeline, Nursema Aktepe, Turkia Almoustafa, Avi Bar-Massada,\nMar\u00eda Eugenia Beget, Matthias Boer, Ross Bradstock, Tegan Brown, et al. Globe-LFMC 2.0, an enhanced and\nupdated dataset for live fuel moisture content research.Scientific data, 11(1):332, 2024.\n[59]Daniele Zanaga, Ruben Van De Kerchove, Dirk Daems, Wanda De Keersmaecker, Carsten Brockmann, Grit\nKirches, Jan Wevers, Oliver Cartus, Maurizio Santoro, Steffen Fritz, et al. ESA WorldCover 10 m 2021 v200.\nESA WorldCover Project, 2022.\n[60]Xiao Xiang Zhu, Jingliang Hu, Chunping Qiu, Yilei Shi, Jian Kang, Lichao Mou, Hossein Bagheri, Matthias\nHaberle, Yuansheng Hua, Rong Huang, et al. So2sat lcz42: A benchmark data set for the classification of global\nlocal climate zones [software and data sets].IEEE Geoscience and Remote Sensing Magazine, 8(3):76\u201389, 2020.\n16\nA Research Benchmarks\nWe describe the research benchmarks introduced in Section 3.2 in more detail below. We also share our\nobservations on limitations of certain benchmarks.\nGEO-Bench modifies benchmarks to form a unified and consistent collection of datasets:\nm-bigearthnetis modified from BigEarthNet [ 45], which involves multi-label land cover classification of\n120\u00d7120Sentinel-2 image crops. It consists of 19 classes, such as arable land, inland wetlands, and urban\nfabric. The original dataset contains 549,488 examples, but the modified subset in GEO-Bench contains only\n22,000, with 20,000 for training, 1,000 for validation, and 1,000 for testing.\nm-so2satis modified from So2Sat LCZ42 [ 60], which involves image-level classification of local climate zones\nfrom co-registered Sentinel-1 and Sentinel-2 crops. It consists of 17 classes, such as high-rise, industrial, and\nwater bodies. The original dataset contains 400,673 examples, but the modified subset in GEO-Bench contains\nonly 21,964, with 19,992 for training, 986 for validation, and 986 for testing.\nm-brick-kilnis modified from the Brick Kiln Classification Dataset in Bangladesh [ 31]. The original dataset\ninvolves image-level classification of whether or not high-resolution224 \u00d7224satellite image crops from\nDigitalGlobe contain at least one kiln, and contains 6,329 positive examples and 67,284 negative examples.\nThe modified dataset in GEO-Bench performs the same task on corresponding64 \u00d764Sentinel-2 crops, and\ncontains only 17,061 examples, with 15,063 for training, 999 for validation, and 999 for testing. While finding\nkilns in Sentinel-2 images is a challenging task, we find that the nature of the negatives in the GEO-Bench\nversion of the dataset make the classification task too easy; for example, many negatives seem to have only\ndark pixels, making it easy to distinguish them.\nm-forestnetis modified from ForestNet [ 24], which involves image-level classification of deforestation drivers\nfrom a composite332 \u00d7332Landsat 8 satellite image captured within five years after each forest loss event.\nThere are four driver categories: plantation, smallholder agriculture, grassland/shrubland, and other. The\noriginal dataset contains 2,756 examples. The modified subset in GEO-Bench contains 6,464 examples for\ntraining, 989 examples for validation, and 993 examples for testing; we could not determine where the\nadditional examples came from.\nm-eurosatis modified from EuroSat [ 22], which involves image-level land use and land cover classification\nfrom64 \u00d764Sentinel-2 image crops. It consists of 10 classes, such as annual crop, river, and highway. The\noriginal dataset contains 27,000 examples, but the modified subset in GEO-Bench contains only 4,000, with\n2,000 for training, 1,000 for validation, and 1,000 for testing.\nm-cashewplantis modified from the Smallholder Cashew Plantations in Benin Dataset [ 27], which involves\nsegmentation of256 \u00d7256Sentinel-2 image crops. It consists of six classes relating to cashew plantations:\nwell-managed plantation, poorly-managed plantation, non-plantation, uncertain, residential, and background.\nThe modified dataset contains 1,800 examples, with 1,350 for training, 400 for validation, and 50 for testing.\nMultiple models were sensitive to input patch size on this dataset, so for models that had a variable patch\nsize, we swept input patch size and report the best result. Ultimately this is likely an effect of the labels being\nlarge polygons instead of per-pixel labels.\nm-SA-crop-typeis modified from the South Africa Crop Type Competition dataset [ 57], which involves crop\ntype segmentation of256 \u00d7256Sentinel-2 and Sentinel-1 image crops. It consists of 10 classes, such as fallow,\nwine grapes, and wheat. The modified dataset in GEO-Bench only uses the Sentinel-2 images, and contains\n5,000 examples, with 3,000 for training, 1,000 for validation, and 1,000 for testing.\nAll of the GEO-Bench datasets share a significant limitation: although the tasks involve labels that do not\nchange rapidly over time, the input consists of a single satellite image or image pair. We find that remote\nsensing models generally perform much better with multiple input images, and argue that single-image inputs\nshould only be used for tasks like vessel detection where the labels are only valid for one timestep.\nWe compare on five additional datasets outside of GEO-Bench:\nBreizhCrops[ 42] involves crop type classification from single-pixel Sentinel-2 time series. It consists of nine\nclasses, such as wheat, corn, and permanent meadows. It contains 610K examples.\n17\nCropHarvest[ 48] involves binary cropland classification from single-pixel time series. The provided time series\ninclude Sentinel-2 and Sentinel-1 satellite image observations, as well as elevation from SRTM and weather\ndata from ERA-5. It contains 95,186 examples.\nPASTIS[17] involves crop type segmentation from Sentinel-1 and Sentinel-2 image time series, with128 \u00d7128\nimage crops. It consists of 19 classes, such as grapevine, spring barley, and soybeans. It contains 2,433\nexamples.\nMADOS[ 29] involves marine debris segmentation in80 \u00d780Sentinel-2 image crops. It consists of 15 classes,\nsuch as oil spills, dense sargassum, and foam. It contains 2,803 examples. A key limitation with MADOS is\nthat it provides custom-processed images, making it difficult to apply foundation models with their intended\nnormalization statistics. Additionally, the dataset includes a lot of rare classes that greatly affect mIoU in the\ntest set, making metrics highly variable across runs of the same model with different seeds.\nSen1Floods11involves binary water segmentation in512 \u00d7512Sentinel-2 image crops that focus on flooded\nareas. It contains 4,831 examples. All of the remote sensing models we tested get between 78-80% accuracy,\nand we find that the accuracy is not well correlated with other benchmarks. However, Sen1Floods11 is one of\nthe few Sentinel-1 benchmarks.\nB Partner Tasks\nWe describe the partner tasks introduced in Section 3.2 in more detail below.\nAWF -African Wildlife Foundation (AWF)Land cover classification in southern Kenya. The dataset contains\n1,459 examples with 9 classes, which range from lava forest and agriculture to urban development. The AWF\nteam used Planet imagery as the main reference to annotate these examples.\nLive Fuel Moisture Content -NASA JPLRegression dataset of 41,214 examples from Globe-LFMC-2.0 [ 58]\nlabeled with the LFMC value. We partner with NASA JPL to deploy a model trained on this data. LFMC\npredictions are used to understand wildfire risk.\nMangrove -Global Mangrove WatchClassification dataset of 100,000 coastal areas into 3 classes: mangrove\nforest, water, or other. Mangrove maps across different years are used to understand mangrove growth and\nloss.\nNandi -CGIARCrop-type classification in Nandi County, Kenya. The dataset contains 6,924 examples with 6\ncategories (coffee, maize, sugarcane, etc.). The ground-truth labels were collected through field surveys.\nEcosystem type mapping is similar, but only uses six timesteps of input images:\nGEA North Africa -Global Ecosystem AtlasEcosystem type classification of 2,361 examples in a region of North\nAfrica, and labels correspond to the 110 categories in level 3 of the IUCN Global Ecosystem Typology [19].\nThe other tasks are more unique:\nForest Loss Driver -Amazon ConservationClassification dataset for the cause of forest loss in the Amazon\nrainforest into 10 classes (mining, logging, agriculture, etc.). The input consists of 4 Sentinel-2 images captured\nbefore the forest loss and 4 images captured after the forest loss. Driver predictions are used to prioritize\nenforcement and litigation efforts to deter further human-caused forest loss.\nMarine Infrastructure -SkylightGlobal marine infrastructure detection dataset containing 7,197 examples\nlabeled as offshore platform or wind turbine. The input consists of a time series of 4 Sentinel-2 or Sentinel-2 +\nSentinel-1 images.\nVessel Detection, Type, Length -SkylightThree object detection tasks to detect vessels in Landsat (8,000\nexamples), Sentinel-1 (1,776 examples), and Sentinel-2 (45,545 examples) images, one classification task to\npredict the vessel type in Sentinel-2 images centered at detected vessels (584,432 examples), and one regression\ntask to estimate the vessel length in Sentinel-2 images (584,432 examples). For all of these tasks, the input is\na single image.\nSolar Farm Detection: Binary segmentation dataset containing 3,561 examples densely labeled with solar farm\npolygons. The input consists of 4 timesteps, either Sentinel-2 or Sentinel-2 + Sentinel-1. Solar farm maps are\n18\nused to understand the global rate of renewable energy deployment over time.\nC Additional Ablations\nm-bigearthnet m-so2sat m-brick-kiln m-forestnet m-eurosat BreizhCrops PASTIS PASTIS MADOS Sen1Floods11 Average Average Rank\nS2 S2 S2 L8 S2 S2 S1 S2 S2 S1\nAcc. Acc. Acc. Acc. Acc. F1 F1 F1 F1 F1\nMAE 60.6 48.1 96.2 42.0 89.3 71.531.146.6 68.7 78.3 63.2 5.1\nOnly S2 Data 53.7 45.9 91.3 - 89.271.7- 42.4 69.5 - 46.4 -\nNo Maps 59.5 58.6 95.246.092.6 71.4 29.1 48.0 70.2 77.9 64.9 4.7\nNo Agricultural Maps 60.9 66.5 94.346.093.9 71.4 29.0 48.5 71.4 78.8 66.1 3.6\nRandom Masking 60.767.494.7 43.5 91.8 70.3 24.7 51.1 71.9 77.8 65.4 4.7\nNo Inst. Contrastive Loss 60.5 65.6 93.6 44.9 93.6 70.2 28.5 51.4 72.1 78.4 65.9 4.7\nPatch Disc Loss 62.0 62.196.344.8 94.0 70.3 29.6 50.074.1 79.366.2 3.0\nFinal Recipe62.365.9 94.2 45.894.671.4 29.452.271.7 78.866.6 2.9\nTable 6Ablation experiment selectively removing components of OlmoEarth base model.\nIn addition to the ablations in Section 3.6, we conduct a second set of ablations in Table 6. Our second set of\nablations evaluates the contributions of components of our final model and training recipe by removing them\nindividually, with the exception of the top row which is a MAE baseline. These models are trained for 300,000\nsteps. In the data ablation section we see the Sentinel-2 only model perform relatively poorly, however the\n\u201cNo Maps\" run (only observational data) maintains relatively high performance. While our model can benefit\nfrom labeled data we still see good performance with pure self-supervised training.\nBuilding remote sensing foundation models necessitates some tradeoffs. While our final model is not the best\nin every metric it retains high performance across the board and has the best average score and lowest average\nper-task rank.\nD Comparison to AlphaEarth Foundations\nThe AlphaEarth foundation model [ 9] is comparable to OlmoEarth in that both draw on similar data sources\nand were designed to support similar downstream tasks. Rather than releasing the model, Google released\nonly the global, annualized embeddings computed by AlphaEarth. We compare OlmoEarth both as a frozen\nfeature extractor (where, like AlphaEarth, only embeddings are used) and as an end-to-end finetune-able\nmodel.\nIt is expensive to export and download AlphaEarth embeddings from Google Earth Engine: our export jobs\nfor32 \u00d732crops took 26 EECU-seconds on average, or $290 for a dataset with 100K crops. Thus, we were\nonly able to evaluate AlphaEarth on five tasks: three classification tasks (Nandi, AWF, and Ecosystem), one\nper-pixel regression task (LFMC), and one segmentation task (Solar Farm).\nSince the AlphaEarth model has not been released, we can\u2019t evaluate AlphaEarth under a finetuning regime.\nWe assess the performance of the annualized AlphaEarth embeddings compared to the OlmoEarth embeddings\nfrom the ViT Base encoder using a simple KNN classifier. We use the timestep of AlphaEarth embeddings\nthat has the highest overlap with the time range of the labels. To assess the benefits of more complex decoders,\nwe use the partner task decoders described in Section 3.4, while sweeping over the input size (AlphaEarth\nembeddings already capture spatial context, so we find that a smaller input size performs better).\nWith a KNN-classifier, OlmoEarth outperforms AlphaEarth on the Nandi and AWF tasks, while AEF\noutperforms OlmoEarth on the Ecosystem mapping task. However, OlmoEarth benefits significantly from full\nfine-tuning, with the fine-tuned models outperforming the best possible with AlphaEarth on all five tasks.\nThis underscores the value of an open model that makes per-task fine-tuning possible.\n19\nNandi AWF Ecosystem LFMC Solar Farm\nModel Training Acc. Acc. Acc. L1 mIOU\nAEF kNN 55.6 81 60.6 - -\nAEF Frozen + Decoder 66.0 75.9 61.2 23.1 77.5\nAEF Full Fine-tuning Not Possible\nOlmoEarth kNN 66.2 82 59.3 - -\nOlmoEarth Frozen + Decoder 62.9 84.0 61.1 19.9 84.8\nOlmoEarth Full Fine-tuning82.2 86.0 62.4 17.9 86.7\nTable 7Comparing AlphaEarth Foundation (AEF) embeddings with OlmoEarth ViT Base model using three different\ntraining strategies: kNN, frozen backbone + decoder, and decoder with full fine-tuning. For these evaluations, we use\nthe \u201cpartner task\u201d decoders described in Section 3.4.\nFigure 5An example instance from them_cashew_plantdataset: note the coarse, polygonal labels\nE Patch Size Analysis form_cashew_plant\nWe observe that for the m_cashew_plant evaluation task, larger patch sizes lead to better performance for\nmodels that support variable patch sizes, such as OlmoEarth and Galileo. Table 8 summarizes the linear\nprobing and fine-tuning results form_cashew_plantacross different patch sizes.\nThis effect is unusual: a smaller patch size typically improves performance (e.g. Figure 4 of [ 50]). We\nhypothesize that this is due to the spatially coarse labels in the dataset, which are polygons instead of pixels\n(Figure 5).\nModelPatch 4\u00d74 Patch 8\u00d78 Patch 16\u00d716\nLP FT LP FT LP FT\nOlmoEarth-Base 27.7 71.9 27.9 76.2 32.3 79.8\nGalileo 24.3 73.0 25.6 76.9 28.9 78.8\nTable 8Performance (mIoU) comparison (LP = Linear Probing, FT = Fine-tuning) across patch sizes.\n20\nStep 1\nLabel & Validate \nStep 2\nFinetune & Register Model Task De\ufb01nition \n\u25cfAnnotation Type : Point, Polygon, Bounding box \n\u25cfTask Type : Classi\ufb01cation, Regression, \nSegmentation, Object detection \n\u25cfModalities : Sentinel-2, Sentinel-1, Landsat \n\u25cfSpatial Context : e.g., 1km x 1km \n\u25cfTemporal Context : Single-date, Multi-date \nDataset Preparation \n\u25cfWindow Creation : Per annotation \n\u25cfTrain/Val/Test split : Random split, \nSpatial split, Temporal split \n\u25cfLayers: Monthly Sentinel-2 bands, Monthly \nSentinel-1 bands, etc. \nModel Con\ufb01guration \n\u25cfPretrained Backbone : Helios-Base, etc. \n\u25cfDecoder Architecture : Linear Head, U-Net, \nFaster-RCNN, etc. \n\u25cfHyperparameters : Epochs, Batch size, Learning \nrate, etc.\n\u25cfData Augmentation : Random cropping, \nHorizontal/vertical \ufb02ipping, etc. \nStep 4\nReview & Publish \nStep 3\nRun Inference \nFigure 6 OlmoEarth Platform: End-to-End Workflow (using crop type mapping as an example).The platform\nenables users to complete the full process from data labeling to map publishing:Step 1:Label and review annotations,\nStep 2:Fine-tune and register models for specific tasks,Step 3:Run inference on selected areas and time ranges, and\nStep 4:Review and publish the final maps.\nF OlmoEarth Platform\nOlmoEarth Platform is an end-to-end solution that combines our foundation models with data management\ntools designed for organizations working on environmental challenges. The platform handles the complete\nworkflow from satellite data collection through labeling, model fine-tuning, and inference, eliminating the need\nfor organizations to manage GPU infrastructure or deep learning expertise. By making our models accessible,\nOlmoEarth Platform solves the last-mile problem of translating research into practical tools for applications\nincluding conservation, climate action, and food security.\n21\nFigure 7\nThey may\ntake our\nIntroduction,\nbut they\u2019ll never take\nour Antarctica!\n22\n",
    "title": "OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation",
    "authors": [
      "Henry Herzog",
      "Favyen Bastani",
      "Yawen Zhang",
      "Gabriel Tseng",
      "Joseph Redmon",
      "Hadrien Sablon",
      "Ryan Park",
      "Jacob Morrison",
      "Alexandra Buraczynski",
      "Karen Farley",
      "Joshua Hansen",
      "Andrew Howe",
      "Patrick Alan Johnson",
      "Mark Otterlee",
      "Ted Schmitt",
      "Hunter Pitelka",
      "Stephen Daspit",
      "Rachel Ratner",
      "Christopher Wilhelm",
      "Sebastian Wood",
      "Mike Jacobi",
      "Hannah Kerner",
      "Evan Shelhamer",
      "Ali Farhadi",
      "Ranjay Krishna",
      "Patrick Beukema"
    ],
    "published": "2025-11-17",
    "arxiv_id": "2511.13655v1",
    "num_pages": 22,
    "num_chars": 75457
  }
]